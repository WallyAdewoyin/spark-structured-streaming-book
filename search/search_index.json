{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Internals of Spark Structured Streaming (Apache Spark 3.0.1) \u00b6 Welcome to The Internals of Spark Structured Streaming online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Structured Streaming as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Spark Structured Streaming and Streaming Queries .","title":"Welcome"},{"location":"#the-internals-of-spark-structured-streaming-apache-spark-301","text":"Welcome to The Internals of Spark Structured Streaming online book! I'm Jacek Laskowski , a Seasoned IT Professional specializing in Apache Spark , Delta Lake , Apache Kafka and Kafka Streams . I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Structured Streaming as much as I have. Flannery O'Connor I write to discover what I know. \"The Internals Of\" series I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page. Expect text and code snippets from a variety of public sources. Attribution follows. Now, let me introduce you to Spark Structured Streaming and Streaming Queries .","title":"The Internals of Spark Structured Streaming (Apache Spark 3.0.1)"},{"location":"ContinuousExecution/","text":"ContinuousExecution \u2014 Stream Execution Engine of Continuous Stream Processing \u00b6 ContinuousExecution is the stream execution engine of Continuous Stream Processing . ContinuousExecution is < > when StreamingQueryManager is requested to create a streaming query with a < > and a < > (when DataStreamWriter is requested to start an execution of the streaming query ). ContinuousExecution can only run streaming queries with < > with < > data source. [[sources]] ContinuousExecution supports one < > only in a < > (and asserts it when < > and < >). When requested for available streaming sources , ContinuousExecution simply gives the < >. import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.Continuous(1.minute)) // <-- Gives ContinuousExecution .queryName(\"rate2console\") .start import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) // The following gives access to the internals // And to ContinuousExecution import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution val continuousEngine = engine.asInstanceOf[ContinuousExecution] assert(continuousEngine.trigger == Trigger.Continuous(1.minute)) When < > (for a streaming query), ContinuousExecution is given the < >. The analyzed logical plan is immediately transformed to include a < > for every < > with < > data source (and is the < > internally). NOTE: ContinuousExecution uses the same instance of ContinuousExecutionRelation for the same instances of < > with < > data source. When requested to < >, ContinuousExecution collects < > data sources (inside < >) from the < > and requests each and every ContinuousReadSupport to < > (that are stored in < > internal registry). [[EPOCH_COORDINATOR_ID_KEY]] ContinuousExecution uses __epoch_coordinator_id local property for...FIXME [[START_EPOCH_KEY]] ContinuousExecution uses __continuous_start_epoch local property for...FIXME [[EPOCH_INTERVAL_KEY]] ContinuousExecution uses __continuous_epoch_interval local property for...FIXME [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution=ALL Refer to < >. \u00b6 === [[runActivatedStream]] Running Activated Streaming Query -- runActivatedStream Method [source, scala] \u00b6 runActivatedStream(sparkSessionForStream: SparkSession): Unit \u00b6 runActivatedStream simply runs the streaming query in continuous mode as long as the state is ACTIVE . runActivatedStream is part of StreamExecution abstraction. === [[runContinuous]] Running Streaming Query in Continuous Mode -- runContinuous Internal Method [source, scala] \u00b6 runContinuous(sparkSessionForQuery: SparkSession): Unit \u00b6 runContinuous initializes the < > internal registry by traversing the < > to find < > leaf logical operators and requests their < > to < > (with the sources metadata directory under the checkpoint directory ). runContinuous initializes the uniqueSources internal registry to be the < > distinct. runContinuous < > (they may or may not be available). runContinuous transforms the < >. For every < > runContinuous finds the corresponding < > (in the < >), requests it to < > (from their JSON representation), and then < >. In the end, runContinuous creates a StreamingDataSourceV2Relation (with the read schema of the ContinuousReader and the ContinuousReader itself). runContinuous rewires the transformed plan (with the StreamingDataSourceV2Relation ) to use the new attributes from the source (the reader). NOTE: CurrentTimestamp and CurrentDate expressions are not supported for continuous processing. runContinuous requests the < > to < > (with the run ID of the streaming query ). runContinuous creates a < > (with the < > and the transformed logical query plan). runContinuous finds the only < > (of the only StreamingDataSourceV2Relation ) in the query plan with the WriteToContinuousDataSource . [[runContinuous-queryPlanning]] In queryPlanning time-tracking section , runContinuous creates an < > (that becomes the lastExecution ) that is immediately executed (i.e. the entire query execution pipeline is executed up to and including executedPlan ). runContinuous sets the following local properties: __is_continuous_processing as true < > as the currentBatchId < > as the < >, i.e. runId followed by -- with a random UUID < > as the interval of the < > runContinuous uses the EpochCoordinatorRef helper to < > (with the < >, the < >, the < >, and the currentBatchId ). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. runContinuous creates a daemon < > and starts it immediately. [[runContinuous-runContinuous]] In runContinuous time-tracking section , runContinuous requests the physical query plan (of the IncrementalExecution ) to execute (that simply requests the physical operator to doExecute and generate an RDD[InternalRow] ). runContinuous is used when ContinuousExecution is requested to < >. ==== [[runContinuous-epoch-update-thread]] Epoch Update Thread runContinuous creates an epoch update thread that...FIXME ==== [[getStartOffsets]] Getting Start Offsets From Checkpoint -- getStartOffsets Internal Method [source, scala] \u00b6 getStartOffsets(sparkSessionToRunBatches: SparkSession): OffsetSeq \u00b6 getStartOffsets ...FIXME NOTE: getStartOffsets is used exclusively when ContinuousExecution is requested to < >. === [[commit]] Committing Epoch -- commit Method [source, scala] \u00b6 commit(epoch: Long): Unit \u00b6 In essence, commit < > the given epoch to commit log and the committedOffsets , and requests the < > to < >. In the end, commit < > from the offset and commit logs (to keep spark.sql.streaming.minBatchesToRetain entries only). Internally, commit recordTriggerOffsets (with the from and to offsets as the committedOffsets and availableOffsets , respectively). At this point, commit may simply return when the stream execution thread is no longer alive (died). commit requests the commit log to < > for the epoch. commit requests the single < > to < > for the epoch (from the offset write-ahead log ). commit adds the single < > and the offset (for the epoch) to the committedOffsets registry. commit requests the single < > to < >. commit requests the offset and commit logs to < > to keep spark.sql.streaming.minBatchesToRetain only. commit then acquires the awaitProgressLock , wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock . NOTE: commit supports only one continuous source (registered in the < > internal registry). commit asserts that the given epoch is available in the offsetLog internal registry (i.e. the offset for the given epoch has been reported before). commit is used when EpochCoordinator is requested to commitEpoch . === [[addOffset]] addOffset Method [source, scala] \u00b6 addOffset( epoch: Long, reader: ContinuousReader, partitionOffsets: Seq[PartitionOffset]): Unit In essense, addOffset requests the given < > to < > (with the given PartitionOffsets ) and then requests the OffsetSeqLog to < >. Internally, addOffset requests the given < > to < > (with the given PartitionOffsets ) and to get the current \"global\" offset back. addOffset then requests the OffsetSeqLog to < > the current \"global\" offset for the given epoch . addOffset requests the OffsetSeqLog for the < >. If the offsets at the current and previous epochs are the same, addOffset turns the noNewData internal flag on. addOffset then acquires the awaitProgressLock , wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock . NOTE: addOffset supports exactly one < >. addOffset is used when EpochCoordinator is requested to < >. === [[logicalPlan]] Analyzed Logical Plan of Streaming Query -- logicalPlan Property [source, scala] \u00b6 logicalPlan: LogicalPlan \u00b6 logicalPlan resolves < > leaf logical operators (with a < > source) to < > leaf logical operators. Internally, logicalPlan transforms the < > as follows: . For every < > leaf logical operator with a < > source, logicalPlan looks it up for the corresponding < > (if available in the internal lookup registry) or creates a ContinuousExecutionRelation (with the ContinuousReadSupport source, the options and the output attributes of the StreamingRelationV2 operator) . For any other StreamingRelationV2 , logicalPlan throws an UnsupportedOperationException : + Data source [name] does not support continuous processing. logicalPlan is part of the StreamExecution abstraction. Creating Instance \u00b6 ContinuousExecution takes the following when created: [[sparkSession]] SparkSession [[name]] The name of the structured query [[checkpointRoot]] Path to the checkpoint directory (aka metadata directory ) [[analyzedPlan]] Analyzed logical query plan ( LogicalPlan ) [[sink]] < > [[trigger]] < > [[triggerClock]] Clock [[outputMode]] < > [[extraOptions]] Options ( Map[String, String] ) [[deleteCheckpointOnStop]] deleteCheckpointOnStop flag to control whether to delete the checkpoint directory on stop ContinuousExecution initializes the < >. === [[stop]] Stopping Stream Processing (Execution of Streaming Query) -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 NOTE: stop is part of the < > to stop a streaming query. stop transitions the streaming query to TERMINATED state. If the queryExecutionThread is alive (i.e. it has been started and has not yet died), stop interrupts it and waits for this thread to die. In the end, stop prints out the following INFO message to the logs: Query [prettyIdString] was stopped Note prettyIdString is in the format of queryName [id = [id], runId = [runId]] . === [[awaitEpoch]] awaitEpoch Internal Method [source, scala] \u00b6 awaitEpoch(epoch: Long): Unit \u00b6 awaitEpoch ...FIXME NOTE: awaitEpoch seems to be used exclusively in tests. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | continuousSources a| [[continuousSources]] [source, scala] \u00b6 continuousSources: Seq[ContinuousReader] \u00b6 Registry of < > (in the < >) As asserted in < > and < > there could only be exactly one ContinuousReaders registered. Used when ContinuousExecution is requested to < >, < >, and < > Use < > to access the current value | currentEpochCoordinatorId | [[currentEpochCoordinatorId]] FIXME Used when...FIXME | triggerExecutor a| [[triggerExecutor]] < > for the < >: ProcessingTimeExecutor for < > Used when...FIXME NOTE: StreamExecution throws an IllegalStateException when the < > is not a < >. |===","title":"ContinuousExecution"},{"location":"ContinuousExecution/#continuousexecution-stream-execution-engine-of-continuous-stream-processing","text":"ContinuousExecution is the stream execution engine of Continuous Stream Processing . ContinuousExecution is < > when StreamingQueryManager is requested to create a streaming query with a < > and a < > (when DataStreamWriter is requested to start an execution of the streaming query ). ContinuousExecution can only run streaming queries with < > with < > data source. [[sources]] ContinuousExecution supports one < > only in a < > (and asserts it when < > and < >). When requested for available streaming sources , ContinuousExecution simply gives the < >. import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.Continuous(1.minute)) // <-- Gives ContinuousExecution .queryName(\"rate2console\") .start import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) // The following gives access to the internals // And to ContinuousExecution import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) import org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution val continuousEngine = engine.asInstanceOf[ContinuousExecution] assert(continuousEngine.trigger == Trigger.Continuous(1.minute)) When < > (for a streaming query), ContinuousExecution is given the < >. The analyzed logical plan is immediately transformed to include a < > for every < > with < > data source (and is the < > internally). NOTE: ContinuousExecution uses the same instance of ContinuousExecutionRelation for the same instances of < > with < > data source. When requested to < >, ContinuousExecution collects < > data sources (inside < >) from the < > and requests each and every ContinuousReadSupport to < > (that are stored in < > internal registry). [[EPOCH_COORDINATOR_ID_KEY]] ContinuousExecution uses __epoch_coordinator_id local property for...FIXME [[START_EPOCH_KEY]] ContinuousExecution uses __continuous_start_epoch local property for...FIXME [[EPOCH_INTERVAL_KEY]] ContinuousExecution uses __continuous_epoch_interval local property for...FIXME [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution=ALL","title":"ContinuousExecution &mdash; Stream Execution Engine of Continuous Stream Processing"},{"location":"ContinuousExecution/#refer-to","text":"=== [[runActivatedStream]] Running Activated Streaming Query -- runActivatedStream Method","title":"Refer to &lt;&gt;."},{"location":"ContinuousExecution/#source-scala","text":"","title":"[source, scala]"},{"location":"ContinuousExecution/#runactivatedstreamsparksessionforstream-sparksession-unit","text":"runActivatedStream simply runs the streaming query in continuous mode as long as the state is ACTIVE . runActivatedStream is part of StreamExecution abstraction. === [[runContinuous]] Running Streaming Query in Continuous Mode -- runContinuous Internal Method","title":"runActivatedStream(sparkSessionForStream: SparkSession): Unit"},{"location":"ContinuousExecution/#source-scala_1","text":"","title":"[source, scala]"},{"location":"ContinuousExecution/#runcontinuoussparksessionforquery-sparksession-unit","text":"runContinuous initializes the < > internal registry by traversing the < > to find < > leaf logical operators and requests their < > to < > (with the sources metadata directory under the checkpoint directory ). runContinuous initializes the uniqueSources internal registry to be the < > distinct. runContinuous < > (they may or may not be available). runContinuous transforms the < >. For every < > runContinuous finds the corresponding < > (in the < >), requests it to < > (from their JSON representation), and then < >. In the end, runContinuous creates a StreamingDataSourceV2Relation (with the read schema of the ContinuousReader and the ContinuousReader itself). runContinuous rewires the transformed plan (with the StreamingDataSourceV2Relation ) to use the new attributes from the source (the reader). NOTE: CurrentTimestamp and CurrentDate expressions are not supported for continuous processing. runContinuous requests the < > to < > (with the run ID of the streaming query ). runContinuous creates a < > (with the < > and the transformed logical query plan). runContinuous finds the only < > (of the only StreamingDataSourceV2Relation ) in the query plan with the WriteToContinuousDataSource . [[runContinuous-queryPlanning]] In queryPlanning time-tracking section , runContinuous creates an < > (that becomes the lastExecution ) that is immediately executed (i.e. the entire query execution pipeline is executed up to and including executedPlan ). runContinuous sets the following local properties: __is_continuous_processing as true < > as the currentBatchId < > as the < >, i.e. runId followed by -- with a random UUID < > as the interval of the < > runContinuous uses the EpochCoordinatorRef helper to < > (with the < >, the < >, the < >, and the currentBatchId ). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. runContinuous creates a daemon < > and starts it immediately. [[runContinuous-runContinuous]] In runContinuous time-tracking section , runContinuous requests the physical query plan (of the IncrementalExecution ) to execute (that simply requests the physical operator to doExecute and generate an RDD[InternalRow] ). runContinuous is used when ContinuousExecution is requested to < >. ==== [[runContinuous-epoch-update-thread]] Epoch Update Thread runContinuous creates an epoch update thread that...FIXME ==== [[getStartOffsets]] Getting Start Offsets From Checkpoint -- getStartOffsets Internal Method","title":"runContinuous(sparkSessionForQuery: SparkSession): Unit"},{"location":"ContinuousExecution/#source-scala_2","text":"","title":"[source, scala]"},{"location":"ContinuousExecution/#getstartoffsetssparksessiontorunbatches-sparksession-offsetseq","text":"getStartOffsets ...FIXME NOTE: getStartOffsets is used exclusively when ContinuousExecution is requested to < >. === [[commit]] Committing Epoch -- commit Method","title":"getStartOffsets(sparkSessionToRunBatches: SparkSession): OffsetSeq"},{"location":"ContinuousExecution/#source-scala_3","text":"","title":"[source, scala]"},{"location":"ContinuousExecution/#commitepoch-long-unit","text":"In essence, commit < > the given epoch to commit log and the committedOffsets , and requests the < > to < >. In the end, commit < > from the offset and commit logs (to keep spark.sql.streaming.minBatchesToRetain entries only). Internally, commit recordTriggerOffsets (with the from and to offsets as the committedOffsets and availableOffsets , respectively). At this point, commit may simply return when the stream execution thread is no longer alive (died). commit requests the commit log to < > for the epoch. commit requests the single < > to < > for the epoch (from the offset write-ahead log ). commit adds the single < > and the offset (for the epoch) to the committedOffsets registry. commit requests the single < > to < >. commit requests the offset and commit logs to < > to keep spark.sql.streaming.minBatchesToRetain only. commit then acquires the awaitProgressLock , wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock . NOTE: commit supports only one continuous source (registered in the < > internal registry). commit asserts that the given epoch is available in the offsetLog internal registry (i.e. the offset for the given epoch has been reported before). commit is used when EpochCoordinator is requested to commitEpoch . === [[addOffset]] addOffset Method","title":"commit(epoch: Long): Unit"},{"location":"ContinuousExecution/#source-scala_4","text":"addOffset( epoch: Long, reader: ContinuousReader, partitionOffsets: Seq[PartitionOffset]): Unit In essense, addOffset requests the given < > to < > (with the given PartitionOffsets ) and then requests the OffsetSeqLog to < >. Internally, addOffset requests the given < > to < > (with the given PartitionOffsets ) and to get the current \"global\" offset back. addOffset then requests the OffsetSeqLog to < > the current \"global\" offset for the given epoch . addOffset requests the OffsetSeqLog for the < >. If the offsets at the current and previous epochs are the same, addOffset turns the noNewData internal flag on. addOffset then acquires the awaitProgressLock , wakes up all threads waiting for the awaitProgressLockCondition and in the end releases the awaitProgressLock . NOTE: addOffset supports exactly one < >. addOffset is used when EpochCoordinator is requested to < >. === [[logicalPlan]] Analyzed Logical Plan of Streaming Query -- logicalPlan Property","title":"[source, scala]"},{"location":"ContinuousExecution/#source-scala_5","text":"","title":"[source, scala]"},{"location":"ContinuousExecution/#logicalplan-logicalplan","text":"logicalPlan resolves < > leaf logical operators (with a < > source) to < > leaf logical operators. Internally, logicalPlan transforms the < > as follows: . For every < > leaf logical operator with a < > source, logicalPlan looks it up for the corresponding < > (if available in the internal lookup registry) or creates a ContinuousExecutionRelation (with the ContinuousReadSupport source, the options and the output attributes of the StreamingRelationV2 operator) . For any other StreamingRelationV2 , logicalPlan throws an UnsupportedOperationException : + Data source [name] does not support continuous processing. logicalPlan is part of the StreamExecution abstraction.","title":"logicalPlan: LogicalPlan"},{"location":"ContinuousExecution/#creating-instance","text":"ContinuousExecution takes the following when created: [[sparkSession]] SparkSession [[name]] The name of the structured query [[checkpointRoot]] Path to the checkpoint directory (aka metadata directory ) [[analyzedPlan]] Analyzed logical query plan ( LogicalPlan ) [[sink]] < > [[trigger]] < > [[triggerClock]] Clock [[outputMode]] < > [[extraOptions]] Options ( Map[String, String] ) [[deleteCheckpointOnStop]] deleteCheckpointOnStop flag to control whether to delete the checkpoint directory on stop ContinuousExecution initializes the < >. === [[stop]] Stopping Stream Processing (Execution of Streaming Query) -- stop Method","title":"Creating Instance"},{"location":"ContinuousExecution/#source-scala_6","text":"","title":"[source, scala]"},{"location":"ContinuousExecution/#stop-unit","text":"NOTE: stop is part of the < > to stop a streaming query. stop transitions the streaming query to TERMINATED state. If the queryExecutionThread is alive (i.e. it has been started and has not yet died), stop interrupts it and waits for this thread to die. In the end, stop prints out the following INFO message to the logs: Query [prettyIdString] was stopped Note prettyIdString is in the format of queryName [id = [id], runId = [runId]] . === [[awaitEpoch]] awaitEpoch Internal Method","title":"stop(): Unit"},{"location":"ContinuousExecution/#source-scala_7","text":"","title":"[source, scala]"},{"location":"ContinuousExecution/#awaitepochepoch-long-unit","text":"awaitEpoch ...FIXME NOTE: awaitEpoch seems to be used exclusively in tests. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | continuousSources a| [[continuousSources]]","title":"awaitEpoch(epoch: Long): Unit"},{"location":"ContinuousExecution/#source-scala_8","text":"","title":"[source, scala]"},{"location":"ContinuousExecution/#continuoussources-seqcontinuousreader","text":"Registry of < > (in the < >) As asserted in < > and < > there could only be exactly one ContinuousReaders registered. Used when ContinuousExecution is requested to < >, < >, and < > Use < > to access the current value | currentEpochCoordinatorId | [[currentEpochCoordinatorId]] FIXME Used when...FIXME | triggerExecutor a| [[triggerExecutor]] < > for the < >: ProcessingTimeExecutor for < > Used when...FIXME NOTE: StreamExecution throws an IllegalStateException when the < > is not a < >. |===","title":"continuousSources: Seq[ContinuousReader]"},{"location":"DataStreamWriter/","text":"DataStreamWriter \u2014 Writing Datasets To Streaming Sink \u00b6 DataStreamWriter is the < > to describe when and what rows of a streaming query are sent out to the < >. DataStreamWriter is available using Dataset.writeStream method (on a streaming query). import org.apache.spark.sql.streaming.DataStreamWriter import org.apache.spark.sql.Row val streamingQuery: Dataset[Long] = ... assert(streamingQuery.isStreaming) val writer: DataStreamWriter[Row] = streamingQuery.writeStream [[methods]] .DataStreamWriter's Methods [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | < > a| [source, scala] \u00b6 foreach(writer: ForeachWriter[T]): DataStreamWriter[T] \u00b6 Sets spark-sql-streaming-ForeachWriter.md[ForeachWriter] in the full control of streaming writes | foreachBatch a| [[foreachBatch]] [source, scala] \u00b6 foreachBatch( function: (Dataset[T], Long) => Unit): DataStreamWriter[T] ( New in 2.4.0 ) Sets the < > to foreachBatch and the < > to the given function. As per https://issues.apache.org/jira/browse/SPARK-24565[SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame], the purpose of the method is to expose the micro-batch output as a dataframe for the following: Pass the output rows of each batch to a library that is designed for the batch jobs only Reuse batch data sources for output whose streaming version does not exist Multi-writes where the output rows are written to multiple outputs by writing twice for every batch | format a| [[format]] [source, scala] \u00b6 format(source: String): DataStreamWriter[T] \u00b6 Specifies the format of the < > (aka output format ) The format is used internally as the name ( alias ) of the < > to use to write the data to | < > a| [source, scala] \u00b6 option(key: String, value: Boolean): DataStreamWriter[T] option(key: String, value: Double): DataStreamWriter[T] option(key: String, value: Long): DataStreamWriter[T] option(key: String, value: String): DataStreamWriter[T] | options a| [[options]] [source, scala] \u00b6 options(options: Map[String, String]): DataStreamWriter[T] \u00b6 Specifies the configuration options of a data sink NOTE: You could use < > method if you prefer specifying the options one by one or there is only one in use. | < > a| [source, scala] \u00b6 outputMode(outputMode: OutputMode): DataStreamWriter[T] outputMode(outputMode: String): DataStreamWriter[T] Specifies the < > | partitionBy a| [[partitionBy]] [source, scala] \u00b6 partitionBy(colNames: String*): DataStreamWriter[T] \u00b6 | < > a| [source, scala] \u00b6 queryName(queryName: String): DataStreamWriter[T] \u00b6 Assigns the name of a query | < > a| [source, scala] \u00b6 start(): StreamingQuery start(path: String): StreamingQuery // <1> <1> Explicit path (that could also be specified as an < >) Creates and immediately starts a < > | < > a| [source, scala] \u00b6 trigger(trigger: Trigger): DataStreamWriter[T] \u00b6 Sets the spark-sql-streaming-Trigger.md[Trigger] for how often a streaming query should be executed and the result saved. |=== [NOTE] \u00b6 A streaming query is a spark-sql-Dataset.md[Dataset] with a spark-sql-LogicalPlan.md#isStreaming[streaming logical plan]. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ import org.apache.spark.sql.DataFrame val rates: DataFrame = spark. readStream. format(\"rate\"). load scala> rates.isStreaming res1: Boolean = true scala> rates.queryExecution.logical.isStreaming res2: Boolean = true ==== Like the batch DataFrameWriter , DataStreamWriter has a direct support for many < > and < >. [source, scala] \u00b6 // see above for writer definition // Save dataset in JSON format writer.format(\"json\") In the end, you start the actual continuous writing of the result of executing a Dataset to a sink using < > operator. [source, scala] \u00b6 writer.save \u00b6 Beside the above operators, there are the following to work with a Dataset as a whole. NOTE: hive < > for streaming writing (and leads to a AnalysisException ). NOTE: DataFrameWriter is responsible for writing in a batch fashion. === [[option]] Specifying Write Option -- option Method [source, scala] \u00b6 option(key: String, value: String): DataStreamWriter[T] option(key: String, value: Boolean): DataStreamWriter[T] option(key: String, value: Long): DataStreamWriter[T] option(key: String, value: Double): DataStreamWriter[T] Internally, option adds the key and value to < > internal option registry. === [[outputMode]] Specifying Output Mode -- outputMode Method [source, scala] \u00b6 outputMode(outputMode: String): DataStreamWriter[T] outputMode(outputMode: OutputMode): DataStreamWriter[T] outputMode specifies the spark-sql-streaming-OutputMode.md[output mode] of a streaming query, i.e. what data is sent out to a spark-sql-streaming-Sink.md[streaming sink] when there is new data available in streaming data sources . NOTE: When not defined explicitly, outputMode defaults to < > output mode. outputMode can be specified by name or one of the < > values. === [[queryName]] Setting Query Name -- queryName method [source, scala] \u00b6 queryName(queryName: String): DataStreamWriter[T] \u00b6 queryName sets the name of a StreamingQuery.md[streaming query]. Internally, it is just an additional < > with the key queryName . === [[trigger]] Setting How Often to Execute Streaming Query -- trigger method [source, scala] \u00b6 trigger(trigger: Trigger): DataStreamWriter[T] \u00b6 trigger method sets the time interval of the trigger (that executes a batch runner) for a streaming query. NOTE: Trigger specifies how often results should be produced by a StreamingQuery.md[StreamingQuery]. See spark-sql-streaming-Trigger.md[Trigger]. The default trigger is spark-sql-streaming-Trigger.md#ProcessingTime[ProcessingTime(0L)] that runs a streaming query as often as possible. TIP: Consult spark-sql-streaming-Trigger.md[Trigger] to learn about Trigger and ProcessingTime types. === [[start]] Creating and Starting Execution of Streaming Query -- start Method [source, scala] \u00b6 start(): StreamingQuery start(path: String): StreamingQuery // <1> <1> Sets path option to path and passes the call on to start() start starts a streaming query. start gives a StreamingQuery.md[StreamingQuery] to control the execution of the continuous query. NOTE: Whether or not you have to specify path option depends on the streaming sink in use. Internally, start branches off per source . memory foreach other formats ...FIXME [[start-options]] .start's Options [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Option | Description | queryName | Name of active streaming query | [[checkpointLocation]] checkpointLocation | Directory for checkpointing (and to store query metadata like offsets before and after being processed, the query id , etc.) |=== start reports a AnalysisException when source is hive . [source, scala] \u00b6 val q = spark. readStream. text(\"server-logs/*\"). writeStream. format(\"hive\") \u2190 hive format used as a streaming sink scala> q.start org.apache.spark.sql.AnalysisException: Hive data source can only be used with tables, you can not write files of Hive data source directly.; at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:234) ... 48 elided NOTE: Define options using < > or < > methods. === [[foreach]] Making ForeachWriter in Charge of Streaming Writes -- foreach method [source, scala] \u00b6 foreach(writer: ForeachWriter[T]): DataStreamWriter[T] \u00b6 foreach sets the input spark-sql-streaming-ForeachWriter.md[ForeachWriter] to be in control of streaming writes. Internally, foreach sets the streaming output < > as foreach and foreachWriter as the input writer . NOTE: foreach uses SparkSession to access SparkContext to clean the ForeachWriter . [NOTE] \u00b6 foreach reports an IllegalArgumentException when writer is null . foreach writer cannot be null \u00b6 === [[internal-properties]] Internal Properties [cols=\"20m,20,60\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description extraOptions [[extraOptions]] | foreachBatchWriter | null a| [[foreachBatchWriter]] [source, scala] \u00b6 foreachBatchWriter: (Dataset[T], Long) => Unit \u00b6 The function that is used as the batch writer in the < > for < > foreachWriter [[foreachWriter]] partitioningColumns [[partitioningColumns]] source [[source]] | outputMode | < > | [[outputMode-property]] spark-sql-streaming-OutputMode.md[OutputMode] of the streaming sink Set using < > method. trigger [[trigger-property]] ===","title":"DataStreamWriter"},{"location":"DataStreamWriter/#datastreamwriter-writing-datasets-to-streaming-sink","text":"DataStreamWriter is the < > to describe when and what rows of a streaming query are sent out to the < >. DataStreamWriter is available using Dataset.writeStream method (on a streaming query). import org.apache.spark.sql.streaming.DataStreamWriter import org.apache.spark.sql.Row val streamingQuery: Dataset[Long] = ... assert(streamingQuery.isStreaming) val writer: DataStreamWriter[Row] = streamingQuery.writeStream [[methods]] .DataStreamWriter's Methods [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | < > a|","title":"DataStreamWriter &mdash; Writing Datasets To Streaming Sink"},{"location":"DataStreamWriter/#source-scala","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#foreachwriter-foreachwritert-datastreamwritert","text":"Sets spark-sql-streaming-ForeachWriter.md[ForeachWriter] in the full control of streaming writes | foreachBatch a| [[foreachBatch]]","title":"foreach(writer: ForeachWriter[T]): DataStreamWriter[T]"},{"location":"DataStreamWriter/#source-scala_1","text":"foreachBatch( function: (Dataset[T], Long) => Unit): DataStreamWriter[T] ( New in 2.4.0 ) Sets the < > to foreachBatch and the < > to the given function. As per https://issues.apache.org/jira/browse/SPARK-24565[SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame], the purpose of the method is to expose the micro-batch output as a dataframe for the following: Pass the output rows of each batch to a library that is designed for the batch jobs only Reuse batch data sources for output whose streaming version does not exist Multi-writes where the output rows are written to multiple outputs by writing twice for every batch | format a| [[format]]","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_2","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#formatsource-string-datastreamwritert","text":"Specifies the format of the < > (aka output format ) The format is used internally as the name ( alias ) of the < > to use to write the data to | < > a|","title":"format(source: String): DataStreamWriter[T]"},{"location":"DataStreamWriter/#source-scala_3","text":"option(key: String, value: Boolean): DataStreamWriter[T] option(key: String, value: Double): DataStreamWriter[T] option(key: String, value: Long): DataStreamWriter[T] option(key: String, value: String): DataStreamWriter[T] | options a| [[options]]","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_4","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#optionsoptions-mapstring-string-datastreamwritert","text":"Specifies the configuration options of a data sink NOTE: You could use < > method if you prefer specifying the options one by one or there is only one in use. | < > a|","title":"options(options: Map[String, String]): DataStreamWriter[T]"},{"location":"DataStreamWriter/#source-scala_5","text":"outputMode(outputMode: OutputMode): DataStreamWriter[T] outputMode(outputMode: String): DataStreamWriter[T] Specifies the < > | partitionBy a| [[partitionBy]]","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_6","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#partitionbycolnames-string-datastreamwritert","text":"| < > a|","title":"partitionBy(colNames: String*): DataStreamWriter[T]"},{"location":"DataStreamWriter/#source-scala_7","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#querynamequeryname-string-datastreamwritert","text":"Assigns the name of a query | < > a|","title":"queryName(queryName: String): DataStreamWriter[T]"},{"location":"DataStreamWriter/#source-scala_8","text":"start(): StreamingQuery start(path: String): StreamingQuery // <1> <1> Explicit path (that could also be specified as an < >) Creates and immediately starts a < > | < > a|","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_9","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#triggertrigger-trigger-datastreamwritert","text":"Sets the spark-sql-streaming-Trigger.md[Trigger] for how often a streaming query should be executed and the result saved. |===","title":"trigger(trigger: Trigger): DataStreamWriter[T]"},{"location":"DataStreamWriter/#note","text":"A streaming query is a spark-sql-Dataset.md[Dataset] with a spark-sql-LogicalPlan.md#isStreaming[streaming logical plan].","title":"[NOTE]"},{"location":"DataStreamWriter/#source-scala_10","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ import org.apache.spark.sql.DataFrame val rates: DataFrame = spark. readStream. format(\"rate\"). load scala> rates.isStreaming res1: Boolean = true scala> rates.queryExecution.logical.isStreaming res2: Boolean = true ==== Like the batch DataFrameWriter , DataStreamWriter has a direct support for many < > and < >.","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_11","text":"// see above for writer definition // Save dataset in JSON format writer.format(\"json\") In the end, you start the actual continuous writing of the result of executing a Dataset to a sink using < > operator.","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_12","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#writersave","text":"Beside the above operators, there are the following to work with a Dataset as a whole. NOTE: hive < > for streaming writing (and leads to a AnalysisException ). NOTE: DataFrameWriter is responsible for writing in a batch fashion. === [[option]] Specifying Write Option -- option Method","title":"writer.save"},{"location":"DataStreamWriter/#source-scala_13","text":"option(key: String, value: String): DataStreamWriter[T] option(key: String, value: Boolean): DataStreamWriter[T] option(key: String, value: Long): DataStreamWriter[T] option(key: String, value: Double): DataStreamWriter[T] Internally, option adds the key and value to < > internal option registry. === [[outputMode]] Specifying Output Mode -- outputMode Method","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_14","text":"outputMode(outputMode: String): DataStreamWriter[T] outputMode(outputMode: OutputMode): DataStreamWriter[T] outputMode specifies the spark-sql-streaming-OutputMode.md[output mode] of a streaming query, i.e. what data is sent out to a spark-sql-streaming-Sink.md[streaming sink] when there is new data available in streaming data sources . NOTE: When not defined explicitly, outputMode defaults to < > output mode. outputMode can be specified by name or one of the < > values. === [[queryName]] Setting Query Name -- queryName method","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_15","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#querynamequeryname-string-datastreamwritert_1","text":"queryName sets the name of a StreamingQuery.md[streaming query]. Internally, it is just an additional < > with the key queryName . === [[trigger]] Setting How Often to Execute Streaming Query -- trigger method","title":"queryName(queryName: String): DataStreamWriter[T]"},{"location":"DataStreamWriter/#source-scala_16","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#triggertrigger-trigger-datastreamwritert_1","text":"trigger method sets the time interval of the trigger (that executes a batch runner) for a streaming query. NOTE: Trigger specifies how often results should be produced by a StreamingQuery.md[StreamingQuery]. See spark-sql-streaming-Trigger.md[Trigger]. The default trigger is spark-sql-streaming-Trigger.md#ProcessingTime[ProcessingTime(0L)] that runs a streaming query as often as possible. TIP: Consult spark-sql-streaming-Trigger.md[Trigger] to learn about Trigger and ProcessingTime types. === [[start]] Creating and Starting Execution of Streaming Query -- start Method","title":"trigger(trigger: Trigger): DataStreamWriter[T]"},{"location":"DataStreamWriter/#source-scala_17","text":"start(): StreamingQuery start(path: String): StreamingQuery // <1> <1> Sets path option to path and passes the call on to start() start starts a streaming query. start gives a StreamingQuery.md[StreamingQuery] to control the execution of the continuous query. NOTE: Whether or not you have to specify path option depends on the streaming sink in use. Internally, start branches off per source . memory foreach other formats ...FIXME [[start-options]] .start's Options [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Option | Description | queryName | Name of active streaming query | [[checkpointLocation]] checkpointLocation | Directory for checkpointing (and to store query metadata like offsets before and after being processed, the query id , etc.) |=== start reports a AnalysisException when source is hive .","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_18","text":"val q = spark. readStream. text(\"server-logs/*\"). writeStream. format(\"hive\") \u2190 hive format used as a streaming sink scala> q.start org.apache.spark.sql.AnalysisException: Hive data source can only be used with tables, you can not write files of Hive data source directly.; at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:234) ... 48 elided NOTE: Define options using < > or < > methods. === [[foreach]] Making ForeachWriter in Charge of Streaming Writes -- foreach method","title":"[source, scala]"},{"location":"DataStreamWriter/#source-scala_19","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#foreachwriter-foreachwritert-datastreamwritert_1","text":"foreach sets the input spark-sql-streaming-ForeachWriter.md[ForeachWriter] to be in control of streaming writes. Internally, foreach sets the streaming output < > as foreach and foreachWriter as the input writer . NOTE: foreach uses SparkSession to access SparkContext to clean the ForeachWriter .","title":"foreach(writer: ForeachWriter[T]): DataStreamWriter[T]"},{"location":"DataStreamWriter/#note_1","text":"foreach reports an IllegalArgumentException when writer is null .","title":"[NOTE]"},{"location":"DataStreamWriter/#foreach-writer-cannot-be-null","text":"=== [[internal-properties]] Internal Properties [cols=\"20m,20,60\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description extraOptions [[extraOptions]] | foreachBatchWriter | null a| [[foreachBatchWriter]]","title":"foreach writer cannot be null\n"},{"location":"DataStreamWriter/#source-scala_20","text":"","title":"[source, scala]"},{"location":"DataStreamWriter/#foreachbatchwriter-datasett-long-unit","text":"The function that is used as the batch writer in the < > for < > foreachWriter [[foreachWriter]] partitioningColumns [[partitioningColumns]] source [[source]] | outputMode | < > | [[outputMode-property]] spark-sql-streaming-OutputMode.md[OutputMode] of the streaming sink Set using < > method. trigger [[trigger-property]] ===","title":"foreachBatchWriter: (Dataset[T], Long) =&gt; Unit"},{"location":"EventTimeWatermark/","text":"EventTimeWatermark Unary Logical Operator \u00b6 EventTimeWatermark is a unary logical operator that is < > to represent Dataset.withWatermark operator in a logical query plan of a streaming query. [NOTE] \u00b6 A unary logical operator ( UnaryNode ) is a logical operator with a single < > logical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan.html[UnaryNode ] (and logical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 When requested for the < >, EventTimeWatermark logical operator goes over the output attributes of the < > logical operator to find the matching attribute based on the < > attribute and updates it to include spark.watermarkDelayMs metadata key with the < > interval (< >). EventTimeWatermark is resolved ( planned ) to EventTimeWatermarkExec physical operator in StatefulAggregationStrategy execution planning strategy. [NOTE] \u00b6 EliminateEventTimeWatermark logical optimization rule (i.e. Rule[LogicalPlan] ) removes EventTimeWatermark logical operator from a logical plan if the < > logical operator is not streaming, i.e. when Dataset.withWatermark operator is used on a batch query. val logs = spark. read. // <-- batch non-streaming query that makes `EliminateEventTimeWatermark` rule applicable format(\"text\"). load(\"logs\") // logs is a batch Dataset assert(!logs.isStreaming) val q = logs. withWatermark(eventTime = \"timestamp\", delayThreshold = \"30 seconds\") // <-- creates EventTimeWatermark scala> println(q.queryExecution.logical.numberedTreeString) // <-- no EventTimeWatermark as it was removed immediately 00 Relation[value#0] text \u00b6 === [[creating-instance]] Creating EventTimeWatermark Instance EventTimeWatermark takes the following to be created: [[eventTime]] Watermark column ( Attribute ) [[delay]] Watermark delay ( CalendarInterval ) [[child]] Child logical operator ( LogicalPlan ) === [[output]] Output Schema -- output Property [source, scala] \u00b6 output: Seq[Attribute] \u00b6 NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output finds < > column in the output schema of the < > logical operator and updates the Metadata of the column with < > key and the milliseconds for the delay. output removes < > key from the other columns. [source, scala] \u00b6 // FIXME How to access/show the eventTime column with the metadata updated to include spark.watermarkDelayMs? import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark val etw = q.queryExecution.logical.asInstanceOf[EventTimeWatermark] scala> etw.output.toStructType.printTreeString root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) === [[watermarkDelayMs]][[delayKey]] Watermark Metadata (Marker) -- spark.watermarkDelayMs Metadata Key spark.watermarkDelayMs metadata key is used to mark one of the < > as the watermark attribute ( eventTime watermark ). === [[getDelayMs]] Converting Human-Friendly CalendarInterval to Milliseconds -- getDelayMs Object Method [source, scala] \u00b6 getDelayMs( delay: CalendarInterval): Long getDelayMs ...FIXME NOTE: getDelayMs is used when...FIXME","title":"EventTimeWatermark Unary Logical Operator"},{"location":"EventTimeWatermark/#eventtimewatermark-unary-logical-operator","text":"EventTimeWatermark is a unary logical operator that is < > to represent Dataset.withWatermark operator in a logical query plan of a streaming query.","title":"EventTimeWatermark Unary Logical Operator"},{"location":"EventTimeWatermark/#note","text":"A unary logical operator ( UnaryNode ) is a logical operator with a single < > logical operator.","title":"[NOTE]"},{"location":"EventTimeWatermark/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-logicalplanhtmlunarynode-and-logical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"When requested for the < >, EventTimeWatermark logical operator goes over the output attributes of the < > logical operator to find the matching attribute based on the < > attribute and updates it to include spark.watermarkDelayMs metadata key with the < > interval (< >). EventTimeWatermark is resolved ( planned ) to EventTimeWatermarkExec physical operator in StatefulAggregationStrategy execution planning strategy.","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan.html[UnaryNode] (and logical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"EventTimeWatermark/#note_1","text":"EliminateEventTimeWatermark logical optimization rule (i.e. Rule[LogicalPlan] ) removes EventTimeWatermark logical operator from a logical plan if the < > logical operator is not streaming, i.e. when Dataset.withWatermark operator is used on a batch query.","title":"[NOTE]"},{"location":"EventTimeWatermark/#val-logs-spark-read-batch-non-streaming-query-that-makes-eliminateeventtimewatermark-rule-applicable-formattext-loadlogs-logs-is-a-batch-dataset-assertlogsisstreaming-val-q-logs-withwatermarkeventtime-timestamp-delaythreshold-30-seconds-creates-eventtimewatermark-scala-printlnqqueryexecutionlogicalnumberedtreestring-no-eventtimewatermark-as-it-was-removed-immediately-00-relationvalue0-text","text":"=== [[creating-instance]] Creating EventTimeWatermark Instance EventTimeWatermark takes the following to be created: [[eventTime]] Watermark column ( Attribute ) [[delay]] Watermark delay ( CalendarInterval ) [[child]] Child logical operator ( LogicalPlan ) === [[output]] Output Schema -- output Property","title":"val logs = spark.\n  read. // &lt;-- batch non-streaming query that makes `EliminateEventTimeWatermark` rule applicable\n  format(&quot;text&quot;).\n  load(&quot;logs&quot;)\n\n// logs is a batch Dataset\nassert(!logs.isStreaming)\n\nval q = logs.\n  withWatermark(eventTime = &quot;timestamp&quot;, delayThreshold = &quot;30 seconds&quot;) // &lt;-- creates EventTimeWatermark\nscala&gt; println(q.queryExecution.logical.numberedTreeString) // &lt;-- no EventTimeWatermark as it was removed immediately\n00 Relation[value#0] text\n"},{"location":"EventTimeWatermark/#source-scala","text":"","title":"[source, scala]"},{"location":"EventTimeWatermark/#output-seqattribute","text":"NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output finds < > column in the output schema of the < > logical operator and updates the Metadata of the column with < > key and the milliseconds for the delay. output removes < > key from the other columns.","title":"output: Seq[Attribute]"},{"location":"EventTimeWatermark/#source-scala_1","text":"// FIXME How to access/show the eventTime column with the metadata updated to include spark.watermarkDelayMs? import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark val etw = q.queryExecution.logical.asInstanceOf[EventTimeWatermark] scala> etw.output.toStructType.printTreeString root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) === [[watermarkDelayMs]][[delayKey]] Watermark Metadata (Marker) -- spark.watermarkDelayMs Metadata Key spark.watermarkDelayMs metadata key is used to mark one of the < > as the watermark attribute ( eventTime watermark ). === [[getDelayMs]] Converting Human-Friendly CalendarInterval to Milliseconds -- getDelayMs Object Method","title":"[source, scala]"},{"location":"EventTimeWatermark/#source-scala_2","text":"getDelayMs( delay: CalendarInterval): Long getDelayMs ...FIXME NOTE: getDelayMs is used when...FIXME","title":"[source, scala]"},{"location":"GroupState/","text":"GroupState -- Group State in Arbitrary Stateful Streaming Aggregation \u00b6 GroupState is an < > of < > (of type S ) in Arbitrary Stateful Streaming Aggregation . GroupState is used with the following KeyValueGroupedDataset operations: mapGroupsWithState flatMapGroupsWithState GroupState is created separately for every aggregation key to hold a state as an aggregation state value . [[contract]] .GroupState Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | exists a| [[exists]] [source, scala] \u00b6 exists: Boolean \u00b6 Checks whether the state value exists or not If not exists, < > throws a NoSuchElementException . Use < > instead. | get a| [[get]] [source, scala] \u00b6 get: S \u00b6 Gets the state value if it < > or throws a NoSuchElementException | getCurrentProcessingTimeMs a| [[getCurrentProcessingTimeMs]] [source, scala] \u00b6 getCurrentProcessingTimeMs(): Long \u00b6 Gets the current processing time (as milliseconds in epoch time) | getCurrentWatermarkMs a| [[getCurrentWatermarkMs]] [source, scala] \u00b6 getCurrentWatermarkMs(): Long \u00b6 Gets the current event time watermark (as milliseconds in epoch time) | getOption a| [[getOption]] [source, scala] \u00b6 getOption: Option[S] \u00b6 Gets the state value as a Scala Option (regardless whether it < > or not) Used when: InputProcessor is requested to callFunctionAndUpdateState (when the row iterator is consumed and a state value has been updated, removed or timeout changed) GroupStateImpl is requested for the textual representation | hasTimedOut a| [[hasTimedOut]] [source, scala] \u00b6 hasTimedOut: Boolean \u00b6 Whether the state (for a given key) has timed out or not. Can only be true when timeouts are enabled using < > | remove a| [[remove]] [source, scala] \u00b6 remove(): Unit \u00b6 Removes the state | setTimeoutDuration a| [[setTimeoutDuration]] [source, scala] \u00b6 setTimeoutDuration(durationMs: Long): Unit setTimeoutDuration(duration: String): Unit Specifies the timeout duration for the state key (in millis or as a string, e.g. \"10 seconds\", \"1 hour\") for < > | setTimeoutTimestamp a| [[setTimeoutTimestamp]] [source, scala] \u00b6 setTimeoutTimestamp(timestamp: java.sql.Date): Unit setTimeoutTimestamp( timestamp: java.sql.Date, additionalDuration: String): Unit setTimeoutTimestamp(timestampMs: Long): Unit setTimeoutTimestamp( timestampMs: Long, additionalDuration: String): Unit Specifies the timeout timestamp for the state key for < > | update a| [[update]] [source, scala] \u00b6 update(newState: S): Unit \u00b6 Updates the state (sets the state to a new value) |=== [[implementations]] GroupStateImpl is the default and only known implementation of the < > in Spark Structured Streaming.","title":"GroupState"},{"location":"GroupState/#groupstate-group-state-in-arbitrary-stateful-streaming-aggregation","text":"GroupState is an < > of < > (of type S ) in Arbitrary Stateful Streaming Aggregation . GroupState is used with the following KeyValueGroupedDataset operations: mapGroupsWithState flatMapGroupsWithState GroupState is created separately for every aggregation key to hold a state as an aggregation state value . [[contract]] .GroupState Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | exists a| [[exists]]","title":"GroupState -- Group State in Arbitrary Stateful Streaming Aggregation"},{"location":"GroupState/#source-scala","text":"","title":"[source, scala]"},{"location":"GroupState/#exists-boolean","text":"Checks whether the state value exists or not If not exists, < > throws a NoSuchElementException . Use < > instead. | get a| [[get]]","title":"exists: Boolean"},{"location":"GroupState/#source-scala_1","text":"","title":"[source, scala]"},{"location":"GroupState/#get-s","text":"Gets the state value if it < > or throws a NoSuchElementException | getCurrentProcessingTimeMs a| [[getCurrentProcessingTimeMs]]","title":"get: S"},{"location":"GroupState/#source-scala_2","text":"","title":"[source, scala]"},{"location":"GroupState/#getcurrentprocessingtimems-long","text":"Gets the current processing time (as milliseconds in epoch time) | getCurrentWatermarkMs a| [[getCurrentWatermarkMs]]","title":"getCurrentProcessingTimeMs(): Long"},{"location":"GroupState/#source-scala_3","text":"","title":"[source, scala]"},{"location":"GroupState/#getcurrentwatermarkms-long","text":"Gets the current event time watermark (as milliseconds in epoch time) | getOption a| [[getOption]]","title":"getCurrentWatermarkMs(): Long"},{"location":"GroupState/#source-scala_4","text":"","title":"[source, scala]"},{"location":"GroupState/#getoption-options","text":"Gets the state value as a Scala Option (regardless whether it < > or not) Used when: InputProcessor is requested to callFunctionAndUpdateState (when the row iterator is consumed and a state value has been updated, removed or timeout changed) GroupStateImpl is requested for the textual representation | hasTimedOut a| [[hasTimedOut]]","title":"getOption: Option[S]"},{"location":"GroupState/#source-scala_5","text":"","title":"[source, scala]"},{"location":"GroupState/#hastimedout-boolean","text":"Whether the state (for a given key) has timed out or not. Can only be true when timeouts are enabled using < > | remove a| [[remove]]","title":"hasTimedOut: Boolean"},{"location":"GroupState/#source-scala_6","text":"","title":"[source, scala]"},{"location":"GroupState/#remove-unit","text":"Removes the state | setTimeoutDuration a| [[setTimeoutDuration]]","title":"remove(): Unit"},{"location":"GroupState/#source-scala_7","text":"setTimeoutDuration(durationMs: Long): Unit setTimeoutDuration(duration: String): Unit Specifies the timeout duration for the state key (in millis or as a string, e.g. \"10 seconds\", \"1 hour\") for < > | setTimeoutTimestamp a| [[setTimeoutTimestamp]]","title":"[source, scala]"},{"location":"GroupState/#source-scala_8","text":"setTimeoutTimestamp(timestamp: java.sql.Date): Unit setTimeoutTimestamp( timestamp: java.sql.Date, additionalDuration: String): Unit setTimeoutTimestamp(timestampMs: Long): Unit setTimeoutTimestamp( timestampMs: Long, additionalDuration: String): Unit Specifies the timeout timestamp for the state key for < > | update a| [[update]]","title":"[source, scala]"},{"location":"GroupState/#source-scala_9","text":"","title":"[source, scala]"},{"location":"GroupState/#updatenewstate-s-unit","text":"Updates the state (sets the state to a new value) |=== [[implementations]] GroupStateImpl is the default and only known implementation of the < > in Spark Structured Streaming.","title":"update(newState: S): Unit"},{"location":"GroupStateImpl/","text":"GroupStateImpl \u00b6 GroupStateImpl is the default and only known GroupState in Spark Structured Streaming. GroupStateImpl holds per-group < > of type S per group key. GroupStateImpl is < > when GroupStateImpl helper object is requested for the following: < > < > === [[creating-instance]] Creating GroupStateImpl Instance GroupStateImpl takes the following to be created: [[optionalValue]] State value (of type S ) [[batchProcessingTimeMs]] < > [[eventTimeWatermarkMs]] eventTimeWatermarkMs [[timeoutConf]] < > [[hasTimedOut]] hasTimedOut flag [[watermarkPresent]] watermarkPresent flag GroupStateImpl initializes the < >. === [[createForStreaming]] Creating GroupStateImpl for Streaming Query -- createForStreaming Factory Method [source, scala] \u00b6 createForStreaming S : GroupStateImpl[S] createForStreaming simply creates a < > with the given input arguments. NOTE: createForStreaming is used exclusively when InputProcessor is requested to callFunctionAndUpdateState (when InputProcessor is requested to processNewData and processTimedOutState ). === [[createForBatch]] Creating GroupStateImpl for Batch Query -- createForBatch Factory Method [source, scala] \u00b6 createForBatch( timeoutConf: GroupStateTimeout, watermarkPresent: Boolean): GroupStateImpl[Any] createForBatch ...FIXME NOTE: createForBatch is used when...FIXME === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString ...FIXME === [[setTimeoutDuration]] Specifying Timeout Duration for ProcessingTimeTimeout -- setTimeoutDuration Method [source, scala] \u00b6 setTimeoutDuration(durationMs: Long): Unit \u00b6 setTimeoutDuration ...FIXME setTimeoutDuration is part of the GroupState abstraction. === [[setTimeoutTimestamp]] Specifying Timeout Timestamp for EventTimeTimeout -- setTimeoutTimestamp Method [source, scala] \u00b6 setTimeoutTimestamp(durationMs: Long): Unit \u00b6 setTimeoutTimestamp ...FIXME setTimeoutTimestamp is part of the GroupState abstraction. === [[getCurrentProcessingTimeMs]] Getting Processing Time -- getCurrentProcessingTimeMs Method [source, scala] \u00b6 getCurrentProcessingTimeMs(): Long \u00b6 getCurrentProcessingTimeMs simply returns the < >. getCurrentProcessingTimeMs is part of the GroupState abstraction. === [[update]] Updating State -- update Method [source, scala] \u00b6 update(newValue: S): Unit \u00b6 update ...FIXME update is part of the GroupState abstraction. === [[remove]] Removing State -- remove Method [source, scala] \u00b6 remove(): Unit \u00b6 remove ...FIXME remove is part of the GroupState abstraction. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | value a| [[value]] FIXME Used when...FIXME | defined a| [[defined]] FIXME Used when...FIXME | updated a| [[updated]][[hasUpdated]] Updated flag that says whether the state has been < > or not Default: false Disabled ( false ) when GroupStateImpl is requested to < > Enabled ( true ) when GroupStateImpl is requested to < > | removed a| [[removed]][[hasRemoved]] Removed flag that says whether the state is marked < > or not Default: false Disabled ( false ) when GroupStateImpl is requested to < > Enabled ( true ) when GroupStateImpl is requested to < > | timeoutTimestamp a| [[timeoutTimestamp]][[getTimeoutTimestamp]] Current timeout timestamp (in millis) for < > or < > [[NO_TIMESTAMP]] Default: -1 Defined using < > (for EventTimeTimeout ) and < > (for ProcessingTimeTimeout ) |===","title":"GroupStateImpl"},{"location":"GroupStateImpl/#groupstateimpl","text":"GroupStateImpl is the default and only known GroupState in Spark Structured Streaming. GroupStateImpl holds per-group < > of type S per group key. GroupStateImpl is < > when GroupStateImpl helper object is requested for the following: < > < > === [[creating-instance]] Creating GroupStateImpl Instance GroupStateImpl takes the following to be created: [[optionalValue]] State value (of type S ) [[batchProcessingTimeMs]] < > [[eventTimeWatermarkMs]] eventTimeWatermarkMs [[timeoutConf]] < > [[hasTimedOut]] hasTimedOut flag [[watermarkPresent]] watermarkPresent flag GroupStateImpl initializes the < >. === [[createForStreaming]] Creating GroupStateImpl for Streaming Query -- createForStreaming Factory Method","title":"GroupStateImpl"},{"location":"GroupStateImpl/#source-scala","text":"createForStreaming S : GroupStateImpl[S] createForStreaming simply creates a < > with the given input arguments. NOTE: createForStreaming is used exclusively when InputProcessor is requested to callFunctionAndUpdateState (when InputProcessor is requested to processNewData and processTimedOutState ). === [[createForBatch]] Creating GroupStateImpl for Batch Query -- createForBatch Factory Method","title":"[source, scala]"},{"location":"GroupStateImpl/#source-scala_1","text":"createForBatch( timeoutConf: GroupStateTimeout, watermarkPresent: Boolean): GroupStateImpl[Any] createForBatch ...FIXME NOTE: createForBatch is used when...FIXME === [[toString]] Textual Representation -- toString Method","title":"[source, scala]"},{"location":"GroupStateImpl/#source-scala_2","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString ...FIXME === [[setTimeoutDuration]] Specifying Timeout Duration for ProcessingTimeTimeout -- setTimeoutDuration Method","title":"toString: String"},{"location":"GroupStateImpl/#source-scala_3","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#settimeoutdurationdurationms-long-unit","text":"setTimeoutDuration ...FIXME setTimeoutDuration is part of the GroupState abstraction. === [[setTimeoutTimestamp]] Specifying Timeout Timestamp for EventTimeTimeout -- setTimeoutTimestamp Method","title":"setTimeoutDuration(durationMs: Long): Unit"},{"location":"GroupStateImpl/#source-scala_4","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#settimeouttimestampdurationms-long-unit","text":"setTimeoutTimestamp ...FIXME setTimeoutTimestamp is part of the GroupState abstraction. === [[getCurrentProcessingTimeMs]] Getting Processing Time -- getCurrentProcessingTimeMs Method","title":"setTimeoutTimestamp(durationMs: Long): Unit"},{"location":"GroupStateImpl/#source-scala_5","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#getcurrentprocessingtimems-long","text":"getCurrentProcessingTimeMs simply returns the < >. getCurrentProcessingTimeMs is part of the GroupState abstraction. === [[update]] Updating State -- update Method","title":"getCurrentProcessingTimeMs(): Long"},{"location":"GroupStateImpl/#source-scala_6","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#updatenewvalue-s-unit","text":"update ...FIXME update is part of the GroupState abstraction. === [[remove]] Removing State -- remove Method","title":"update(newValue: S): Unit"},{"location":"GroupStateImpl/#source-scala_7","text":"","title":"[source, scala]"},{"location":"GroupStateImpl/#remove-unit","text":"remove ...FIXME remove is part of the GroupState abstraction. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | value a| [[value]] FIXME Used when...FIXME | defined a| [[defined]] FIXME Used when...FIXME | updated a| [[updated]][[hasUpdated]] Updated flag that says whether the state has been < > or not Default: false Disabled ( false ) when GroupStateImpl is requested to < > Enabled ( true ) when GroupStateImpl is requested to < > | removed a| [[removed]][[hasRemoved]] Removed flag that says whether the state is marked < > or not Default: false Disabled ( false ) when GroupStateImpl is requested to < > Enabled ( true ) when GroupStateImpl is requested to < > | timeoutTimestamp a| [[timeoutTimestamp]][[getTimeoutTimestamp]] Current timeout timestamp (in millis) for < > or < > [[NO_TIMESTAMP]] Default: -1 Defined using < > (for EventTimeTimeout ) and < > (for ProcessingTimeTimeout ) |===","title":"remove(): Unit"},{"location":"InputProcessor/","text":"InputProcessor \u00b6 InputProcessor is a helper class that is used to update state (in the state store ) of a single partition of a FlatMapGroupsWithStateExec physical operator. Creating Instance \u00b6 InputProcessor takes the following to be created: StateStore InputProcessor is created when FlatMapGroupsWithStateExec physical operator is executed (for storeUpdateFunction while processing rows per partition with a corresponding per-partition state store). StateStore \u00b6 InputProcessor is given a StateStore when created . The StateStore manages the per-group state (and is used when processing new data and timed-out state data , and in the \"all rows processed\" callback ). Processing New Data \u00b6 processNewData ( dataIter : Iterator [ InternalRow ]) : Iterator [ InternalRow ] processNewData creates a grouped iterator of (of pairs of) per-group state keys and the row values from the given data iterator ( dataIter ) with the grouping attributes and the output schema of the child operator (of the parent FlatMapGroupsWithStateExec physical operator). For every per-group state key (in the grouped iterator), processNewData requests the StateManager (of the parent FlatMapGroupsWithStateExec physical operator) to get the state (from the StateStore ) and callFunctionAndUpdateState (with the hasTimedOut flag off). processNewData is used when FlatMapGroupsWithStateExec physical operator is executed. Processing Timed-Out State Data \u00b6 processTimedOutState () : Iterator [ InternalRow ] processTimedOutState does nothing and simply returns an empty iterator for GroupStateTimeout.NoTimeout . With timeout enabled , processTimedOutState gets the current timeout threshold per GroupStateTimeout : batchTimestampMs for ProcessingTimeTimeout eventTimeWatermark for EventTimeTimeout processTimedOutState creates an iterator of timed-out state data by requesting the StateManager for all the available state data (in the StateStore ) and takes only the state data with timeout defined and below the current timeout threshold. In the end, for every timed-out state data, processTimedOutState callFunctionAndUpdateState (with the hasTimedOut flag on). processTimedOutState is used when FlatMapGroupsWithStateExec physical operator is executed. callFunctionAndUpdateState Internal Method \u00b6 callFunctionAndUpdateState ( stateData : StateData , valueRowIter : Iterator [ InternalRow ], hasTimedOut : Boolean ) : Iterator [ InternalRow ] callFunctionAndUpdateState is used when InputProcessor is requested to process new data and timed-out state data with the given hasTimedOut flag is off and on, respectively. callFunctionAndUpdateState creates a key object by requesting the given StateData for the UnsafeRow of the key ( keyRow ) and converts it to an object (using the internal state key converter ). callFunctionAndUpdateState creates value objects by taking every value row (from the given valueRowIter iterator) and converts them to objects (using the internal state value converter ). callFunctionAndUpdateState creates a new GroupStateImpl with the following: The current state value (of the given StateData ) that could possibly be null The batchTimestampMs of the parent FlatMapGroupsWithStateExec operator (that could possibly be -1 ) The event-time watermark of the parent FlatMapGroupsWithStateExec operator (that could possibly be -1 ) The GroupStateTimeout of the parent FlatMapGroupsWithStateExec operator The watermarkPresent flag of the parent FlatMapGroupsWithStateExec operator The given hasTimedOut flag callFunctionAndUpdateState then executes the user-defined state function (of the parent FlatMapGroupsWithStateExec operator) on the key object, value objects, and the newly-created GroupStateImpl . For every output value from the user-defined state function, callFunctionAndUpdateState updates numOutputRows performance metric and wraps the values to an internal row (using the internal output value converter ). In the end, callFunctionAndUpdateState returns a Iterator[InternalRow] which calls the completion function right after rows have been processed (so the iterator is considered fully consumed). \"All Rows Processed\" Callback \u00b6 onIteratorCompletion : Unit onIteratorCompletion branches off per whether the GroupStateImpl has been marked removed and no timeout timestamp is specified or not. When the GroupStateImpl has been marked removed and no timeout timestamp is specified, onIteratorCompletion does the following: . Requests the StateManager (of the parent FlatMapGroupsWithStateExec operator) to remove the state (from the StateStore for the key row of the given StateData ) . Increments the numUpdatedStateRows performance metric Otherwise, when the GroupStateImpl has not been marked removed or the timeout timestamp is specified, onIteratorCompletion checks whether the timeout timestamp has changed by comparing the timeout timestamps of the GroupStateImpl and the given StateData . (only when the GroupStateImpl has been updated , removed or the timeout timestamp changed) onIteratorCompletion does the following: . Requests the StateManager (of the parent FlatMapGroupsWithStateExec operator) to persist the state (in the StateStore with the key row, updated state object, and the timeout timestamp of the given StateData ) . Increments the numUpdatedStateRows performance metrics onIteratorCompletion is used when InputProcessor is requested to callFunctionAndUpdateState (right after rows have been processed) Converters \u00b6 Output Value Converter \u00b6 An output value converter (of type Any => InternalRow ) to wrap a given output value (from the user-defined state function) to a row The data type of the row is specified as the data type of the output object attribute when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState . State Key Converter \u00b6 A state key converter (of type InternalRow => Any ) to deserialize a given row (for a per-group state key) to the current state value The deserialization expression for keys is specified as the key deserializer expression when the parent FlatMapGroupsWithStateExec operator is created The data type of state keys is specified as the grouping attributes when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState . State Value Converter \u00b6 A state value converter (of type InternalRow => Any ) to deserialize a given row (for a per-group state value) to a Scala value The deserialization expression for values is specified as the value deserializer expression when the parent FlatMapGroupsWithStateExec operator is created The data type of state values is specified as the data attributes when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState .","title":"InputProcessor"},{"location":"InputProcessor/#inputprocessor","text":"InputProcessor is a helper class that is used to update state (in the state store ) of a single partition of a FlatMapGroupsWithStateExec physical operator.","title":"InputProcessor"},{"location":"InputProcessor/#creating-instance","text":"InputProcessor takes the following to be created: StateStore InputProcessor is created when FlatMapGroupsWithStateExec physical operator is executed (for storeUpdateFunction while processing rows per partition with a corresponding per-partition state store).","title":"Creating Instance"},{"location":"InputProcessor/#statestore","text":"InputProcessor is given a StateStore when created . The StateStore manages the per-group state (and is used when processing new data and timed-out state data , and in the \"all rows processed\" callback ).","title":" StateStore"},{"location":"InputProcessor/#processing-new-data","text":"processNewData ( dataIter : Iterator [ InternalRow ]) : Iterator [ InternalRow ] processNewData creates a grouped iterator of (of pairs of) per-group state keys and the row values from the given data iterator ( dataIter ) with the grouping attributes and the output schema of the child operator (of the parent FlatMapGroupsWithStateExec physical operator). For every per-group state key (in the grouped iterator), processNewData requests the StateManager (of the parent FlatMapGroupsWithStateExec physical operator) to get the state (from the StateStore ) and callFunctionAndUpdateState (with the hasTimedOut flag off). processNewData is used when FlatMapGroupsWithStateExec physical operator is executed.","title":" Processing New Data"},{"location":"InputProcessor/#processing-timed-out-state-data","text":"processTimedOutState () : Iterator [ InternalRow ] processTimedOutState does nothing and simply returns an empty iterator for GroupStateTimeout.NoTimeout . With timeout enabled , processTimedOutState gets the current timeout threshold per GroupStateTimeout : batchTimestampMs for ProcessingTimeTimeout eventTimeWatermark for EventTimeTimeout processTimedOutState creates an iterator of timed-out state data by requesting the StateManager for all the available state data (in the StateStore ) and takes only the state data with timeout defined and below the current timeout threshold. In the end, for every timed-out state data, processTimedOutState callFunctionAndUpdateState (with the hasTimedOut flag on). processTimedOutState is used when FlatMapGroupsWithStateExec physical operator is executed.","title":" Processing Timed-Out State Data"},{"location":"InputProcessor/#callfunctionandupdatestate-internal-method","text":"callFunctionAndUpdateState ( stateData : StateData , valueRowIter : Iterator [ InternalRow ], hasTimedOut : Boolean ) : Iterator [ InternalRow ] callFunctionAndUpdateState is used when InputProcessor is requested to process new data and timed-out state data with the given hasTimedOut flag is off and on, respectively. callFunctionAndUpdateState creates a key object by requesting the given StateData for the UnsafeRow of the key ( keyRow ) and converts it to an object (using the internal state key converter ). callFunctionAndUpdateState creates value objects by taking every value row (from the given valueRowIter iterator) and converts them to objects (using the internal state value converter ). callFunctionAndUpdateState creates a new GroupStateImpl with the following: The current state value (of the given StateData ) that could possibly be null The batchTimestampMs of the parent FlatMapGroupsWithStateExec operator (that could possibly be -1 ) The event-time watermark of the parent FlatMapGroupsWithStateExec operator (that could possibly be -1 ) The GroupStateTimeout of the parent FlatMapGroupsWithStateExec operator The watermarkPresent flag of the parent FlatMapGroupsWithStateExec operator The given hasTimedOut flag callFunctionAndUpdateState then executes the user-defined state function (of the parent FlatMapGroupsWithStateExec operator) on the key object, value objects, and the newly-created GroupStateImpl . For every output value from the user-defined state function, callFunctionAndUpdateState updates numOutputRows performance metric and wraps the values to an internal row (using the internal output value converter ). In the end, callFunctionAndUpdateState returns a Iterator[InternalRow] which calls the completion function right after rows have been processed (so the iterator is considered fully consumed).","title":" callFunctionAndUpdateState Internal Method"},{"location":"InputProcessor/#all-rows-processed-callback","text":"onIteratorCompletion : Unit onIteratorCompletion branches off per whether the GroupStateImpl has been marked removed and no timeout timestamp is specified or not. When the GroupStateImpl has been marked removed and no timeout timestamp is specified, onIteratorCompletion does the following: . Requests the StateManager (of the parent FlatMapGroupsWithStateExec operator) to remove the state (from the StateStore for the key row of the given StateData ) . Increments the numUpdatedStateRows performance metric Otherwise, when the GroupStateImpl has not been marked removed or the timeout timestamp is specified, onIteratorCompletion checks whether the timeout timestamp has changed by comparing the timeout timestamps of the GroupStateImpl and the given StateData . (only when the GroupStateImpl has been updated , removed or the timeout timestamp changed) onIteratorCompletion does the following: . Requests the StateManager (of the parent FlatMapGroupsWithStateExec operator) to persist the state (in the StateStore with the key row, updated state object, and the timeout timestamp of the given StateData ) . Increments the numUpdatedStateRows performance metrics onIteratorCompletion is used when InputProcessor is requested to callFunctionAndUpdateState (right after rows have been processed)","title":" \"All Rows Processed\" Callback"},{"location":"InputProcessor/#converters","text":"","title":"Converters"},{"location":"InputProcessor/#output-value-converter","text":"An output value converter (of type Any => InternalRow ) to wrap a given output value (from the user-defined state function) to a row The data type of the row is specified as the data type of the output object attribute when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState .","title":" Output Value Converter"},{"location":"InputProcessor/#state-key-converter","text":"A state key converter (of type InternalRow => Any ) to deserialize a given row (for a per-group state key) to the current state value The deserialization expression for keys is specified as the key deserializer expression when the parent FlatMapGroupsWithStateExec operator is created The data type of state keys is specified as the grouping attributes when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState .","title":" State Key Converter"},{"location":"InputProcessor/#state-value-converter","text":"A state value converter (of type InternalRow => Any ) to deserialize a given row (for a per-group state value) to a Scala value The deserialization expression for values is specified as the value deserializer expression when the parent FlatMapGroupsWithStateExec operator is created The data type of state values is specified as the data attributes when the parent FlatMapGroupsWithStateExec operator is created Used when InputProcessor is requested to callFunctionAndUpdateState .","title":" State Value Converter"},{"location":"KeyValueGroupedDataset/","text":"== [[KeyValueGroupedDataset]] KeyValueGroupedDataset -- Streaming Aggregation KeyValueGroupedDataset represents a grouped dataset as a result of Dataset.groupByKey operator (that aggregates records by a grouping function). // Dataset[T] groupByKey(func: T => K): KeyValueGroupedDataset[K, T] [source, scala] \u00b6 import java.sql.Timestamp val numGroups = spark. readStream. format(\"rate\"). load. as[(Timestamp, Long)]. groupByKey { case (time, value) => value % 2 } scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] KeyValueGroupedDataset is also < > for < > and < > operators. [source, scala] \u00b6 scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] scala> :type numGroups.keyAs[String] org.apache.spark.sql.KeyValueGroupedDataset[String,(java.sql.Timestamp, Long)] [source, scala] \u00b6 scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] val mapped = numGroups.mapValues { case (ts, n) => s\"($ts, $n)\" } scala> :type mapped org.apache.spark.sql.KeyValueGroupedDataset[Long,String] KeyValueGroupedDataset works for batch and streaming aggregations, but shines the most when used for < >. [source, scala] \u00b6 scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ numGroups. mapGroups { case(group, values) => values.size }. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). start Batch: 0 \u00b6 +-----+ |value| +-----+ +-----+ Batch: 1 \u00b6 +-----+ |value| +-----+ | 3| | 2| +-----+ Batch: 2 \u00b6 +-----+ |value| +-----+ | 5| | 5| +-----+ // Eventually... spark.streams.active.foreach(_.stop) The most prestigious use case of KeyValueGroupedDataset however is Arbitrary Stateful Streaming Aggregation that allows for accumulating streaming state (by means of GroupState ) using < > and the more advanced < > operators. [[operators]] .KeyValueGroupedDataset's Operators [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Operator | Description | agg a| [[agg]] [source, scala] \u00b6 agg U1 : Dataset[(K, U1)] agg U1, U2 : Dataset[(K, U1, U2)] agg U1, U2, U3 : Dataset[(K, U1, U2, U3)] agg U1, U2, U3, U4 : Dataset[(K, U1, U2, U3, U4)] | cogroup a| [[cogroup]] [source, scala] \u00b6 cogroup U, R : Encoder ( f: (K, Iterator[V], Iterator[U]) => TraversableOnce[R]): Dataset[R] | count a| [[count]] [source, scala] \u00b6 count(): Dataset[(K, Long)] \u00b6 | flatMapGroups a| [[flatMapGroups]] [source, scala] \u00b6 flatMapGroups U : Encoder : Dataset[U] \u00b6 | flatMapGroupsWithState a| [[flatMapGroupsWithState]] flatMapGroupsWithState [ S: Encoder , U: Encoder ]( outputMode : OutputMode , timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => Iterator [ U ]) : Dataset [ U ] Arbitrary Stateful Streaming Aggregation - streaming aggregation with explicit state and state timeout Note The difference between this flatMapGroupsWithState and mapGroupsWithState operators is the state function that generates zero or more elements (that are in turn the rows in the result streaming Dataset ). | keyAs a| [[keyAs]] [source, scala] \u00b6 keys: Dataset[K] keyAs[L : Encoder]: KeyValueGroupedDataset[L, V] | mapGroups a| [[mapGroups]] [source, scala] \u00b6 mapGroups U : Encoder : Dataset[U] \u00b6 | spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState.md[mapGroupsWithState] a| [[mapGroupsWithState]] mapGroupsWithState [ S: Encoder , U: Encoder ]( func : ( K , Iterator [ V ], GroupState [ S ]) => U ) : Dataset [ U ] mapGroupsWithState [ S: Encoder , U: Encoder ]( timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => U ) : Dataset [ U ] Creates a new Dataset with FlatMapGroupsWithState logical operator Note The difference between mapGroupsWithState and flatMapGroupsWithState is the state function that generates exactly one element (that is in turn the row in the result Dataset ). | mapValues a| [[mapValues]] [source, scala] \u00b6 mapValues W : Encoder : KeyValueGroupedDataset[K, W] \u00b6 | reduceGroups a| [[reduceGroups]] [source, scala] \u00b6 reduceGroups(f: (V, V) => V): Dataset[(K, V)] \u00b6 |=== === [[creating-instance]] Creating KeyValueGroupedDataset Instance KeyValueGroupedDataset takes the following when created: [[kEncoder]] Encoder for keys [[vEncoder]] Encoder for values [[queryExecution]] QueryExecution [[dataAttributes]] Data attributes [[groupingAttributes]] Grouping attributes","title":"KeyValueGroupedDataset"},{"location":"KeyValueGroupedDataset/#source-scala","text":"import java.sql.Timestamp val numGroups = spark. readStream. format(\"rate\"). load. as[(Timestamp, Long)]. groupByKey { case (time, value) => value % 2 } scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] KeyValueGroupedDataset is also < > for < > and < > operators.","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#source-scala_1","text":"scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] scala> :type numGroups.keyAs[String] org.apache.spark.sql.KeyValueGroupedDataset[String,(java.sql.Timestamp, Long)]","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#source-scala_2","text":"scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] val mapped = numGroups.mapValues { case (ts, n) => s\"($ts, $n)\" } scala> :type mapped org.apache.spark.sql.KeyValueGroupedDataset[Long,String] KeyValueGroupedDataset works for batch and streaming aggregations, but shines the most when used for < >.","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#source-scala_3","text":"scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ numGroups. mapGroups { case(group, values) => values.size }. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). start","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#batch-0","text":"+-----+ |value| +-----+ +-----+","title":"Batch: 0"},{"location":"KeyValueGroupedDataset/#batch-1","text":"+-----+ |value| +-----+ | 3| | 2| +-----+","title":"Batch: 1"},{"location":"KeyValueGroupedDataset/#batch-2","text":"+-----+ |value| +-----+ | 5| | 5| +-----+ // Eventually... spark.streams.active.foreach(_.stop) The most prestigious use case of KeyValueGroupedDataset however is Arbitrary Stateful Streaming Aggregation that allows for accumulating streaming state (by means of GroupState ) using < > and the more advanced < > operators. [[operators]] .KeyValueGroupedDataset's Operators [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Operator | Description | agg a| [[agg]]","title":"Batch: 2"},{"location":"KeyValueGroupedDataset/#source-scala_4","text":"agg U1 : Dataset[(K, U1)] agg U1, U2 : Dataset[(K, U1, U2)] agg U1, U2, U3 : Dataset[(K, U1, U2, U3)] agg U1, U2, U3, U4 : Dataset[(K, U1, U2, U3, U4)] | cogroup a| [[cogroup]]","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#source-scala_5","text":"cogroup U, R : Encoder ( f: (K, Iterator[V], Iterator[U]) => TraversableOnce[R]): Dataset[R] | count a| [[count]]","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#source-scala_6","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#count-datasetk-long","text":"| flatMapGroups a| [[flatMapGroups]]","title":"count(): Dataset[(K, Long)]"},{"location":"KeyValueGroupedDataset/#source-scala_7","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#flatmapgroupsu-encoder-datasetu","text":"| flatMapGroupsWithState a| [[flatMapGroupsWithState]] flatMapGroupsWithState [ S: Encoder , U: Encoder ]( outputMode : OutputMode , timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => Iterator [ U ]) : Dataset [ U ] Arbitrary Stateful Streaming Aggregation - streaming aggregation with explicit state and state timeout Note The difference between this flatMapGroupsWithState and mapGroupsWithState operators is the state function that generates zero or more elements (that are in turn the rows in the result streaming Dataset ). | keyAs a| [[keyAs]]","title":"flatMapGroupsU : Encoder: Dataset[U]"},{"location":"KeyValueGroupedDataset/#source-scala_8","text":"keys: Dataset[K] keyAs[L : Encoder]: KeyValueGroupedDataset[L, V] | mapGroups a| [[mapGroups]]","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#source-scala_9","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#mapgroupsu-encoder-datasetu","text":"| spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState.md[mapGroupsWithState] a| [[mapGroupsWithState]] mapGroupsWithState [ S: Encoder , U: Encoder ]( func : ( K , Iterator [ V ], GroupState [ S ]) => U ) : Dataset [ U ] mapGroupsWithState [ S: Encoder , U: Encoder ]( timeoutConf : GroupStateTimeout )( func : ( K , Iterator [ V ], GroupState [ S ]) => U ) : Dataset [ U ] Creates a new Dataset with FlatMapGroupsWithState logical operator Note The difference between mapGroupsWithState and flatMapGroupsWithState is the state function that generates exactly one element (that is in turn the row in the result Dataset ). | mapValues a| [[mapValues]]","title":"mapGroupsU : Encoder: Dataset[U]"},{"location":"KeyValueGroupedDataset/#source-scala_10","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#mapvaluesw-encoder-keyvaluegroupeddatasetk-w","text":"| reduceGroups a| [[reduceGroups]]","title":"mapValuesW : Encoder: KeyValueGroupedDataset[K, W]"},{"location":"KeyValueGroupedDataset/#source-scala_11","text":"","title":"[source, scala]"},{"location":"KeyValueGroupedDataset/#reducegroupsf-v-v-v-datasetk-v","text":"|=== === [[creating-instance]] Creating KeyValueGroupedDataset Instance KeyValueGroupedDataset takes the following when created: [[kEncoder]] Encoder for keys [[vEncoder]] Encoder for values [[queryExecution]] QueryExecution [[dataAttributes]] Data attributes [[groupingAttributes]] Grouping attributes","title":"reduceGroups(f: (V, V) =&gt; V): Dataset[(K, V)]"},{"location":"MicroBatchExecution/","text":"MicroBatchExecution \u2014 Stream Execution Engine of Micro-Batch Stream Processing \u00b6 MicroBatchExecution is the stream execution engine in Micro-Batch Stream Processing . MicroBatchExecution is < > when StreamingQueryManager is requested to create a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ) with the following: Any type of < > but < > Any type of < > but < > import org.apache.spark.sql.streaming.Trigger val query = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") // <-- not a StreamWriteSupport sink .option(\"truncate\", false) .trigger(Trigger.Once) // <-- Gives MicroBatchExecution .queryName(\"rate2console\") .start // The following gives access to the internals // And to MicroBatchExecution import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = query.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) import org.apache.spark.sql.execution.streaming.MicroBatchExecution val microBatchEngine = engine.asInstanceOf[MicroBatchExecution] assert(microBatchEngine.trigger == Trigger.Once) Once < >, MicroBatchExecution (as a stream execution engine ) is requested to < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.MicroBatchExecution to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MicroBatchExecution=ALL Refer to < >. \u00b6 Creating Instance \u00b6 MicroBatchExecution takes the following to be created: [[sparkSession]] SparkSession [[name]] Name of the streaming query [[checkpointRoot]] Path of the checkpoint directory [[analyzedPlan]] Analyzed logical query plan of the streaming query ( LogicalPlan ) [[sink]] < > [[trigger]] < > [[triggerClock]] Trigger clock ( Clock ) [[outputMode]] < > [[extraOptions]] Extra options ( Map[String, String] ) [[deleteCheckpointOnStop]] deleteCheckpointOnStop flag to control whether to delete the checkpoint directory on stop MicroBatchExecution initializes the < >. === [[triggerExecutor]] MicroBatchExecution and TriggerExecutor -- triggerExecutor Property [source, scala] \u00b6 triggerExecutor: TriggerExecutor \u00b6 triggerExecutor is the < > of the streaming query that is how micro-batches are executed at regular intervals. triggerExecutor is initialized based on the given < > (that was used to create the MicroBatchExecution ): < > for < > < > for < > (aka < > trigger) triggerExecutor throws an IllegalStateException when the < > is not one of the < >. Unknown type of trigger: [trigger] NOTE: triggerExecutor is used exclusively when StreamExecution is requested to < > (at regular intervals). === [[runActivatedStream]] Running Activated Streaming Query -- runActivatedStream Method [source, scala] \u00b6 runActivatedStream( sparkSessionForStream: SparkSession): Unit runActivatedStream simply requests the < > to execute micro-batches using the < > (until MicroBatchExecution is terminated due to a query stop or a failure). runActivatedStream is part of StreamExecution abstraction. ==== [[batchRunner]][[batch-runner]] TriggerExecutor's Batch Runner The batch runner (of the < >) is executed as long as the MicroBatchExecution is active . NOTE: trigger and batch are considered equivalent and used interchangeably. [[runActivatedStream-startTrigger]] The batch runner initializes query progress for the new trigger (aka startTrigger ). [[runActivatedStream-triggerExecution]][[runActivatedStream-triggerExecution-populateStartOffsets]] The batch runner starts triggerExecution execution phase that is made up of the following steps: . < > before the first \"zero\" batch (at every start or restart) . < > . < > At the start or restart ( resume ) of a streaming query (when the < > is uninitialized and -1 ), the batch runner < > and then prints out the following INFO message to the logs (using the committedOffsets internal registry): Stream started from [committedOffsets] The batch runner sets the human-readable description for any Spark job submitted (that streaming sources may submit to get new data) as the batch description . [[runActivatedStream-triggerExecution-isCurrentBatchConstructed]] The batch runner < > (when the < > internal flag is off). The batch runner < > (with the committed and available offsets). The batch runner updates the current StreamingQueryStatus with the < > for isDataAvailable property. [[runActivatedStream-triggerExecution-runBatch]] With the < > flag enabled ( true ), the batch runner updates the status message to one of the following (per < >) and < >. Processing new data No new data but cleaning up state With the < > flag disabled ( false ), the batch runner simply updates the status message to the following: Waiting for data to arrive [[runActivatedStream-triggerExecution-finishTrigger]] The batch runner finalizes query progress for the trigger (with a flag that indicates whether the current batch had new data). With the < > flag enabled ( true ), the batch runner increments the < > and turns the < > flag off ( false ). With the < > flag disabled ( false ), the batch runner simply sleeps (as long as configured using the spark.sql.streaming.pollingDelay configuration property). In the end, the batch runner updates the status message to the following status and returns whether the MicroBatchExecution is active or not. Waiting for next trigger === [[populateStartOffsets]] Populating Start Offsets From Checkpoint (Resuming from Checkpoint) -- populateStartOffsets Internal Method [source, scala] \u00b6 populateStartOffsets( sparkSessionToRunBatches: SparkSession): Unit populateStartOffsets requests the Offset Write-Ahead Log for the < > (i.e. < >). NOTE: The batch id could not be available in the write-ahead log when a streaming query started with a new log or no batch was persisted ( added ) to the log before. populateStartOffsets branches off based on whether the latest committed batch was < > or < >. NOTE: populateStartOffsets is used exclusively when MicroBatchExecution is requested to < > ( before the first \"zero\" micro-batch ). ==== [[populateStartOffsets-getLatest-available]] Latest Committed Batch Available When the latest committed batch id with the metadata was available in the Offset Write-Ahead Log , populateStartOffsets (re)initializes the internal state as follows: Sets the current batch ID to the latest committed batch ID found Turns the < > internal flag on ( true ) Sets the < > to the offsets (from the metadata) When the latest batch ID found is greater than 0 , populateStartOffsets requests the Offset Write-Ahead Log for the < > or throws an IllegalStateException if not found. batch [latestBatchId - 1] doesn't exist populateStartOffsets sets the < > to the second latest committed offsets. [[populateStartOffsets-getLatest-available-offsetSeqMetadata]] populateStartOffsets updates the offset metadata. CAUTION: FIXME Describe me populateStartOffsets requests the Offset Commit Log for the < > (i.e. < >). CAUTION: FIXME Describe me When the latest committed batch id with metadata was found which is exactly the latest batch ID (found in the Offset Commit Log ), populateStartOffsets ...FIXME When the latest committed batch id with metadata was found, but it is not exactly the second latest batch ID (found in the Offset Commit Log ), populateStartOffsets prints out the following WARN message to the logs: [options=\"wrap\"] \u00b6 Batch completion log latest batch id is [latestCommittedBatchId], which is not trailing batchid [latestBatchId] by one \u00b6 When no commit log present in the Offset Commit Log , populateStartOffsets prints out the following INFO message to the logs: no commit log present In the end, populateStartOffsets prints out the following DEBUG message to the logs: Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets] ==== [[populateStartOffsets-getLatest-not-available]] No Latest Committed Batch When the latest committed batch id with the metadata could not be found in the Offset Write-Ahead Log , it is assumed that the streaming query is started for the very first time (or the checkpoint location has changed). populateStartOffsets prints out the following INFO message to the logs: Starting new streaming query. [[populateStartOffsets-currentBatchId-0]] populateStartOffsets sets the current batch ID to 0 and creates a new < >. === [[constructNextBatch]] Constructing Or Skipping Next Streaming Micro-Batch -- constructNextBatch Internal Method [source, scala] \u00b6 constructNextBatch( noDataBatchesEnabled: Boolean): Boolean NOTE: constructNextBatch will only be executed when the < > internal flag is enabled ( true ). constructNextBatch performs the following steps: . < > (of the streaming query) . < > . < > . < > In the end, constructNextBatch returns < >. NOTE: constructNextBatch is used exclusively when MicroBatchExecution is requested to < >. ==== [[constructNextBatch-latestOffsets]] Requesting Latest Offsets from Streaming Sources (getOffset, setOffsetRange and getEndOffset Phases) constructNextBatch firstly requests every streaming source for the latest offsets. NOTE: constructNextBatch checks out the latest offset in every streaming data source sequentially, i.e. one data source at a time. .MicroBatchExecution's Getting Offsets From Streaming Sources image::images/MicroBatchExecution-constructNextBatch.png[align=\"center\"] For every streaming source (Data Source API V1), constructNextBatch updates the status message to the following: Getting offsets from [source] [[constructNextBatch-getOffset]] In getOffset time-tracking section , constructNextBatch requests the Source for the < >. For every < > (Data Source API V2), constructNextBatch updates the status message to the following: Getting offsets from [source] [[constructNextBatch-setOffsetRange]] In setOffsetRange time-tracking section , constructNextBatch finds the available offsets of the source (in the < > internal registry) and, if found, requests the MicroBatchReader to < > (from < >). constructNextBatch requests the MicroBatchReader to < >. [[constructNextBatch-getEndOffset]] In getEndOffset time-tracking section , constructNextBatch requests the MicroBatchReader for the < >. Updating availableOffsets StreamProgress with Latest Available Offsets \u00b6 constructNextBatch updates the availableOffsets StreamProgress with the latest reported offsets. Updating Batch Metadata with Current Event-Time Watermark and Batch Timestamp \u00b6 constructNextBatch updates the batch metadata with the current < > (from the < >) and the batch timestamp. ==== [[constructNextBatch-shouldConstructNextBatch]] Checking Whether to Construct Next Micro-Batch or Not (Skip It) constructNextBatch checks whether or not the next streaming micro-batch should be constructed ( lastExecutionRequiresAnotherBatch ). constructNextBatch uses the last IncrementalExecution if the < > (using the batch metadata ) and the given noDataBatchesEnabled flag is enabled ( true ). constructNextBatch also < >. NOTE: shouldConstructNextBatch local flag is enabled ( true ) when < > or the < > (and the given noDataBatchesEnabled flag is enabled). constructNextBatch prints out the following TRACE message to the logs: noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch] constructNextBatch branches off per whether to < > or < > the next batch (per shouldConstructNextBatch flag in the above TRACE message). ==== [[constructNextBatch-shouldConstructNextBatch-enabled]] Constructing Next Micro-Batch -- shouldConstructNextBatch Flag Enabled With the < > flag enabled ( true ), constructNextBatch updates the status message to the following: Writing offsets to log [[constructNextBatch-walCommit]] In walCommit time-tracking section , constructNextBatch requests the availableOffsets StreamProgress to < > (with the < > and the current batch metadata (event-time watermark and timestamp) ) that is in turn added to the write-ahead log for the current batch ID . constructNextBatch prints out the following INFO message to the logs: Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata] NOTE: FIXME ( if (currentBatchId != 0) ... ) NOTE: FIXME ( if (minLogEntriesToMaintain < currentBatchId) ... ) constructNextBatch turns the noNewData internal flag off ( false ). In case of a failure while < > to the write-ahead log , constructNextBatch throws an AssertionError : Concurrent update to the log. Multiple streaming jobs detected for [currentBatchId] Skipping Next Micro-Batch -- shouldConstructNextBatch Flag Disabled \u00b6 With the < > flag disabled ( false ), constructNextBatch turns the noNewData flag on ( true ) and wakes up ( notifies ) all threads waiting for the awaitProgressLockCondition lock. Running Single Streaming Micro-Batch \u00b6 runBatch ( sparkSessionToRunBatch : SparkSession ) : Unit runBatch prints out the following DEBUG message to the logs (with the current batch ID ): Running batch [currentBatchId] runBatch then performs the following steps (aka phases ): . < > . < > . < > . < > . < > . < > . < > . < > . < > In the end, runBatch prints out the following DEBUG message to the logs (with the current batch ID ): Completed batch [currentBatchId] NOTE: runBatch is used exclusively when MicroBatchExecution is requested to < > (and there is new data to process). getBatch Phase -- Creating Logical Query Plans For Unprocessed Data From Sources and MicroBatchReaders \u00b6 In getBatch time-tracking section , runBatch goes over the available offsets and processes every < > and < > (associated with the available offsets) to create logical query plans ( newData ) for data processing (per offset ranges). NOTE: runBatch requests sources and readers for data per offset range sequentially, one by one. .StreamExecution's Running Single Streaming Batch (getBatch Phase) image::images/StreamExecution-runBatch-getBatch.png[align=\"center\"] getBatch Phase and Sources \u00b6 For a Source (with the available < > different from the committedOffsets registry), runBatch does the following: Requests the committedOffsets for the committed offsets for the Source (if available) Requests the Source for a dataframe for the offset range (the current and available offsets) runBatch prints out the following DEBUG message to the logs. Retrieving data from [source]: [current] -> [available] In the end, runBatch returns the Source and the logical plan of the streaming dataset (for the offset range). In case the Source returns a dataframe that is not streaming, runBatch throws an AssertionError : DataFrame returned by getBatch from [source] did not have isStreaming=true\\n[logicalQueryPlan] ==== [[runBatch-getBatch-MicroBatchReader]] getBatch Phase and MicroBatchReaders For a < > (with the available < > different from the committedOffsets registry), runBatch does the following: Requests the committedOffsets for the committed offsets for the MicroBatchReader (if available) Requests the MicroBatchReader to < > (if available) Requests the MicroBatchReader to < > (only for < >) Requests the MicroBatchReader to < > (the current and available offsets) runBatch prints out the following DEBUG message to the logs. Retrieving data from [reader]: [current] -> [availableV2] runBatch looks up the DataSourceV2 and the options for the MicroBatchReader (in the < > internal registry). In the end, runBatch requests the MicroBatchReader for the < > and creates a StreamingDataSourceV2Relation logical operator (with the read schema, the DataSourceV2 , options, and the MicroBatchReader ). ==== [[runBatch-newBatchesPlan]] Transforming Logical Plan to Include Sources and MicroBatchReaders with New Data .StreamExecution's Running Single Streaming Batch (and Transforming Logical Plan for New Data) image::images/StreamExecution-runBatch-newBatchesPlan.png[align=\"center\"] runBatch transforms the < > to include < > ( newBatchesPlan with logical plans to process data that has arrived since the last batch). For every < > (with a < >), runBatch tries to find the corresponding logical plan for processing new data. NOTE: < > logical operator is used to represent a streaming source or reader in the < > (of a streaming query). If the logical plan is found, runBatch makes the plan a child operator of Project (with Aliases ) logical operator and replaces the StreamingExecutionRelation . Otherwise, if not found, runBatch simply creates an empty streaming LocalRelation (for scanning data from an empty local collection). In case the number of columns in dataframes with new data and StreamingExecutionRelation 's do not match, runBatch throws an AssertionError : Invalid batch: [output] != [dataPlan.output] ==== [[runBatch-newAttributePlan]] Transforming CurrentTimestamp and CurrentDate Expressions (Per Batch Metadata) runBatch replaces all CurrentTimestamp and CurrentDate expressions in the < > with the < > (based on the batch metadata ). Note CurrentTimestamp and CurrentDate expressions correspond to current_timestamp and current_date standard function, respectively. ==== [[runBatch-triggerLogicalPlan]] Adapting Transformed Logical Plan to Sink with StreamWriteSupport runBatch adapts the < > for the new < > sinks (per the type of the < >). For a < > (Data Source API V2), runBatch requests the StreamWriteSupport for a < > (for the runId , the output schema, the < >, and the < >). runBatch then creates a WriteToDataSourceV2 logical operator with a new < > as a child operator (for the current batch ID and the < >). For a < > (Data Source API V1), runBatch changes nothing. For any other < > type, runBatch simply throws an IllegalArgumentException : unknown sink type for [sink] ==== [[runBatch-setLocalProperty]] Setting Local Properties runBatch sets the < >. [[runBatch-setLocalProperty-local-properties]] .runBatch's Local Properties [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Local Property | Value | < > a| currentBatchId | __is_continuous_processing a| false |=== ==== [[runBatch-queryPlanning]] queryPlanning Phase -- Creating and Preparing IncrementalExecution for Execution .StreamExecution's Query Planning (queryPlanning Phase) image::images/StreamExecution-runBatch-queryPlanning.png[align=\"center\"] In queryPlanning time-tracking section , runBatch creates a new IncrementalExecution with the following: < > < > state < > Run ID Batch ID Batch Metadata (Event-Time Watermark and Timestamp) In the end (of the queryPlanning phase), runBatch requests the IncrementalExecution to prepare the transformed logical plan for execution (i.e. execute the executedPlan query execution phase). TIP: Read up on the executedPlan query execution phase in https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-QueryExecution.html[The Internals of Spark SQL]. ==== [[runBatch-nextBatch]] nextBatch Phase -- Creating DataFrame (with IncrementalExecution for New Data) .StreamExecution Creates DataFrame with New Data image::images/StreamExecution-runBatch-nextBatch.png[align=\"center\"] runBatch creates a new DataFrame with the new < >. The DataFrame represents the result of executing the current micro-batch of the streaming query. ==== [[runBatch-addBatch]] addBatch Phase -- Adding DataFrame With New Data to Sink .StreamExecution Adds DataFrame With New Data to Sink image::images/StreamExecution-runBatch-addBatch.png[align=\"center\"] In addBatch time-tracking section , runBatch adds the DataFrame with new data to the < >. For a < > (Data Source API V1), runBatch simply requests the Sink to < > (with the batch ID ). For a < > (Data Source API V2), runBatch simply requests the DataFrame with new data to collect (which simply forces execution of the < >). NOTE: runBatch uses SQLExecution.withNewExecutionId to execute and track all the Spark jobs under one execution id (so it is reported as one single multi-job execution, e.g. in web UI). NOTE: SQLExecution.withNewExecutionId posts a SparkListenerSQLExecutionStart event before execution and a SparkListenerSQLExecutionEnd event right afterwards. Tip Register SparkListener to get notified about the SQL execution events ( SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd ). ==== [[runBatch-updateWatermark-commitLog]] Updating Watermark and Committing Offsets to Offset Commit Log runBatch requests the < > to < > (with the executedPlan of the < >). runBatch requests the Offset Commit Log to < > (with the current batch ID and < > of the < >). In the end, runBatch < > the available offsets to the committed offsets (and updates the < > of every < > with new data in the current micro-batch). Stopping Stream Processing (Execution of Streaming Query) \u00b6 stop () : Unit stop sets the state to TERMINATED . When the stream execution thread is alive, stop requests the current SparkContext to cancelJobGroup identified by the runId and waits for this thread to die. Just to make sure that there are no more streaming jobs, stop requests the current SparkContext to cancelJobGroup identified by the runId again. In the end, stop prints out the following INFO message to the logs: Query [prettyIdString] was stopped stop is part of the StreamingQuery abstraction. === [[isNewDataAvailable]] Checking Whether New Data Is Available (Based on Available and Committed Offsets) -- isNewDataAvailable Internal Method [source, scala] \u00b6 isNewDataAvailable: Boolean \u00b6 isNewDataAvailable checks whether there is a streaming source (in the < >) for which < > are different from the available offsets or not available (committed) at all. isNewDataAvailable is positive ( true ) when there is at least one such streaming source. NOTE: isNewDataAvailable is used when MicroBatchExecution is requested to < > and < >. === [[logicalPlan]] Analyzed Logical Plan With Unique StreamingExecutionRelation Operators -- logicalPlan Lazy Property [source, scala] \u00b6 logicalPlan: LogicalPlan \u00b6 logicalPlan is part of the StreamExecution abstraction. logicalPlan resolves ( replaces ) < >, < > logical operators to < > logical operators. logicalPlan uses the transformed logical plan to set the uniqueSources and < > internal registries to be the < > of all the StreamingExecutionRelations unique and not, respectively. NOTE: logicalPlan is a Scala lazy value and so the initialization is guaranteed to happen only once at the first access (and is cached for later use afterwards). Internally, logicalPlan transforms the < >. For every < > logical operator, logicalPlan tries to replace it with the < > that was used earlier for the same StreamingRelation (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the DataSource to < > with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using Source [source] from DataSourceV1 named '[sourceName]' [dataSourceV1] For every < > logical operator with a < > data source (which is not on the list of < >), logicalPlan tries to replace it with the < > that was used earlier for the same StreamingRelationV2 (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the MicroBatchReadSupport to < > with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using MicroBatchReader [reader] from DataSourceV2 named '[sourceName]' [dataSourceV2] For every other < > logical operator, logicalPlan tries to replace it with the < > that was used earlier for the same StreamingRelationV2 (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the StreamingRelation for the underlying < > that is in turn requested to < > with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using Source [source] from DataSourceV2 named '[sourceName]' [dataSourceV2] logicalPlan requests the transformed analyzed logical plan for all StreamingExecutionRelations that are then requested for < >, and saves them as the < > internal registry. In the end, logicalPlan sets the uniqueSources internal registry to be the unique BaseStreamingSources above. logicalPlan throws an AssertionError when not executed on the stream execution thread . logicalPlan must be initialized in QueryExecutionThread but the current thread was [currentThread] === [[BATCH_ID_KEY]][[streaming.sql.batchId]] streaming.sql.batchId Local Property MicroBatchExecution defines streaming.sql.batchId as the name of the local property to be the current batch or epoch IDs (that Spark tasks can use) streaming.sql.batchId is used when: MicroBatchExecution is requested to < > (and sets the property to be the current batch ID) DataWritingSparkTask is requested to run (and needs an epoch ID) === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | isCurrentBatchConstructed a| [[isCurrentBatchConstructed]] Flag to control whether to < > ( true ) or not ( false ) Default: false When disabled ( false ), changed to whatever < > gives back when < > Disabled ( false ) after < > (when enabled after < >) Enabled ( true ) when < > (when < >) and < > (using the Offset Write-Ahead Log ) Disabled ( false ) when < > (when < >) and < > when the latest offset checkpointed (written) to the offset write-ahead log has been successfully processed and < > to the Offset Commit Log | readerToDataSourceMap a| [[readerToDataSourceMap]] ( Map[MicroBatchReader, (DataSourceV2, Map[String, String])] ) | sources a| [[sources]] < > (of the < > of the < > of the streaming query) Default: (empty) sources is part of the ProgressReporter abstraction. Initialized when MicroBatchExecution is requested for the < > Used when: < > (for the available and committed offsets) < > (and persisting offsets to write-ahead log) | watermarkTracker a| [[watermarkTracker]] < > that is created when MicroBatchExecution is requested to < > (when requested to < >) |===","title":"MicroBatchExecution"},{"location":"MicroBatchExecution/#microbatchexecution-stream-execution-engine-of-micro-batch-stream-processing","text":"MicroBatchExecution is the stream execution engine in Micro-Batch Stream Processing . MicroBatchExecution is < > when StreamingQueryManager is requested to create a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ) with the following: Any type of < > but < > Any type of < > but < > import org.apache.spark.sql.streaming.Trigger val query = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") // <-- not a StreamWriteSupport sink .option(\"truncate\", false) .trigger(Trigger.Once) // <-- Gives MicroBatchExecution .queryName(\"rate2console\") .start // The following gives access to the internals // And to MicroBatchExecution import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = query.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) import org.apache.spark.sql.execution.streaming.MicroBatchExecution val microBatchEngine = engine.asInstanceOf[MicroBatchExecution] assert(microBatchEngine.trigger == Trigger.Once) Once < >, MicroBatchExecution (as a stream execution engine ) is requested to < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.MicroBatchExecution to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MicroBatchExecution=ALL","title":"MicroBatchExecution &mdash; Stream Execution Engine of Micro-Batch Stream Processing"},{"location":"MicroBatchExecution/#refer-to","text":"","title":"Refer to &lt;&gt;."},{"location":"MicroBatchExecution/#creating-instance","text":"MicroBatchExecution takes the following to be created: [[sparkSession]] SparkSession [[name]] Name of the streaming query [[checkpointRoot]] Path of the checkpoint directory [[analyzedPlan]] Analyzed logical query plan of the streaming query ( LogicalPlan ) [[sink]] < > [[trigger]] < > [[triggerClock]] Trigger clock ( Clock ) [[outputMode]] < > [[extraOptions]] Extra options ( Map[String, String] ) [[deleteCheckpointOnStop]] deleteCheckpointOnStop flag to control whether to delete the checkpoint directory on stop MicroBatchExecution initializes the < >. === [[triggerExecutor]] MicroBatchExecution and TriggerExecutor -- triggerExecutor Property","title":"Creating Instance"},{"location":"MicroBatchExecution/#source-scala","text":"","title":"[source, scala]"},{"location":"MicroBatchExecution/#triggerexecutor-triggerexecutor","text":"triggerExecutor is the < > of the streaming query that is how micro-batches are executed at regular intervals. triggerExecutor is initialized based on the given < > (that was used to create the MicroBatchExecution ): < > for < > < > for < > (aka < > trigger) triggerExecutor throws an IllegalStateException when the < > is not one of the < >. Unknown type of trigger: [trigger] NOTE: triggerExecutor is used exclusively when StreamExecution is requested to < > (at regular intervals). === [[runActivatedStream]] Running Activated Streaming Query -- runActivatedStream Method","title":"triggerExecutor: TriggerExecutor"},{"location":"MicroBatchExecution/#source-scala_1","text":"runActivatedStream( sparkSessionForStream: SparkSession): Unit runActivatedStream simply requests the < > to execute micro-batches using the < > (until MicroBatchExecution is terminated due to a query stop or a failure). runActivatedStream is part of StreamExecution abstraction. ==== [[batchRunner]][[batch-runner]] TriggerExecutor's Batch Runner The batch runner (of the < >) is executed as long as the MicroBatchExecution is active . NOTE: trigger and batch are considered equivalent and used interchangeably. [[runActivatedStream-startTrigger]] The batch runner initializes query progress for the new trigger (aka startTrigger ). [[runActivatedStream-triggerExecution]][[runActivatedStream-triggerExecution-populateStartOffsets]] The batch runner starts triggerExecution execution phase that is made up of the following steps: . < > before the first \"zero\" batch (at every start or restart) . < > . < > At the start or restart ( resume ) of a streaming query (when the < > is uninitialized and -1 ), the batch runner < > and then prints out the following INFO message to the logs (using the committedOffsets internal registry): Stream started from [committedOffsets] The batch runner sets the human-readable description for any Spark job submitted (that streaming sources may submit to get new data) as the batch description . [[runActivatedStream-triggerExecution-isCurrentBatchConstructed]] The batch runner < > (when the < > internal flag is off). The batch runner < > (with the committed and available offsets). The batch runner updates the current StreamingQueryStatus with the < > for isDataAvailable property. [[runActivatedStream-triggerExecution-runBatch]] With the < > flag enabled ( true ), the batch runner updates the status message to one of the following (per < >) and < >. Processing new data No new data but cleaning up state With the < > flag disabled ( false ), the batch runner simply updates the status message to the following: Waiting for data to arrive [[runActivatedStream-triggerExecution-finishTrigger]] The batch runner finalizes query progress for the trigger (with a flag that indicates whether the current batch had new data). With the < > flag enabled ( true ), the batch runner increments the < > and turns the < > flag off ( false ). With the < > flag disabled ( false ), the batch runner simply sleeps (as long as configured using the spark.sql.streaming.pollingDelay configuration property). In the end, the batch runner updates the status message to the following status and returns whether the MicroBatchExecution is active or not. Waiting for next trigger === [[populateStartOffsets]] Populating Start Offsets From Checkpoint (Resuming from Checkpoint) -- populateStartOffsets Internal Method","title":"[source, scala]"},{"location":"MicroBatchExecution/#source-scala_2","text":"populateStartOffsets( sparkSessionToRunBatches: SparkSession): Unit populateStartOffsets requests the Offset Write-Ahead Log for the < > (i.e. < >). NOTE: The batch id could not be available in the write-ahead log when a streaming query started with a new log or no batch was persisted ( added ) to the log before. populateStartOffsets branches off based on whether the latest committed batch was < > or < >. NOTE: populateStartOffsets is used exclusively when MicroBatchExecution is requested to < > ( before the first \"zero\" micro-batch ). ==== [[populateStartOffsets-getLatest-available]] Latest Committed Batch Available When the latest committed batch id with the metadata was available in the Offset Write-Ahead Log , populateStartOffsets (re)initializes the internal state as follows: Sets the current batch ID to the latest committed batch ID found Turns the < > internal flag on ( true ) Sets the < > to the offsets (from the metadata) When the latest batch ID found is greater than 0 , populateStartOffsets requests the Offset Write-Ahead Log for the < > or throws an IllegalStateException if not found. batch [latestBatchId - 1] doesn't exist populateStartOffsets sets the < > to the second latest committed offsets. [[populateStartOffsets-getLatest-available-offsetSeqMetadata]] populateStartOffsets updates the offset metadata. CAUTION: FIXME Describe me populateStartOffsets requests the Offset Commit Log for the < > (i.e. < >). CAUTION: FIXME Describe me When the latest committed batch id with metadata was found which is exactly the latest batch ID (found in the Offset Commit Log ), populateStartOffsets ...FIXME When the latest committed batch id with metadata was found, but it is not exactly the second latest batch ID (found in the Offset Commit Log ), populateStartOffsets prints out the following WARN message to the logs:","title":"[source, scala]"},{"location":"MicroBatchExecution/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"MicroBatchExecution/#batch-completion-log-latest-batch-id-is-latestcommittedbatchid-which-is-not-trailing-batchid-latestbatchid-by-one","text":"When no commit log present in the Offset Commit Log , populateStartOffsets prints out the following INFO message to the logs: no commit log present In the end, populateStartOffsets prints out the following DEBUG message to the logs: Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets] ==== [[populateStartOffsets-getLatest-not-available]] No Latest Committed Batch When the latest committed batch id with the metadata could not be found in the Offset Write-Ahead Log , it is assumed that the streaming query is started for the very first time (or the checkpoint location has changed). populateStartOffsets prints out the following INFO message to the logs: Starting new streaming query. [[populateStartOffsets-currentBatchId-0]] populateStartOffsets sets the current batch ID to 0 and creates a new < >. === [[constructNextBatch]] Constructing Or Skipping Next Streaming Micro-Batch -- constructNextBatch Internal Method","title":"Batch completion log latest batch id is [latestCommittedBatchId], which is not trailing batchid [latestBatchId] by one"},{"location":"MicroBatchExecution/#source-scala_3","text":"constructNextBatch( noDataBatchesEnabled: Boolean): Boolean NOTE: constructNextBatch will only be executed when the < > internal flag is enabled ( true ). constructNextBatch performs the following steps: . < > (of the streaming query) . < > . < > . < > In the end, constructNextBatch returns < >. NOTE: constructNextBatch is used exclusively when MicroBatchExecution is requested to < >. ==== [[constructNextBatch-latestOffsets]] Requesting Latest Offsets from Streaming Sources (getOffset, setOffsetRange and getEndOffset Phases) constructNextBatch firstly requests every streaming source for the latest offsets. NOTE: constructNextBatch checks out the latest offset in every streaming data source sequentially, i.e. one data source at a time. .MicroBatchExecution's Getting Offsets From Streaming Sources image::images/MicroBatchExecution-constructNextBatch.png[align=\"center\"] For every streaming source (Data Source API V1), constructNextBatch updates the status message to the following: Getting offsets from [source] [[constructNextBatch-getOffset]] In getOffset time-tracking section , constructNextBatch requests the Source for the < >. For every < > (Data Source API V2), constructNextBatch updates the status message to the following: Getting offsets from [source] [[constructNextBatch-setOffsetRange]] In setOffsetRange time-tracking section , constructNextBatch finds the available offsets of the source (in the < > internal registry) and, if found, requests the MicroBatchReader to < > (from < >). constructNextBatch requests the MicroBatchReader to < >. [[constructNextBatch-getEndOffset]] In getEndOffset time-tracking section , constructNextBatch requests the MicroBatchReader for the < >.","title":"[source, scala]"},{"location":"MicroBatchExecution/#updating-availableoffsets-streamprogress-with-latest-available-offsets","text":"constructNextBatch updates the availableOffsets StreamProgress with the latest reported offsets.","title":" Updating availableOffsets StreamProgress with Latest Available Offsets"},{"location":"MicroBatchExecution/#updating-batch-metadata-with-current-event-time-watermark-and-batch-timestamp","text":"constructNextBatch updates the batch metadata with the current < > (from the < >) and the batch timestamp. ==== [[constructNextBatch-shouldConstructNextBatch]] Checking Whether to Construct Next Micro-Batch or Not (Skip It) constructNextBatch checks whether or not the next streaming micro-batch should be constructed ( lastExecutionRequiresAnotherBatch ). constructNextBatch uses the last IncrementalExecution if the < > (using the batch metadata ) and the given noDataBatchesEnabled flag is enabled ( true ). constructNextBatch also < >. NOTE: shouldConstructNextBatch local flag is enabled ( true ) when < > or the < > (and the given noDataBatchesEnabled flag is enabled). constructNextBatch prints out the following TRACE message to the logs: noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch] constructNextBatch branches off per whether to < > or < > the next batch (per shouldConstructNextBatch flag in the above TRACE message). ==== [[constructNextBatch-shouldConstructNextBatch-enabled]] Constructing Next Micro-Batch -- shouldConstructNextBatch Flag Enabled With the < > flag enabled ( true ), constructNextBatch updates the status message to the following: Writing offsets to log [[constructNextBatch-walCommit]] In walCommit time-tracking section , constructNextBatch requests the availableOffsets StreamProgress to < > (with the < > and the current batch metadata (event-time watermark and timestamp) ) that is in turn added to the write-ahead log for the current batch ID . constructNextBatch prints out the following INFO message to the logs: Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata] NOTE: FIXME ( if (currentBatchId != 0) ... ) NOTE: FIXME ( if (minLogEntriesToMaintain < currentBatchId) ... ) constructNextBatch turns the noNewData internal flag off ( false ). In case of a failure while < > to the write-ahead log , constructNextBatch throws an AssertionError : Concurrent update to the log. Multiple streaming jobs detected for [currentBatchId]","title":" Updating Batch Metadata with Current Event-Time Watermark and Batch Timestamp"},{"location":"MicroBatchExecution/#skipping-next-micro-batch-shouldconstructnextbatch-flag-disabled","text":"With the < > flag disabled ( false ), constructNextBatch turns the noNewData flag on ( true ) and wakes up ( notifies ) all threads waiting for the awaitProgressLockCondition lock.","title":" Skipping Next Micro-Batch -- shouldConstructNextBatch Flag Disabled"},{"location":"MicroBatchExecution/#running-single-streaming-micro-batch","text":"runBatch ( sparkSessionToRunBatch : SparkSession ) : Unit runBatch prints out the following DEBUG message to the logs (with the current batch ID ): Running batch [currentBatchId] runBatch then performs the following steps (aka phases ): . < > . < > . < > . < > . < > . < > . < > . < > . < > In the end, runBatch prints out the following DEBUG message to the logs (with the current batch ID ): Completed batch [currentBatchId] NOTE: runBatch is used exclusively when MicroBatchExecution is requested to < > (and there is new data to process).","title":" Running Single Streaming Micro-Batch"},{"location":"MicroBatchExecution/#getbatch-phase-creating-logical-query-plans-for-unprocessed-data-from-sources-and-microbatchreaders","text":"In getBatch time-tracking section , runBatch goes over the available offsets and processes every < > and < > (associated with the available offsets) to create logical query plans ( newData ) for data processing (per offset ranges). NOTE: runBatch requests sources and readers for data per offset range sequentially, one by one. .StreamExecution's Running Single Streaming Batch (getBatch Phase) image::images/StreamExecution-runBatch-getBatch.png[align=\"center\"]","title":" getBatch Phase -- Creating Logical Query Plans For Unprocessed Data From Sources and MicroBatchReaders"},{"location":"MicroBatchExecution/#getbatch-phase-and-sources","text":"For a Source (with the available < > different from the committedOffsets registry), runBatch does the following: Requests the committedOffsets for the committed offsets for the Source (if available) Requests the Source for a dataframe for the offset range (the current and available offsets) runBatch prints out the following DEBUG message to the logs. Retrieving data from [source]: [current] -> [available] In the end, runBatch returns the Source and the logical plan of the streaming dataset (for the offset range). In case the Source returns a dataframe that is not streaming, runBatch throws an AssertionError : DataFrame returned by getBatch from [source] did not have isStreaming=true\\n[logicalQueryPlan] ==== [[runBatch-getBatch-MicroBatchReader]] getBatch Phase and MicroBatchReaders For a < > (with the available < > different from the committedOffsets registry), runBatch does the following: Requests the committedOffsets for the committed offsets for the MicroBatchReader (if available) Requests the MicroBatchReader to < > (if available) Requests the MicroBatchReader to < > (only for < >) Requests the MicroBatchReader to < > (the current and available offsets) runBatch prints out the following DEBUG message to the logs. Retrieving data from [reader]: [current] -> [availableV2] runBatch looks up the DataSourceV2 and the options for the MicroBatchReader (in the < > internal registry). In the end, runBatch requests the MicroBatchReader for the < > and creates a StreamingDataSourceV2Relation logical operator (with the read schema, the DataSourceV2 , options, and the MicroBatchReader ). ==== [[runBatch-newBatchesPlan]] Transforming Logical Plan to Include Sources and MicroBatchReaders with New Data .StreamExecution's Running Single Streaming Batch (and Transforming Logical Plan for New Data) image::images/StreamExecution-runBatch-newBatchesPlan.png[align=\"center\"] runBatch transforms the < > to include < > ( newBatchesPlan with logical plans to process data that has arrived since the last batch). For every < > (with a < >), runBatch tries to find the corresponding logical plan for processing new data. NOTE: < > logical operator is used to represent a streaming source or reader in the < > (of a streaming query). If the logical plan is found, runBatch makes the plan a child operator of Project (with Aliases ) logical operator and replaces the StreamingExecutionRelation . Otherwise, if not found, runBatch simply creates an empty streaming LocalRelation (for scanning data from an empty local collection). In case the number of columns in dataframes with new data and StreamingExecutionRelation 's do not match, runBatch throws an AssertionError : Invalid batch: [output] != [dataPlan.output] ==== [[runBatch-newAttributePlan]] Transforming CurrentTimestamp and CurrentDate Expressions (Per Batch Metadata) runBatch replaces all CurrentTimestamp and CurrentDate expressions in the < > with the < > (based on the batch metadata ). Note CurrentTimestamp and CurrentDate expressions correspond to current_timestamp and current_date standard function, respectively. ==== [[runBatch-triggerLogicalPlan]] Adapting Transformed Logical Plan to Sink with StreamWriteSupport runBatch adapts the < > for the new < > sinks (per the type of the < >). For a < > (Data Source API V2), runBatch requests the StreamWriteSupport for a < > (for the runId , the output schema, the < >, and the < >). runBatch then creates a WriteToDataSourceV2 logical operator with a new < > as a child operator (for the current batch ID and the < >). For a < > (Data Source API V1), runBatch changes nothing. For any other < > type, runBatch simply throws an IllegalArgumentException : unknown sink type for [sink] ==== [[runBatch-setLocalProperty]] Setting Local Properties runBatch sets the < >. [[runBatch-setLocalProperty-local-properties]] .runBatch's Local Properties [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Local Property | Value | < > a| currentBatchId | __is_continuous_processing a| false |=== ==== [[runBatch-queryPlanning]] queryPlanning Phase -- Creating and Preparing IncrementalExecution for Execution .StreamExecution's Query Planning (queryPlanning Phase) image::images/StreamExecution-runBatch-queryPlanning.png[align=\"center\"] In queryPlanning time-tracking section , runBatch creates a new IncrementalExecution with the following: < > < > state < > Run ID Batch ID Batch Metadata (Event-Time Watermark and Timestamp) In the end (of the queryPlanning phase), runBatch requests the IncrementalExecution to prepare the transformed logical plan for execution (i.e. execute the executedPlan query execution phase). TIP: Read up on the executedPlan query execution phase in https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-QueryExecution.html[The Internals of Spark SQL]. ==== [[runBatch-nextBatch]] nextBatch Phase -- Creating DataFrame (with IncrementalExecution for New Data) .StreamExecution Creates DataFrame with New Data image::images/StreamExecution-runBatch-nextBatch.png[align=\"center\"] runBatch creates a new DataFrame with the new < >. The DataFrame represents the result of executing the current micro-batch of the streaming query. ==== [[runBatch-addBatch]] addBatch Phase -- Adding DataFrame With New Data to Sink .StreamExecution Adds DataFrame With New Data to Sink image::images/StreamExecution-runBatch-addBatch.png[align=\"center\"] In addBatch time-tracking section , runBatch adds the DataFrame with new data to the < >. For a < > (Data Source API V1), runBatch simply requests the Sink to < > (with the batch ID ). For a < > (Data Source API V2), runBatch simply requests the DataFrame with new data to collect (which simply forces execution of the < >). NOTE: runBatch uses SQLExecution.withNewExecutionId to execute and track all the Spark jobs under one execution id (so it is reported as one single multi-job execution, e.g. in web UI). NOTE: SQLExecution.withNewExecutionId posts a SparkListenerSQLExecutionStart event before execution and a SparkListenerSQLExecutionEnd event right afterwards. Tip Register SparkListener to get notified about the SQL execution events ( SparkListenerSQLExecutionStart and SparkListenerSQLExecutionEnd ). ==== [[runBatch-updateWatermark-commitLog]] Updating Watermark and Committing Offsets to Offset Commit Log runBatch requests the < > to < > (with the executedPlan of the < >). runBatch requests the Offset Commit Log to < > (with the current batch ID and < > of the < >). In the end, runBatch < > the available offsets to the committed offsets (and updates the < > of every < > with new data in the current micro-batch).","title":" getBatch Phase and Sources"},{"location":"MicroBatchExecution/#stopping-stream-processing-execution-of-streaming-query","text":"stop () : Unit stop sets the state to TERMINATED . When the stream execution thread is alive, stop requests the current SparkContext to cancelJobGroup identified by the runId and waits for this thread to die. Just to make sure that there are no more streaming jobs, stop requests the current SparkContext to cancelJobGroup identified by the runId again. In the end, stop prints out the following INFO message to the logs: Query [prettyIdString] was stopped stop is part of the StreamingQuery abstraction. === [[isNewDataAvailable]] Checking Whether New Data Is Available (Based on Available and Committed Offsets) -- isNewDataAvailable Internal Method","title":" Stopping Stream Processing (Execution of Streaming Query)"},{"location":"MicroBatchExecution/#source-scala_4","text":"","title":"[source, scala]"},{"location":"MicroBatchExecution/#isnewdataavailable-boolean","text":"isNewDataAvailable checks whether there is a streaming source (in the < >) for which < > are different from the available offsets or not available (committed) at all. isNewDataAvailable is positive ( true ) when there is at least one such streaming source. NOTE: isNewDataAvailable is used when MicroBatchExecution is requested to < > and < >. === [[logicalPlan]] Analyzed Logical Plan With Unique StreamingExecutionRelation Operators -- logicalPlan Lazy Property","title":"isNewDataAvailable: Boolean"},{"location":"MicroBatchExecution/#source-scala_5","text":"","title":"[source, scala]"},{"location":"MicroBatchExecution/#logicalplan-logicalplan","text":"logicalPlan is part of the StreamExecution abstraction. logicalPlan resolves ( replaces ) < >, < > logical operators to < > logical operators. logicalPlan uses the transformed logical plan to set the uniqueSources and < > internal registries to be the < > of all the StreamingExecutionRelations unique and not, respectively. NOTE: logicalPlan is a Scala lazy value and so the initialization is guaranteed to happen only once at the first access (and is cached for later use afterwards). Internally, logicalPlan transforms the < >. For every < > logical operator, logicalPlan tries to replace it with the < > that was used earlier for the same StreamingRelation (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the DataSource to < > with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using Source [source] from DataSourceV1 named '[sourceName]' [dataSourceV1] For every < > logical operator with a < > data source (which is not on the list of < >), logicalPlan tries to replace it with the < > that was used earlier for the same StreamingRelationV2 (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the MicroBatchReadSupport to < > with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using MicroBatchReader [reader] from DataSourceV2 named '[sourceName]' [dataSourceV2] For every other < > logical operator, logicalPlan tries to replace it with the < > that was used earlier for the same StreamingRelationV2 (if used multiple times in the plan) or creates a new one. While creating a new StreamingExecutionRelation , logicalPlan requests the StreamingRelation for the underlying < > that is in turn requested to < > with the metadata path as sources/uniqueID directory in the checkpoint root directory . logicalPlan prints out the following INFO message to the logs: Using Source [source] from DataSourceV2 named '[sourceName]' [dataSourceV2] logicalPlan requests the transformed analyzed logical plan for all StreamingExecutionRelations that are then requested for < >, and saves them as the < > internal registry. In the end, logicalPlan sets the uniqueSources internal registry to be the unique BaseStreamingSources above. logicalPlan throws an AssertionError when not executed on the stream execution thread . logicalPlan must be initialized in QueryExecutionThread but the current thread was [currentThread] === [[BATCH_ID_KEY]][[streaming.sql.batchId]] streaming.sql.batchId Local Property MicroBatchExecution defines streaming.sql.batchId as the name of the local property to be the current batch or epoch IDs (that Spark tasks can use) streaming.sql.batchId is used when: MicroBatchExecution is requested to < > (and sets the property to be the current batch ID) DataWritingSparkTask is requested to run (and needs an epoch ID) === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | isCurrentBatchConstructed a| [[isCurrentBatchConstructed]] Flag to control whether to < > ( true ) or not ( false ) Default: false When disabled ( false ), changed to whatever < > gives back when < > Disabled ( false ) after < > (when enabled after < >) Enabled ( true ) when < > (when < >) and < > (using the Offset Write-Ahead Log ) Disabled ( false ) when < > (when < >) and < > when the latest offset checkpointed (written) to the offset write-ahead log has been successfully processed and < > to the Offset Commit Log | readerToDataSourceMap a| [[readerToDataSourceMap]] ( Map[MicroBatchReader, (DataSourceV2, Map[String, String])] ) | sources a| [[sources]] < > (of the < > of the < > of the streaming query) Default: (empty) sources is part of the ProgressReporter abstraction. Initialized when MicroBatchExecution is requested for the < > Used when: < > (for the available and committed offsets) < > (and persisting offsets to write-ahead log) | watermarkTracker a| [[watermarkTracker]] < > that is created when MicroBatchExecution is requested to < > (when requested to < >) |===","title":"logicalPlan: LogicalPlan"},{"location":"SQLConf/","text":"SQLConf \u2014 Internal Configuration Store \u00b6 SQLConf is an internal configuration store for parameters and hints used to configure a Spark Structured Streaming application (and Spark SQL applications in general). Tip Find out more on SQLConf in The Internals of Spark SQL streamingMetricsEnabled \u00b6 spark.sql.streaming.metricsEnabled configuration property Used when StreamExecution is requested to runStream [[accessor-methods]] .SQLConf's Property Accessor Methods [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | Method Name / Property | Description | continuousStreamingExecutorQueueSize < > a| [[continuousStreamingExecutorQueueSize]] Used when: DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < >) ContinuousCoalesceExec unary physical operator is requested to execute | continuousStreamingExecutorPollIntervalMs < > a| [[continuousStreamingExecutorPollIntervalMs]] Used exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < >) | disabledV2StreamingMicroBatchReaders < > a| [[disabledV2StreamingMicroBatchReaders]] Used exclusively when MicroBatchExecution is requested for the < > (of a streaming query) | fileSourceLogDeletion < > a| [[fileSourceLogDeletion]][[FILE_SOURCE_LOG_DELETION]] Used exclusively when FileStreamSourceLog is requested for the < > | fileSourceLogCleanupDelay < > a| [[fileSourceLogCleanupDelay]][[FILE_SOURCE_LOG_CLEANUP_DELAY]] Used exclusively when FileStreamSourceLog is requested for the < > | fileSourceLogCompactInterval < > a| [[fileSourceLogCompactInterval]][[FILE_SOURCE_LOG_COMPACT_INTERVAL]] Used exclusively when FileStreamSourceLog is requested for the < > | FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION < > a| [[FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION]] Used when: < > execution planning strategy is requested to plan a streaming query (and creates a FlatMapGroupsWithStateExec physical operator for every FlatMapGroupsWithState logical operator) Among the < > | minBatchesToRetain < > a| [[minBatchesToRetain]] Used when: CompactibleFileStreamLog is < > StreamExecution is created StateStoreConf is < > | SHUFFLE_PARTITIONS spark.sql.shuffle.partitions a| [[SHUFFLE_PARTITIONS]] See https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-properties.html#spark.sql.shuffle.partitions[spark.sql.shuffle.partitions ] in The Internals of Spark SQL. | stateStoreMinDeltasForSnapshot < > a| [[stateStoreMinDeltasForSnapshot]] Used (as < >) exclusively when HDFSBackedStateStoreProvider is requested to < > | stateStoreProviderClass < > a| [[stateStoreProviderClass]] Used when: StateStoreWriter is requested to stateStoreCustomMetrics (when StateStoreWriter is requested for the metrics and getProgress ) StateStoreConf is < > | STREAMING_AGGREGATION_STATE_FORMAT_VERSION < > a| [[STREAMING_AGGREGATION_STATE_FORMAT_VERSION]] Used when: < > execution planning strategy is executed OffsetSeqMetadata is requested for the < > and the < > | STREAMING_CHECKPOINT_FILE_MANAGER_CLASS < > a| [[STREAMING_CHECKPOINT_FILE_MANAGER_CLASS]] Used exclusively when CheckpointFileManager helper object is requested to < > | streamingMetricsEnabled < > a| [[streamingMetricsEnabled]] Used exclusively when StreamExecution is requested for runStream (to control whether to register a metrics reporter for a streaming query) | STREAMING_MULTIPLE_WATERMARK_POLICY < > a| [[STREAMING_MULTIPLE_WATERMARK_POLICY]] | streamingNoDataMicroBatchesEnabled < > a| [[streamingNoDataMicroBatchesEnabled]][[STREAMING_NO_DATA_MICRO_BATCHES_ENABLED]] Used exclusively when MicroBatchExecution stream execution engine is requested to < > | streamingNoDataProgressEventInterval < > a| [[streamingNoDataProgressEventInterval]] Used exclusively for ProgressReporter | streamingPollingDelay < > a| [[streamingPollingDelay]][[STREAMING_POLLING_DELAY]] Used exclusively when StreamExecution is created | streamingProgressRetention < > a| [[streamingProgressRetention]][[STREAMING_PROGRESS_RETENTION]] Used exclusively when ProgressReporter is requested to update progress of streaming query (and possibly remove an excess) |===","title":"SQLConf"},{"location":"SQLConf/#sqlconf-internal-configuration-store","text":"SQLConf is an internal configuration store for parameters and hints used to configure a Spark Structured Streaming application (and Spark SQL applications in general). Tip Find out more on SQLConf in The Internals of Spark SQL","title":"SQLConf &mdash; Internal Configuration Store"},{"location":"SQLConf/#streamingmetricsenabled","text":"spark.sql.streaming.metricsEnabled configuration property Used when StreamExecution is requested to runStream [[accessor-methods]] .SQLConf's Property Accessor Methods [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | Method Name / Property | Description | continuousStreamingExecutorQueueSize < > a| [[continuousStreamingExecutorQueueSize]] Used when: DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < >) ContinuousCoalesceExec unary physical operator is requested to execute | continuousStreamingExecutorPollIntervalMs < > a| [[continuousStreamingExecutorPollIntervalMs]] Used exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < >) | disabledV2StreamingMicroBatchReaders < > a| [[disabledV2StreamingMicroBatchReaders]] Used exclusively when MicroBatchExecution is requested for the < > (of a streaming query) | fileSourceLogDeletion < > a| [[fileSourceLogDeletion]][[FILE_SOURCE_LOG_DELETION]] Used exclusively when FileStreamSourceLog is requested for the < > | fileSourceLogCleanupDelay < > a| [[fileSourceLogCleanupDelay]][[FILE_SOURCE_LOG_CLEANUP_DELAY]] Used exclusively when FileStreamSourceLog is requested for the < > | fileSourceLogCompactInterval < > a| [[fileSourceLogCompactInterval]][[FILE_SOURCE_LOG_COMPACT_INTERVAL]] Used exclusively when FileStreamSourceLog is requested for the < > | FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION < > a| [[FLATMAPGROUPSWITHSTATE_STATE_FORMAT_VERSION]] Used when: < > execution planning strategy is requested to plan a streaming query (and creates a FlatMapGroupsWithStateExec physical operator for every FlatMapGroupsWithState logical operator) Among the < > | minBatchesToRetain < > a| [[minBatchesToRetain]] Used when: CompactibleFileStreamLog is < > StreamExecution is created StateStoreConf is < > | SHUFFLE_PARTITIONS spark.sql.shuffle.partitions a| [[SHUFFLE_PARTITIONS]] See https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-properties.html#spark.sql.shuffle.partitions[spark.sql.shuffle.partitions ] in The Internals of Spark SQL. | stateStoreMinDeltasForSnapshot < > a| [[stateStoreMinDeltasForSnapshot]] Used (as < >) exclusively when HDFSBackedStateStoreProvider is requested to < > | stateStoreProviderClass < > a| [[stateStoreProviderClass]] Used when: StateStoreWriter is requested to stateStoreCustomMetrics (when StateStoreWriter is requested for the metrics and getProgress ) StateStoreConf is < > | STREAMING_AGGREGATION_STATE_FORMAT_VERSION < > a| [[STREAMING_AGGREGATION_STATE_FORMAT_VERSION]] Used when: < > execution planning strategy is executed OffsetSeqMetadata is requested for the < > and the < > | STREAMING_CHECKPOINT_FILE_MANAGER_CLASS < > a| [[STREAMING_CHECKPOINT_FILE_MANAGER_CLASS]] Used exclusively when CheckpointFileManager helper object is requested to < > | streamingMetricsEnabled < > a| [[streamingMetricsEnabled]] Used exclusively when StreamExecution is requested for runStream (to control whether to register a metrics reporter for a streaming query) | STREAMING_MULTIPLE_WATERMARK_POLICY < > a| [[STREAMING_MULTIPLE_WATERMARK_POLICY]] | streamingNoDataMicroBatchesEnabled < > a| [[streamingNoDataMicroBatchesEnabled]][[STREAMING_NO_DATA_MICRO_BATCHES_ENABLED]] Used exclusively when MicroBatchExecution stream execution engine is requested to < > | streamingNoDataProgressEventInterval < > a| [[streamingNoDataProgressEventInterval]] Used exclusively for ProgressReporter | streamingPollingDelay < > a| [[streamingPollingDelay]][[STREAMING_POLLING_DELAY]] Used exclusively when StreamExecution is created | streamingProgressRetention < > a| [[streamingProgressRetention]][[STREAMING_PROGRESS_RETENTION]] Used exclusively when ProgressReporter is requested to update progress of streaming query (and possibly remove an excess) |===","title":" streamingMetricsEnabled"},{"location":"Source/","text":"Source \u2014 Streaming Source in Micro-Batch Stream Processing \u00b6 Source is an extension of the SparkDataStream abstraction for streaming sources for \"streamed reading\" of continually arriving data for a streaming query (identified by offset ). Source is used in Micro-Batch Stream Processing . Source is created using StreamSourceProvider.createSource (and DataSource.createSource ). For fault tolerance, Source must be able to replay an arbitrary sequence of past data in a stream using a range of offsets. This is the assumption so Structured Streaming can achieve end-to-end exactly-once guarantees. Contract \u00b6 commit \u00b6 commit ( end : Offset ) : Unit Commits data up to the end offset (informs the source that Spark has completed processing all data for offsets less than or equal to the end offset and will only request offsets greater than the end offset in the future). Used when MicroBatchExecution stream execution engine is requested to write offsets to a commit log (walCommit phase) while running an activated streaming query . getBatch \u00b6 getBatch ( start : Option [ Offset ], end : Offset ) : DataFrame Generating a streaming DataFrame with data between the start and end offsets Start offset can be undefined ( None ) to indicate that the batch should begin with the first record Used when MicroBatchExecution stream execution engine is requested to run an activated streaming query , namely: Populate start offsets from checkpoint (resuming from checkpoint) Request unprocessed data from all sources (getBatch phase) getOffset \u00b6 getOffset : Option [ Offset ] Latest (maximum) < > of the source (or None to denote no data) Used when < > stream execution engine (< >) is requested for < > while < >. schema \u00b6 schema : StructType Schema of the data from this source Implementations \u00b6 FileStreamSource KafkaSource initialOffset Method \u00b6 initialOffset () : OffsetV2 initialOffset throws an IllegalStateException . initialOffset is part of the SparkDataStream abstraction. deserializeOffset Method \u00b6 deserializeOffset ( json : String ) : OffsetV2 deserializeOffset throws an IllegalStateException . deserializeOffset is part of the SparkDataStream abstraction.","title":"Source"},{"location":"Source/#source-streaming-source-in-micro-batch-stream-processing","text":"Source is an extension of the SparkDataStream abstraction for streaming sources for \"streamed reading\" of continually arriving data for a streaming query (identified by offset ). Source is used in Micro-Batch Stream Processing . Source is created using StreamSourceProvider.createSource (and DataSource.createSource ). For fault tolerance, Source must be able to replay an arbitrary sequence of past data in a stream using a range of offsets. This is the assumption so Structured Streaming can achieve end-to-end exactly-once guarantees.","title":"Source &mdash; Streaming Source in Micro-Batch Stream Processing"},{"location":"Source/#contract","text":"","title":"Contract"},{"location":"Source/#commit","text":"commit ( end : Offset ) : Unit Commits data up to the end offset (informs the source that Spark has completed processing all data for offsets less than or equal to the end offset and will only request offsets greater than the end offset in the future). Used when MicroBatchExecution stream execution engine is requested to write offsets to a commit log (walCommit phase) while running an activated streaming query .","title":" commit"},{"location":"Source/#getbatch","text":"getBatch ( start : Option [ Offset ], end : Offset ) : DataFrame Generating a streaming DataFrame with data between the start and end offsets Start offset can be undefined ( None ) to indicate that the batch should begin with the first record Used when MicroBatchExecution stream execution engine is requested to run an activated streaming query , namely: Populate start offsets from checkpoint (resuming from checkpoint) Request unprocessed data from all sources (getBatch phase)","title":" getBatch"},{"location":"Source/#getoffset","text":"getOffset : Option [ Offset ] Latest (maximum) < > of the source (or None to denote no data) Used when < > stream execution engine (< >) is requested for < > while < >.","title":" getOffset"},{"location":"Source/#schema","text":"schema : StructType Schema of the data from this source","title":" schema"},{"location":"Source/#implementations","text":"FileStreamSource KafkaSource","title":"Implementations"},{"location":"Source/#initialoffset-method","text":"initialOffset () : OffsetV2 initialOffset throws an IllegalStateException . initialOffset is part of the SparkDataStream abstraction.","title":" initialOffset Method"},{"location":"Source/#deserializeoffset-method","text":"deserializeOffset ( json : String ) : OffsetV2 deserializeOffset throws an IllegalStateException . deserializeOffset is part of the SparkDataStream abstraction.","title":" deserializeOffset Method"},{"location":"SparkDataStream/","text":"SparkDataStream \u00b6 SparkDataStream is...FIXME","title":"SparkDataStream"},{"location":"SparkDataStream/#sparkdatastream","text":"SparkDataStream is...FIXME","title":"SparkDataStream"},{"location":"StreamExecution/","text":"StreamExecution \u2014 Stream Execution Engines \u00b6 StreamExecution is the < > of < > (aka streaming query processing engines ) that can < > a < > (on a < >). NOTE: Continuous query , streaming query , continuous Dataset , streaming Dataset are all considered high-level synonyms for an executable entity that stream execution engines run using the < > internally. [[contract]] .StreamExecution Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Property | Description | logicalPlan a| [[logicalPlan]] [source, scala] \u00b6 logicalPlan: LogicalPlan \u00b6 Analyzed logical plan of the streaming query to execute Used when StreamExecution is requested to < > logicalPlan is part of the ProgressReporter abstraction. | runActivatedStream a| [[runActivatedStream]] [source, scala] \u00b6 runActivatedStream( sparkSessionForStream: SparkSession): Unit Executes ( runs ) the activated < > Used exclusively when StreamExecution is requested to < > (when transitioning from INITIALIZING to ACTIVE state) |=== .Streaming Query and Stream Execution Engine [source, scala] import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution [[minLogEntriesToMaintain]][[spark.sql.streaming.minBatchesToRetain]] StreamExecution uses the < > configuration property to allow the < > to discard old log entries (from the < > and < > logs). [[extensions]] .StreamExecutions [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StreamExecution | Description | < > | [[ContinuousExecution]] Used in < > | < > | [[MicroBatchExecution]] Used in < > |=== NOTE: StreamExecution does not support adaptive query execution and cost-based optimizer (and turns them off when requested to < >). StreamExecution is the execution environment of a StreamingQuery.md[single streaming query] (aka streaming Dataset ) that is executed every < > and in the end < >. NOTE: StreamExecution corresponds to a StreamingQuery.md[single streaming query] with one or more streaming sources and exactly one spark-sql-streaming-Sink.md[streaming sink]. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val q = spark. readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.minutes)). start scala> :type q org.apache.spark.sql.streaming.StreamingQuery // Pull out StreamExecution off StreamingQueryWrapper import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper} val se = q.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution When < >, StreamExecution starts a < > that simply < > (and hence the streaming query). .StreamExecution's Starting Streaming Query (on Execution Thread) image::images/StreamExecution-start.png[align=\"center\"] StreamExecution is a ProgressReporter and < > (i.e. when it starts, progresses and terminates) by posting StreamingQueryListener events. import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .text(\"server-logs\") .writeStream .format(\"console\") .queryName(\"debug\") .trigger(Trigger.ProcessingTime(20.seconds)) .start // Enable the log level to see the INFO and DEBUG messages // log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG 17/06/18 21:21:07 INFO StreamExecution: Starting new streaming query. 17/06/18 21:21:07 DEBUG StreamExecution: getOffset took 5 ms 17/06/18 21:21:07 DEBUG StreamExecution: Stream running from {} to {} 17/06/18 21:21:07 DEBUG StreamExecution: triggerExecution took 9 ms 17/06/18 21:21:07 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map()) 17/06/18 21:21:07 INFO StreamExecution: Streaming query made progress: { \"id\" : \"8b57b0bd-fc4a-42eb-81a3-777d7ba5e370\", \"runId\" : \"920b227e-6d02-4a03-a271-c62120258cea\", \"name\" : \"debug\", \"timestamp\" : \"2017-06-18T19:21:07.693Z\", \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"getOffset\" : 5, \"triggerExecution\" : 9 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"FileStreamSource[file:/Users/jacek/dev/oss/spark/server-logs]\", \"startOffset\" : null, \"endOffset\" : null, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@2460208a\" } } 17/06/18 21:21:10 DEBUG StreamExecution: Starting Trigger Calculation 17/06/18 21:21:10 DEBUG StreamExecution: getOffset took 3 ms 17/06/18 21:21:10 DEBUG StreamExecution: triggerExecution took 3 ms 17/06/18 21:21:10 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map()) StreamExecution tracks streaming data sources in < > internal registry. .StreamExecution's uniqueSources Registry of Streaming Data Sources image::images/StreamExecution-uniqueSources.png[align=\"center\"] StreamExecution collects durationMs for the execution units of streaming batches. .StreamExecution's durationMs image::images/StreamExecution-durationMs.png[align=\"center\"] [source, scala] \u00b6 scala> :type q org.apache.spark.sql.streaming.StreamingQuery scala> println(q.lastProgress) { \"id\" : \"03fc78fc-fe19-408c-a1ae-812d0e28fcee\", \"runId\" : \"8c247071-afba-40e5-aad2-0e6f45f22488\", \"name\" : null, \"timestamp\" : \"2017-08-14T20:30:00.004Z\", \"batchId\" : 1, \"numInputRows\" : 432, \"inputRowsPerSecond\" : 0.9993568953312452, \"processedRowsPerSecond\" : 1380.1916932907347, \"durationMs\" : { \"addBatch\" : 237, \"getBatch\" : 26, \"getOffset\" : 0, \"queryPlanning\" : 1, \"triggerExecution\" : 313, \"walCommit\" : 45 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : 0, \"endOffset\" : 432, \"numInputRows\" : 432, \"inputRowsPerSecond\" : 0.9993568953312452, \"processedRowsPerSecond\" : 1380.1916932907347 } ], \"sink\" : { \"description\" : \"ConsoleSink[numRows=20, truncate=true]\" } } StreamExecution uses < > and < > metadata logs for write-ahead log (to record offsets to be processed) and that have already been processed and committed to a streaming sink, respectively. TIP: Monitor offsets and commits metadata logs to know the progress of a streaming query. StreamExecution < > for 10 milliseconds (when no data was available to process in a batch). Use spark-sql-streaming-properties.md#spark.sql.streaming.pollingDelay[spark.sql.streaming.pollingDelay] Spark property to control the delay. [[id]] Every StreamExecution is uniquely identified by an ID of the streaming query (which is the id of the < >). NOTE: Since the < > is persisted (to the metadata file in the < >), the streaming query ID \"survives\" query restarts as long as the checkpoint directory is preserved. [[runId]] StreamExecution is also uniquely identified by a run ID of the streaming query . A run ID is a randomly-generated 128-bit universally unique identifier (UUID) that is assigned at the time StreamExecution is created. NOTE: runId does not \"survive\" query restarts and will always be different yet unique (across all active queries). [NOTE] \u00b6 The < >, < > and < > are all unique across all active queries (in a StreamingQueryManager ). The difference is that: < > is optional and user-defined < > is a UUID that is auto-generated at the time StreamExecution is created and persisted to metadata checkpoint file * < > is a UUID that is auto-generated every time StreamExecution is created \u00b6 [[streamMetadata]] StreamExecution uses a < > that is < > in the metadata file in the < >. If the metadata file is available it is < > and is the way to recover the < > of a streaming query when resumed (i.e. restarted after a failure or a planned stop). [[IS_CONTINUOUS_PROCESSING]] StreamExecution uses __is_continuous_processing local property (default: false ) to differentiate between < > ( true ) and < > ( false ) which is used when StateStoreRDD is requested to < > (and < > for a given version). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.StreamExecution to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating StreamExecution Instance StreamExecution takes the following to be created: [[sparkSession]] SparkSession [[name]] Name of the streaming query (can also be null ) [[checkpointRoot]] Path of the checkpoint directory (aka metadata directory ) [[analyzedPlan]] Streaming query (as an analyzed logical query plan, i.e. LogicalPlan ) [[sink]] < > [[trigger]] < > [[triggerClock]] Clock [[outputMode]] < > [[deleteCheckpointOnStop]] deleteCheckpointOnStop flag (to control whether to delete the checkpoint directory on stop) StreamExecution initializes the < >. NOTE: StreamExecution is a Scala abstract class and cannot be < > directly. It is created indirectly when the < > are. === [[offsetLog]] Write-Ahead Log (WAL) of Offsets -- offsetLog Property [source, scala] \u00b6 offsetLog: OffsetSeqLog \u00b6 offsetLog is a < > (of < >) with offsets < >. offsetLog is used as Write-Ahead Log of Offsets to < > of the data about to be processed in every trigger. NOTE: Metadata log or metadata checkpoint are synonyms and are often used interchangeably. The number of entries in the OffsetSeqLog is controlled using < > configuration property (default: 100 ). < > discard ( purge ) offsets from the offsets metadata log when the < > (in < >) or the < > (in < >) is above the threshold. [NOTE] \u00b6 offsetLog is used when: ContinuousExecution stream execution engine is requested to < >, < >, and < > * MicroBatchExecution stream execution engine is requested to < > and < > \u00b6 === [[state]] State of Streaming Query (Execution) -- state Property [source, scala] \u00b6 state: AtomicReference[State] \u00b6 state indicates the internal state of execution of the streaming query (as https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicReference.html[java.util.concurrent.atomic.AtomicReference ]). [[states]] .States [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | ACTIVE a| [[ACTIVE]] StreamExecution has been requested to < > (and is about to < >) | INITIALIZING a| [[INITIALIZING]] StreamExecution has been < > | TERMINATED a| [[TERMINATED]] Used to indicate that: MicroBatchExecution has been requested to < > ContinuousExecution has been requested to < > StreamExecution has been requested to < > (and has finished < >) | RECONFIGURING a| [[RECONFIGURING]] Used only when ContinuousExecution is requested to < > (and the < > indicated a < >) |=== === [[availableOffsets]] Available Offsets (StreamProgress) -- availableOffsets Property [source, scala] \u00b6 availableOffsets: StreamProgress \u00b6 availableOffsets is a < > to track what data (by < >) is available for processing for every streaming source in the < > (and have not yet been < >). availableOffsets works in tandem with the < > internal registry. availableOffsets is < > when StreamExecution is < > (i.e. no offsets are reported for any streaming source in the streaming query). availableOffsets is used when: MicroBatchExecution stream execution engine is requested to < >, < >, < > and < > ContinuousExecution stream execution engine is requested to < > StreamExecution is requested for the < > === [[committedOffsets]] Committed Offsets (StreamProgress) -- committedOffsets Property [source, scala] \u00b6 committedOffsets: StreamProgress \u00b6 committedOffsets is a < > to track what data (by < >) has already been processed and committed (to the sink or state stores) for every streaming source in the < >. committedOffsets works in tandem with the < > internal registry. committedOffsets is used when: MicroBatchExecution stream execution engine is requested for the < >, to < > and < > ContinuousExecution stream execution engine is requested for the < > and to < > StreamExecution is requested for the < > === [[resolvedCheckpointRoot]] Fully-Qualified (Resolved) Path to Checkpoint Root Directory -- resolvedCheckpointRoot Property [source, scala] \u00b6 resolvedCheckpointRoot: String \u00b6 resolvedCheckpointRoot is a fully-qualified path of the given < >. The given < > is defined using checkpointLocation option or the < > configuration property with queryName option. checkpointLocation and queryName options are defined when StreamingQueryManager is requested to create a streaming query . resolvedCheckpointRoot is used when < > and when StreamExecution finishes < >. resolvedCheckpointRoot is used for the < > (while transforming < > and planning StreamingRelation logical operators to corresponding StreamingExecutionRelation physical operators with the streaming data sources created passing in the path to sources directory to store checkpointing metadata). [TIP] \u00b6 You can see resolvedCheckpointRoot in the INFO message when StreamExecution is < >. Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. \u00b6 Internally, resolvedCheckpointRoot creates a Hadoop org.apache.hadoop.fs.Path for < > and makes it qualified. NOTE: resolvedCheckpointRoot uses SparkSession to access SessionState for a Hadoop configuration. === [[commitLog]] Offset Commit Log -- commits Metadata Checkpoint Directory StreamExecution uses offset commit log (< > with commits < >) for streaming batches successfully executed (with a single file per batch with a file name being the batch id) or committed epochs. NOTE: Metadata log or metadata checkpoint are synonyms and are often used interchangeably. commitLog is used by the < > for the following: MicroBatchExecution is requested to < > (that in turn requests to < > at the very beginning of the streaming query execution and later regularly every < >) ContinuousExecution is requested to < > (that in turn requests to < > at the very beginning of the streaming query execution and later regularly every < >) Last Query Execution Of Streaming Query (IncrementalExecution) \u00b6 lastExecution : IncrementalExecution lastExecution is part of the ProgressReporter abstraction. lastExecution is a < > (a QueryExecution of a streaming query) of the most recent ( last ) execution. lastExecution is created when the < > are requested for the following: MicroBatchExecution is requested to < > (when in < >) ContinuousExecution stream execution engine is requested to < > (when in < >) lastExecution is used when: StreamExecution is requested to < > (via < >) ProgressReporter is requested to extractStateOperatorMetrics , extractExecutionStats , and extractSourceToNumInputRows MicroBatchExecution stream execution engine is requested to < > (based on < >), < > (when in < > and < >) ContinuousExecution stream execution engine is requested to < > (when in < >) For debugging query execution of streaming queries (using debugCodegen ) === [[explain]] Explaining Streaming Query -- explain Method [source, scala] \u00b6 explain(): Unit // <1> explain(extended: Boolean): Unit <1> Turns the extended flag off ( false ) explain simply prints out < > to the standard output. NOTE: explain is used when...FIXME === [[explainInternal]] explainInternal Method [source, scala] \u00b6 explainInternal(extended: Boolean): String \u00b6 explainInternal ...FIXME [NOTE] \u00b6 explainInternal is used when: StreamExecution is requested to < > * StreamingQueryWrapper is requested to < > \u00b6 === [[stopSources]] Stopping Streaming Sources and Readers -- stopSources Method [source, scala] \u00b6 stopSources(): Unit \u00b6 stopSources requests every < > (in the < >) to < >. In case of an non-fatal exception, stopSources prints out the following WARN message to the logs: Failed to stop streaming source: [source]. Resources may have leaked. [NOTE] \u00b6 stopSources is used when: StreamExecution is requested to < > (and < > successfully or not) * ContinuousExecution is requested to < > (and terminates) \u00b6 Running Stream Processing \u00b6 runStream () : Unit runStream simply prepares the environment to < >. NOTE: runStream is used exclusively when the < > is requested to < > (when DataStreamWriter is requested to start an execution of the streaming query ). Internally, runStream sets the job group (to all the Spark jobs started by this thread) as follows: < > for the job group ID < > for the job group description (to display in web UI) interruptOnCancel flag on [NOTE] \u00b6 runStream uses the < > to access SparkContext and assign the job group id. Read up on https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-SparkContext.html#setJobGroup[SparkContext.setJobGroup ] method in https://bit.ly/apache-spark-internals[The Internals of Apache Spark] book. \u00b6 runStream sets sql.streaming.queryId local property to < >. runStream requests the MetricsSystem to register the < > when < > configuration property is on (default: off / false ). runStream notifies < > that the streaming query has been started (by < > a new < > event with < >, < >, and < >). .StreamingQueryListener Notified about Query's Start (onQueryStarted) image::images/StreamingQueryListener-onQueryStarted.png[align=\"center\"] runStream unblocks the < > (by decrementing the count of the < > that when 0 lets the starting thread continue). CAUTION: FIXME A picture with two parallel lanes for the starting thread and daemon one for the query. runStream updates the status message to be Initializing sources . [[runStream-initializing-sources]] runStream initializes the < >. NOTE: The < > is a lazy value in Scala and is initialized when requested the very first time. runStream disables adaptive query execution and cost-based join optimization (by turning spark.sql.adaptive.enabled and spark.sql.cbo.enabled configuration properties off, respectively). runStream creates a new \"zero\" < >. (Only when in < > state) runStream enters < > state: Decrements the count of < > [[runStream-runActivatedStream]] < > (which is different per < >, i.e. < > or < >). NOTE: runBatches does the main work only when first started (i.e. when < > is INITIALIZING ). [[runStream-stopped]] runStream ...FIXME (describe the failed and stop states) Once < > has finished executing batches, runBatches updates the status message to Stopped . NOTE: < > finishes executing batches when < > returns whether the streaming query is stopped or not (which is when the internal < > is not TERMINATED ). [[runBatches-catch-isInterruptedByStop]] [[runBatches-catch-IOException]] [[runStream-catch-Throwable]] CAUTION: FIXME Describe catch block for exception handling ==== [[runStream-finally]] Running Stream Processing -- finally Block runStream releases the < > and < > locks. runStream < >. runStream sets the < > to < >. runStream sets the StreamingQueryStatus with the isTriggerActive and isDataAvailable flags off ( false ). runStream removes the < > from the application's MetricsSystem . runStream requests the StreamingQueryManager to handle termination of a streaming query . runStream creates a new < > (with the < > and < > of the streaming query) and < >. [[runStream-finally-deleteCheckpointOnStop]] With the < > flag enabled and no < > reported, runStream deletes the < > recursively. In the end, runStream releases the < > lock. ==== [[runBatches-batch-runner]] TriggerExecutor's Batch Runner Batch Runner (aka batchRunner ) is an executable block executed by < > in < >. batchRunner < >. As long as the query is not stopped (i.e. < > is not TERMINATED ), batchRunner executes the streaming batch for the trigger. In triggerExecution time-tracking section , runBatches branches off per < >. .Current Batch Execution per currentBatchId [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | currentBatchId < 0 | currentBatchId >= 0 a| < > Setting Job Description as < > DEBUG Stream running from [committedOffsets] to [availableOffsets] | 1. < > |=== If there is < > in the sources, batchRunner marks < > with isDataAvailable enabled. [NOTE] \u00b6 You can check out the status of a StreamingQuery.md[streaming query] using StreamingQuery.md#status[status] method. [source, scala] \u00b6 scala> spark.streams.active(0).status res1: org.apache.spark.sql.streaming.StreamingQueryStatus = { \"message\" : \"Waiting for next trigger\", \"isDataAvailable\" : false, \"isTriggerActive\" : false } ==== batchRunner then updates the status message to Processing new data and < >. .StreamExecution's Running Batches (on Execution Thread) image::images/StreamExecution-runBatches.png[align=\"center\"] [[runBatches-batch-runner-finishTrigger]] After triggerExecution section has finished, batchRunner finishes the streaming batch for the trigger (and collects query execution statistics). When there was < > in the sources, batchRunner updates committed offsets (by spark-sql-streaming-CommitLog.md#add[adding] the < > to < > and adding < > to < >). You should see the following DEBUG message in the logs: DEBUG batch $currentBatchId committed batchRunner increments the < > and sets the job description for all the following Spark jobs to < >. [[runBatches-batchRunner-no-data]] When no < > in the sources to process, batchRunner does the following: Marks < > with isDataAvailable disabled Updates the status message to Waiting for data to arrive Sleeps the current thread for < > milliseconds. batchRunner updates the status message to Waiting for next trigger and returns whether the query is currently active or not (so < > can decide whether to finish executing the batches or not) === [[start]] Starting Streaming Query (on Stream Execution Thread) -- start Method [source, scala] \u00b6 start(): Unit \u00b6 When called, start prints out the following INFO message to the logs: Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. start then starts the < > (as a daemon thread). NOTE: start uses Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#start--++[java.lang.Thread.start ] to run the streaming query on a separate execution thread. NOTE: When started, a streaming query runs in its own execution thread on JVM. In the end, start pauses the main thread (using the < > until StreamExecution is requested to < > that in turn sends a < > to all streaming listeners followed by decrementing the count of the < >). start is used when StreamingQueryManager is requested to start a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ). === [[checkpointFile]] Path to Checkpoint Directory -- checkpointFile Internal Method [source, scala] \u00b6 checkpointFile(name: String): String \u00b6 checkpointFile gives the path of a directory with name in < >. NOTE: checkpointFile uses Hadoop's org.apache.hadoop.fs.Path . NOTE: checkpointFile is used for < >, < >, < >, and < > (for < >). Posting StreamingQueryListener Event \u00b6 postEvent ( event : StreamingQueryListener.Event ) : Unit postEvent is a part of ProgressReporter abstraction. postEvent simply requests the StreamingQueryManager to post the input event (to the StreamingQueryListenerBus in the current SparkSession ). Note postEvent uses SparkSession to access the current StreamingQueryManager . postEvent is used when: ProgressReporter is requested to report update progress (while finishing a trigger ) StreamExecution < > (and announces starting a streaming query by posting a spark-sql-streaming-StreamingQueryListener.md#QueryStartedEvent[QueryStartedEvent] and query termination by posting a spark-sql-streaming-StreamingQueryListener.md#QueryTerminatedEvent[QueryTerminatedEvent]) === [[processAllAvailable]] Waiting Until No New Data Available in Sources or Query Has Been Terminated -- processAllAvailable Method [source, scala] \u00b6 processAllAvailable(): Unit \u00b6 NOTE: processAllAvailable is a part of < >. processAllAvailable reports the < > if reported (and returns immediately). NOTE: < > is reported exclusively when StreamExecution is requested to < > (that terminated with an exception). processAllAvailable returns immediately when StreamExecution is no longer < > (in TERMINATED state). processAllAvailable acquires a lock on the < > and turns the < > internal flag off ( false ). processAllAvailable keeps polling with 10-second pauses (locked on < >) until < > flag is turned on ( true ) or StreamExecution is no longer < > (in TERMINATED state). NOTE: The 10-second pause is hardcoded and cannot be changed. In the end, processAllAvailable releases < > lock. processAllAvailable throws an IllegalStateException when executed on the < >: Cannot wait for a query state from the same thread that is running the query === [[queryExecutionThread]] Stream Execution Thread -- queryExecutionThread Property [source, scala] \u00b6 queryExecutionThread: QueryExecutionThread \u00b6 queryExecutionThread is a Java thread of execution ( https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.util.Thread ]) that < >. queryExecutionThread is started (as a daemon thread) when StreamExecution is requested to < >. At that time, start prints out the following INFO message to the logs (with the < > and the < >): Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. When started, queryExecutionThread sets the < > and < >. queryExecutionThread uses the name stream execution thread for [id] (that uses < > for the id, i.e. queryName [id = [id], runId = [runId]] ). queryExecutionThread is a QueryExecutionThread that is a custom UninterruptibleThread from Apache Spark with runUninterruptibly method for running a block of code without being interrupted by Thread.interrupt() . [TIP] \u00b6 Use Java's http://docs.oracle.com/javase/8/docs/technotes/guides/management/jconsole.html[jconsole ] or https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html[jstack ] to monitor stream execution threads. $ jstack <driver-pid> | grep -e \"stream execution thread\" \"stream execution thread for kafka-topic1 [id =... \u00b6 === [[toDebugString]] Internal String Representation -- toDebugString Internal Method [source, scala] \u00b6 toDebugString(includeLogicalPlan: Boolean): String \u00b6 toDebugString ...FIXME NOTE: toDebugString is used exclusively when StreamExecution is requested to < > (and an exception is caught). === [[offsetSeqMetadata]] Current Batch Metadata (Event-Time Watermark and Timestamp) -- offsetSeqMetadata Internal Property [source, scala] \u00b6 offsetSeqMetadata: OffsetSeqMetadata \u00b6 offsetSeqMetadata is a < >. offsetSeqMetadata is used to create an < > in the queryPlanning phase of the < > and < > execution engines. offsetSeqMetadata is initialized (with 0 for batchWatermarkMs and batchTimestampMs ) when StreamExecution is requested to < >. offsetSeqMetadata is then updated (with the current event-time watermark and timestamp) when MicroBatchExecution is requested to < >. NOTE: MicroBatchExecution uses the < > for the current event-time watermark and the < > for the current batch timestamp. offsetSeqMetadata is stored ( checkpointed ) in < > of MicroBatchExecution (and printed out as INFO message to the logs). offsetSeqMetadata is restored ( re-created ) from a checkpointed state when MicroBatchExecution is requested to < >. offsetSeqMetadata is part of the ProgressReporter abstraction. === [[isActive]] isActive Method [source, scala] \u00b6 isActive: Boolean \u00b6 NOTE: isActive is part of the < > to indicate whether a streaming query is active ( true ) or not ( false ). isActive is enabled ( true ) as long as the < > is not < >. === [[exception]] exception Method [source, scala] \u00b6 exception: Option[StreamingQueryException] \u00b6 NOTE: exception is part of the < > to indicate whether a streaming query...FIXME exception ...FIXME === [[getBatchDescriptionString]] Human-Readable HTML Description of Spark Jobs (for web UI) -- getBatchDescriptionString Method [source, scala] \u00b6 getBatchDescriptionString: String \u00b6 getBatchDescriptionString is a human-readable description (in HTML format) that uses the optional < > if defined, the < >, the < > and batchDescription that can be init (for the < > negative) or the < > itself. getBatchDescriptionString is of the following format: [subs=-macros] \u00b6 [name] id = [id] runId = [runId] batch = [batchDescription] \u00b6 .Monitoring Streaming Query using web UI (Spark Jobs) image::images/StreamExecution-getBatchDescriptionString-webUI.png[align=\"center\"] [NOTE] \u00b6 getBatchDescriptionString is used when: MicroBatchExecution stream execution engine is requested to < > (as the job description of any Spark jobs triggerred as part of query execution) * StreamExecution is requested to < > (as the job group description of any Spark jobs triggerred as part of query execution) \u00b6 === [[noNewData]] No New Data Available -- noNewData Internal Flag [source, scala] \u00b6 noNewData: Boolean \u00b6 noNewData is a flag that indicates that a batch has completed with no new data left and < > could stop waiting till all streaming data is processed. Default: false Turned on ( true ) when: MicroBatchExecution stream execution engine is requested to < > (while < >) ContinuousExecution stream execution engine is requested to < > Turned off ( false ) when: MicroBatchExecution stream execution engine is requested to < > (right after the < > phase) StreamExecution is requested to < > === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | awaitProgressLock | [[awaitProgressLock]] Java's fair reentrant mutual exclusion https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/ReentrantLock.html[java.util.concurrent.locks.ReentrantLock ] (that favors granting access to the longest-waiting thread under contention) | awaitProgressLockCondition a| [[awaitProgressLockCondition]] Lock | callSite | [[callSite]] | currentBatchId a| [[currentBatchId]] Current batch ID Starts at -1 when StreamExecution is < > 0 when StreamExecution < > (and < > is empty, i.e. no offset files in offsets directory in checkpoint) Incremented when StreamExecution < > and finishes a trigger that had < > (right after < >). | initializationLatch | [[initializationLatch]] | newData a| [[newData]] newData : Map [ BaseStreamingSource , LogicalPlan ] Registry of the < > (in the < >) that have new data available in the current batch. The new data is a streaming DataFrame . newData is part of the ProgressReporter abstraction. Set exclusively when StreamExecution is requested to < > (while < >). Used exclusively when StreamExecution is requested to < > (while < >). | pollingDelayMs | [[pollingDelayMs]] Time delay before polling new data again when no data was available Set to spark-sql-streaming-properties.md#spark.sql.streaming.pollingDelay[spark.sql.streaming.pollingDelay] Spark property. Used when StreamExecution has started < > (and < >). | prettyIdString a| [[prettyIdString]] Pretty-identified string for identification in logs (with < > if defined). // query name set queryName [id = xyz, runId = abc] // no query name [id = xyz, runId = abc] | startLatch | [[startLatch]] Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountDownLatch.html[java.util.concurrent.CountDownLatch ] with count 1 . Used when StreamExecution is requested to < > to pause the main thread until StreamExecution was requested to < >. | streamDeathCause | [[streamDeathCause]] StreamingQueryException | uniqueSources a| [[uniqueSources]] Unique < > (after being collected as StreamingExecutionRelation from the < >). NOTE: spark-sql-streaming-StreamingExecutionRelation.md[StreamingExecutionRelation] is a leaf logical operator (i.e. LogicalPlan ) that represents a streaming data source (and corresponds to a single spark-sql-streaming-StreamingRelation.md[StreamingRelation] in < > of a streaming Dataset). Used when StreamExecution : < > (and gets new offsets for every streaming data source) < > |=== Streaming Metrics \u00b6 StreamExecution uses MetricsReporter for reporting streaming metrics. MetricsReporter is created with the following source name (with name if defined or id ): spark.streaming.[name or id] MetricsReporter is registered only when spark.sql.streaming.metricsEnabled configuration property is enabled (when StreamExecution is requested to runStream ). MetricsReporter is deactivated ( removed ) when a streaming query is stopped (when StreamExecution is requested to runStream ).","title":"StreamExecution"},{"location":"StreamExecution/#streamexecution-stream-execution-engines","text":"StreamExecution is the < > of < > (aka streaming query processing engines ) that can < > a < > (on a < >). NOTE: Continuous query , streaming query , continuous Dataset , streaming Dataset are all considered high-level synonyms for an executable entity that stream execution engines run using the < > internally. [[contract]] .StreamExecution Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Property | Description | logicalPlan a| [[logicalPlan]]","title":"StreamExecution &mdash; Stream Execution Engines"},{"location":"StreamExecution/#source-scala","text":"","title":"[source, scala]"},{"location":"StreamExecution/#logicalplan-logicalplan","text":"Analyzed logical plan of the streaming query to execute Used when StreamExecution is requested to < > logicalPlan is part of the ProgressReporter abstraction. | runActivatedStream a| [[runActivatedStream]]","title":"logicalPlan: LogicalPlan"},{"location":"StreamExecution/#source-scala_1","text":"runActivatedStream( sparkSessionForStream: SparkSession): Unit Executes ( runs ) the activated < > Used exclusively when StreamExecution is requested to < > (when transitioning from INITIALIZING to ACTIVE state) |=== .Streaming Query and Stream Execution Engine [source, scala] import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution [[minLogEntriesToMaintain]][[spark.sql.streaming.minBatchesToRetain]] StreamExecution uses the < > configuration property to allow the < > to discard old log entries (from the < > and < > logs). [[extensions]] .StreamExecutions [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StreamExecution | Description | < > | [[ContinuousExecution]] Used in < > | < > | [[MicroBatchExecution]] Used in < > |=== NOTE: StreamExecution does not support adaptive query execution and cost-based optimizer (and turns them off when requested to < >). StreamExecution is the execution environment of a StreamingQuery.md[single streaming query] (aka streaming Dataset ) that is executed every < > and in the end < >. NOTE: StreamExecution corresponds to a StreamingQuery.md[single streaming query] with one or more streaming sources and exactly one spark-sql-streaming-Sink.md[streaming sink].","title":"[source, scala]"},{"location":"StreamExecution/#source-scala_2","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val q = spark. readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.minutes)). start scala> :type q org.apache.spark.sql.streaming.StreamingQuery // Pull out StreamExecution off StreamingQueryWrapper import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper} val se = q.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution When < >, StreamExecution starts a < > that simply < > (and hence the streaming query). .StreamExecution's Starting Streaming Query (on Execution Thread) image::images/StreamExecution-start.png[align=\"center\"] StreamExecution is a ProgressReporter and < > (i.e. when it starts, progresses and terminates) by posting StreamingQueryListener events. import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .text(\"server-logs\") .writeStream .format(\"console\") .queryName(\"debug\") .trigger(Trigger.ProcessingTime(20.seconds)) .start // Enable the log level to see the INFO and DEBUG messages // log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG 17/06/18 21:21:07 INFO StreamExecution: Starting new streaming query. 17/06/18 21:21:07 DEBUG StreamExecution: getOffset took 5 ms 17/06/18 21:21:07 DEBUG StreamExecution: Stream running from {} to {} 17/06/18 21:21:07 DEBUG StreamExecution: triggerExecution took 9 ms 17/06/18 21:21:07 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map()) 17/06/18 21:21:07 INFO StreamExecution: Streaming query made progress: { \"id\" : \"8b57b0bd-fc4a-42eb-81a3-777d7ba5e370\", \"runId\" : \"920b227e-6d02-4a03-a271-c62120258cea\", \"name\" : \"debug\", \"timestamp\" : \"2017-06-18T19:21:07.693Z\", \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"getOffset\" : 5, \"triggerExecution\" : 9 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"FileStreamSource[file:/Users/jacek/dev/oss/spark/server-logs]\", \"startOffset\" : null, \"endOffset\" : null, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@2460208a\" } } 17/06/18 21:21:10 DEBUG StreamExecution: Starting Trigger Calculation 17/06/18 21:21:10 DEBUG StreamExecution: getOffset took 3 ms 17/06/18 21:21:10 DEBUG StreamExecution: triggerExecution took 3 ms 17/06/18 21:21:10 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map()) StreamExecution tracks streaming data sources in < > internal registry. .StreamExecution's uniqueSources Registry of Streaming Data Sources image::images/StreamExecution-uniqueSources.png[align=\"center\"] StreamExecution collects durationMs for the execution units of streaming batches. .StreamExecution's durationMs image::images/StreamExecution-durationMs.png[align=\"center\"]","title":"[source, scala]"},{"location":"StreamExecution/#source-scala_3","text":"scala> :type q org.apache.spark.sql.streaming.StreamingQuery scala> println(q.lastProgress) { \"id\" : \"03fc78fc-fe19-408c-a1ae-812d0e28fcee\", \"runId\" : \"8c247071-afba-40e5-aad2-0e6f45f22488\", \"name\" : null, \"timestamp\" : \"2017-08-14T20:30:00.004Z\", \"batchId\" : 1, \"numInputRows\" : 432, \"inputRowsPerSecond\" : 0.9993568953312452, \"processedRowsPerSecond\" : 1380.1916932907347, \"durationMs\" : { \"addBatch\" : 237, \"getBatch\" : 26, \"getOffset\" : 0, \"queryPlanning\" : 1, \"triggerExecution\" : 313, \"walCommit\" : 45 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : 0, \"endOffset\" : 432, \"numInputRows\" : 432, \"inputRowsPerSecond\" : 0.9993568953312452, \"processedRowsPerSecond\" : 1380.1916932907347 } ], \"sink\" : { \"description\" : \"ConsoleSink[numRows=20, truncate=true]\" } } StreamExecution uses < > and < > metadata logs for write-ahead log (to record offsets to be processed) and that have already been processed and committed to a streaming sink, respectively. TIP: Monitor offsets and commits metadata logs to know the progress of a streaming query. StreamExecution < > for 10 milliseconds (when no data was available to process in a batch). Use spark-sql-streaming-properties.md#spark.sql.streaming.pollingDelay[spark.sql.streaming.pollingDelay] Spark property to control the delay. [[id]] Every StreamExecution is uniquely identified by an ID of the streaming query (which is the id of the < >). NOTE: Since the < > is persisted (to the metadata file in the < >), the streaming query ID \"survives\" query restarts as long as the checkpoint directory is preserved. [[runId]] StreamExecution is also uniquely identified by a run ID of the streaming query . A run ID is a randomly-generated 128-bit universally unique identifier (UUID) that is assigned at the time StreamExecution is created. NOTE: runId does not \"survive\" query restarts and will always be different yet unique (across all active queries).","title":"[source, scala]"},{"location":"StreamExecution/#note","text":"The < >, < > and < > are all unique across all active queries (in a StreamingQueryManager ). The difference is that: < > is optional and user-defined < > is a UUID that is auto-generated at the time StreamExecution is created and persisted to metadata checkpoint file","title":"[NOTE]"},{"location":"StreamExecution/#is-a-uuid-that-is-auto-generated-every-time-streamexecution-is-created","text":"[[streamMetadata]] StreamExecution uses a < > that is < > in the metadata file in the < >. If the metadata file is available it is < > and is the way to recover the < > of a streaming query when resumed (i.e. restarted after a failure or a planned stop). [[IS_CONTINUOUS_PROCESSING]] StreamExecution uses __is_continuous_processing local property (default: false ) to differentiate between < > ( true ) and < > ( false ) which is used when StateStoreRDD is requested to < > (and < > for a given version). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.StreamExecution to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=ALL","title":"* &lt;&gt; is a UUID that is auto-generated every time StreamExecution is created"},{"location":"StreamExecution/#refer-to","text":"=== [[creating-instance]] Creating StreamExecution Instance StreamExecution takes the following to be created: [[sparkSession]] SparkSession [[name]] Name of the streaming query (can also be null ) [[checkpointRoot]] Path of the checkpoint directory (aka metadata directory ) [[analyzedPlan]] Streaming query (as an analyzed logical query plan, i.e. LogicalPlan ) [[sink]] < > [[trigger]] < > [[triggerClock]] Clock [[outputMode]] < > [[deleteCheckpointOnStop]] deleteCheckpointOnStop flag (to control whether to delete the checkpoint directory on stop) StreamExecution initializes the < >. NOTE: StreamExecution is a Scala abstract class and cannot be < > directly. It is created indirectly when the < > are. === [[offsetLog]] Write-Ahead Log (WAL) of Offsets -- offsetLog Property","title":"Refer to &lt;&gt;."},{"location":"StreamExecution/#source-scala_4","text":"","title":"[source, scala]"},{"location":"StreamExecution/#offsetlog-offsetseqlog","text":"offsetLog is a < > (of < >) with offsets < >. offsetLog is used as Write-Ahead Log of Offsets to < > of the data about to be processed in every trigger. NOTE: Metadata log or metadata checkpoint are synonyms and are often used interchangeably. The number of entries in the OffsetSeqLog is controlled using < > configuration property (default: 100 ). < > discard ( purge ) offsets from the offsets metadata log when the < > (in < >) or the < > (in < >) is above the threshold.","title":"offsetLog: OffsetSeqLog"},{"location":"StreamExecution/#note_1","text":"offsetLog is used when: ContinuousExecution stream execution engine is requested to < >, < >, and < >","title":"[NOTE]"},{"location":"StreamExecution/#microbatchexecution-stream-execution-engine-is-requested-to-and","text":"=== [[state]] State of Streaming Query (Execution) -- state Property","title":"* MicroBatchExecution stream execution engine is requested to &lt;&gt; and &lt;&gt;"},{"location":"StreamExecution/#source-scala_5","text":"","title":"[source, scala]"},{"location":"StreamExecution/#state-atomicreferencestate","text":"state indicates the internal state of execution of the streaming query (as https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicReference.html[java.util.concurrent.atomic.AtomicReference ]). [[states]] .States [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | ACTIVE a| [[ACTIVE]] StreamExecution has been requested to < > (and is about to < >) | INITIALIZING a| [[INITIALIZING]] StreamExecution has been < > | TERMINATED a| [[TERMINATED]] Used to indicate that: MicroBatchExecution has been requested to < > ContinuousExecution has been requested to < > StreamExecution has been requested to < > (and has finished < >) | RECONFIGURING a| [[RECONFIGURING]] Used only when ContinuousExecution is requested to < > (and the < > indicated a < >) |=== === [[availableOffsets]] Available Offsets (StreamProgress) -- availableOffsets Property","title":"state: AtomicReference[State]"},{"location":"StreamExecution/#source-scala_6","text":"","title":"[source, scala]"},{"location":"StreamExecution/#availableoffsets-streamprogress","text":"availableOffsets is a < > to track what data (by < >) is available for processing for every streaming source in the < > (and have not yet been < >). availableOffsets works in tandem with the < > internal registry. availableOffsets is < > when StreamExecution is < > (i.e. no offsets are reported for any streaming source in the streaming query). availableOffsets is used when: MicroBatchExecution stream execution engine is requested to < >, < >, < > and < > ContinuousExecution stream execution engine is requested to < > StreamExecution is requested for the < > === [[committedOffsets]] Committed Offsets (StreamProgress) -- committedOffsets Property","title":"availableOffsets: StreamProgress"},{"location":"StreamExecution/#source-scala_7","text":"","title":"[source, scala]"},{"location":"StreamExecution/#committedoffsets-streamprogress","text":"committedOffsets is a < > to track what data (by < >) has already been processed and committed (to the sink or state stores) for every streaming source in the < >. committedOffsets works in tandem with the < > internal registry. committedOffsets is used when: MicroBatchExecution stream execution engine is requested for the < >, to < > and < > ContinuousExecution stream execution engine is requested for the < > and to < > StreamExecution is requested for the < > === [[resolvedCheckpointRoot]] Fully-Qualified (Resolved) Path to Checkpoint Root Directory -- resolvedCheckpointRoot Property","title":"committedOffsets: StreamProgress"},{"location":"StreamExecution/#source-scala_8","text":"","title":"[source, scala]"},{"location":"StreamExecution/#resolvedcheckpointroot-string","text":"resolvedCheckpointRoot is a fully-qualified path of the given < >. The given < > is defined using checkpointLocation option or the < > configuration property with queryName option. checkpointLocation and queryName options are defined when StreamingQueryManager is requested to create a streaming query . resolvedCheckpointRoot is used when < > and when StreamExecution finishes < >. resolvedCheckpointRoot is used for the < > (while transforming < > and planning StreamingRelation logical operators to corresponding StreamingExecutionRelation physical operators with the streaming data sources created passing in the path to sources directory to store checkpointing metadata).","title":"resolvedCheckpointRoot: String"},{"location":"StreamExecution/#tip","text":"You can see resolvedCheckpointRoot in the INFO message when StreamExecution is < >.","title":"[TIP]"},{"location":"StreamExecution/#starting-prettyidstring-use-resolvedcheckpointroot-to-store-the-query-checkpoint","text":"Internally, resolvedCheckpointRoot creates a Hadoop org.apache.hadoop.fs.Path for < > and makes it qualified. NOTE: resolvedCheckpointRoot uses SparkSession to access SessionState for a Hadoop configuration. === [[commitLog]] Offset Commit Log -- commits Metadata Checkpoint Directory StreamExecution uses offset commit log (< > with commits < >) for streaming batches successfully executed (with a single file per batch with a file name being the batch id) or committed epochs. NOTE: Metadata log or metadata checkpoint are synonyms and are often used interchangeably. commitLog is used by the < > for the following: MicroBatchExecution is requested to < > (that in turn requests to < > at the very beginning of the streaming query execution and later regularly every < >) ContinuousExecution is requested to < > (that in turn requests to < > at the very beginning of the streaming query execution and later regularly every < >)","title":"Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint.\n"},{"location":"StreamExecution/#last-query-execution-of-streaming-query-incrementalexecution","text":"lastExecution : IncrementalExecution lastExecution is part of the ProgressReporter abstraction. lastExecution is a < > (a QueryExecution of a streaming query) of the most recent ( last ) execution. lastExecution is created when the < > are requested for the following: MicroBatchExecution is requested to < > (when in < >) ContinuousExecution stream execution engine is requested to < > (when in < >) lastExecution is used when: StreamExecution is requested to < > (via < >) ProgressReporter is requested to extractStateOperatorMetrics , extractExecutionStats , and extractSourceToNumInputRows MicroBatchExecution stream execution engine is requested to < > (based on < >), < > (when in < > and < >) ContinuousExecution stream execution engine is requested to < > (when in < >) For debugging query execution of streaming queries (using debugCodegen ) === [[explain]] Explaining Streaming Query -- explain Method","title":" Last Query Execution Of Streaming Query (IncrementalExecution)"},{"location":"StreamExecution/#source-scala_9","text":"explain(): Unit // <1> explain(extended: Boolean): Unit <1> Turns the extended flag off ( false ) explain simply prints out < > to the standard output. NOTE: explain is used when...FIXME === [[explainInternal]] explainInternal Method","title":"[source, scala]"},{"location":"StreamExecution/#source-scala_10","text":"","title":"[source, scala]"},{"location":"StreamExecution/#explaininternalextended-boolean-string","text":"explainInternal ...FIXME","title":"explainInternal(extended: Boolean): String"},{"location":"StreamExecution/#note_2","text":"explainInternal is used when: StreamExecution is requested to < >","title":"[NOTE]"},{"location":"StreamExecution/#streamingquerywrapper-is-requested-to","text":"=== [[stopSources]] Stopping Streaming Sources and Readers -- stopSources Method","title":"* StreamingQueryWrapper is requested to &lt;&gt;"},{"location":"StreamExecution/#source-scala_11","text":"","title":"[source, scala]"},{"location":"StreamExecution/#stopsources-unit","text":"stopSources requests every < > (in the < >) to < >. In case of an non-fatal exception, stopSources prints out the following WARN message to the logs: Failed to stop streaming source: [source]. Resources may have leaked.","title":"stopSources(): Unit"},{"location":"StreamExecution/#note_3","text":"stopSources is used when: StreamExecution is requested to < > (and < > successfully or not)","title":"[NOTE]"},{"location":"StreamExecution/#continuousexecution-is-requested-to-and-terminates","text":"","title":"* ContinuousExecution is requested to &lt;&gt; (and terminates)"},{"location":"StreamExecution/#running-stream-processing","text":"runStream () : Unit runStream simply prepares the environment to < >. NOTE: runStream is used exclusively when the < > is requested to < > (when DataStreamWriter is requested to start an execution of the streaming query ). Internally, runStream sets the job group (to all the Spark jobs started by this thread) as follows: < > for the job group ID < > for the job group description (to display in web UI) interruptOnCancel flag on","title":" Running Stream Processing"},{"location":"StreamExecution/#note_4","text":"runStream uses the < > to access SparkContext and assign the job group id.","title":"[NOTE]"},{"location":"StreamExecution/#read-up-on-httpsjaceklaskowskigitbooksiomastering-apache-sparkspark-sparkcontexthtmlsetjobgroupsparkcontextsetjobgroup-method-in-httpsbitlyapache-spark-internalsthe-internals-of-apache-spark-book","text":"runStream sets sql.streaming.queryId local property to < >. runStream requests the MetricsSystem to register the < > when < > configuration property is on (default: off / false ). runStream notifies < > that the streaming query has been started (by < > a new < > event with < >, < >, and < >). .StreamingQueryListener Notified about Query's Start (onQueryStarted) image::images/StreamingQueryListener-onQueryStarted.png[align=\"center\"] runStream unblocks the < > (by decrementing the count of the < > that when 0 lets the starting thread continue). CAUTION: FIXME A picture with two parallel lanes for the starting thread and daemon one for the query. runStream updates the status message to be Initializing sources . [[runStream-initializing-sources]] runStream initializes the < >. NOTE: The < > is a lazy value in Scala and is initialized when requested the very first time. runStream disables adaptive query execution and cost-based join optimization (by turning spark.sql.adaptive.enabled and spark.sql.cbo.enabled configuration properties off, respectively). runStream creates a new \"zero\" < >. (Only when in < > state) runStream enters < > state: Decrements the count of < > [[runStream-runActivatedStream]] < > (which is different per < >, i.e. < > or < >). NOTE: runBatches does the main work only when first started (i.e. when < > is INITIALIZING ). [[runStream-stopped]] runStream ...FIXME (describe the failed and stop states) Once < > has finished executing batches, runBatches updates the status message to Stopped . NOTE: < > finishes executing batches when < > returns whether the streaming query is stopped or not (which is when the internal < > is not TERMINATED ). [[runBatches-catch-isInterruptedByStop]] [[runBatches-catch-IOException]] [[runStream-catch-Throwable]] CAUTION: FIXME Describe catch block for exception handling ==== [[runStream-finally]] Running Stream Processing -- finally Block runStream releases the < > and < > locks. runStream < >. runStream sets the < > to < >. runStream sets the StreamingQueryStatus with the isTriggerActive and isDataAvailable flags off ( false ). runStream removes the < > from the application's MetricsSystem . runStream requests the StreamingQueryManager to handle termination of a streaming query . runStream creates a new < > (with the < > and < > of the streaming query) and < >. [[runStream-finally-deleteCheckpointOnStop]] With the < > flag enabled and no < > reported, runStream deletes the < > recursively. In the end, runStream releases the < > lock. ==== [[runBatches-batch-runner]] TriggerExecutor's Batch Runner Batch Runner (aka batchRunner ) is an executable block executed by < > in < >. batchRunner < >. As long as the query is not stopped (i.e. < > is not TERMINATED ), batchRunner executes the streaming batch for the trigger. In triggerExecution time-tracking section , runBatches branches off per < >. .Current Batch Execution per currentBatchId [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | currentBatchId < 0 | currentBatchId >= 0 a| < > Setting Job Description as < > DEBUG Stream running from [committedOffsets] to [availableOffsets] | 1. < > |=== If there is < > in the sources, batchRunner marks < > with isDataAvailable enabled.","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-SparkContext.html#setJobGroup[SparkContext.setJobGroup] method in https://bit.ly/apache-spark-internals[The Internals of Apache Spark] book."},{"location":"StreamExecution/#note_5","text":"You can check out the status of a StreamingQuery.md[streaming query] using StreamingQuery.md#status[status] method.","title":"[NOTE]"},{"location":"StreamExecution/#source-scala_12","text":"scala> spark.streams.active(0).status res1: org.apache.spark.sql.streaming.StreamingQueryStatus = { \"message\" : \"Waiting for next trigger\", \"isDataAvailable\" : false, \"isTriggerActive\" : false } ==== batchRunner then updates the status message to Processing new data and < >. .StreamExecution's Running Batches (on Execution Thread) image::images/StreamExecution-runBatches.png[align=\"center\"] [[runBatches-batch-runner-finishTrigger]] After triggerExecution section has finished, batchRunner finishes the streaming batch for the trigger (and collects query execution statistics). When there was < > in the sources, batchRunner updates committed offsets (by spark-sql-streaming-CommitLog.md#add[adding] the < > to < > and adding < > to < >). You should see the following DEBUG message in the logs: DEBUG batch $currentBatchId committed batchRunner increments the < > and sets the job description for all the following Spark jobs to < >. [[runBatches-batchRunner-no-data]] When no < > in the sources to process, batchRunner does the following: Marks < > with isDataAvailable disabled Updates the status message to Waiting for data to arrive Sleeps the current thread for < > milliseconds. batchRunner updates the status message to Waiting for next trigger and returns whether the query is currently active or not (so < > can decide whether to finish executing the batches or not) === [[start]] Starting Streaming Query (on Stream Execution Thread) -- start Method","title":"[source, scala]"},{"location":"StreamExecution/#source-scala_13","text":"","title":"[source, scala]"},{"location":"StreamExecution/#start-unit","text":"When called, start prints out the following INFO message to the logs: Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. start then starts the < > (as a daemon thread). NOTE: start uses Java's ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#start--++[java.lang.Thread.start ] to run the streaming query on a separate execution thread. NOTE: When started, a streaming query runs in its own execution thread on JVM. In the end, start pauses the main thread (using the < > until StreamExecution is requested to < > that in turn sends a < > to all streaming listeners followed by decrementing the count of the < >). start is used when StreamingQueryManager is requested to start a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ). === [[checkpointFile]] Path to Checkpoint Directory -- checkpointFile Internal Method","title":"start(): Unit"},{"location":"StreamExecution/#source-scala_14","text":"","title":"[source, scala]"},{"location":"StreamExecution/#checkpointfilename-string-string","text":"checkpointFile gives the path of a directory with name in < >. NOTE: checkpointFile uses Hadoop's org.apache.hadoop.fs.Path . NOTE: checkpointFile is used for < >, < >, < >, and < > (for < >).","title":"checkpointFile(name: String): String"},{"location":"StreamExecution/#posting-streamingquerylistener-event","text":"postEvent ( event : StreamingQueryListener.Event ) : Unit postEvent is a part of ProgressReporter abstraction. postEvent simply requests the StreamingQueryManager to post the input event (to the StreamingQueryListenerBus in the current SparkSession ). Note postEvent uses SparkSession to access the current StreamingQueryManager . postEvent is used when: ProgressReporter is requested to report update progress (while finishing a trigger ) StreamExecution < > (and announces starting a streaming query by posting a spark-sql-streaming-StreamingQueryListener.md#QueryStartedEvent[QueryStartedEvent] and query termination by posting a spark-sql-streaming-StreamingQueryListener.md#QueryTerminatedEvent[QueryTerminatedEvent]) === [[processAllAvailable]] Waiting Until No New Data Available in Sources or Query Has Been Terminated -- processAllAvailable Method","title":" Posting StreamingQueryListener Event"},{"location":"StreamExecution/#source-scala_15","text":"","title":"[source, scala]"},{"location":"StreamExecution/#processallavailable-unit","text":"NOTE: processAllAvailable is a part of < >. processAllAvailable reports the < > if reported (and returns immediately). NOTE: < > is reported exclusively when StreamExecution is requested to < > (that terminated with an exception). processAllAvailable returns immediately when StreamExecution is no longer < > (in TERMINATED state). processAllAvailable acquires a lock on the < > and turns the < > internal flag off ( false ). processAllAvailable keeps polling with 10-second pauses (locked on < >) until < > flag is turned on ( true ) or StreamExecution is no longer < > (in TERMINATED state). NOTE: The 10-second pause is hardcoded and cannot be changed. In the end, processAllAvailable releases < > lock. processAllAvailable throws an IllegalStateException when executed on the < >: Cannot wait for a query state from the same thread that is running the query === [[queryExecutionThread]] Stream Execution Thread -- queryExecutionThread Property","title":"processAllAvailable(): Unit"},{"location":"StreamExecution/#source-scala_16","text":"","title":"[source, scala]"},{"location":"StreamExecution/#queryexecutionthread-queryexecutionthread","text":"queryExecutionThread is a Java thread of execution ( https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.util.Thread ]) that < >. queryExecutionThread is started (as a daemon thread) when StreamExecution is requested to < >. At that time, start prints out the following INFO message to the logs (with the < > and the < >): Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint. When started, queryExecutionThread sets the < > and < >. queryExecutionThread uses the name stream execution thread for [id] (that uses < > for the id, i.e. queryName [id = [id], runId = [runId]] ). queryExecutionThread is a QueryExecutionThread that is a custom UninterruptibleThread from Apache Spark with runUninterruptibly method for running a block of code without being interrupted by Thread.interrupt() .","title":"queryExecutionThread: QueryExecutionThread"},{"location":"StreamExecution/#tip_1","text":"Use Java's http://docs.oracle.com/javase/8/docs/technotes/guides/management/jconsole.html[jconsole ] or https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html[jstack ] to monitor stream execution threads.","title":"[TIP]"},{"location":"StreamExecution/#jstack-driver-pid-grep-e-stream-execution-thread-stream-execution-thread-for-kafka-topic1-id","text":"=== [[toDebugString]] Internal String Representation -- toDebugString Internal Method","title":"$ jstack &lt;driver-pid&gt; | grep -e &quot;stream execution thread&quot;\n&quot;stream execution thread for kafka-topic1 [id =...\n"},{"location":"StreamExecution/#source-scala_17","text":"","title":"[source, scala]"},{"location":"StreamExecution/#todebugstringincludelogicalplan-boolean-string","text":"toDebugString ...FIXME NOTE: toDebugString is used exclusively when StreamExecution is requested to < > (and an exception is caught). === [[offsetSeqMetadata]] Current Batch Metadata (Event-Time Watermark and Timestamp) -- offsetSeqMetadata Internal Property","title":"toDebugString(includeLogicalPlan: Boolean): String"},{"location":"StreamExecution/#source-scala_18","text":"","title":"[source, scala]"},{"location":"StreamExecution/#offsetseqmetadata-offsetseqmetadata","text":"offsetSeqMetadata is a < >. offsetSeqMetadata is used to create an < > in the queryPlanning phase of the < > and < > execution engines. offsetSeqMetadata is initialized (with 0 for batchWatermarkMs and batchTimestampMs ) when StreamExecution is requested to < >. offsetSeqMetadata is then updated (with the current event-time watermark and timestamp) when MicroBatchExecution is requested to < >. NOTE: MicroBatchExecution uses the < > for the current event-time watermark and the < > for the current batch timestamp. offsetSeqMetadata is stored ( checkpointed ) in < > of MicroBatchExecution (and printed out as INFO message to the logs). offsetSeqMetadata is restored ( re-created ) from a checkpointed state when MicroBatchExecution is requested to < >. offsetSeqMetadata is part of the ProgressReporter abstraction. === [[isActive]] isActive Method","title":"offsetSeqMetadata: OffsetSeqMetadata"},{"location":"StreamExecution/#source-scala_19","text":"","title":"[source, scala]"},{"location":"StreamExecution/#isactive-boolean","text":"NOTE: isActive is part of the < > to indicate whether a streaming query is active ( true ) or not ( false ). isActive is enabled ( true ) as long as the < > is not < >. === [[exception]] exception Method","title":"isActive: Boolean"},{"location":"StreamExecution/#source-scala_20","text":"","title":"[source, scala]"},{"location":"StreamExecution/#exception-optionstreamingqueryexception","text":"NOTE: exception is part of the < > to indicate whether a streaming query...FIXME exception ...FIXME === [[getBatchDescriptionString]] Human-Readable HTML Description of Spark Jobs (for web UI) -- getBatchDescriptionString Method","title":"exception: Option[StreamingQueryException]"},{"location":"StreamExecution/#source-scala_21","text":"","title":"[source, scala]"},{"location":"StreamExecution/#getbatchdescriptionstring-string","text":"getBatchDescriptionString is a human-readable description (in HTML format) that uses the optional < > if defined, the < >, the < > and batchDescription that can be init (for the < > negative) or the < > itself. getBatchDescriptionString is of the following format:","title":"getBatchDescriptionString: String"},{"location":"StreamExecution/#subs-macros","text":"","title":"[subs=-macros]"},{"location":"StreamExecution/#nameid-idrunid-runidbatch-batchdescription","text":".Monitoring Streaming Query using web UI (Spark Jobs) image::images/StreamExecution-getBatchDescriptionString-webUI.png[align=\"center\"]","title":"[name]id = [id]runId = [runId]batch = [batchDescription]"},{"location":"StreamExecution/#note_6","text":"getBatchDescriptionString is used when: MicroBatchExecution stream execution engine is requested to < > (as the job description of any Spark jobs triggerred as part of query execution)","title":"[NOTE]"},{"location":"StreamExecution/#streamexecution-is-requested-to-as-the-job-group-description-of-any-spark-jobs-triggerred-as-part-of-query-execution","text":"=== [[noNewData]] No New Data Available -- noNewData Internal Flag","title":"* StreamExecution is requested to &lt;&gt; (as the job group description of any Spark jobs triggerred as part of query execution)"},{"location":"StreamExecution/#source-scala_22","text":"","title":"[source, scala]"},{"location":"StreamExecution/#nonewdata-boolean","text":"noNewData is a flag that indicates that a batch has completed with no new data left and < > could stop waiting till all streaming data is processed. Default: false Turned on ( true ) when: MicroBatchExecution stream execution engine is requested to < > (while < >) ContinuousExecution stream execution engine is requested to < > Turned off ( false ) when: MicroBatchExecution stream execution engine is requested to < > (right after the < > phase) StreamExecution is requested to < > === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | awaitProgressLock | [[awaitProgressLock]] Java's fair reentrant mutual exclusion https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/ReentrantLock.html[java.util.concurrent.locks.ReentrantLock ] (that favors granting access to the longest-waiting thread under contention) | awaitProgressLockCondition a| [[awaitProgressLockCondition]] Lock | callSite | [[callSite]] | currentBatchId a| [[currentBatchId]] Current batch ID Starts at -1 when StreamExecution is < > 0 when StreamExecution < > (and < > is empty, i.e. no offset files in offsets directory in checkpoint) Incremented when StreamExecution < > and finishes a trigger that had < > (right after < >). | initializationLatch | [[initializationLatch]] | newData a| [[newData]] newData : Map [ BaseStreamingSource , LogicalPlan ] Registry of the < > (in the < >) that have new data available in the current batch. The new data is a streaming DataFrame . newData is part of the ProgressReporter abstraction. Set exclusively when StreamExecution is requested to < > (while < >). Used exclusively when StreamExecution is requested to < > (while < >). | pollingDelayMs | [[pollingDelayMs]] Time delay before polling new data again when no data was available Set to spark-sql-streaming-properties.md#spark.sql.streaming.pollingDelay[spark.sql.streaming.pollingDelay] Spark property. Used when StreamExecution has started < > (and < >). | prettyIdString a| [[prettyIdString]] Pretty-identified string for identification in logs (with < > if defined). // query name set queryName [id = xyz, runId = abc] // no query name [id = xyz, runId = abc] | startLatch | [[startLatch]] Java's https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountDownLatch.html[java.util.concurrent.CountDownLatch ] with count 1 . Used when StreamExecution is requested to < > to pause the main thread until StreamExecution was requested to < >. | streamDeathCause | [[streamDeathCause]] StreamingQueryException | uniqueSources a| [[uniqueSources]] Unique < > (after being collected as StreamingExecutionRelation from the < >). NOTE: spark-sql-streaming-StreamingExecutionRelation.md[StreamingExecutionRelation] is a leaf logical operator (i.e. LogicalPlan ) that represents a streaming data source (and corresponds to a single spark-sql-streaming-StreamingRelation.md[StreamingRelation] in < > of a streaming Dataset). Used when StreamExecution : < > (and gets new offsets for every streaming data source) < > |===","title":"noNewData: Boolean"},{"location":"StreamExecution/#streaming-metrics","text":"StreamExecution uses MetricsReporter for reporting streaming metrics. MetricsReporter is created with the following source name (with name if defined or id ): spark.streaming.[name or id] MetricsReporter is registered only when spark.sql.streaming.metricsEnabled configuration property is enabled (when StreamExecution is requested to runStream ). MetricsReporter is deactivated ( removed ) when a streaming query is stopped (when StreamExecution is requested to runStream ).","title":" Streaming Metrics"},{"location":"StreamSourceProvider/","text":"== [[StreamSourceProvider]] StreamSourceProvider Contract -- Streaming Source Providers for Micro-Batch Stream Processing (Data Source API V1) StreamSourceProvider is the < > of < > that can < > for a format (e.g. text file) or system (e.g. Apache Kafka). StreamSourceProvider is part of Data Source API V1 and used in < > only. [[contract]] .StreamSourceProvider Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | createSource a| [[createSource]] [source, scala] \u00b6 createSource( sqlContext: SQLContext, metadataPath: String, schema: Option[StructType], providerName: String, parameters: Map[String, String]): Source Creates a streaming source NOTE: metadataPath is the value of the optional user-specified checkpointLocation option or resolved by StreamingQueryManager . Used exclusively when DataSource is requested to < > (when MicroBatchExecution is requested to < >) | sourceSchema a| [[sourceSchema]] [source, scala] \u00b6 sourceSchema( sqlContext: SQLContext, schema: Option[StructType], providerName: String, parameters: Map[String, String]): (String, StructType) The name and schema of the streaming source Used exclusively when DataSource is requested for < > (when MicroBatchExecution is requested to < >) |=== [[implementations]] NOTE: < > is the only known StreamSourceProvider in Spark Structured Streaming.","title":"StreamSourceProvider"},{"location":"StreamSourceProvider/#source-scala","text":"createSource( sqlContext: SQLContext, metadataPath: String, schema: Option[StructType], providerName: String, parameters: Map[String, String]): Source Creates a streaming source NOTE: metadataPath is the value of the optional user-specified checkpointLocation option or resolved by StreamingQueryManager . Used exclusively when DataSource is requested to < > (when MicroBatchExecution is requested to < >) | sourceSchema a| [[sourceSchema]]","title":"[source, scala]"},{"location":"StreamSourceProvider/#source-scala_1","text":"sourceSchema( sqlContext: SQLContext, schema: Option[StructType], providerName: String, parameters: Map[String, String]): (String, StructType) The name and schema of the streaming source Used exclusively when DataSource is requested for < > (when MicroBatchExecution is requested to < >) |=== [[implementations]] NOTE: < > is the only known StreamSourceProvider in Spark Structured Streaming.","title":"[source, scala]"},{"location":"StreamingQuery/","text":"StreamingQuery \u00b6 StreamingQuery is the < > of streaming queries that are executed continuously and concurrently (i.e. on a separate thread ). StreamingQuery is a Scala trait with the only implementation being StreamExecution (and less importanly StreamingQueryWrapper ). [[contract]] .StreamingQuery Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | awaitTermination a| [[awaitTermination]] [source, scala] \u00b6 awaitTermination(): Unit awaitTermination(timeoutMs: Long): Boolean Used when...FIXME | exception a| [[exception]] [source, scala] \u00b6 exception: Option[StreamingQueryException] \u00b6 StreamingQueryException if the query has finished due to an exception Used when...FIXME | explain a| [[explain]] [source, scala] \u00b6 explain(): Unit explain(extended: Boolean): Unit Used when...FIXME | id a| [[id]] [source, scala] \u00b6 id: UUID \u00b6 The unique identifier of the streaming query (that does not change across restarts unlike < >) Used when...FIXME | isActive a| [[isActive]] [source, scala] \u00b6 isActive: Boolean \u00b6 Indicates whether the streaming query is active ( true ) or not ( false ) Used when...FIXME | lastProgress a| [[lastProgress]] [source, scala] \u00b6 lastProgress: StreamingQueryProgress \u00b6 The last StreamingQueryProgress of the streaming query Used when...FIXME | name a| [[name]] [source, scala] \u00b6 name: String \u00b6 The name of the query that is unique across all active queries Used when...FIXME | processAllAvailable a| [[processAllAvailable]] [source, scala] \u00b6 processAllAvailable(): Unit \u00b6 Pauses ( blocks ) the current thread until the streaming query has no more data to be processed or has been < > Intended for testing | recentProgress a| [[recentProgress]] [source, scala] \u00b6 recentProgress: Array[StreamingQueryProgress] \u00b6 Collection of the recent StreamingQueryProgress updates. Used when...FIXME | runId a| [[runId]] [source, scala] \u00b6 runId: UUID \u00b6 The unique identifier of the current execution of the streaming query (that is different every restart unlike < >) Used when...FIXME | sparkSession a| [[sparkSession]] [source, scala] \u00b6 sparkSession: SparkSession \u00b6 Used when...FIXME | status a| [[status]] [source, scala] \u00b6 status: StreamingQueryStatus \u00b6 StreamingQueryStatus of the streaming query (as StreamExecution has accumulated being a ProgressReporter while running the streaming query) Used when...FIXME | stop a| [[stop]] [source, scala] \u00b6 stop(): Unit \u00b6 Stops the streaming query |=== StreamingQuery can be in two states: active (started) inactive (stopped) If inactive, StreamingQuery may have transitioned into the state due to an StreamingQueryException (that is available under exception ). StreamingQuery tracks current state of all the sources, i.e. SourceStatus , as sourceStatuses . There could only be a single streaming sink for a StreamingQuery with many streaming sources . StreamingQuery can be stopped by stop or an exception.","title":"StreamingQuery"},{"location":"StreamingQuery/#streamingquery","text":"StreamingQuery is the < > of streaming queries that are executed continuously and concurrently (i.e. on a separate thread ). StreamingQuery is a Scala trait with the only implementation being StreamExecution (and less importanly StreamingQueryWrapper ). [[contract]] .StreamingQuery Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | awaitTermination a| [[awaitTermination]]","title":"StreamingQuery"},{"location":"StreamingQuery/#source-scala","text":"awaitTermination(): Unit awaitTermination(timeoutMs: Long): Boolean Used when...FIXME | exception a| [[exception]]","title":"[source, scala]"},{"location":"StreamingQuery/#source-scala_1","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#exception-optionstreamingqueryexception","text":"StreamingQueryException if the query has finished due to an exception Used when...FIXME | explain a| [[explain]]","title":"exception: Option[StreamingQueryException]"},{"location":"StreamingQuery/#source-scala_2","text":"explain(): Unit explain(extended: Boolean): Unit Used when...FIXME | id a| [[id]]","title":"[source, scala]"},{"location":"StreamingQuery/#source-scala_3","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#id-uuid","text":"The unique identifier of the streaming query (that does not change across restarts unlike < >) Used when...FIXME | isActive a| [[isActive]]","title":"id: UUID"},{"location":"StreamingQuery/#source-scala_4","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#isactive-boolean","text":"Indicates whether the streaming query is active ( true ) or not ( false ) Used when...FIXME | lastProgress a| [[lastProgress]]","title":"isActive: Boolean"},{"location":"StreamingQuery/#source-scala_5","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#lastprogress-streamingqueryprogress","text":"The last StreamingQueryProgress of the streaming query Used when...FIXME | name a| [[name]]","title":"lastProgress: StreamingQueryProgress"},{"location":"StreamingQuery/#source-scala_6","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#name-string","text":"The name of the query that is unique across all active queries Used when...FIXME | processAllAvailable a| [[processAllAvailable]]","title":"name: String"},{"location":"StreamingQuery/#source-scala_7","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#processallavailable-unit","text":"Pauses ( blocks ) the current thread until the streaming query has no more data to be processed or has been < > Intended for testing | recentProgress a| [[recentProgress]]","title":"processAllAvailable(): Unit"},{"location":"StreamingQuery/#source-scala_8","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#recentprogress-arraystreamingqueryprogress","text":"Collection of the recent StreamingQueryProgress updates. Used when...FIXME | runId a| [[runId]]","title":"recentProgress: Array[StreamingQueryProgress]"},{"location":"StreamingQuery/#source-scala_9","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#runid-uuid","text":"The unique identifier of the current execution of the streaming query (that is different every restart unlike < >) Used when...FIXME | sparkSession a| [[sparkSession]]","title":"runId: UUID"},{"location":"StreamingQuery/#source-scala_10","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#sparksession-sparksession","text":"Used when...FIXME | status a| [[status]]","title":"sparkSession: SparkSession"},{"location":"StreamingQuery/#source-scala_11","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#status-streamingquerystatus","text":"StreamingQueryStatus of the streaming query (as StreamExecution has accumulated being a ProgressReporter while running the streaming query) Used when...FIXME | stop a| [[stop]]","title":"status: StreamingQueryStatus"},{"location":"StreamingQuery/#source-scala_12","text":"","title":"[source, scala]"},{"location":"StreamingQuery/#stop-unit","text":"Stops the streaming query |=== StreamingQuery can be in two states: active (started) inactive (stopped) If inactive, StreamingQuery may have transitioned into the state due to an StreamingQueryException (that is available under exception ). StreamingQuery tracks current state of all the sources, i.e. SourceStatus , as sourceStatuses . There could only be a single streaming sink for a StreamingQuery with many streaming sources . StreamingQuery can be stopped by stop or an exception.","title":"stop(): Unit"},{"location":"StreamingQueryListenerBus/","text":"StreamingQueryListenerBus \u2014 Event Bus for Streaming Events \u00b6 StreamingQueryListenerBus is an event bus ( ListenerBus[StreamingQueryListener, StreamingQueryListener.Event] ) for < > of < > (that eventually are delivered to < >). StreamingQueryListenerBus is created for StreamingQueryManager (once per SparkSession ). StreamingQueryListenerBus is also a SparkListener and registers itself with the < > (of the SparkSession ) to < >. Creating Instance \u00b6 StreamingQueryListenerBus takes the following when created: [[sparkListenerBus]] LiveListenerBus StreamingQueryListenerBus registers itself with the < >. === [[activeQueryRunIds]] Run IDs of Active Streaming Queries [source, scala] \u00b6 activeQueryRunIds: HashSet[UUID] \u00b6 activeQueryRunIds is an internal registry of < > of active streaming queries in the SparkSession . A runId is added when StreamingQueryListenerBus is requested to < > A runId is removed when StreamingQueryListenerBus is requested to < > activeQueryRunIds is used internally to < > (so the events gets sent out to streaming queries in the SparkSession ). === [[post]] Posting Streaming Event to LiveListenerBus -- post Method [source, scala] \u00b6 post(event: StreamingQueryListener.Event): Unit \u00b6 post simply posts the input event directly to the < > unless it is a < >. For a < >, post adds the runId (of the streaming query that has been started) to the < > internal registry first, posts the event to the < > and then < >. post is used when StreamingQueryManager is requested to post a streaming event . Posting Streaming Event to StreamingQueryListeners \u00b6 doPostEvent ( listener : StreamingQueryListener , event : StreamingQueryListener.Event ) : Unit doPostEvent is part of the ListenerBus abstraction ( Spark Core ). doPostEvent branches per the type of < >: For a < >, requests the < > to < > For a < >, requests the < > to < > For a < >, requests the < > to < > For any other event, doPostEvent simply does nothing ( swallows it ). === [[postToAll]] postToAll Method [source, scala] \u00b6 postToAll(event: Event): Unit \u00b6 NOTE: postToAll is part of Spark Core's ListenerBus contract to post an event to all registered listeners. postToAll first requests the parent ListenerBus to post the event to all registered listeners. For a < >, postToAll simply removes the runId (of the streaming query that has been terminated) from the < > internal registry.","title":"StreamingQueryListenerBus"},{"location":"StreamingQueryListenerBus/#streamingquerylistenerbus-event-bus-for-streaming-events","text":"StreamingQueryListenerBus is an event bus ( ListenerBus[StreamingQueryListener, StreamingQueryListener.Event] ) for < > of < > (that eventually are delivered to < >). StreamingQueryListenerBus is created for StreamingQueryManager (once per SparkSession ). StreamingQueryListenerBus is also a SparkListener and registers itself with the < > (of the SparkSession ) to < >.","title":"StreamingQueryListenerBus &mdash; Event Bus for Streaming Events"},{"location":"StreamingQueryListenerBus/#creating-instance","text":"StreamingQueryListenerBus takes the following when created: [[sparkListenerBus]] LiveListenerBus StreamingQueryListenerBus registers itself with the < >. === [[activeQueryRunIds]] Run IDs of Active Streaming Queries","title":"Creating Instance"},{"location":"StreamingQueryListenerBus/#source-scala","text":"","title":"[source, scala]"},{"location":"StreamingQueryListenerBus/#activequeryrunids-hashsetuuid","text":"activeQueryRunIds is an internal registry of < > of active streaming queries in the SparkSession . A runId is added when StreamingQueryListenerBus is requested to < > A runId is removed when StreamingQueryListenerBus is requested to < > activeQueryRunIds is used internally to < > (so the events gets sent out to streaming queries in the SparkSession ). === [[post]] Posting Streaming Event to LiveListenerBus -- post Method","title":"activeQueryRunIds: HashSet[UUID]"},{"location":"StreamingQueryListenerBus/#source-scala_1","text":"","title":"[source, scala]"},{"location":"StreamingQueryListenerBus/#postevent-streamingquerylistenerevent-unit","text":"post simply posts the input event directly to the < > unless it is a < >. For a < >, post adds the runId (of the streaming query that has been started) to the < > internal registry first, posts the event to the < > and then < >. post is used when StreamingQueryManager is requested to post a streaming event .","title":"post(event: StreamingQueryListener.Event): Unit"},{"location":"StreamingQueryListenerBus/#posting-streaming-event-to-streamingquerylisteners","text":"doPostEvent ( listener : StreamingQueryListener , event : StreamingQueryListener.Event ) : Unit doPostEvent is part of the ListenerBus abstraction ( Spark Core ). doPostEvent branches per the type of < >: For a < >, requests the < > to < > For a < >, requests the < > to < > For a < >, requests the < > to < > For any other event, doPostEvent simply does nothing ( swallows it ). === [[postToAll]] postToAll Method","title":" Posting Streaming Event to StreamingQueryListeners"},{"location":"StreamingQueryListenerBus/#source-scala_2","text":"","title":"[source, scala]"},{"location":"StreamingQueryListenerBus/#posttoallevent-event-unit","text":"NOTE: postToAll is part of Spark Core's ListenerBus contract to post an event to all registered listeners. postToAll first requests the parent ListenerBus to post the event to all registered listeners. For a < >, postToAll simply removes the runId (of the streaming query that has been terminated) from the < > internal registry.","title":"postToAll(event: Event): Unit"},{"location":"StreamingQueryManager/","text":"StreamingQueryManager \u2014 Streaming Query Management \u00b6 StreamingQueryManager is the < > for < > of a < >. [[methods]] .StreamingQueryManager API [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | < > a| [source, scala] \u00b6 active: Array[StreamingQuery] \u00b6 Active < > | < > a| [source, scala] \u00b6 addListener(listener: StreamingQueryListener): Unit \u00b6 Registers ( adds ) a StreamingQueryListener | < > a| [source, scala] \u00b6 awaitAnyTermination(): Unit awaitAnyTermination(timeoutMs: Long): Boolean Waits until any streaming query terminats or timeoutMs elapses | < > a| [source, scala] \u00b6 get(id: String): StreamingQuery get(id: UUID): StreamingQuery Gets the < > by < > | < > a| [source, scala] \u00b6 removeListener( listener: StreamingQueryListener): Unit De-registers ( removes ) the StreamingQueryListener | < > a| [source, scala] \u00b6 resetTerminated(): Unit \u00b6 Resets the internal registry of the terminated streaming queries (that lets < > to be used again) |=== StreamingQueryManager is available using SparkSession.streams property. [source, scala] \u00b6 scala> :type spark org.apache.spark.sql.SparkSession scala> :type spark.streams org.apache.spark.sql.streaming.StreamingQueryManager StreamingQueryManager is < > when SessionState is created. .StreamingQueryManager image::images/StreamingQueryManager.png[align=\"center\"] TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SessionState.html[SessionState ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] gitbook. StreamingQueryManager is used (internally) to < >. .StreamingQueryManager Creates StreamingQuery (and StreamExecution) image::images/StreamingQueryManager-createQuery.png[align=\"center\"] StreamingQueryManager is < >. [[creating-instance]][[sparkSession]] StreamingQueryManager takes a single SparkSession when created. === [[listenerBus]] StreamingQueryListenerBus -- listenerBus Internal Property [source, scala] \u00b6 listenerBus: StreamingQueryListenerBus \u00b6 listenerBus is a StreamingQueryListenerBus (for the current < >) that is created immediately when StreamingQueryManager is < >. listenerBus is used for the following: < > or < > a given StreamingQueryListener < > (and notify < >) === [[active]] Getting All Active Streaming Queries -- active Method [source, scala] \u00b6 active: Array[StreamingQuery] \u00b6 active gets < >. === [[get]] Getting Active Continuous Query By Name -- get Method [source, scala] \u00b6 get(name: String): StreamingQuery \u00b6 get method returns a StreamingQuery.md[StreamingQuery] by name . It may throw an IllegalArgumentException when no StreamingQuery exists for the name . java.lang.IllegalArgumentException: There is no active query with name hello at org.apache.spark.sql.StreamingQueryManager$$anonfun$get$1.apply(StreamingQueryManager.scala:59) at org.apache.spark.sql.StreamingQueryManager$$anonfun$get$1.apply(StreamingQueryManager.scala:59) at scala.collection.MapLike$class.getOrElse(MapLike.scala:128) at scala.collection.AbstractMap.getOrElse(Map.scala:59) at org.apache.spark.sql.StreamingQueryManager.get(StreamingQueryManager.scala:58) ... 49 elided Registering StreamingQueryListener \u00b6 addListener ( listener : StreamingQueryListener ) : Unit addListener requests the StreamingQueryListenerBus to add the input StreamingQueryListener . De-Registering StreamingQueryListener \u00b6 removeListener ( listener : StreamingQueryListener ) : Unit removeListener requests StreamingQueryListenerBus to remove the input StreamingQueryListener . === [[awaitAnyTermination]] Waiting for Any Streaming Query Termination -- awaitAnyTermination Method [source, scala] \u00b6 awaitAnyTermination(): Unit awaitAnyTermination(timeoutMs: Long): Boolean awaitAnyTermination acquires a lock on < > and waits until any streaming query has finished (i.e. < > is available) or timeoutMs has expired. awaitAnyTermination re-throws the StreamingQueryException from < > if StreamingQuery.md#exception[it reported one]. === [[resetTerminated]] resetTerminated Method [source, scala] \u00b6 resetTerminated(): Unit \u00b6 resetTerminated forgets about the past-terminated query (so that < > can be used again to wait for a new streaming query termination). Internally, resetTerminated acquires a lock on < > and simply resets < > (i.e. sets it to null ). === [[createQuery]] Creating Streaming Query -- createQuery Internal Method [source, scala] \u00b6 createQuery( userSpecifiedName: Option[String], userSpecifiedCheckpointLocation: Option[String], df: DataFrame, extraOptions: Map[String, String], sink: BaseStreamingSink, outputMode: OutputMode, useTempCheckpointLocation: Boolean, recoverFromCheckpointLocation: Boolean, trigger: Trigger, triggerClock: Clock): StreamingQueryWrapper createQuery creates a spark-sql-streaming-StreamingQueryWrapper.md#creating-instance[StreamingQueryWrapper] (for a StreamExecution.md#creating-instance[StreamExecution] per the input user-defined properties). Internally, createQuery first finds the name of the checkpoint directory of a query (aka checkpoint location ) in the following order: . Exactly the input userSpecifiedCheckpointLocation if defined . spark-sql-streaming-properties.md#spark.sql.streaming.checkpointLocation[spark.sql.streaming.checkpointLocation] Spark property if defined for the parent directory with a subdirectory per the optional userSpecifiedName (or a randomly-generated UUID) . (only when useTempCheckpointLocation is enabled) A temporary directory (as specified by java.io.tmpdir JVM property) with a subdirectory with temporary prefix. NOTE: userSpecifiedCheckpointLocation can be any path that is acceptable by Hadoop's https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/Path.html[Path ]. If the directory name for the checkpoint location could not be found, createQuery reports a AnalysisException . checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...) createQuery reports a AnalysisException when the input recoverFromCheckpointLocation flag is turned off but there is offsets directory in the checkpoint location. createQuery makes sure that the logical plan of the structured query is analyzed (i.e. no logical errors have been found). Unless spark-sql-streaming-properties.md#spark.sql.streaming.unsupportedOperationCheck[spark.sql.streaming.unsupportedOperationCheck] Spark property is turned on, createQuery spark-sql-streaming-UnsupportedOperationChecker.md#checkForStreaming[checks the logical plan of the streaming query for unsupported operations]. (only when spark.sql.adaptive.enabled Spark property is turned on) createQuery prints out a WARN message to the logs: WARN spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled. In the end, createQuery creates a spark-sql-streaming-StreamingQueryWrapper.md#creating-instance[StreamingQueryWrapper] with a new < >. [NOTE] \u00b6 recoverFromCheckpointLocation flag corresponds to recoverFromCheckpointLocation flag that StreamingQueryManager uses to < > and which is enabled by default (and is in fact the only place where createQuery is used). memory sink has the flag enabled for spark-sql-streaming-OutputMode.md#Complete[Complete] output mode only foreach sink has the flag always enabled console sink has the flag always disabled * all other sinks have the flag always enabled \u00b6 NOTE: userSpecifiedName corresponds to queryName option (that can be defined using DataStreamWriter 's queryName method) while userSpecifiedCheckpointLocation is checkpointLocation option. NOTE: createQuery is used when StreamingQueryManager is requested to start a streaming query (when DataStreamWriter is requested to start an execution of a streaming query ). === [[startQuery]] Starting Streaming Query Execution -- startQuery Internal Method [source, scala] \u00b6 startQuery( userSpecifiedName: Option[String], userSpecifiedCheckpointLocation: Option[String], df: DataFrame, extraOptions: Map[String, String], sink: BaseStreamingSink, outputMode: OutputMode, useTempCheckpointLocation: Boolean = false, recoverFromCheckpointLocation: Boolean = true, trigger: Trigger = ProcessingTime(0), triggerClock: Clock = new SystemClock()): StreamingQuery startQuery starts a StreamingQuery.md[streaming query] and returns a handle to it. NOTE: trigger defaults to 0 milliseconds (as spark-sql-streaming-Trigger.md#ProcessingTime[ProcessingTime(0)]). Internally, startQuery first < >, registers it in < > internal registry (by the id ), requests it for the underlying < > and starts it . In the end, startQuery returns the < > (as part of the fluent API so you can chain operators) or throws the exception that was reported when attempting to start the query. startQuery throws an IllegalArgumentException when there is another query registered under name . startQuery looks it up in the < > internal registry. Cannot start query with name [name] as a query with that name is already active startQuery throws an IllegalStateException when a query is started again from checkpoint. startQuery looks it up in < > internal registry. Cannot start query with id [id] as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active. startQuery is used when DataStreamWriter is requested to start an execution of the streaming query . === [[postListenerEvent]] Posting StreamingQueryListener Event to StreamingQueryListenerBus -- postListenerEvent Internal Method [source, scala] \u00b6 postListenerEvent(event: StreamingQueryListener.Event): Unit \u00b6 postListenerEvent simply posts the input event to the internal < >. postListenerEvent is used when StreamExecution is requested to post a streaming event . === [[notifyQueryTermination]] Handling Termination of Streaming Query (and Deactivating Query in StateStoreCoordinator) -- notifyQueryTermination Internal Method [source, scala] \u00b6 notifyQueryTermination(terminatedQuery: StreamingQuery): Unit \u00b6 notifyQueryTermination removes the terminatedQuery from < > internal registry (by the StreamingQuery.md#id[query id]). notifyQueryTermination records the terminatedQuery in < > internal registry (when no earlier streaming query was recorded or the terminatedQuery terminated due to an exception). notifyQueryTermination notifies others that are blocked on < >. In the end, notifyQueryTermination requests < > to spark-sql-streaming-StateStoreCoordinatorRef.md#deactivateInstances[deactivate all active runs of the streaming query]. .StreamingQueryManager's Marking Streaming Query as Terminated image::images/StreamingQueryManager-notifyQueryTermination.png[align=\"center\"] notifyQueryTermination is used when StreamExecution is requested to run a streaming query and the query has finished (running streaming batches) (with or without an exception). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | activeQueries | [[activeQueries]] Registry of < > per UUID Used when StreamingQueryManager is requested for < >, < >, < > and < >. | activeQueriesLock | [[activeQueriesLock]] | awaitTerminationLock | [[awaitTerminationLock]] | lastTerminatedQuery a| [[lastTerminatedQuery]] < > that has recently been terminated, i.e. StreamingQuery.md#stop[stopped] or StreamingQuery.md#exception[due to an exception]. null when no streaming query has terminated yet or < >. Used in < > to know when a streaming query has terminated Set when StreamingQueryManager < > | stateStoreCoordinator a| [[stateStoreCoordinator]] < > to the StateStoreCoordinator RPC Endpoint spark-sql-streaming-StateStoreCoordinatorRef.md#forDriver[Created] when StreamingQueryManager is < > Used when: StreamingQueryManager < > Stateful operators are executed, i.e. FlatMapGroupsWithStateExec , spark-sql-streaming-StateStoreRestoreExec.md#doExecute[StateStoreRestoreExec], StateStoreSaveExec.md#doExecute[StateStoreSaveExec], physical-operators/StreamingDeduplicateExec.md#doExecute[StreamingDeduplicateExec] and physical-operators/StreamingSymmetricHashJoinExec.md#doExecute[StreamingSymmetricHashJoinExec] spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[Creating StateStoreRDD (with storeUpdateFunction aborting StateStore when a task fails)] |===","title":"StreamingQueryManager"},{"location":"StreamingQueryManager/#streamingquerymanager-streaming-query-management","text":"StreamingQueryManager is the < > for < > of a < >. [[methods]] .StreamingQueryManager API [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | < > a|","title":"StreamingQueryManager &mdash; Streaming Query Management"},{"location":"StreamingQueryManager/#source-scala","text":"","title":"[source, scala]"},{"location":"StreamingQueryManager/#active-arraystreamingquery","text":"Active < > | < > a|","title":"active: Array[StreamingQuery]"},{"location":"StreamingQueryManager/#source-scala_1","text":"","title":"[source, scala]"},{"location":"StreamingQueryManager/#addlistenerlistener-streamingquerylistener-unit","text":"Registers ( adds ) a StreamingQueryListener | < > a|","title":"addListener(listener: StreamingQueryListener): Unit"},{"location":"StreamingQueryManager/#source-scala_2","text":"awaitAnyTermination(): Unit awaitAnyTermination(timeoutMs: Long): Boolean Waits until any streaming query terminats or timeoutMs elapses | < > a|","title":"[source, scala]"},{"location":"StreamingQueryManager/#source-scala_3","text":"get(id: String): StreamingQuery get(id: UUID): StreamingQuery Gets the < > by < > | < > a|","title":"[source, scala]"},{"location":"StreamingQueryManager/#source-scala_4","text":"removeListener( listener: StreamingQueryListener): Unit De-registers ( removes ) the StreamingQueryListener | < > a|","title":"[source, scala]"},{"location":"StreamingQueryManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"StreamingQueryManager/#resetterminated-unit","text":"Resets the internal registry of the terminated streaming queries (that lets < > to be used again) |=== StreamingQueryManager is available using SparkSession.streams property.","title":"resetTerminated(): Unit"},{"location":"StreamingQueryManager/#source-scala_6","text":"scala> :type spark org.apache.spark.sql.SparkSession scala> :type spark.streams org.apache.spark.sql.streaming.StreamingQueryManager StreamingQueryManager is < > when SessionState is created. .StreamingQueryManager image::images/StreamingQueryManager.png[align=\"center\"] TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SessionState.html[SessionState ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] gitbook. StreamingQueryManager is used (internally) to < >. .StreamingQueryManager Creates StreamingQuery (and StreamExecution) image::images/StreamingQueryManager-createQuery.png[align=\"center\"] StreamingQueryManager is < >. [[creating-instance]][[sparkSession]] StreamingQueryManager takes a single SparkSession when created. === [[listenerBus]] StreamingQueryListenerBus -- listenerBus Internal Property","title":"[source, scala]"},{"location":"StreamingQueryManager/#source-scala_7","text":"","title":"[source, scala]"},{"location":"StreamingQueryManager/#listenerbus-streamingquerylistenerbus","text":"listenerBus is a StreamingQueryListenerBus (for the current < >) that is created immediately when StreamingQueryManager is < >. listenerBus is used for the following: < > or < > a given StreamingQueryListener < > (and notify < >) === [[active]] Getting All Active Streaming Queries -- active Method","title":"listenerBus: StreamingQueryListenerBus"},{"location":"StreamingQueryManager/#source-scala_8","text":"","title":"[source, scala]"},{"location":"StreamingQueryManager/#active-arraystreamingquery_1","text":"active gets < >. === [[get]] Getting Active Continuous Query By Name -- get Method","title":"active: Array[StreamingQuery]"},{"location":"StreamingQueryManager/#source-scala_9","text":"","title":"[source, scala]"},{"location":"StreamingQueryManager/#getname-string-streamingquery","text":"get method returns a StreamingQuery.md[StreamingQuery] by name . It may throw an IllegalArgumentException when no StreamingQuery exists for the name . java.lang.IllegalArgumentException: There is no active query with name hello at org.apache.spark.sql.StreamingQueryManager$$anonfun$get$1.apply(StreamingQueryManager.scala:59) at org.apache.spark.sql.StreamingQueryManager$$anonfun$get$1.apply(StreamingQueryManager.scala:59) at scala.collection.MapLike$class.getOrElse(MapLike.scala:128) at scala.collection.AbstractMap.getOrElse(Map.scala:59) at org.apache.spark.sql.StreamingQueryManager.get(StreamingQueryManager.scala:58) ... 49 elided","title":"get(name: String): StreamingQuery"},{"location":"StreamingQueryManager/#registering-streamingquerylistener","text":"addListener ( listener : StreamingQueryListener ) : Unit addListener requests the StreamingQueryListenerBus to add the input StreamingQueryListener .","title":" Registering StreamingQueryListener"},{"location":"StreamingQueryManager/#de-registering-streamingquerylistener","text":"removeListener ( listener : StreamingQueryListener ) : Unit removeListener requests StreamingQueryListenerBus to remove the input StreamingQueryListener . === [[awaitAnyTermination]] Waiting for Any Streaming Query Termination -- awaitAnyTermination Method","title":" De-Registering StreamingQueryListener"},{"location":"StreamingQueryManager/#source-scala_10","text":"awaitAnyTermination(): Unit awaitAnyTermination(timeoutMs: Long): Boolean awaitAnyTermination acquires a lock on < > and waits until any streaming query has finished (i.e. < > is available) or timeoutMs has expired. awaitAnyTermination re-throws the StreamingQueryException from < > if StreamingQuery.md#exception[it reported one]. === [[resetTerminated]] resetTerminated Method","title":"[source, scala]"},{"location":"StreamingQueryManager/#source-scala_11","text":"","title":"[source, scala]"},{"location":"StreamingQueryManager/#resetterminated-unit_1","text":"resetTerminated forgets about the past-terminated query (so that < > can be used again to wait for a new streaming query termination). Internally, resetTerminated acquires a lock on < > and simply resets < > (i.e. sets it to null ). === [[createQuery]] Creating Streaming Query -- createQuery Internal Method","title":"resetTerminated(): Unit"},{"location":"StreamingQueryManager/#source-scala_12","text":"createQuery( userSpecifiedName: Option[String], userSpecifiedCheckpointLocation: Option[String], df: DataFrame, extraOptions: Map[String, String], sink: BaseStreamingSink, outputMode: OutputMode, useTempCheckpointLocation: Boolean, recoverFromCheckpointLocation: Boolean, trigger: Trigger, triggerClock: Clock): StreamingQueryWrapper createQuery creates a spark-sql-streaming-StreamingQueryWrapper.md#creating-instance[StreamingQueryWrapper] (for a StreamExecution.md#creating-instance[StreamExecution] per the input user-defined properties). Internally, createQuery first finds the name of the checkpoint directory of a query (aka checkpoint location ) in the following order: . Exactly the input userSpecifiedCheckpointLocation if defined . spark-sql-streaming-properties.md#spark.sql.streaming.checkpointLocation[spark.sql.streaming.checkpointLocation] Spark property if defined for the parent directory with a subdirectory per the optional userSpecifiedName (or a randomly-generated UUID) . (only when useTempCheckpointLocation is enabled) A temporary directory (as specified by java.io.tmpdir JVM property) with a subdirectory with temporary prefix. NOTE: userSpecifiedCheckpointLocation can be any path that is acceptable by Hadoop's https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/Path.html[Path ]. If the directory name for the checkpoint location could not be found, createQuery reports a AnalysisException . checkpointLocation must be specified either through option(\"checkpointLocation\", ...) or SparkSession.conf.set(\"spark.sql.streaming.checkpointLocation\", ...) createQuery reports a AnalysisException when the input recoverFromCheckpointLocation flag is turned off but there is offsets directory in the checkpoint location. createQuery makes sure that the logical plan of the structured query is analyzed (i.e. no logical errors have been found). Unless spark-sql-streaming-properties.md#spark.sql.streaming.unsupportedOperationCheck[spark.sql.streaming.unsupportedOperationCheck] Spark property is turned on, createQuery spark-sql-streaming-UnsupportedOperationChecker.md#checkForStreaming[checks the logical plan of the streaming query for unsupported operations]. (only when spark.sql.adaptive.enabled Spark property is turned on) createQuery prints out a WARN message to the logs: WARN spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled. In the end, createQuery creates a spark-sql-streaming-StreamingQueryWrapper.md#creating-instance[StreamingQueryWrapper] with a new < >.","title":"[source, scala]"},{"location":"StreamingQueryManager/#note","text":"recoverFromCheckpointLocation flag corresponds to recoverFromCheckpointLocation flag that StreamingQueryManager uses to < > and which is enabled by default (and is in fact the only place where createQuery is used). memory sink has the flag enabled for spark-sql-streaming-OutputMode.md#Complete[Complete] output mode only foreach sink has the flag always enabled console sink has the flag always disabled","title":"[NOTE]"},{"location":"StreamingQueryManager/#all-other-sinks-have-the-flag-always-enabled","text":"NOTE: userSpecifiedName corresponds to queryName option (that can be defined using DataStreamWriter 's queryName method) while userSpecifiedCheckpointLocation is checkpointLocation option. NOTE: createQuery is used when StreamingQueryManager is requested to start a streaming query (when DataStreamWriter is requested to start an execution of a streaming query ). === [[startQuery]] Starting Streaming Query Execution -- startQuery Internal Method","title":"* all other sinks have the flag always enabled"},{"location":"StreamingQueryManager/#source-scala_13","text":"startQuery( userSpecifiedName: Option[String], userSpecifiedCheckpointLocation: Option[String], df: DataFrame, extraOptions: Map[String, String], sink: BaseStreamingSink, outputMode: OutputMode, useTempCheckpointLocation: Boolean = false, recoverFromCheckpointLocation: Boolean = true, trigger: Trigger = ProcessingTime(0), triggerClock: Clock = new SystemClock()): StreamingQuery startQuery starts a StreamingQuery.md[streaming query] and returns a handle to it. NOTE: trigger defaults to 0 milliseconds (as spark-sql-streaming-Trigger.md#ProcessingTime[ProcessingTime(0)]). Internally, startQuery first < >, registers it in < > internal registry (by the id ), requests it for the underlying < > and starts it . In the end, startQuery returns the < > (as part of the fluent API so you can chain operators) or throws the exception that was reported when attempting to start the query. startQuery throws an IllegalArgumentException when there is another query registered under name . startQuery looks it up in the < > internal registry. Cannot start query with name [name] as a query with that name is already active startQuery throws an IllegalStateException when a query is started again from checkpoint. startQuery looks it up in < > internal registry. Cannot start query with id [id] as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active. startQuery is used when DataStreamWriter is requested to start an execution of the streaming query . === [[postListenerEvent]] Posting StreamingQueryListener Event to StreamingQueryListenerBus -- postListenerEvent Internal Method","title":"[source, scala]"},{"location":"StreamingQueryManager/#source-scala_14","text":"","title":"[source, scala]"},{"location":"StreamingQueryManager/#postlistenereventevent-streamingquerylistenerevent-unit","text":"postListenerEvent simply posts the input event to the internal < >. postListenerEvent is used when StreamExecution is requested to post a streaming event . === [[notifyQueryTermination]] Handling Termination of Streaming Query (and Deactivating Query in StateStoreCoordinator) -- notifyQueryTermination Internal Method","title":"postListenerEvent(event: StreamingQueryListener.Event): Unit"},{"location":"StreamingQueryManager/#source-scala_15","text":"","title":"[source, scala]"},{"location":"StreamingQueryManager/#notifyqueryterminationterminatedquery-streamingquery-unit","text":"notifyQueryTermination removes the terminatedQuery from < > internal registry (by the StreamingQuery.md#id[query id]). notifyQueryTermination records the terminatedQuery in < > internal registry (when no earlier streaming query was recorded or the terminatedQuery terminated due to an exception). notifyQueryTermination notifies others that are blocked on < >. In the end, notifyQueryTermination requests < > to spark-sql-streaming-StateStoreCoordinatorRef.md#deactivateInstances[deactivate all active runs of the streaming query]. .StreamingQueryManager's Marking Streaming Query as Terminated image::images/StreamingQueryManager-notifyQueryTermination.png[align=\"center\"] notifyQueryTermination is used when StreamExecution is requested to run a streaming query and the query has finished (running streaming batches) (with or without an exception). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | activeQueries | [[activeQueries]] Registry of < > per UUID Used when StreamingQueryManager is requested for < >, < >, < > and < >. | activeQueriesLock | [[activeQueriesLock]] | awaitTerminationLock | [[awaitTerminationLock]] | lastTerminatedQuery a| [[lastTerminatedQuery]] < > that has recently been terminated, i.e. StreamingQuery.md#stop[stopped] or StreamingQuery.md#exception[due to an exception]. null when no streaming query has terminated yet or < >. Used in < > to know when a streaming query has terminated Set when StreamingQueryManager < > | stateStoreCoordinator a| [[stateStoreCoordinator]] < > to the StateStoreCoordinator RPC Endpoint spark-sql-streaming-StateStoreCoordinatorRef.md#forDriver[Created] when StreamingQueryManager is < > Used when: StreamingQueryManager < > Stateful operators are executed, i.e. FlatMapGroupsWithStateExec , spark-sql-streaming-StateStoreRestoreExec.md#doExecute[StateStoreRestoreExec], StateStoreSaveExec.md#doExecute[StateStoreSaveExec], physical-operators/StreamingDeduplicateExec.md#doExecute[StreamingDeduplicateExec] and physical-operators/StreamingSymmetricHashJoinExec.md#doExecute[StreamingSymmetricHashJoinExec] spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[Creating StateStoreRDD (with storeUpdateFunction aborting StateStore when a task fails)] |===","title":"notifyQueryTermination(terminatedQuery: StreamingQuery): Unit"},{"location":"arbitrary-stateful-streaming-aggregation/","text":"Arbitrary Stateful Streaming Aggregation \u00b6 Arbitrary Stateful Streaming Aggregation is a streaming aggregation query that uses the following KeyValueGroupedDataset operators: mapGroupsWithState for implicit state logic flatMapGroupsWithState for explicit state logic KeyValueGroupedDataset represents a grouped dataset as a result of Dataset.groupByKey operator. mapGroupsWithState and flatMapGroupsWithState operators use GroupState as group streaming aggregation state that is created separately for every aggregation key with an aggregation state value (of a user-defined type). mapGroupsWithState and flatMapGroupsWithState operators use GroupStateTimeout as an aggregation state timeout that defines when a GroupState is considered timed-out ( expired ). Demos \u00b6 Use the following demos and complete applications to learn more: Demo: Internals of FlatMapGroupsWithStateExec Physical Operator Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator groupByKey Streaming Aggregation in Update Mode FlatMapGroupsWithStateApp Performance Metrics \u00b6 Arbitrary Stateful Streaming Aggregation uses performance metrics (of the StateStoreWriter through FlatMapGroupsWithStateExec physical operator). Internals \u00b6 One of the most important internal execution components of Arbitrary Stateful Streaming Aggregation is FlatMapGroupsWithStateExec physical operator. When executed, FlatMapGroupsWithStateExec first validates a selected GroupStateTimeout : For ProcessingTimeTimeout , batch timeout threshold has to be defined For EventTimeTimeout , event-time watermark has to be defined and the input schema has the watermark attribute Note FIXME When are the above requirements met? FlatMapGroupsWithStateExec physical operator then mapPartitionsWithStateStore with a custom storeUpdateFunction of the following signature: ( StateStore , Iterator [ T ]) => Iterator [ U ] While generating the recipe, FlatMapGroupsWithStateExec uses StateStoreOps extension method object to register a listener that is executed on a task completion. The listener makes sure that a given StateStore has all state changes either committed or aborted . In the end, FlatMapGroupsWithStateExec creates a new StateStoreRDD and adds it to the RDD lineage. StateStoreRDD is used to properly distribute tasks across executors (per preferred locations ) with help of StateStoreCoordinator (that runs on the driver). StateStoreRDD uses StateStore helper to look up a StateStore by StateStoreProviderId and store version. FlatMapGroupsWithStateExec physical operator uses state managers that are different than state managers for Streaming Aggregation . StateStore abstraction is the same as in Streaming Aggregation . One of the important execution steps is when InputProcessor (of FlatMapGroupsWithStateExec physical operator) is requested to callFunctionAndUpdateState . That executes the user-defined state function on a per-group state key object, value objects, and a GroupStateImpl .","title":"Arbitrary Stateful Streaming Aggregation"},{"location":"arbitrary-stateful-streaming-aggregation/#arbitrary-stateful-streaming-aggregation","text":"Arbitrary Stateful Streaming Aggregation is a streaming aggregation query that uses the following KeyValueGroupedDataset operators: mapGroupsWithState for implicit state logic flatMapGroupsWithState for explicit state logic KeyValueGroupedDataset represents a grouped dataset as a result of Dataset.groupByKey operator. mapGroupsWithState and flatMapGroupsWithState operators use GroupState as group streaming aggregation state that is created separately for every aggregation key with an aggregation state value (of a user-defined type). mapGroupsWithState and flatMapGroupsWithState operators use GroupStateTimeout as an aggregation state timeout that defines when a GroupState is considered timed-out ( expired ).","title":"Arbitrary Stateful Streaming Aggregation"},{"location":"arbitrary-stateful-streaming-aggregation/#demos","text":"Use the following demos and complete applications to learn more: Demo: Internals of FlatMapGroupsWithStateExec Physical Operator Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator groupByKey Streaming Aggregation in Update Mode FlatMapGroupsWithStateApp","title":"Demos"},{"location":"arbitrary-stateful-streaming-aggregation/#performance-metrics","text":"Arbitrary Stateful Streaming Aggregation uses performance metrics (of the StateStoreWriter through FlatMapGroupsWithStateExec physical operator).","title":" Performance Metrics"},{"location":"arbitrary-stateful-streaming-aggregation/#internals","text":"One of the most important internal execution components of Arbitrary Stateful Streaming Aggregation is FlatMapGroupsWithStateExec physical operator. When executed, FlatMapGroupsWithStateExec first validates a selected GroupStateTimeout : For ProcessingTimeTimeout , batch timeout threshold has to be defined For EventTimeTimeout , event-time watermark has to be defined and the input schema has the watermark attribute Note FIXME When are the above requirements met? FlatMapGroupsWithStateExec physical operator then mapPartitionsWithStateStore with a custom storeUpdateFunction of the following signature: ( StateStore , Iterator [ T ]) => Iterator [ U ] While generating the recipe, FlatMapGroupsWithStateExec uses StateStoreOps extension method object to register a listener that is executed on a task completion. The listener makes sure that a given StateStore has all state changes either committed or aborted . In the end, FlatMapGroupsWithStateExec creates a new StateStoreRDD and adds it to the RDD lineage. StateStoreRDD is used to properly distribute tasks across executors (per preferred locations ) with help of StateStoreCoordinator (that runs on the driver). StateStoreRDD uses StateStore helper to look up a StateStore by StateStoreProviderId and store version. FlatMapGroupsWithStateExec physical operator uses state managers that are different than state managers for Streaming Aggregation . StateStore abstraction is the same as in Streaming Aggregation . One of the important execution steps is when InputProcessor (of FlatMapGroupsWithStateExec physical operator) is requested to callFunctionAndUpdateState . That executes the user-defined state function on a per-group state key object, value objects, and a GroupStateImpl .","title":" Internals"},{"location":"logging/","text":"== [[Logging]] Logging CAUTION: FIXME","title":"Logging"},{"location":"micro-batch-stream-processing/","text":"== Micro-Batch Stream Processing (Structured Streaming V1) Micro-Batch Stream Processing is a stream processing model in Spark Structured Streaming that is used for streaming queries with < > and < > triggers. Micro-batch stream processing uses < > stream execution engine. Micro-batch stream processing supports < > data sources. Micro-batch stream processing is often referred to as Structured Streaming V1 . [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.ProcessingTime(1.minute)) // \u2190 Uses MicroBatchExecution for execution .queryName(\"rate2console\") .start assert(sq.isActive) scala> sq.explain == Physical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@678e6267 +- *(1) Project [timestamp#54, value#55L] +- *(1) ScanV2 rate[timestamp#54, value#55L] // sq.stop \u00b6 === [[execution-phases]] Execution Phases (Processing Cycle) Once < > stream processing engine is requested to < >, the query execution goes through the following execution phases every trigger: . [[triggerExecution]] < > . < > for Sources or < > for < > . < > . < > . < > . < > . < > Execution phases with execution times are available using < > under durationMs . scala> :type sq org.apache.spark.sql.streaming.StreamingQuery sq.lastProgress.durationMs.get(\"walCommit\") Tip Enable INFO logging level for StreamExecution logger to be notified about durations. 17/08/11 09:04:17 INFO StreamExecution: Streaming query made progress: { \"id\" : \"ec8f8228-90f6-4e1f-8ad2-80222affed63\", \"runId\" : \"f605c134-cfb0-4378-88c1-159d8a7c232e\", \"name\" : \"rates-to-console\", \"timestamp\" : \"2017-08-11T07:04:17.373Z\", \"batchId\" : 0, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { // <-- Durations (in millis) \"addBatch\" : 38, \"getBatch\" : 1, \"getOffset\" : 0, \"queryPlanning\" : 1, \"triggerExecution\" : 62, \"walCommit\" : 19 }, Monitoring \u00b6 MicroBatchExecution posts events to announce when a streaming query is started and stopped as well as after every micro-batch. < > interface can be used to intercept the events and act accordingly. After < > MicroBatchExecution is requested to finish up a streaming batch (trigger) and generate a StreamingQueryProgress (with execution statistics). MicroBatchExecution prints out the following DEBUG message to the logs: Execution stats: [executionStats] MicroBatchExecution posts a QueryProgressEvent with the StreamingQueryProgress and prints out the following INFO message to the logs: Streaming query made progress: [newProgress]","title":"Micro-Batch Stream Processing"},{"location":"micro-batch-stream-processing/#source-scala","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.ProcessingTime(1.minute)) // \u2190 Uses MicroBatchExecution for execution .queryName(\"rate2console\") .start assert(sq.isActive) scala> sq.explain == Physical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@678e6267 +- *(1) Project [timestamp#54, value#55L] +- *(1) ScanV2 rate[timestamp#54, value#55L]","title":"[source, scala]"},{"location":"micro-batch-stream-processing/#sqstop","text":"=== [[execution-phases]] Execution Phases (Processing Cycle) Once < > stream processing engine is requested to < >, the query execution goes through the following execution phases every trigger: . [[triggerExecution]] < > . < > for Sources or < > for < > . < > . < > . < > . < > . < > Execution phases with execution times are available using < > under durationMs . scala> :type sq org.apache.spark.sql.streaming.StreamingQuery sq.lastProgress.durationMs.get(\"walCommit\") Tip Enable INFO logging level for StreamExecution logger to be notified about durations. 17/08/11 09:04:17 INFO StreamExecution: Streaming query made progress: { \"id\" : \"ec8f8228-90f6-4e1f-8ad2-80222affed63\", \"runId\" : \"f605c134-cfb0-4378-88c1-159d8a7c232e\", \"name\" : \"rates-to-console\", \"timestamp\" : \"2017-08-11T07:04:17.373Z\", \"batchId\" : 0, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { // <-- Durations (in millis) \"addBatch\" : 38, \"getBatch\" : 1, \"getOffset\" : 0, \"queryPlanning\" : 1, \"triggerExecution\" : 62, \"walCommit\" : 19 },","title":"// sq.stop"},{"location":"micro-batch-stream-processing/#monitoring","text":"MicroBatchExecution posts events to announce when a streaming query is started and stopped as well as after every micro-batch. < > interface can be used to intercept the events and act accordingly. After < > MicroBatchExecution is requested to finish up a streaming batch (trigger) and generate a StreamingQueryProgress (with execution statistics). MicroBatchExecution prints out the following DEBUG message to the logs: Execution stats: [executionStats] MicroBatchExecution posts a QueryProgressEvent with the StreamingQueryProgress and prints out the following INFO message to the logs: Streaming query made progress: [newProgress]","title":"Monitoring"},{"location":"spark-sql-streaming-BaseStreamingSink/","text":"== [[BaseStreamingSink]] BaseStreamingSink Contract -- Base of Streaming Writers and Sinks BaseStreamingSink is the abstraction of < > with the only purpose of sharing a common abstraction between the former Data Source API V1 (< >) and the modern Data Source API V2 (until Spark Structured Streaming migrates to the Data Source API V2 fully). BaseStreamingSink defines no methods. [[extensions]] .BaseStreamingSinks (Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | BaseStreamingSink | Description | < > | [[MemorySinkBase]] Base contract for data sinks in < > | < > | [[Sink]] Streaming sinks for < > (based on Data Source API V1) | < > | [[StreamWriteSupport]] Data source writers (based on Data Source API V2) |===","title":"BaseStreamingSink"},{"location":"spark-sql-streaming-BaseStreamingSource/","text":"== [[BaseStreamingSource]] BaseStreamingSource Contract -- Base of Streaming Readers and Sources BaseStreamingSource is the < > of < > that can be < >. The main purpose of BaseStreamingSource is to share a common abstraction between the former Data Source API V1 (< >) and the modern Data Source API V2 (until Spark Structured Streaming migrates to the Data Source API V2 fully). [[contract]] .BaseStreamingSource Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | stop a| [[stop]] [source, java] \u00b6 void stop() \u00b6 Stops the streaming source or reader (and frees up any resources it may have allocated) Used when: StreamExecution is requested to stop streaming sources and readers DataStreamReader is requested to < > (for read schema) |=== [[extensions]] .BaseStreamingSources (Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | BaseStreamingSource | Description | < > | [[ContinuousReader]] Data source readers in < > (based on Data Source API V2) | < > | [[MemoryStreamBase]] Base implementation of < > (for < >) and < > (for < >) | < > | [[MicroBatchReader]] Data source readers in < > (based on Data Source API V2) | Source | [[Source]] Streaming sources for < > (based on Data Source API V1) |===","title":"BaseStreamingSource"},{"location":"spark-sql-streaming-BaseStreamingSource/#source-java","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-BaseStreamingSource/#void-stop","text":"Stops the streaming source or reader (and frees up any resources it may have allocated) Used when: StreamExecution is requested to stop streaming sources and readers DataStreamReader is requested to < > (for read schema) |=== [[extensions]] .BaseStreamingSources (Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | BaseStreamingSource | Description | < > | [[ContinuousReader]] Data source readers in < > (based on Data Source API V2) | < > | [[MemoryStreamBase]] Base implementation of < > (for < >) and < > (for < >) | < > | [[MicroBatchReader]] Data source readers in < > (based on Data Source API V2) | Source | [[Source]] Streaming sources for < > (based on Data Source API V1) |===","title":"void stop()"},{"location":"spark-sql-streaming-CachedKafkaConsumer/","text":"== [[CachedKafkaConsumer]] CachedKafkaConsumer CAUTION: FIXME === [[poll]] poll Internal Method CAUTION: FIXME === [[fetchData]] fetchData Internal Method CAUTION: FIXME","title":"CachedKafkaConsumer"},{"location":"spark-sql-streaming-CheckpointFileManager/","text":"== [[CheckpointFileManager]] CheckpointFileManager Contract CheckpointFileManager is the < > of < > that manage checkpoint files (metadata of streaming batches) on Hadoop DFS-compatible file systems. CheckpointFileManager is < > per < > configuration property if defined before reverting to the available < >. CheckpointFileManager is used exclusively by < >, < > and < >. [[contract]] .CheckpointFileManager Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | createAtomic a| [[createAtomic]] [source, scala] \u00b6 createAtomic( path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream Used when: HDFSMetadataLog is requested to < > (that < >) StreamMetadata helper object is requested to < > HDFSBackedStateStore is requested for the < > HDFSBackedStateStoreProvider is requested to < > | delete a| [[delete]] [source, scala] \u00b6 delete(path: Path): Unit \u00b6 Deletes the given path recursively (if exists) Used when: RenameBasedFSDataOutputStream is requested to cancel CompactibleFileStreamLog is requested to < > (that < >) HDFSMetadataLog is requested to < > and < > HDFSBackedStateStoreProvider is requested to < > (that < >) | exists a| [[exists]] [source, scala] \u00b6 exists(path: Path): Boolean \u00b6 Used when HDFSMetadataLog is < > (to create the < >) and requested for < > | isLocal a| [[isLocal]] [source, scala] \u00b6 isLocal: Boolean \u00b6 Does not seem to be used. | list a| [[list]] [source, scala] \u00b6 list( path: Path): Array[FileStatus] // <1> list( path: Path, filter: PathFilter): Array[FileStatus] <1> Uses PathFilter that accepts all files in the path Lists all files in the given path Used when: HDFSBackedStateStoreProvider is requested for < > CompactibleFileStreamLog is requested for the < > and to < > HDFSMetadataLog is requested for < >, the < >, < >, to < > and < > | mkdirs a| [[mkdirs]] [source, scala] \u00b6 mkdirs(path: Path): Unit \u00b6 Used when: HDFSMetadataLog is < > HDFSBackedStateStoreProvider is requested to < > | open a| [[open]] [source, scala] \u00b6 open(path: Path): FSDataInputStream \u00b6 Opens a file (by the given path) for reading Used when: HDFSMetadataLog is requested for < > HDFSBackedStateStoreProvider is requested to < > (that < >), and < > |=== [[implementations]] .CheckpointFileManagers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | CheckpointFileManager | Description | < > | [[FileContextBasedCheckpointFileManager]] Default CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileContext.html[FileContext ] API for managing checkpoint files (unless < >) | < > | [[FileSystemBasedCheckpointFileManager]] Basic CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] API for managing checkpoint files (that < > that the implementation of FileSystem.rename() is atomic or the correctness and fault-tolerance of Structured Streaming is not guaranteed) |=== === [[create]] Creating CheckpointFileManager Instance -- create Object Method [source, scala] \u00b6 create( path: Path, hadoopConf: Configuration): CheckpointFileManager create finds < > configuration property in the hadoopConf configuration. If found, create simply instantiates whatever CheckpointFileManager implementation is defined. If not found, create creates a < >. In case of UnsupportedFileSystemException , create prints out the following WARN message to the logs and creates ( falls back on ) a < >. Could not use FileContext API for managing Structured Streaming checkpoint files at [path]. Using FileSystem API instead for managing log files. If the implementation of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of your Structured Streaming is not guaranteed. create is used when: HDFSMetadataLog is created StreamMetadata utility is used to write metadata to a file (when StreamExecution is created) HDFSBackedStateStoreProvider is requested for a CheckpointFileManager","title":"CheckpointFileManager"},{"location":"spark-sql-streaming-CheckpointFileManager/#source-scala","text":"createAtomic( path: Path, overwriteIfPossible: Boolean): CancellableFSDataOutputStream Used when: HDFSMetadataLog is requested to < > (that < >) StreamMetadata helper object is requested to < > HDFSBackedStateStore is requested for the < > HDFSBackedStateStoreProvider is requested to < > | delete a| [[delete]]","title":"[source, scala]"},{"location":"spark-sql-streaming-CheckpointFileManager/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CheckpointFileManager/#deletepath-path-unit","text":"Deletes the given path recursively (if exists) Used when: RenameBasedFSDataOutputStream is requested to cancel CompactibleFileStreamLog is requested to < > (that < >) HDFSMetadataLog is requested to < > and < > HDFSBackedStateStoreProvider is requested to < > (that < >) | exists a| [[exists]]","title":"delete(path: Path): Unit"},{"location":"spark-sql-streaming-CheckpointFileManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CheckpointFileManager/#existspath-path-boolean","text":"Used when HDFSMetadataLog is < > (to create the < >) and requested for < > | isLocal a| [[isLocal]]","title":"exists(path: Path): Boolean"},{"location":"spark-sql-streaming-CheckpointFileManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CheckpointFileManager/#islocal-boolean","text":"Does not seem to be used. | list a| [[list]]","title":"isLocal: Boolean"},{"location":"spark-sql-streaming-CheckpointFileManager/#source-scala_4","text":"list( path: Path): Array[FileStatus] // <1> list( path: Path, filter: PathFilter): Array[FileStatus] <1> Uses PathFilter that accepts all files in the path Lists all files in the given path Used when: HDFSBackedStateStoreProvider is requested for < > CompactibleFileStreamLog is requested for the < > and to < > HDFSMetadataLog is requested for < >, the < >, < >, to < > and < > | mkdirs a| [[mkdirs]]","title":"[source, scala]"},{"location":"spark-sql-streaming-CheckpointFileManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CheckpointFileManager/#mkdirspath-path-unit","text":"Used when: HDFSMetadataLog is < > HDFSBackedStateStoreProvider is requested to < > | open a| [[open]]","title":"mkdirs(path: Path): Unit"},{"location":"spark-sql-streaming-CheckpointFileManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CheckpointFileManager/#openpath-path-fsdatainputstream","text":"Opens a file (by the given path) for reading Used when: HDFSMetadataLog is requested for < > HDFSBackedStateStoreProvider is requested to < > (that < >), and < > |=== [[implementations]] .CheckpointFileManagers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | CheckpointFileManager | Description | < > | [[FileContextBasedCheckpointFileManager]] Default CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileContext.html[FileContext ] API for managing checkpoint files (unless < >) | < > | [[FileSystemBasedCheckpointFileManager]] Basic CheckpointFileManager that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] API for managing checkpoint files (that < > that the implementation of FileSystem.rename() is atomic or the correctness and fault-tolerance of Structured Streaming is not guaranteed) |=== === [[create]] Creating CheckpointFileManager Instance -- create Object Method","title":"open(path: Path): FSDataInputStream"},{"location":"spark-sql-streaming-CheckpointFileManager/#source-scala_7","text":"create( path: Path, hadoopConf: Configuration): CheckpointFileManager create finds < > configuration property in the hadoopConf configuration. If found, create simply instantiates whatever CheckpointFileManager implementation is defined. If not found, create creates a < >. In case of UnsupportedFileSystemException , create prints out the following WARN message to the logs and creates ( falls back on ) a < >. Could not use FileContext API for managing Structured Streaming checkpoint files at [path]. Using FileSystem API instead for managing log files. If the implementation of FileSystem.rename() is not atomic, then the correctness and fault-tolerance of your Structured Streaming is not guaranteed. create is used when: HDFSMetadataLog is created StreamMetadata utility is used to write metadata to a file (when StreamExecution is created) HDFSBackedStateStoreProvider is requested for a CheckpointFileManager","title":"[source, scala]"},{"location":"spark-sql-streaming-CommitLog/","text":"CommitLog \u2014 HDFSMetadataLog for Offset Commit Log \u00b6 CommitLog is an < > with < > metadata. CommitLog is < > exclusively for the offset commit log (of StreamExecution ). [[CommitMetadata]][[nextBatchWatermarkMs]] CommitLog uses CommitMetadata for the metadata with nextBatchWatermarkMs attribute (of type Long and the default 0 ). CommitLog < > commit metadata to files with names that are offsets. $ ls -tr [checkpoint-directory]/commits 0 1 2 3 4 5 6 7 8 9 $ cat [checkpoint-directory]/commits/8 v1 {\"nextBatchWatermarkMs\": 0} [[VERSION]] CommitLog uses 1 for the version. [[creating-instance]] CommitLog (like the parent < >) takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[serialize]] Serializing Metadata (Writing Metadata to Persistent Storage) -- serialize Method [source, scala] \u00b6 serialize( metadata: CommitMetadata, out: OutputStream): Unit NOTE: serialize is part of < > to write a metadata in serialized format. serialize writes out the < > prefixed with v on a single line (e.g. v1 ) followed by the given CommitMetadata in JSON format. === [[deserialize]] Deserializing Metadata -- deserialize Method [source, scala] \u00b6 deserialize(in: InputStream): CommitMetadata \u00b6 NOTE: deserialize is part of < > to deserialize a metadata (from an InputStream ). deserialize simply reads ( deserializes ) two lines from the given InputStream for < > and the < > attribute. === [[add-batchId]] add Method [source, scala] \u00b6 add(batchId: Long): Unit \u00b6 add ...FIXME NOTE: add is used when...FIXME === [[add-batchId-metadata]] add Method [source, scala] \u00b6 add(batchId: Long, metadata: String): Boolean \u00b6 NOTE: add is part of < > to...FIXME. add ...FIXME","title":"CommitLog"},{"location":"spark-sql-streaming-CommitLog/#commitlog-hdfsmetadatalog-for-offset-commit-log","text":"CommitLog is an < > with < > metadata. CommitLog is < > exclusively for the offset commit log (of StreamExecution ). [[CommitMetadata]][[nextBatchWatermarkMs]] CommitLog uses CommitMetadata for the metadata with nextBatchWatermarkMs attribute (of type Long and the default 0 ). CommitLog < > commit metadata to files with names that are offsets. $ ls -tr [checkpoint-directory]/commits 0 1 2 3 4 5 6 7 8 9 $ cat [checkpoint-directory]/commits/8 v1 {\"nextBatchWatermarkMs\": 0} [[VERSION]] CommitLog uses 1 for the version. [[creating-instance]] CommitLog (like the parent < >) takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[serialize]] Serializing Metadata (Writing Metadata to Persistent Storage) -- serialize Method","title":"CommitLog &mdash; HDFSMetadataLog for Offset Commit Log"},{"location":"spark-sql-streaming-CommitLog/#source-scala","text":"serialize( metadata: CommitMetadata, out: OutputStream): Unit NOTE: serialize is part of < > to write a metadata in serialized format. serialize writes out the < > prefixed with v on a single line (e.g. v1 ) followed by the given CommitMetadata in JSON format. === [[deserialize]] Deserializing Metadata -- deserialize Method","title":"[source, scala]"},{"location":"spark-sql-streaming-CommitLog/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CommitLog/#deserializein-inputstream-commitmetadata","text":"NOTE: deserialize is part of < > to deserialize a metadata (from an InputStream ). deserialize simply reads ( deserializes ) two lines from the given InputStream for < > and the < > attribute. === [[add-batchId]] add Method","title":"deserialize(in: InputStream): CommitMetadata"},{"location":"spark-sql-streaming-CommitLog/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CommitLog/#addbatchid-long-unit","text":"add ...FIXME NOTE: add is used when...FIXME === [[add-batchId-metadata]] add Method","title":"add(batchId: Long): Unit"},{"location":"spark-sql-streaming-CommitLog/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CommitLog/#addbatchid-long-metadata-string-boolean","text":"NOTE: add is part of < > to...FIXME. add ...FIXME","title":"add(batchId: Long, metadata: String): Boolean"},{"location":"spark-sql-streaming-CommitMetadata/","text":"== [[CommitMetadata]] CommitMetadata CommitMetadata is...FIXME","title":"CommitMetadata"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/","text":"== [[CompactibleFileStreamLog]] CompactibleFileStreamLog Contract -- Compactible Metadata Logs CompactibleFileStreamLog is the < > of the < > for < > that < > every < >. [[minBatchesToRetain]][[spark.sql.streaming.minBatchesToRetain]] CompactibleFileStreamLog uses < > configuration property (default: 100 ) for < >. [[COMPACT_FILE_SUFFIX]] CompactibleFileStreamLog uses .compact suffix for < >, < >, and the < >. [[contract]] .CompactibleFileStreamLog Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | compactLogs a| [[compactLogs]] [source, scala] \u00b6 compactLogs(logs: Seq[T]): Seq[T] \u00b6 Used when CompactibleFileStreamLog is requested to < > and < > | defaultCompactInterval a| [[defaultCompactInterval]] [source, scala] \u00b6 defaultCompactInterval: Int \u00b6 Default < > Used exclusively when CompactibleFileStreamLog is requested for the < > | fileCleanupDelayMs a| [[fileCleanupDelayMs]] [source, scala] \u00b6 fileCleanupDelayMs: Long \u00b6 Used exclusively when CompactibleFileStreamLog is requested to < > | isDeletingExpiredLog a| [[isDeletingExpiredLog]] [source, scala] \u00b6 isDeletingExpiredLog: Boolean \u00b6 Used exclusively when CompactibleFileStreamLog is requested to < > |=== [[implementations]] .CompactibleFileStreamLogs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | CompactibleFileStreamLog | Description | < > | [[FileStreamSinkLog]] | < > | [[FileStreamSourceLog]] CompactibleFileStreamLog (of FileEntry metadata) of < > |=== === [[creating-instance]] Creating CompactibleFileStreamLog Instance CompactibleFileStreamLog takes the following to be created: [[metadataLogVersion]] Metadata version [[sparkSession]] SparkSession [[path]] Path of the metadata log directory NOTE: CompactibleFileStreamLog is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. === [[batchIdToPath]] batchIdToPath Method [source, scala] \u00b6 batchIdToPath(batchId: Long): Path \u00b6 NOTE: batchIdToPath is part of the < > to...FIXME. batchIdToPath ...FIXME === [[pathToBatchId]] pathToBatchId Method [source, scala] \u00b6 pathToBatchId(path: Path): Long \u00b6 NOTE: pathToBatchId is part of the < > to...FIXME. pathToBatchId ...FIXME === [[isBatchFile]] isBatchFile Method [source, scala] \u00b6 isBatchFile(path: Path): Boolean \u00b6 NOTE: isBatchFile is part of the < > to...FIXME. isBatchFile ...FIXME === [[serialize]] Serializing Metadata (Writing Metadata in Serialized Format) -- serialize Method [source, scala] \u00b6 serialize( logData: Array[T], out: OutputStream): Unit NOTE: serialize is part of the < > to serialize metadata (write metadata in serialized format). serialize firstly writes the version header ( v and the < >) out to the given output stream (in UTF_8 ). serialize then writes the log data (serialized using < > library). Entries are separated by new lines. === [[deserialize]] Deserializing Metadata -- deserialize Method [source, scala] \u00b6 deserialize(in: InputStream): Array[T] \u00b6 NOTE: deserialize is part of the < > to...FIXME. deserialize ...FIXME === [[add]] Storing Metadata Of Streaming Batch -- add Method [source, scala] \u00b6 add( batchId: Long, logs: Array[T]): Boolean NOTE: add is part of the < > to store metadata for a batch. add ...FIXME === [[allFiles]] allFiles Method [source, scala] \u00b6 allFiles(): Array[T] \u00b6 allFiles ...FIXME [NOTE] \u00b6 allFiles is used when: FileStreamSource is < > * MetadataLogFileIndex is < > \u00b6 === [[compact]] compact Internal Method [source, scala] \u00b6 compact( batchId: Long, logs: Array[T]): Boolean compact < > (with the streaming batch and the < >). compact ...FIXME In the end, compact < > and requests the parent HDFSMetadataLog to < >. NOTE: compact is used exclusively when CompactibleFileStreamLog is requested to < >. === [[getValidBatchesBeforeCompactionBatch]] getValidBatchesBeforeCompactionBatch Object Method [source, scala] \u00b6 getValidBatchesBeforeCompactionBatch( compactionBatchId: Long, compactInterval: Int): Seq[Long] getValidBatchesBeforeCompactionBatch ...FIXME NOTE: getValidBatchesBeforeCompactionBatch is used exclusively when CompactibleFileStreamLog is requested to < >. === [[isCompactionBatch]] isCompactionBatch Object Method [source, scala] \u00b6 isCompactionBatch(batchId: Long, compactInterval: Int): Boolean \u00b6 isCompactionBatch ...FIXME [NOTE] \u00b6 isCompactionBatch is used when: CompactibleFileStreamLog is requested to < >, < >, < >, and < > * FileStreamSourceLog is requested to < > and < > \u00b6 === [[getBatchIdFromFileName]] getBatchIdFromFileName Object Method [source, scala] \u00b6 getBatchIdFromFileName(fileName: String): Long \u00b6 getBatchIdFromFileName simply removes the < > suffix from the given fileName and converts the remaining part to a number. NOTE: getBatchIdFromFileName is used when CompactibleFileStreamLog is requested to < >, < >, and < >. === [[deleteExpiredLog]] deleteExpiredLog Internal Method [source, scala] \u00b6 deleteExpiredLog( currentBatchId: Long): Unit deleteExpiredLog does nothing and simply returns when the current batch ID incremented ( currentBatchId + 1 ) is below the < > plus the < >. deleteExpiredLog ...FIXME NOTE: deleteExpiredLog is used exclusively when CompactibleFileStreamLog is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | compactInterval a| [[compactInterval]] Compact interval |===","title":"CompactibleFileStreamLog"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#compactlogslogs-seqt-seqt","text":"Used when CompactibleFileStreamLog is requested to < > and < > | defaultCompactInterval a| [[defaultCompactInterval]]","title":"compactLogs(logs: Seq[T]): Seq[T]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#defaultcompactinterval-int","text":"Default < > Used exclusively when CompactibleFileStreamLog is requested for the < > | fileCleanupDelayMs a| [[fileCleanupDelayMs]]","title":"defaultCompactInterval: Int"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#filecleanupdelayms-long","text":"Used exclusively when CompactibleFileStreamLog is requested to < > | isDeletingExpiredLog a| [[isDeletingExpiredLog]]","title":"fileCleanupDelayMs: Long"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#isdeletingexpiredlog-boolean","text":"Used exclusively when CompactibleFileStreamLog is requested to < > |=== [[implementations]] .CompactibleFileStreamLogs [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | CompactibleFileStreamLog | Description | < > | [[FileStreamSinkLog]] | < > | [[FileStreamSourceLog]] CompactibleFileStreamLog (of FileEntry metadata) of < > |=== === [[creating-instance]] Creating CompactibleFileStreamLog Instance CompactibleFileStreamLog takes the following to be created: [[metadataLogVersion]] Metadata version [[sparkSession]] SparkSession [[path]] Path of the metadata log directory NOTE: CompactibleFileStreamLog is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. === [[batchIdToPath]] batchIdToPath Method","title":"isDeletingExpiredLog: Boolean"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#batchidtopathbatchid-long-path","text":"NOTE: batchIdToPath is part of the < > to...FIXME. batchIdToPath ...FIXME === [[pathToBatchId]] pathToBatchId Method","title":"batchIdToPath(batchId: Long): Path"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#pathtobatchidpath-path-long","text":"NOTE: pathToBatchId is part of the < > to...FIXME. pathToBatchId ...FIXME === [[isBatchFile]] isBatchFile Method","title":"pathToBatchId(path: Path): Long"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#isbatchfilepath-path-boolean","text":"NOTE: isBatchFile is part of the < > to...FIXME. isBatchFile ...FIXME === [[serialize]] Serializing Metadata (Writing Metadata in Serialized Format) -- serialize Method","title":"isBatchFile(path: Path): Boolean"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_7","text":"serialize( logData: Array[T], out: OutputStream): Unit NOTE: serialize is part of the < > to serialize metadata (write metadata in serialized format). serialize firstly writes the version header ( v and the < >) out to the given output stream (in UTF_8 ). serialize then writes the log data (serialized using < > library). Entries are separated by new lines. === [[deserialize]] Deserializing Metadata -- deserialize Method","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#deserializein-inputstream-arrayt","text":"NOTE: deserialize is part of the < > to...FIXME. deserialize ...FIXME === [[add]] Storing Metadata Of Streaming Batch -- add Method","title":"deserialize(in: InputStream): Array[T]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_9","text":"add( batchId: Long, logs: Array[T]): Boolean NOTE: add is part of the < > to store metadata for a batch. add ...FIXME === [[allFiles]] allFiles Method","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_10","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#allfiles-arrayt","text":"allFiles ...FIXME","title":"allFiles(): Array[T]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#note","text":"allFiles is used when: FileStreamSource is < >","title":"[NOTE]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#metadatalogfileindex-is","text":"=== [[compact]] compact Internal Method","title":"* MetadataLogFileIndex is &lt;&gt;"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_11","text":"compact( batchId: Long, logs: Array[T]): Boolean compact < > (with the streaming batch and the < >). compact ...FIXME In the end, compact < > and requests the parent HDFSMetadataLog to < >. NOTE: compact is used exclusively when CompactibleFileStreamLog is requested to < >. === [[getValidBatchesBeforeCompactionBatch]] getValidBatchesBeforeCompactionBatch Object Method","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_12","text":"getValidBatchesBeforeCompactionBatch( compactionBatchId: Long, compactInterval: Int): Seq[Long] getValidBatchesBeforeCompactionBatch ...FIXME NOTE: getValidBatchesBeforeCompactionBatch is used exclusively when CompactibleFileStreamLog is requested to < >. === [[isCompactionBatch]] isCompactionBatch Object Method","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_13","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#iscompactionbatchbatchid-long-compactinterval-int-boolean","text":"isCompactionBatch ...FIXME","title":"isCompactionBatch(batchId: Long, compactInterval: Int): Boolean"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#note_1","text":"isCompactionBatch is used when: CompactibleFileStreamLog is requested to < >, < >, < >, and < >","title":"[NOTE]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#filestreamsourcelog-is-requested-to-and","text":"=== [[getBatchIdFromFileName]] getBatchIdFromFileName Object Method","title":"* FileStreamSourceLog is requested to &lt;&gt; and &lt;&gt;"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_14","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#getbatchidfromfilenamefilename-string-long","text":"getBatchIdFromFileName simply removes the < > suffix from the given fileName and converts the remaining part to a number. NOTE: getBatchIdFromFileName is used when CompactibleFileStreamLog is requested to < >, < >, and < >. === [[deleteExpiredLog]] deleteExpiredLog Internal Method","title":"getBatchIdFromFileName(fileName: String): Long"},{"location":"spark-sql-streaming-CompactibleFileStreamLog/#source-scala_15","text":"deleteExpiredLog( currentBatchId: Long): Unit deleteExpiredLog does nothing and simply returns when the current batch ID incremented ( currentBatchId + 1 ) is below the < > plus the < >. deleteExpiredLog ...FIXME NOTE: deleteExpiredLog is used exclusively when CompactibleFileStreamLog is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | compactInterval a| [[compactInterval]] Compact interval |===","title":"[source, scala]"},{"location":"spark-sql-streaming-ConsoleSinkProvider/","text":"== [[ConsoleSinkProvider]] ConsoleSinkProvider ConsoleSinkProvider is a DataSourceV2 with < > for console data source format. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceV2.html[DataSourceV2 Contract] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. ConsoleSinkProvider is a < > and registers itself as the console data source format. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger val q = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") // \u2190 requests ConsoleSinkProvider for a sink .trigger(Trigger.Once) .start scala> println(q.lastProgress.sink) { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@2392cfb1\" } [[createStreamWriter]] When requested for a < >, ConsoleSinkProvider simply creates a < > (with the given schema and options). [[CreatableRelationProvider]] ConsoleSinkProvider is a < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-CreatableRelationProvider.html[CreatableRelationProvider ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. === [[createRelation]] createRelation Method [source, scala] \u00b6 createRelation( sqlContext: SQLContext, mode: SaveMode, parameters: Map[String, String], data: DataFrame): BaseRelation NOTE: createRelation is part of the CreatableRelationProvider Contract to support writing a structured query (a DataFrame) per save mode. createRelation ...FIXME","title":"ConsoleSinkProvider"},{"location":"spark-sql-streaming-ConsoleSinkProvider/#source-scala","text":"import org.apache.spark.sql.streaming.Trigger val q = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") // \u2190 requests ConsoleSinkProvider for a sink .trigger(Trigger.Once) .start scala> println(q.lastProgress.sink) { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSinkProvider@2392cfb1\" } [[createStreamWriter]] When requested for a < >, ConsoleSinkProvider simply creates a < > (with the given schema and options). [[CreatableRelationProvider]] ConsoleSinkProvider is a < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-CreatableRelationProvider.html[CreatableRelationProvider ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. === [[createRelation]] createRelation Method","title":"[source, scala]"},{"location":"spark-sql-streaming-ConsoleSinkProvider/#source-scala_1","text":"createRelation( sqlContext: SQLContext, mode: SaveMode, parameters: Map[String, String], data: DataFrame): BaseRelation NOTE: createRelation is part of the CreatableRelationProvider Contract to support writing a structured query (a DataFrame) per save mode. createRelation ...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-ConsoleWriter/","text":"== [[ConsoleWriter]] ConsoleWriter ConsoleWriter is a < > for console data source format.","title":"ConsoleWriter"},{"location":"spark-sql-streaming-ConsumerStrategy/","text":"== [[ConsumerStrategy]] ConsumerStrategy Contract for KafkaConsumer Providers ConsumerStrategy is the < > for components that can < > using the given Kafka parameters. [[contract]] [[createConsumer]] [source, scala] createConsumer(kafkaParams: java.util.Map[String, Object]): Consumer[Array[Byte], Array[Byte]] \u00b6 [[available-consumerstrategies]] .Available ConsumerStrategies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ConsumerStrategy | createConsumer | [[AssignStrategy]] AssignStrategy | Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assign(java.util.Collection)++[KafkaConsumer.assign(Collection partitions)] | [[SubscribeStrategy]] SubscribeStrategy | Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.Collection)++[KafkaConsumer.subscribe(Collection topics)] | [[SubscribePatternStrategy]] SubscribePatternStrategy a| Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.regex.Pattern,%20org.apache.kafka.clients.consumer.ConsumerRebalanceListener)++[KafkaConsumer.subscribe(Pattern pattern, ConsumerRebalanceListener listener)] with NoOpConsumerRebalanceListener . TIP: Refer to http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern ] for the format of supported topic subscription regex patterns. |===","title":"ConsumerStrategy"},{"location":"spark-sql-streaming-ConsumerStrategy/#createconsumerkafkaparams-javautilmapstring-object-consumerarraybyte-arraybyte","text":"[[available-consumerstrategies]] .Available ConsumerStrategies [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ConsumerStrategy | createConsumer | [[AssignStrategy]] AssignStrategy | Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assign(java.util.Collection)++[KafkaConsumer.assign(Collection partitions)] | [[SubscribeStrategy]] SubscribeStrategy | Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.Collection)++[KafkaConsumer.subscribe(Collection topics)] | [[SubscribePatternStrategy]] SubscribePatternStrategy a| Uses ++ http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.regex.Pattern,%20org.apache.kafka.clients.consumer.ConsumerRebalanceListener)++[KafkaConsumer.subscribe(Pattern pattern, ConsumerRebalanceListener listener)] with NoOpConsumerRebalanceListener . TIP: Refer to http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern ] for the format of supported topic subscription regex patterns. |===","title":"createConsumer(kafkaParams: java.util.Map[String, Object]): Consumer[Array[Byte], Array[Byte]]"},{"location":"spark-sql-streaming-ContinuousDataSourceRDD/","text":"== [[ContinuousDataSourceRDD]] ContinuousDataSourceRDD -- Input RDD of DataSourceV2ScanExec Physical Operator with ContinuousReader ContinuousDataSourceRDD is a specialized RDD ( RDD[InternalRow] ) that is used exclusively for the only input RDD (with the input rows) of DataSourceV2ScanExec leaf physical operator with a < >. ContinuousDataSourceRDD is < > exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (which there is only one actually). [[spark.sql.streaming.continuous.executorQueueSize]] ContinuousDataSourceRDD uses < > configuration property for the < >. [[spark.sql.streaming.continuous.executorPollIntervalMs]] ContinuousDataSourceRDD uses < > configuration property for the < >. [[creating-instance]] ContinuousDataSourceRDD takes the following to be created: [[sc]] SparkContext [[dataQueueSize]] Size of the data queue [[epochPollIntervalMs]] epochPollIntervalMs [[readerInputPartitions]] InputPartition[InternalRow] s [[getPreferredLocations]] ContinuousDataSourceRDD uses InputPartition (of a ContinuousDataSourceRDDPartition ) for preferred host locations (where the input partition reader can run faster). === [[compute]] Computing Partition -- compute Method [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[InternalRow] NOTE: compute is part of the RDD Contract to compute a given partition. compute ...FIXME === [[getPartitions]] getPartitions Method [source, scala] \u00b6 getPartitions: Array[Partition] \u00b6 NOTE: getPartitions is part of the RDD Contract to specify the partitions to < >. getPartitions ...FIXME","title":"ContinuousDataSourceRDD"},{"location":"spark-sql-streaming-ContinuousDataSourceRDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[InternalRow] NOTE: compute is part of the RDD Contract to compute a given partition. compute ...FIXME === [[getPartitions]] getPartitions Method","title":"[source, scala]"},{"location":"spark-sql-streaming-ContinuousDataSourceRDD/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-ContinuousDataSourceRDD/#getpartitions-arraypartition","text":"NOTE: getPartitions is part of the RDD Contract to specify the partitions to < >. getPartitions ...FIXME","title":"getPartitions: Array[Partition]"},{"location":"spark-sql-streaming-ContinuousExecutionRelation/","text":"== [[ContinuousExecutionRelation]] ContinuousExecutionRelation Leaf Logical Operator ContinuousExecutionRelation is a MultiInstanceRelation leaf logical operator. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-LeafNode.html[Leaf Logical Operators] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. ContinuousExecutionRelation is < > (to represent < > with < > data source) when ContinuousExecution is < > (and requested for the < >). [[creating-instance]] ContinuousExecutionRelation takes the following to be created: [[source]] < > [[extraOptions]] Options ( Map[String, String] ) [[output]] Output attributes ( Seq[Attribute] ) [[session]] SparkSession","title":"ContinuousExecutionRelation Leaf Logical Operator"},{"location":"spark-sql-streaming-ContinuousMemoryStream/","text":"== [[ContinuousMemoryStream]] ContinuousMemoryStream ContinuousMemoryStream is...FIXME","title":"ContinuousMemoryStream"},{"location":"spark-sql-streaming-ContinuousQueuedDataReader-DataReaderThread/","text":"== [[DataReaderThread]] DataReaderThread DataReaderThread is...FIXME","title":"DataReaderThread"},{"location":"spark-sql-streaming-ContinuousQueuedDataReader-EpochMarkerGenerator/","text":"== [[EpochMarkerGenerator]] EpochMarkerGenerator Thread EpochMarkerGenerator is...FIXME === [[run]] run Method [source, scala] \u00b6 run(): Unit \u00b6 NOTE: run is part of the https://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html[java.lang.Runnable ] Contract to be executed upon starting a thread. run ...FIXME","title":"EpochMarkerGenerator"},{"location":"spark-sql-streaming-ContinuousQueuedDataReader-EpochMarkerGenerator/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-ContinuousQueuedDataReader-EpochMarkerGenerator/#run-unit","text":"NOTE: run is part of the https://docs.oracle.com/javase/8/docs/api/java/lang/Runnable.html[java.lang.Runnable ] Contract to be executed upon starting a thread. run ...FIXME","title":"run(): Unit"},{"location":"spark-sql-streaming-ContinuousQueuedDataReader/","text":"== [[ContinuousQueuedDataReader]] ContinuousQueuedDataReader ContinuousQueuedDataReader is < > exclusively when ContinuousDataSourceRDD is requested to < >. [[ContinuousRecord]] ContinuousQueuedDataReader uses two types of continuous records : [[EpochMarker]] EpochMarker [[ContinuousRow]] ContinuousRow (with the InternalRow at PartitionOffset ) === [[next]] Fetching Next Row -- next Method [source, scala] \u00b6 next(): InternalRow \u00b6 next ...FIXME NOTE: next is used when...FIXME === [[close]] Closing ContinuousQueuedDataReader -- close Method [source, scala] \u00b6 close(): Unit \u00b6 NOTE: close is part of the https://docs.oracle.com/javase/8/docs/api/java/io/Closeable.html[java.io.Closeable ] to close this stream and release any system resources associated with it. close ...FIXME === [[creating-instance]] Creating ContinuousQueuedDataReader Instance ContinuousQueuedDataReader takes the following to be created: [[partition]] ContinuousDataSourceRDDPartition [[context]] TaskContext [[dataQueueSize]] Size of the < > [[epochPollIntervalMs]] epochPollIntervalMs ContinuousQueuedDataReader initializes the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | coordinatorId a| [[coordinatorId]] Epoch Coordinator Identifier Used when...FIXME | currentOffset a| [[currentOffset]] PartitionOffset Used when...FIXME | dataReaderThread a| [[dataReaderThread]] < > daemon thread that is created and started immediately when ContinuousQueuedDataReader is < > Used when...FIXME | epochCoordEndpoint a| [[epochCoordEndpoint]] RpcEndpointRef of the < > per < > Used when...FIXME | epochMarkerExecutor a| [[epochMarkerExecutor]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledExecutorService.html[java.util.concurrent.ScheduledExecutorService ] Used when...FIXME | epochMarkerGenerator a| [[epochMarkerGenerator]] < > Used when...FIXME | reader a| [[reader]] InputPartitionReader Used when...FIXME | queue a| [[queue]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ArrayBlockingQueue.html[java.util.concurrent.ArrayBlockingQueue ] of < > (of the given < >) Used when...FIXME |===","title":"ContinuousQueuedDataReader"},{"location":"spark-sql-streaming-ContinuousQueuedDataReader/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-ContinuousQueuedDataReader/#next-internalrow","text":"next ...FIXME NOTE: next is used when...FIXME === [[close]] Closing ContinuousQueuedDataReader -- close Method","title":"next(): InternalRow"},{"location":"spark-sql-streaming-ContinuousQueuedDataReader/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-ContinuousQueuedDataReader/#close-unit","text":"NOTE: close is part of the https://docs.oracle.com/javase/8/docs/api/java/io/Closeable.html[java.io.Closeable ] to close this stream and release any system resources associated with it. close ...FIXME === [[creating-instance]] Creating ContinuousQueuedDataReader Instance ContinuousQueuedDataReader takes the following to be created: [[partition]] ContinuousDataSourceRDDPartition [[context]] TaskContext [[dataQueueSize]] Size of the < > [[epochPollIntervalMs]] epochPollIntervalMs ContinuousQueuedDataReader initializes the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | coordinatorId a| [[coordinatorId]] Epoch Coordinator Identifier Used when...FIXME | currentOffset a| [[currentOffset]] PartitionOffset Used when...FIXME | dataReaderThread a| [[dataReaderThread]] < > daemon thread that is created and started immediately when ContinuousQueuedDataReader is < > Used when...FIXME | epochCoordEndpoint a| [[epochCoordEndpoint]] RpcEndpointRef of the < > per < > Used when...FIXME | epochMarkerExecutor a| [[epochMarkerExecutor]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ScheduledExecutorService.html[java.util.concurrent.ScheduledExecutorService ] Used when...FIXME | epochMarkerGenerator a| [[epochMarkerGenerator]] < > Used when...FIXME | reader a| [[reader]] InputPartitionReader Used when...FIXME | queue a| [[queue]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ArrayBlockingQueue.html[java.util.concurrent.ArrayBlockingQueue ] of < > (of the given < >) Used when...FIXME |===","title":"close(): Unit"},{"location":"spark-sql-streaming-ContinuousReadSupport/","text":"== [[ContinuousReadSupport]] ContinuousReadSupport Contract -- Data Sources with ContinuousReaders ContinuousReadSupport is the < > of the DataSourceV2 for < > with a < > for < >. [[contract]][[createContinuousReader]] ContinuousReadSupport defines a single createContinuousReader method to create a < >. [source, java] \u00b6 ContinuousReader createContinuousReader( Optional schema, String checkpointLocation, DataSourceOptions options) createContinuousReader is used when: ContinuousExecution is requested to < > (and finds < > in the < >) DataStreamReader is requested to < > [[implementations]] .ContinuousReadSupports [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | ContinuousReadSupport | Description | < > | [[ContinuousMemoryStream]] Data source provider for memory format | < > | [[KafkaSourceProvider]] Data source provider for kafka format | < > | [[RateStreamProvider]] Data source provider for rate format | < > | [[TextSocketSourceProvider]] Data source provider for socket format |===","title":"ContinuousReadSupport Contract"},{"location":"spark-sql-streaming-ContinuousReadSupport/#source-java","text":"ContinuousReader createContinuousReader( Optional schema, String checkpointLocation, DataSourceOptions options) createContinuousReader is used when: ContinuousExecution is requested to < > (and finds < > in the < >) DataStreamReader is requested to < > [[implementations]] .ContinuousReadSupports [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | ContinuousReadSupport | Description | < > | [[ContinuousMemoryStream]] Data source provider for memory format | < > | [[KafkaSourceProvider]] Data source provider for kafka format | < > | [[RateStreamProvider]] Data source provider for rate format | < > | [[TextSocketSourceProvider]] Data source provider for socket format |===","title":"[source, java]"},{"location":"spark-sql-streaming-ContinuousReader/","text":"== [[ContinuousReader]] ContinuousReader Contract -- Data Source Readers in Continuous Stream Processing ContinuousReader is the < > of Spark SQL's DataSourceReader (and < >) contracts for < > in < >. ContinuousReader is part of the novel Data Source API V2 in Spark SQL. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-data-source-api-v2.html[Data Source API V2] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[contract]] .ContinuousReader Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | commit a| [[commit]] [source, java] \u00b6 void commit(Offset end) \u00b6 Commits the specified < > Used exclusively when ContinuousExecution is requested to < > | deserializeOffset a| [[deserializeOffset]] [source, java] \u00b6 Offset deserializeOffset(String json) \u00b6 Deserializes an < > from JSON representation Used when ContinuousExecution is requested to < > and < > | getStartOffset a| [[getStartOffset]] [source, java] \u00b6 Offset getStartOffset() \u00b6 NOTE: Used exclusively in tests. | mergeOffsets a| [[mergeOffsets]] [source, java] \u00b6 Offset mergeOffsets(PartitionOffset[] offsets) \u00b6 Used exclusively when ContinuousExecution is requested to < > | needsReconfiguration a| [[needsReconfiguration]] [source, java] \u00b6 boolean needsReconfiguration() \u00b6 Indicates that the reader needs reconfiguration (e.g. to generate new input partitions) Used exclusively when ContinuousExecution is requested to < > | setStartOffset a| [[setStartOffset]] [source, java] \u00b6 void setStartOffset(Optional start) \u00b6 Used exclusively when ContinuousExecution is requested to < >. |=== [[implementations]] .ContinuousReaders [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ContinuousReader | Description | < > | [[ContinuousMemoryStream]] | < > | [[KafkaContinuousReader]] | < > | [[RateStreamContinuousReader]] | TextSocketContinuousReader | [[TextSocketContinuousReader]] |===","title":"ContinuousReader Contract"},{"location":"spark-sql-streaming-ContinuousReader/#source-java","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-ContinuousReader/#void-commitoffset-end","text":"Commits the specified < > Used exclusively when ContinuousExecution is requested to < > | deserializeOffset a| [[deserializeOffset]]","title":"void commit(Offset end)"},{"location":"spark-sql-streaming-ContinuousReader/#source-java_1","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-ContinuousReader/#offset-deserializeoffsetstring-json","text":"Deserializes an < > from JSON representation Used when ContinuousExecution is requested to < > and < > | getStartOffset a| [[getStartOffset]]","title":"Offset deserializeOffset(String json)"},{"location":"spark-sql-streaming-ContinuousReader/#source-java_2","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-ContinuousReader/#offset-getstartoffset","text":"NOTE: Used exclusively in tests. | mergeOffsets a| [[mergeOffsets]]","title":"Offset getStartOffset()"},{"location":"spark-sql-streaming-ContinuousReader/#source-java_3","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-ContinuousReader/#offset-mergeoffsetspartitionoffset-offsets","text":"Used exclusively when ContinuousExecution is requested to < > | needsReconfiguration a| [[needsReconfiguration]]","title":"Offset mergeOffsets(PartitionOffset[] offsets)"},{"location":"spark-sql-streaming-ContinuousReader/#source-java_4","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-ContinuousReader/#boolean-needsreconfiguration","text":"Indicates that the reader needs reconfiguration (e.g. to generate new input partitions) Used exclusively when ContinuousExecution is requested to < > | setStartOffset a| [[setStartOffset]]","title":"boolean needsReconfiguration()"},{"location":"spark-sql-streaming-ContinuousReader/#source-java_5","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-ContinuousReader/#void-setstartoffsetoptional-start","text":"Used exclusively when ContinuousExecution is requested to < >. |=== [[implementations]] .ContinuousReaders [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | ContinuousReader | Description | < > | [[ContinuousMemoryStream]] | < > | [[KafkaContinuousReader]] | < > | [[RateStreamContinuousReader]] | TextSocketContinuousReader | [[TextSocketContinuousReader]] |===","title":"void setStartOffset(Optional start)"},{"location":"spark-sql-streaming-ContinuousWriteRDD/","text":"== [[ContinuousWriteRDD]] ContinuousWriteRDD -- RDD of WriteToContinuousDataSourceExec Unary Physical Operator ContinuousWriteRDD is a specialized RDD ( RDD[Unit] ) that is used exclusively as the underlying RDD of WriteToContinuousDataSourceExec unary physical operator to < >. ContinuousWriteRDD is < > exclusively when WriteToContinuousDataSourceExec unary physical operator is requested to < >. [[partitioner]] [[getPartitions]] ContinuousWriteRDD uses the < > for the partitions and the partitioner. [[creating-instance]] ContinuousWriteRDD takes the following to be created: [[prev]] Parent RDD ( RDD[InternalRow] ) [[writeTask]] Write task ( DataWriterFactory[InternalRow] ) === [[compute]] Computing Partition -- compute Method [source, scala] \u00b6 compute( split: Partition, context: TaskContext): Iterator[Unit] NOTE: compute is part of the RDD Contract to compute a partition. compute requests the EpochCoordinatorRef helper for a < > (using the < >). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. compute uses the EpochTracker helper to < > (using the < > local property). [[compute-loop]] compute then executes the following steps (in a loop) until the task (as the given TaskContext ) is killed or completed. compute requests the < > to compute the given partition (that gives an Iterator[InternalRow] ). compute requests the < > to create a DataWriter (for the partition and the task attempt IDs from the given TaskContext and the < > from the EpochTracker helper) and requests it to write all records (from the Iterator[InternalRow] ). compute prints out the following INFO message to the logs: Writer for partition [partitionId] in epoch [epoch] is committing. compute requests the DataWriter to commit (that gives a WriterCommitMessage ). compute requests the EpochCoordinator RPC endpoint reference to send out a < > message (with the WriterCommitMessage ). compute prints out the following INFO message to the logs: Writer for partition [partitionId] in epoch [epoch] is committed. In the end (of the loop), compute uses the EpochTracker helper to < >. In case of an error, compute prints out the following ERROR message to the logs and requests the DataWriter to abort. Writer for partition [partitionId] is aborting. In the end, compute prints out the following ERROR message to the logs: Writer for partition [partitionId] aborted.","title":"ContinuousWriteRDD"},{"location":"spark-sql-streaming-ContinuousWriteRDD/#source-scala","text":"compute( split: Partition, context: TaskContext): Iterator[Unit] NOTE: compute is part of the RDD Contract to compute a partition. compute requests the EpochCoordinatorRef helper for a < > (using the < >). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. compute uses the EpochTracker helper to < > (using the < > local property). [[compute-loop]] compute then executes the following steps (in a loop) until the task (as the given TaskContext ) is killed or completed. compute requests the < > to compute the given partition (that gives an Iterator[InternalRow] ). compute requests the < > to create a DataWriter (for the partition and the task attempt IDs from the given TaskContext and the < > from the EpochTracker helper) and requests it to write all records (from the Iterator[InternalRow] ). compute prints out the following INFO message to the logs: Writer for partition [partitionId] in epoch [epoch] is committing. compute requests the DataWriter to commit (that gives a WriterCommitMessage ). compute requests the EpochCoordinator RPC endpoint reference to send out a < > message (with the WriterCommitMessage ). compute prints out the following INFO message to the logs: Writer for partition [partitionId] in epoch [epoch] is committed. In the end (of the loop), compute uses the EpochTracker helper to < >. In case of an error, compute prints out the following ERROR message to the logs and requests the DataWriter to abort. Writer for partition [partitionId] is aborting. In the end, compute prints out the following ERROR message to the logs: Writer for partition [partitionId] aborted.","title":"[source, scala]"},{"location":"spark-sql-streaming-DataSource/","text":"== [[DataSource]] DataSource\u2009\u2014\u2009Pluggable Data Provider Framework TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSource.html[DataSource \u2009\u2014\u2009Pluggable Data Provider Framework] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. === [[creating-instance]] Creating DataSource Instance DataSource takes the following to be created: [[sparkSession]] SparkSession [[className]] className , i.e. the fully-qualified class name or an alias of the data source [[paths]] Paths (default: Nil , i.e. an empty collection) [[userSpecifiedSchema]] Optional user-defined schema (default: None ) [[partitionColumns]] Names of the partition columns (default: (empty)) [[bucketSpec]] Optional BucketSpec (default: None ) [[options]] Configuration options (default: empty) [[catalogTable]] Optional CatalogTable (default: None ) DataSource initializes the < >. === [[sourceSchema]] Generating Metadata of Streaming Source (Data Source API V1) -- sourceSchema Internal Method [source, scala] \u00b6 sourceSchema(): SourceInfo \u00b6 sourceSchema creates a new instance of the < > and branches off per the type, e.g. < >, < > and < >. NOTE: sourceSchema is used exclusively when DataSource is requested for the < >. ==== [[sourceSchema-StreamSourceProvider]] StreamSourceProvider For a StreamSourceProvider , sourceSchema requests the StreamSourceProvider for the name and schema (of the streaming source ). In the end, sourceSchema returns the name and the schema as part of SourceInfo (with partition columns unspecified). ==== [[sourceSchema-FileFormat]] FileFormat For a FileFormat , sourceSchema ...FIXME ==== [[sourceSchema-other]] Other Types For any other data source type, sourceSchema simply throws an UnsupportedOperationException : Data source [className] does not support streamed reading === [[createSource]] Creating Streaming Source (Micro-Batch Stream Processing / Data Source API V1) -- createSource Method [source, scala] \u00b6 createSource( metadataPath: String): Source createSource creates a new instance of the < > and branches off per the type, e.g. < >, < > and < >. NOTE: createSource is used exclusively when MicroBatchExecution is requested to < >. ==== [[createSource-StreamSourceProvider]] StreamSourceProvider For a StreamSourceProvider , createSource requests the StreamSourceProvider to create a source . ==== [[createSource-FileFormat]] FileFormat For a FileFormat , createSource creates a new < >. createSource throws an IllegalArgumentException when path option was not specified for a FileFormat data source: 'path' is not specified ==== [[createSource-other]] Other Types For any other data source type, createSource simply throws an UnsupportedOperationException : Data source [className] does not support streamed reading === [[createSink]] Creating Streaming Sink -- createSink Method [source, scala] \u00b6 createSink( outputMode: OutputMode): Sink createSink creates a < > for < > or FileFormat data sources. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-FileFormat.html[FileFormat \u2009Data Source] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. Internally, createSink creates a new instance of the < > and branches off per type: For a < >, createSink simply delegates the call and requests it to < > For a FileFormat , createSink creates a < > when path option is specified and the output mode is < > createSink throws a IllegalArgumentException when path option is not specified for a FileFormat data source: 'path' is not specified createSink throws an AnalysisException when the given < > is different from < > for a FileFormat data source: Data source [className] does not support [outputMode] output mode createSink throws an UnsupportedOperationException for unsupported data source formats: Data source [className] does not support streamed writing NOTE: createSink is used exclusively when DataStreamWriter is requested to start a streaming query . === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | providingClass a| [[providingClass]] https://docs.oracle.com/javase/8/docs/api/java/lang/Class.html[java.lang.Class ] for the < > (that can be a fully-qualified class name or an alias of the data source) | sourceInfo a| [[sourceInfo]] [source, scala] \u00b6 sourceInfo: SourceInfo \u00b6 Metadata of a Source with the alias (short name), the schema, and optional partitioning columns sourceInfo is a lazy value and so initialized once (the very first time) when accessed. Used when: DataSource is requested to < > (when MicroBatchExecution is requested to < >) StreamingRelation utility is requested for a < > (when DataStreamReader is requested for a < >) |===","title":"DataSource"},{"location":"spark-sql-streaming-DataSource/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataSource/#sourceschema-sourceinfo","text":"sourceSchema creates a new instance of the < > and branches off per the type, e.g. < >, < > and < >. NOTE: sourceSchema is used exclusively when DataSource is requested for the < >. ==== [[sourceSchema-StreamSourceProvider]] StreamSourceProvider For a StreamSourceProvider , sourceSchema requests the StreamSourceProvider for the name and schema (of the streaming source ). In the end, sourceSchema returns the name and the schema as part of SourceInfo (with partition columns unspecified). ==== [[sourceSchema-FileFormat]] FileFormat For a FileFormat , sourceSchema ...FIXME ==== [[sourceSchema-other]] Other Types For any other data source type, sourceSchema simply throws an UnsupportedOperationException : Data source [className] does not support streamed reading === [[createSource]] Creating Streaming Source (Micro-Batch Stream Processing / Data Source API V1) -- createSource Method","title":"sourceSchema(): SourceInfo"},{"location":"spark-sql-streaming-DataSource/#source-scala_1","text":"createSource( metadataPath: String): Source createSource creates a new instance of the < > and branches off per the type, e.g. < >, < > and < >. NOTE: createSource is used exclusively when MicroBatchExecution is requested to < >. ==== [[createSource-StreamSourceProvider]] StreamSourceProvider For a StreamSourceProvider , createSource requests the StreamSourceProvider to create a source . ==== [[createSource-FileFormat]] FileFormat For a FileFormat , createSource creates a new < >. createSource throws an IllegalArgumentException when path option was not specified for a FileFormat data source: 'path' is not specified ==== [[createSource-other]] Other Types For any other data source type, createSource simply throws an UnsupportedOperationException : Data source [className] does not support streamed reading === [[createSink]] Creating Streaming Sink -- createSink Method","title":"[source, scala]"},{"location":"spark-sql-streaming-DataSource/#source-scala_2","text":"createSink( outputMode: OutputMode): Sink createSink creates a < > for < > or FileFormat data sources. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-FileFormat.html[FileFormat \u2009Data Source] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. Internally, createSink creates a new instance of the < > and branches off per type: For a < >, createSink simply delegates the call and requests it to < > For a FileFormat , createSink creates a < > when path option is specified and the output mode is < > createSink throws a IllegalArgumentException when path option is not specified for a FileFormat data source: 'path' is not specified createSink throws an AnalysisException when the given < > is different from < > for a FileFormat data source: Data source [className] does not support [outputMode] output mode createSink throws an UnsupportedOperationException for unsupported data source formats: Data source [className] does not support streamed writing NOTE: createSink is used exclusively when DataStreamWriter is requested to start a streaming query . === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | providingClass a| [[providingClass]] https://docs.oracle.com/javase/8/docs/api/java/lang/Class.html[java.lang.Class ] for the < > (that can be a fully-qualified class name or an alias of the data source) | sourceInfo a| [[sourceInfo]]","title":"[source, scala]"},{"location":"spark-sql-streaming-DataSource/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataSource/#sourceinfo-sourceinfo","text":"Metadata of a Source with the alias (short name), the schema, and optional partitioning columns sourceInfo is a lazy value and so initialized once (the very first time) when accessed. Used when: DataSource is requested to < > (when MicroBatchExecution is requested to < >) StreamingRelation utility is requested for a < > (when DataStreamReader is requested for a < >) |===","title":"sourceInfo: SourceInfo"},{"location":"spark-sql-streaming-DataStreamReader/","text":"== [[DataStreamReader]] DataStreamReader -- Loading Data from Streaming Source DataStreamReader is the < > to describe how data is < > to a streaming Dataset from a streaming source . [[methods]] .DataStreamReader's Methods [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | < > a| [source, scala] \u00b6 csv(path: String): DataFrame \u00b6 Sets csv as the < > of the data source | format a| [[format]] [source, scala] \u00b6 format(source: String): DataStreamReader \u00b6 Specifies the format of the < > The format is used internally as the name ( alias ) of the streaming source to use to load the data | < > a| [source, scala] \u00b6 json(path: String): DataFrame \u00b6 Sets json as the < > of the data source | < > a| [[load]] [source, scala] \u00b6 load(): DataFrame load(path: String): DataFrame // <1> <1> Explicit path (that could also be specified as an < >) Creates a streaming DataFrame that represents \"loading\" streaming data (and is internally a logical plan with a < > or < > leaf logical operators) | < > a| [source, scala] \u00b6 option(key: String, value: Boolean): DataStreamReader option(key: String, value: Double): DataStreamReader option(key: String, value: Long): DataStreamReader option(key: String, value: String): DataStreamReader Sets a loading option | options a| [[options]] [source, scala] \u00b6 options(options: Map[String, String]): DataStreamReader \u00b6 Specifies the configuration options of a data source NOTE: You could use < > method if you prefer specifying the options one by one or there is only one in use. | < > a| [source, scala] \u00b6 orc(path: String): DataFrame \u00b6 Sets orc as the < > of the data source | < > a| [source, scala] \u00b6 parquet(path: String): DataFrame \u00b6 Sets parquet as the < > of the data source | schema a| [[schema]] [source, scala] \u00b6 schema(schema: StructType): DataStreamReader schema(schemaString: String): DataStreamReader // <1> <1> Uses a DDL-formatted table schema Specifies the < > of the streaming data source (as a StructType or DDL-formatted table schema, e.g. a INT, b STRING ) | < > a| [source, scala] \u00b6 text(path: String): DataFrame \u00b6 Sets text as the < > of the data source | < > a| [source, scala] \u00b6 textFile(path: String): Dataset[String] \u00b6 |=== .DataStreamReader and The Others image::images/DataStreamReader-SparkSession-StreamingRelation.png[align=\"center\"] DataStreamReader is used for a Spark developer to describe how Spark Structured Streaming loads datasets from a streaming source (that < > creates a logical plan for a streaming query). NOTE: DataStreamReader is the Spark developer-friendly API to create a spark-sql-streaming-StreamingRelation.md[StreamingRelation] logical operator (that represents a streaming source in a logical plan). You can access DataStreamReader using SparkSession.readStream method. [source, scala] \u00b6 import org.apache.spark.sql.SparkSession val spark: SparkSession = ... val streamReader = spark.readStream \u00b6 DataStreamReader supports many < > natively and offers the < >: < > < > < > < > NOTE: DataStreamReader assumes < > file format by default that you can change using spark.sql.sources.default property. NOTE: hive source format is not supported. After you have described the streaming pipeline to read datasets from an external streaming data source, you eventually trigger the loading using format-agnostic < > or format-specific (e.g. < >, < >) operators. [[internal-properties]] .DataStreamReader's Internal Properties (in alphabetical order) [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description | [[source]] source | spark.sql.sources.default property | Source format of datasets in a streaming data source | [[userSpecifiedSchema]] userSpecifiedSchema | (empty) | Optional user-defined schema | [[extraOptions]] extraOptions | (empty) | Collection of key-value configuration options |=== === [[option]] Specifying Loading Options -- option Method [source, scala] \u00b6 option(key: String, value: String): DataStreamReader option(key: String, value: Boolean): DataStreamReader option(key: String, value: Long): DataStreamReader option(key: String, value: Double): DataStreamReader option family of methods specifies additional options to a streaming data source. There is support for values of String , Boolean , Long , and Double types for user convenience, and internally are converted to String type. Internally, option sets < > internal property. NOTE: You can also set options in bulk using < > method. You have to do the type conversion yourself, though. === [[load-internals]] Creating Streaming Dataset (to Represent Loading Data From Streaming Source) -- load Method [source, scala] \u00b6 load(): DataFrame load(path: String): DataFrame // <1> <1> Specifies path option before passing the call to parameterless load() load ...FIXME === [[builtin-formats]][[json]][[csv]][[parquet]][[text]][[textFile]] Built-in Formats [source, scala] \u00b6 json(path: String): DataFrame csv(path: String): DataFrame parquet(path: String): DataFrame text(path: String): DataFrame textFile(path: String): Dataset[String] // <1> <1> Returns Dataset[String] not DataFrame DataStreamReader can load streaming datasets from data sources of the following < >: json csv parquet text The methods simply pass calls to < > followed by < >.","title":"DataStreamReader"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#csvpath-string-dataframe","text":"Sets csv as the < > of the data source | format a| [[format]]","title":"csv(path: String): DataFrame"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#formatsource-string-datastreamreader","text":"Specifies the format of the < > The format is used internally as the name ( alias ) of the streaming source to use to load the data | < > a|","title":"format(source: String): DataStreamReader"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#jsonpath-string-dataframe","text":"Sets json as the < > of the data source | < > a| [[load]]","title":"json(path: String): DataFrame"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_3","text":"load(): DataFrame load(path: String): DataFrame // <1> <1> Explicit path (that could also be specified as an < >) Creates a streaming DataFrame that represents \"loading\" streaming data (and is internally a logical plan with a < > or < > leaf logical operators) | < > a|","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_4","text":"option(key: String, value: Boolean): DataStreamReader option(key: String, value: Double): DataStreamReader option(key: String, value: Long): DataStreamReader option(key: String, value: String): DataStreamReader Sets a loading option | options a| [[options]]","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#optionsoptions-mapstring-string-datastreamreader","text":"Specifies the configuration options of a data source NOTE: You could use < > method if you prefer specifying the options one by one or there is only one in use. | < > a|","title":"options(options: Map[String, String]): DataStreamReader"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#orcpath-string-dataframe","text":"Sets orc as the < > of the data source | < > a|","title":"orc(path: String): DataFrame"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#parquetpath-string-dataframe","text":"Sets parquet as the < > of the data source | schema a| [[schema]]","title":"parquet(path: String): DataFrame"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_8","text":"schema(schema: StructType): DataStreamReader schema(schemaString: String): DataStreamReader // <1> <1> Uses a DDL-formatted table schema Specifies the < > of the streaming data source (as a StructType or DDL-formatted table schema, e.g. a INT, b STRING ) | < > a|","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#textpath-string-dataframe","text":"Sets text as the < > of the data source | < > a|","title":"text(path: String): DataFrame"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_10","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#textfilepath-string-datasetstring","text":"|=== .DataStreamReader and The Others image::images/DataStreamReader-SparkSession-StreamingRelation.png[align=\"center\"] DataStreamReader is used for a Spark developer to describe how Spark Structured Streaming loads datasets from a streaming source (that < > creates a logical plan for a streaming query). NOTE: DataStreamReader is the Spark developer-friendly API to create a spark-sql-streaming-StreamingRelation.md[StreamingRelation] logical operator (that represents a streaming source in a logical plan). You can access DataStreamReader using SparkSession.readStream method.","title":"textFile(path: String): Dataset[String]"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_11","text":"import org.apache.spark.sql.SparkSession val spark: SparkSession = ...","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#val-streamreader-sparkreadstream","text":"DataStreamReader supports many < > natively and offers the < >: < > < > < > < > NOTE: DataStreamReader assumes < > file format by default that you can change using spark.sql.sources.default property. NOTE: hive source format is not supported. After you have described the streaming pipeline to read datasets from an external streaming data source, you eventually trigger the loading using format-agnostic < > or format-specific (e.g. < >, < >) operators. [[internal-properties]] .DataStreamReader's Internal Properties (in alphabetical order) [cols=\"1,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Initial Value | Description | [[source]] source | spark.sql.sources.default property | Source format of datasets in a streaming data source | [[userSpecifiedSchema]] userSpecifiedSchema | (empty) | Optional user-defined schema | [[extraOptions]] extraOptions | (empty) | Collection of key-value configuration options |=== === [[option]] Specifying Loading Options -- option Method","title":"val streamReader = spark.readStream"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_12","text":"option(key: String, value: String): DataStreamReader option(key: String, value: Boolean): DataStreamReader option(key: String, value: Long): DataStreamReader option(key: String, value: Double): DataStreamReader option family of methods specifies additional options to a streaming data source. There is support for values of String , Boolean , Long , and Double types for user convenience, and internally are converted to String type. Internally, option sets < > internal property. NOTE: You can also set options in bulk using < > method. You have to do the type conversion yourself, though. === [[load-internals]] Creating Streaming Dataset (to Represent Loading Data From Streaming Source) -- load Method","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_13","text":"load(): DataFrame load(path: String): DataFrame // <1> <1> Specifies path option before passing the call to parameterless load() load ...FIXME === [[builtin-formats]][[json]][[csv]][[parquet]][[text]][[textFile]] Built-in Formats","title":"[source, scala]"},{"location":"spark-sql-streaming-DataStreamReader/#source-scala_14","text":"json(path: String): DataFrame csv(path: String): DataFrame parquet(path: String): DataFrame text(path: String): DataFrame textFile(path: String): Dataset[String] // <1> <1> Returns Dataset[String] not DataFrame DataStreamReader can load streaming datasets from data sources of the following < >: json csv parquet text The methods simply pass calls to < > followed by < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-Deduplicate/","text":"Deduplicate Unary Logical Operator \u00b6 Deduplicate is a unary logical operator that represents dropDuplicates operator. Deduplicate has < > flag enabled for streaming Datasets. val uniqueRates = spark. readStream. format(\"rate\"). load. dropDuplicates(\"value\") // <-- creates Deduplicate logical operator // Note the streaming flag scala> println(uniqueRates.queryExecution.logical.numberedTreeString) 00 Deduplicate [value#33L], true // <-- streaming flag enabled 01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#32, value#33L] CAUTION: FIXME Example with duplicates across batches to show that Deduplicate keeps state and withWatermark operator should also be used to limit how much is stored (to not cause OOM) [NOTE] \u00b6 UnsupportedOperationChecker spark-sql-streaming-UnsupportedOperationChecker.md#checkForStreaming[ensures] that dropDuplicates operator is not used after aggregation on streaming Datasets. The following code is not supported in Structured Streaming and results in an AnalysisException . val counts = spark. readStream. format(\"rate\"). load. groupBy(window($\"timestamp\", \"5 seconds\") as \"group\"). agg(count(\"value\") as \"value_count\"). dropDuplicates // <-- after groupBy import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = counts. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Complete). start org.apache.spark.sql.AnalysisException: dropDuplicates is not supported after aggregation on a streaming DataFrame/Dataset;; \u00b6 [NOTE] \u00b6 Deduplicate logical operator is translated (aka planned ) to: physical-operators/StreamingDeduplicateExec.md[StreamingDeduplicateExec] physical operator in spark-sql-streaming-StreamingDeduplicationStrategy.md[StreamingDeduplicationStrategy] execution planning strategy for streaming Datasets (aka streaming plans ) * Aggregate physical operator in ReplaceDeduplicateWithAggregate execution planning strategy for non-streaming/batch Datasets (aka batch plans ) \u00b6 [[output]] The output schema of Deduplicate is exactly the < >'s output schema. === [[creating-instance]] Creating Deduplicate Instance Deduplicate takes the following when created: [[keys]] Attributes for keys [[child]] Child logical operator (i.e. LogicalPlan ) [[streaming]] Flag whether the logical operator is for streaming (enabled) or batch (disabled) mode","title":"Deduplicate Unary Logical Operator"},{"location":"spark-sql-streaming-Deduplicate/#deduplicate-unary-logical-operator","text":"Deduplicate is a unary logical operator that represents dropDuplicates operator. Deduplicate has < > flag enabled for streaming Datasets. val uniqueRates = spark. readStream. format(\"rate\"). load. dropDuplicates(\"value\") // <-- creates Deduplicate logical operator // Note the streaming flag scala> println(uniqueRates.queryExecution.logical.numberedTreeString) 00 Deduplicate [value#33L], true // <-- streaming flag enabled 01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#32, value#33L] CAUTION: FIXME Example with duplicates across batches to show that Deduplicate keeps state and withWatermark operator should also be used to limit how much is stored (to not cause OOM)","title":"Deduplicate Unary Logical Operator"},{"location":"spark-sql-streaming-Deduplicate/#note","text":"UnsupportedOperationChecker spark-sql-streaming-UnsupportedOperationChecker.md#checkForStreaming[ensures] that dropDuplicates operator is not used after aggregation on streaming Datasets. The following code is not supported in Structured Streaming and results in an AnalysisException .","title":"[NOTE]"},{"location":"spark-sql-streaming-Deduplicate/#val-counts-spark-readstream-formatrate-load-groupbywindowtimestamp-5-seconds-as-group-aggcountvalue-as-value_count-dropduplicates-after-groupby-import-scalaconcurrentduration_-import-orgapachesparksqlstreamingoutputmode-trigger-val-sq-counts-writestream-formatconsole-triggertriggerprocessingtime10seconds-outputmodeoutputmodecomplete-start-orgapachesparksqlanalysisexception-dropduplicates-is-not-supported-after-aggregation-on-a-streaming-dataframedataset","text":"","title":"val counts = spark.\n  readStream.\n  format(&quot;rate&quot;).\n  load.\n  groupBy(window($&quot;timestamp&quot;, &quot;5 seconds&quot;) as &quot;group&quot;).\n  agg(count(&quot;value&quot;) as &quot;value_count&quot;).\n  dropDuplicates  // &lt;-- after groupBy\n\nimport scala.concurrent.duration._\nimport org.apache.spark.sql.streaming.{OutputMode, Trigger}\nval sq = counts.\n  writeStream.\n  format(&quot;console&quot;).\n  trigger(Trigger.ProcessingTime(10.seconds)).\n  outputMode(OutputMode.Complete).\n  start\norg.apache.spark.sql.AnalysisException: dropDuplicates is not supported after aggregation on a streaming DataFrame/Dataset;;\n"},{"location":"spark-sql-streaming-Deduplicate/#note_1","text":"Deduplicate logical operator is translated (aka planned ) to: physical-operators/StreamingDeduplicateExec.md[StreamingDeduplicateExec] physical operator in spark-sql-streaming-StreamingDeduplicationStrategy.md[StreamingDeduplicationStrategy] execution planning strategy for streaming Datasets (aka streaming plans )","title":"[NOTE]"},{"location":"spark-sql-streaming-Deduplicate/#aggregate-physical-operator-in-replacededuplicatewithaggregate-execution-planning-strategy-for-non-streamingbatch-datasets-aka-batch-plans","text":"[[output]] The output schema of Deduplicate is exactly the < >'s output schema. === [[creating-instance]] Creating Deduplicate Instance Deduplicate takes the following when created: [[keys]] Attributes for keys [[child]] Child logical operator (i.e. LogicalPlan ) [[streaming]] Flag whether the logical operator is for streaming (enabled) or batch (disabled) mode","title":"* Aggregate physical operator in ReplaceDeduplicateWithAggregate execution planning strategy for non-streaming/batch Datasets (aka batch plans)"},{"location":"spark-sql-streaming-EpochCoordinator/","text":"== [[EpochCoordinator]] EpochCoordinator RPC Endpoint -- Coordinating Epochs and Offsets Across Partition Tasks EpochCoordinator is a ThreadSafeRpcEndpoint that tracks offsets and epochs ( coordinates epochs ) by handling < > (in < > and < > modes) from...FIXME EpochCoordinator is < > (using < > factory method) when ContinuousExecution is requested to < >. [[messages]] [[EpochCoordinatorMessage]] .EpochCoordinator RPC Endpoint's Messages [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Message | Description a| CommitPartitionEpoch [[CommitPartitionEpoch-partitionId]] Partition ID [[CommitPartitionEpoch-epoch]] Epoch [[CommitPartitionEpoch-message]] DataSource API V2's WriterCommitMessage | [[CommitPartitionEpoch]] Sent out (in one-way asynchronous mode) exclusively when ContinuousWriteRDD is requested to < > (after all rows were written down to a streaming sink) | GetCurrentEpoch | [[GetCurrentEpoch]] Sent out (in request-response synchronous mode) exclusively when EpochMarkerGenerator thread is requested to < > | IncrementAndGetEpoch | [[IncrementAndGetEpoch]] Sent out (in request-response synchronous mode) exclusively when ContinuousExecution is requested to < > (and start a separate epoch update thread) a| ReportPartitionOffset [[ReportPartitionOffset-partitionId]] Partition ID [[ReportPartitionOffset-epoch]] Epoch [[ReportPartitionOffset-offset]] < > | [[ReportPartitionOffset]] Sent out (in one-way asynchronous mode) exclusively when ContinuousQueuedDataReader is requested for the < > to be read in the current epoch, and the epoch is done a| SetReaderPartitions [[SetReaderPartitions-numPartitions]] Number of partitions | [[SetReaderPartitions]] Sent out (in request-response synchronous mode) exclusively when DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (for a < > and is about to create a < >) The < > is exactly the number of InputPartitions from the ContinuousReader . a| SetWriterPartitions [[SetWriterPartitions-numPartitions]] Number of partitions | [[SetWriterPartitions]] Sent out (in request-response synchronous mode) exclusively when WriteToContinuousDataSourceExec leaf physical operator is requested to < > (and requests a < > to collect that simply never finishes...and that's the trick of continuous mode) a| StopContinuousExecutionWrites | [[StopContinuousExecutionWrites]] Sent out (in request-response synchronous mode) exclusively when ContinuousExecution is requested to < > (and it finishes successfully or not) |=== [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef* logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.EpochCoordinatorRef*=ALL Refer to < >. \u00b6 === [[receive]] Receiving Messages (Fire-And-Forget One-Way Mode) -- receive Method [source, scala] \u00b6 receive: PartialFunction[Any, Unit] \u00b6 NOTE: receive is part of the RpcEndpoint Contract in Apache Spark to receive messages in fire-and-forget one-way mode. receive handles the following messages: < > < > With the < > turned on, receive simply swallows messages and does nothing. === [[receiveAndReply]] Receiving Messages (Request-Response Two-Way Mode) -- receiveAndReply Method [source, scala] \u00b6 receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] \u00b6 NOTE: receiveAndReply is part of the RpcEndpoint Contract in Apache Spark to receive and reply to messages in request-response two-way mode. receiveAndReply handles the following messages: < > < > < > < > < > ==== [[resolveCommitsAtEpoch]] resolveCommitsAtEpoch Internal Method [source, scala] \u00b6 resolveCommitsAtEpoch(epoch: Long): Unit \u00b6 resolveCommitsAtEpoch ...FIXME NOTE: resolveCommitsAtEpoch is used exclusively when EpochCoordinator is requested to handle < > and < > messages. ==== [[commitEpoch]] commitEpoch Internal Method [source, scala] \u00b6 commitEpoch( epoch: Long, messages: Iterable[WriterCommitMessage]): Unit commitEpoch ...FIXME NOTE: commitEpoch is used exclusively when EpochCoordinator is requested to < >. === [[creating-instance]] Creating EpochCoordinator Instance EpochCoordinator takes the following to be created: [[writer]] < > [[reader]] < > [[query]] < > [[startEpoch]] Start epoch [[session]] SparkSession [[rpcEnv]] RpcEnv EpochCoordinator initializes the < >. === [[create]] Registering EpochCoordinator RPC Endpoint -- create Factory Method [source, scala] \u00b6 create( writer: StreamWriter, reader: ContinuousReader, query: ContinuousExecution, epochCoordinatorId: String, startEpoch: Long, session: SparkSession, env: SparkEnv): RpcEndpointRef create simply < > and requests the RpcEnv to register a RPC endpoint as EpochCoordinator-[id] (where id is the given epochCoordinatorId ). create prints out the following INFO message to the logs: Registered EpochCoordinator endpoint NOTE: create is used exclusively when ContinuousExecution is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | queryWritesStopped | [[queryWritesStopped]] Flag that indicates whether to drop messages ( true ) or not ( false ) when requested to < > Default: false Turned on ( true ) when requested to < > |===","title":"EpochCoordinator RPC Endpoint"},{"location":"spark-sql-streaming-EpochCoordinator/#refer-to","text":"=== [[receive]] Receiving Messages (Fire-And-Forget One-Way Mode) -- receive Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-EpochCoordinator/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-EpochCoordinator/#receive-partialfunctionany-unit","text":"NOTE: receive is part of the RpcEndpoint Contract in Apache Spark to receive messages in fire-and-forget one-way mode. receive handles the following messages: < > < > With the < > turned on, receive simply swallows messages and does nothing. === [[receiveAndReply]] Receiving Messages (Request-Response Two-Way Mode) -- receiveAndReply Method","title":"receive: PartialFunction[Any, Unit]"},{"location":"spark-sql-streaming-EpochCoordinator/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-EpochCoordinator/#receiveandreplycontext-rpccallcontext-partialfunctionany-unit","text":"NOTE: receiveAndReply is part of the RpcEndpoint Contract in Apache Spark to receive and reply to messages in request-response two-way mode. receiveAndReply handles the following messages: < > < > < > < > < > ==== [[resolveCommitsAtEpoch]] resolveCommitsAtEpoch Internal Method","title":"receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit]"},{"location":"spark-sql-streaming-EpochCoordinator/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-EpochCoordinator/#resolvecommitsatepochepoch-long-unit","text":"resolveCommitsAtEpoch ...FIXME NOTE: resolveCommitsAtEpoch is used exclusively when EpochCoordinator is requested to handle < > and < > messages. ==== [[commitEpoch]] commitEpoch Internal Method","title":"resolveCommitsAtEpoch(epoch: Long): Unit"},{"location":"spark-sql-streaming-EpochCoordinator/#source-scala_3","text":"commitEpoch( epoch: Long, messages: Iterable[WriterCommitMessage]): Unit commitEpoch ...FIXME NOTE: commitEpoch is used exclusively when EpochCoordinator is requested to < >. === [[creating-instance]] Creating EpochCoordinator Instance EpochCoordinator takes the following to be created: [[writer]] < > [[reader]] < > [[query]] < > [[startEpoch]] Start epoch [[session]] SparkSession [[rpcEnv]] RpcEnv EpochCoordinator initializes the < >. === [[create]] Registering EpochCoordinator RPC Endpoint -- create Factory Method","title":"[source, scala]"},{"location":"spark-sql-streaming-EpochCoordinator/#source-scala_4","text":"create( writer: StreamWriter, reader: ContinuousReader, query: ContinuousExecution, epochCoordinatorId: String, startEpoch: Long, session: SparkSession, env: SparkEnv): RpcEndpointRef create simply < > and requests the RpcEnv to register a RPC endpoint as EpochCoordinator-[id] (where id is the given epochCoordinatorId ). create prints out the following INFO message to the logs: Registered EpochCoordinator endpoint NOTE: create is used exclusively when ContinuousExecution is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | queryWritesStopped | [[queryWritesStopped]] Flag that indicates whether to drop messages ( true ) or not ( false ) when requested to < > Default: false Turned on ( true ) when requested to < > |===","title":"[source, scala]"},{"location":"spark-sql-streaming-EpochCoordinatorRef/","text":"== [[EpochCoordinatorRef]] EpochCoordinatorRef EpochCoordinatorRef is...FIXME === [[create]] Creating Remote Reference to EpochCoordinator RPC Endpoint -- create Factory Method [source, scala] \u00b6 create( writer: StreamWriter, reader: ContinuousReader, query: ContinuousExecution, epochCoordinatorId: String, startEpoch: Long, session: SparkSession, env: SparkEnv): RpcEndpointRef create ...FIXME NOTE: create is used exclusively when ContinuousExecution is requested to < >. === [[get]] Getting Remote Reference to EpochCoordinator RPC Endpoint -- get Factory Method [source, scala] \u00b6 get(id: String, env: SparkEnv): RpcEndpointRef \u00b6 get ...FIXME [NOTE] \u00b6 get is used when: DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < > for a < >) ContinuousQueuedDataReader is created (and initializes the < >) EpochMarkerGenerator is created (and initializes the < >) ContinuousWriteRDD is requested to < > * WriteToContinuousDataSourceExec is requested to < > \u00b6","title":"EpochCoordinatorRef"},{"location":"spark-sql-streaming-EpochCoordinatorRef/#source-scala","text":"create( writer: StreamWriter, reader: ContinuousReader, query: ContinuousExecution, epochCoordinatorId: String, startEpoch: Long, session: SparkSession, env: SparkEnv): RpcEndpointRef create ...FIXME NOTE: create is used exclusively when ContinuousExecution is requested to < >. === [[get]] Getting Remote Reference to EpochCoordinator RPC Endpoint -- get Factory Method","title":"[source, scala]"},{"location":"spark-sql-streaming-EpochCoordinatorRef/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-EpochCoordinatorRef/#getid-string-env-sparkenv-rpcendpointref","text":"get ...FIXME","title":"get(id: String, env: SparkEnv): RpcEndpointRef"},{"location":"spark-sql-streaming-EpochCoordinatorRef/#note","text":"get is used when: DataSourceV2ScanExec leaf physical operator is requested for the input RDDs (and creates a < > for a < >) ContinuousQueuedDataReader is created (and initializes the < >) EpochMarkerGenerator is created (and initializes the < >) ContinuousWriteRDD is requested to < >","title":"[NOTE]"},{"location":"spark-sql-streaming-EpochCoordinatorRef/#writetocontinuousdatasourceexec-is-requested-to","text":"","title":"* WriteToContinuousDataSourceExec is requested to &lt;&gt;"},{"location":"spark-sql-streaming-EpochTracker/","text":"== [[EpochTracker]] EpochTracker EpochTracker is...FIXME === [[getCurrentEpoch]] Current Epoch -- getCurrentEpoch Method [source, scala] \u00b6 getCurrentEpoch: Option[Long] \u00b6 getCurrentEpoch ...FIXME NOTE: getCurrentEpoch is used when...FIXME === [[incrementCurrentEpoch]] Advancing (Incrementing) Epoch -- incrementCurrentEpoch Method [source, scala] \u00b6 incrementCurrentEpoch(): Unit \u00b6 incrementCurrentEpoch ...FIXME NOTE: incrementCurrentEpoch is used when...FIXME","title":"EpochTracker"},{"location":"spark-sql-streaming-EpochTracker/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-EpochTracker/#getcurrentepoch-optionlong","text":"getCurrentEpoch ...FIXME NOTE: getCurrentEpoch is used when...FIXME === [[incrementCurrentEpoch]] Advancing (Incrementing) Epoch -- incrementCurrentEpoch Method","title":"getCurrentEpoch: Option[Long]"},{"location":"spark-sql-streaming-EpochTracker/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-EpochTracker/#incrementcurrentepoch-unit","text":"incrementCurrentEpoch ...FIXME NOTE: incrementCurrentEpoch is used when...FIXME","title":"incrementCurrentEpoch(): Unit"},{"location":"spark-sql-streaming-EventTimeStatsAccum/","text":"EventTimeStatsAccum Accumulator \u00b6 EventTimeStatsAccum is a Spark accumulator that is used for the < > (that EventTimeWatermarkExec physical operator uses for event-time watermark): [[max]] Maximum value [[min]] Minimum value [[avg]] Average value [[count]] Number of updates (count) EventTimeStatsAccum is < > and registered exclusively for EventTimeWatermarkExec physical operator. Note When EventTimeWatermarkExec physical operator is requested to execute, every task simply < > the values of the event-time watermark column to the EventTimeStatsAccum accumulator. As per design of Spark accumulators in Apache Spark, accumulator updates are automatically sent out ( propagated ) from tasks to the driver every heartbeat and then they are accumulated together. [[currentStats]] [[creating-instance]] EventTimeStatsAccum takes a single < > to be created (default: < >). === [[add]] Accumulating Value -- add Method add ( v : Long ) : Unit add is part of the AccumulatorV2 abstraction. add simply requests the < > to < > the given v value. add is used when EventTimeWatermarkExec physical operator is executed. === [[EventTimeStats]] EventTimeStats EventTimeStats is a Scala case class for the < >. [[zero]] EventTimeStats defines a special value zero with the following values: Long.MinValue for the < > Long.MaxValue for the < > 0.0 for the < > 0L for the < > === [[EventTimeStats-add]] EventTimeStats.add Method [source, scala] \u00b6 add(eventTime: Long): Unit \u00b6 add simply updates the < > per given eventTime . NOTE: add is used exclusively when EventTimeStatsAccum is requested to < >. === [[EventTimeStats-merge]] EventTimeStats.merge Method [source, scala] \u00b6 merge(that: EventTimeStats): Unit \u00b6 merge ...FIXME NOTE: merge is used when...FIXME","title":"EventTimeStatsAccum"},{"location":"spark-sql-streaming-EventTimeStatsAccum/#eventtimestatsaccum-accumulator","text":"EventTimeStatsAccum is a Spark accumulator that is used for the < > (that EventTimeWatermarkExec physical operator uses for event-time watermark): [[max]] Maximum value [[min]] Minimum value [[avg]] Average value [[count]] Number of updates (count) EventTimeStatsAccum is < > and registered exclusively for EventTimeWatermarkExec physical operator. Note When EventTimeWatermarkExec physical operator is requested to execute, every task simply < > the values of the event-time watermark column to the EventTimeStatsAccum accumulator. As per design of Spark accumulators in Apache Spark, accumulator updates are automatically sent out ( propagated ) from tasks to the driver every heartbeat and then they are accumulated together. [[currentStats]] [[creating-instance]] EventTimeStatsAccum takes a single < > to be created (default: < >). === [[add]] Accumulating Value -- add Method add ( v : Long ) : Unit add is part of the AccumulatorV2 abstraction. add simply requests the < > to < > the given v value. add is used when EventTimeWatermarkExec physical operator is executed. === [[EventTimeStats]] EventTimeStats EventTimeStats is a Scala case class for the < >. [[zero]] EventTimeStats defines a special value zero with the following values: Long.MinValue for the < > Long.MaxValue for the < > 0.0 for the < > 0L for the < > === [[EventTimeStats-add]] EventTimeStats.add Method","title":"EventTimeStatsAccum Accumulator"},{"location":"spark-sql-streaming-EventTimeStatsAccum/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-EventTimeStatsAccum/#addeventtime-long-unit","text":"add simply updates the < > per given eventTime . NOTE: add is used exclusively when EventTimeStatsAccum is requested to < >. === [[EventTimeStats-merge]] EventTimeStats.merge Method","title":"add(eventTime: Long): Unit"},{"location":"spark-sql-streaming-EventTimeStatsAccum/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-EventTimeStatsAccum/#mergethat-eventtimestats-unit","text":"merge ...FIXME NOTE: merge is used when...FIXME","title":"merge(that: EventTimeStats): Unit"},{"location":"spark-sql-streaming-FileContextBasedCheckpointFileManager/","text":"== [[FileContextBasedCheckpointFileManager]] FileContextBasedCheckpointFileManager FileContextBasedCheckpointFileManager is...FIXME","title":"FileContextBasedCheckpointFileManager"},{"location":"spark-sql-streaming-FileStreamSink/","text":"== [[FileStreamSink]] FileStreamSink -- Streaming Sink for File-Based Data Sources FileStreamSink is a concrete < > that writes out the results of a streaming query to files (of the specified < >) in the < >. [source, scala] \u00b6 import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = in. writeStream. format(\"parquet\"). option(\"path\", \"parquet-output-dir\"). option(\"checkpointLocation\", \"checkpoint-dir\"). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Append). start FileStreamSink is < > exclusively when DataSource is requested to < > for a file-based data source (i.e. FileFormat ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-FileFormat.html[FileFormat ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. FileStreamSink supports spark-sql-streaming-OutputMode.md#Append[Append output mode] only. FileStreamSink uses spark-sql-SQLConf.md#spark.sql.streaming.fileSink.log.deletion[spark.sql.streaming.fileSink.log.deletion] (as isDeletingExpiredLog ) [[toString]] The textual representation of FileStreamSink is FileSink[path] [[metadataDir]] FileStreamSink uses _spark_metadata directory for...FIXME [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.FileStreamSink to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSink=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating FileStreamSink Instance FileStreamSink takes the following to be created: [[sparkSession]] SparkSession [[path]] Root directory [[fileFormat]] FileFormat [[partitionColumnNames]] Names of the partition columns [[options]] Configuration options FileStreamSink initializes the < >. === [[addBatch]] \"Adding\" Batch of Data to Sink -- addBatch Method [source, scala] \u00b6 addBatch( batchId: Long, data: DataFrame): Unit NOTE: addBatch is a part of spark-sql-streaming-Sink.md#addBatch[Sink Contract] to \"add\" a batch of data to the sink. addBatch ...FIXME === [[basicWriteJobStatsTracker]] Creating BasicWriteJobStatsTracker -- basicWriteJobStatsTracker Internal Method [source, scala] \u00b6 basicWriteJobStatsTracker: BasicWriteJobStatsTracker \u00b6 basicWriteJobStatsTracker simply creates a BasicWriteJobStatsTracker with the basic metrics: number of written files bytes of written output number of output rows number of dynamic partitions TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-BasicWriteJobStatsTracker.html[BasicWriteJobStatsTracker ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. NOTE: basicWriteJobStatsTracker is used exclusively when FileStreamSink is requested to < >. === [[hasMetadata]] hasMetadata Object Method [source, scala] \u00b6 hasMetadata( path: Seq[String], hadoopConf: Configuration): Boolean hasMetadata ...FIXME [NOTE] \u00b6 hasMetadata is used when: DataSource (Spark SQL) is requested to resolve a FileFormat relation ( resolveRelation ) and creates a HadoopFsRelation * FileStreamSource is requested to < > \u00b6 === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | basePath a| [[basePath]] Base path (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ] for the given < >) Used when...FIXME | logPath a| [[logPath]] Metadata log path (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ] for the < > and the < >) Used exclusively to create the < > | fileLog a| [[fileLog]] < > (for the < > and the < >) Used exclusively when FileStreamSink is requested to < > | hadoopConf a| [[hadoopConf]] Hadoop's http://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] Used when...FIXME |===","title":"FileStreamSink"},{"location":"spark-sql-streaming-FileStreamSink/#source-scala","text":"import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = in. writeStream. format(\"parquet\"). option(\"path\", \"parquet-output-dir\"). option(\"checkpointLocation\", \"checkpoint-dir\"). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Append). start FileStreamSink is < > exclusively when DataSource is requested to < > for a file-based data source (i.e. FileFormat ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-FileFormat.html[FileFormat ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. FileStreamSink supports spark-sql-streaming-OutputMode.md#Append[Append output mode] only. FileStreamSink uses spark-sql-SQLConf.md#spark.sql.streaming.fileSink.log.deletion[spark.sql.streaming.fileSink.log.deletion] (as isDeletingExpiredLog ) [[toString]] The textual representation of FileStreamSink is FileSink[path] [[metadataDir]] FileStreamSink uses _spark_metadata directory for...FIXME [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.FileStreamSink to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSink=ALL","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSink/#refer-to","text":"=== [[creating-instance]] Creating FileStreamSink Instance FileStreamSink takes the following to be created: [[sparkSession]] SparkSession [[path]] Root directory [[fileFormat]] FileFormat [[partitionColumnNames]] Names of the partition columns [[options]] Configuration options FileStreamSink initializes the < >. === [[addBatch]] \"Adding\" Batch of Data to Sink -- addBatch Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-FileStreamSink/#source-scala_1","text":"addBatch( batchId: Long, data: DataFrame): Unit NOTE: addBatch is a part of spark-sql-streaming-Sink.md#addBatch[Sink Contract] to \"add\" a batch of data to the sink. addBatch ...FIXME === [[basicWriteJobStatsTracker]] Creating BasicWriteJobStatsTracker -- basicWriteJobStatsTracker Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSink/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSink/#basicwritejobstatstracker-basicwritejobstatstracker","text":"basicWriteJobStatsTracker simply creates a BasicWriteJobStatsTracker with the basic metrics: number of written files bytes of written output number of output rows number of dynamic partitions TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-BasicWriteJobStatsTracker.html[BasicWriteJobStatsTracker ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. NOTE: basicWriteJobStatsTracker is used exclusively when FileStreamSink is requested to < >. === [[hasMetadata]] hasMetadata Object Method","title":"basicWriteJobStatsTracker: BasicWriteJobStatsTracker"},{"location":"spark-sql-streaming-FileStreamSink/#source-scala_3","text":"hasMetadata( path: Seq[String], hadoopConf: Configuration): Boolean hasMetadata ...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSink/#note","text":"hasMetadata is used when: DataSource (Spark SQL) is requested to resolve a FileFormat relation ( resolveRelation ) and creates a HadoopFsRelation","title":"[NOTE]"},{"location":"spark-sql-streaming-FileStreamSink/#filestreamsource-is-requested-to","text":"=== [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | basePath a| [[basePath]] Base path (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ] for the given < >) Used when...FIXME | logPath a| [[logPath]] Metadata log path (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ] for the < > and the < >) Used exclusively to create the < > | fileLog a| [[fileLog]] < > (for the < > and the < >) Used exclusively when FileStreamSink is requested to < > | hadoopConf a| [[hadoopConf]] Hadoop's http://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] Used when...FIXME |===","title":"* FileStreamSource is requested to &lt;&gt;"},{"location":"spark-sql-streaming-FileStreamSinkLog/","text":"== [[FileStreamSinkLog]] FileStreamSinkLog FileStreamSinkLog is a concrete < > (of < >) for < > and < >. [[VERSION]] FileStreamSinkLog uses 1 for the version. [[ADD_ACTION]] FileStreamSinkLog uses add action to create new < >. [[DELETE_ACTION]] FileStreamSinkLog uses delete action to mark < > that should be excluded from < >. === [[creating-instance]] Creating FileStreamSinkLog Instance FileStreamSinkLog (like the parent < >) takes the following to be created: [[metadataLogVersion]] Metadata version [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[compactLogs]] compactLogs Method [source, scala] \u00b6 compactLogs(logs: Seq[SinkFileStatus]): Seq[SinkFileStatus] \u00b6 NOTE: compactLogs is part of the < > to...FIXME. compactLogs ...FIXME","title":"FileStreamSinkLog"},{"location":"spark-sql-streaming-FileStreamSinkLog/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSinkLog/#compactlogslogs-seqsinkfilestatus-seqsinkfilestatus","text":"NOTE: compactLogs is part of the < > to...FIXME. compactLogs ...FIXME","title":"compactLogs(logs: Seq[SinkFileStatus]): Seq[SinkFileStatus]"},{"location":"spark-sql-streaming-FileStreamSource/","text":"FileStreamSource \u00b6 FileStreamSource is a Source that reads text files from path directory as they appear. It uses LongOffset offsets. NOTE: It is used by spark-sql-datasource.md#createSource[DataSource.createSource] for FileFormat . You can provide the < > of the data and dataFrameBuilder - the function to build a DataFrame in < > at instantiation time. [source, scala] \u00b6 // NOTE The source directory must exist // mkdir text-logs val df = spark.readStream .format(\"text\") .option(\"maxFilesPerTrigger\", 1) .load(\"text-logs\") scala> df.printSchema root |-- value: string (nullable = true) Batches are indexed. It lives in org.apache.spark.sql.execution.streaming package. [source, scala] \u00b6 import org.apache.spark.sql.types._ val schema = StructType( StructField(\"id\", LongType, nullable = false) :: StructField(\"name\", StringType, nullable = false) :: StructField(\"score\", DoubleType, nullable = false) :: Nil) // You should have input-json directory available val in = spark.readStream .format(\"json\") .schema(schema) .load(\"input-json\") val input = in.transform { ds => println(\"transform executed\") // \u2190 it's going to be executed once only ds } scala> input.isStreaming res9: Boolean = true It tracks already-processed files in seenFiles hash map. [TIP] \u00b6 Enable DEBUG or TRACE logging level for org.apache.spark.sql.execution.streaming.FileStreamSource to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSource=TRACE Refer to spark-sql-streaming-logging.md[Logging]. \u00b6 === [[creating-instance]] Creating FileStreamSource Instance CAUTION: FIXME === [[options]] Options ==== [[maxFilesPerTrigger]] maxFilesPerTrigger maxFilesPerTrigger option specifies the maximum number of files per trigger (batch). It limits the file stream source to read the maxFilesPerTrigger number of files specified at a time and hence enables rate limiting. It allows for a static set of files be used like a stream for testing as the file set is processed maxFilesPerTrigger number of files at a time. === [[schema]] schema If the schema is specified at instantiation time (using optional dataSchema constructor parameter) it is returned. Otherwise, fetchAllFiles internal method is called to list all the files in a directory. When there is at least one file the schema is calculated using dataFrameBuilder constructor parameter function. Else, an IllegalArgumentException(\"No schema specified\") is thrown unless it is for text provider (as providerName constructor parameter) where the default schema with a single value column of type StringType is assumed. NOTE: text as the value of providerName constructor parameter denotes text file stream provider . === [[getOffset]] getOffset Method [source, scala] \u00b6 getOffset: Option[Offset] \u00b6 getOffset ...FIXME The maximum offset ( getOffset ) is calculated by fetching all the files in path excluding files that start with _ (underscore). When computing the maximum offset using getOffset , you should see the following DEBUG message in the logs: Listed ${files.size} in ${(endTime.toDouble - startTime) / 1000000}ms When computing the maximum offset using getOffset , it also filters out the files that were already seen (tracked in seenFiles internal registry). You should see the following DEBUG message in the logs (depending on the status of a file): new file: $file // or old file: $file getOffset is part of the Source abstraction. === [[getBatch]] Generating DataFrame for Streaming Batch -- getBatch Method FileStreamSource.getBatch asks < > for the batch. You should see the following INFO and DEBUG messages in the logs: INFO Processing ${files.length} files from ${startId + 1}:$endId DEBUG Streaming ${files.mkString(\", \")} The method to create a result batch is given at instantiation time (as dataFrameBuilder constructor parameter). === [[metadataLog]] metadataLog metadataLog is a metadata storage using metadataPath path (which is a constructor parameter). NOTE: It extends HDFSMetadataLog[Seq[String]] . CAUTION: FIXME Review HDFSMetadataLog === [[fetchMaxOffset]] fetchMaxOffset Internal Method [source, scala] \u00b6 fetchMaxOffset(): FileStreamSourceOffset \u00b6 fetchMaxOffset ...FIXME NOTE: fetchMaxOffset is used exclusively when FileStreamSource is requested to < >. === [[fetchAllFiles]] fetchAllFiles Internal Method [source, scala] \u00b6 fetchAllFiles(): Seq[(String, Long)] \u00b6 fetchAllFiles ...FIXME NOTE: fetchAllFiles is used exclusively when FileStreamSource is requested to < >. === [[allFilesUsingMetadataLogFileIndex]] allFilesUsingMetadataLogFileIndex Internal Method [source, scala] \u00b6 allFilesUsingMetadataLogFileIndex(): Seq[FileStatus] \u00b6 allFilesUsingMetadataLogFileIndex simply creates a new < > and requests it to allFiles . NOTE: allFilesUsingMetadataLogFileIndex is used exclusively when FileStreamSource is requested to < > (when requested for < > when FileStreamSource is requested to < >).","title":"FileStreamSource"},{"location":"spark-sql-streaming-FileStreamSource/#filestreamsource","text":"FileStreamSource is a Source that reads text files from path directory as they appear. It uses LongOffset offsets. NOTE: It is used by spark-sql-datasource.md#createSource[DataSource.createSource] for FileFormat . You can provide the < > of the data and dataFrameBuilder - the function to build a DataFrame in < > at instantiation time.","title":"FileStreamSource"},{"location":"spark-sql-streaming-FileStreamSource/#source-scala","text":"// NOTE The source directory must exist // mkdir text-logs val df = spark.readStream .format(\"text\") .option(\"maxFilesPerTrigger\", 1) .load(\"text-logs\") scala> df.printSchema root |-- value: string (nullable = true) Batches are indexed. It lives in org.apache.spark.sql.execution.streaming package.","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSource/#source-scala_1","text":"import org.apache.spark.sql.types._ val schema = StructType( StructField(\"id\", LongType, nullable = false) :: StructField(\"name\", StringType, nullable = false) :: StructField(\"score\", DoubleType, nullable = false) :: Nil) // You should have input-json directory available val in = spark.readStream .format(\"json\") .schema(schema) .load(\"input-json\") val input = in.transform { ds => println(\"transform executed\") // \u2190 it's going to be executed once only ds } scala> input.isStreaming res9: Boolean = true It tracks already-processed files in seenFiles hash map.","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSource/#tip","text":"Enable DEBUG or TRACE logging level for org.apache.spark.sql.execution.streaming.FileStreamSource to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FileStreamSource=TRACE","title":"[TIP]"},{"location":"spark-sql-streaming-FileStreamSource/#refer-to-spark-sql-streaming-loggingmdlogging","text":"=== [[creating-instance]] Creating FileStreamSource Instance CAUTION: FIXME === [[options]] Options ==== [[maxFilesPerTrigger]] maxFilesPerTrigger maxFilesPerTrigger option specifies the maximum number of files per trigger (batch). It limits the file stream source to read the maxFilesPerTrigger number of files specified at a time and hence enables rate limiting. It allows for a static set of files be used like a stream for testing as the file set is processed maxFilesPerTrigger number of files at a time. === [[schema]] schema If the schema is specified at instantiation time (using optional dataSchema constructor parameter) it is returned. Otherwise, fetchAllFiles internal method is called to list all the files in a directory. When there is at least one file the schema is calculated using dataFrameBuilder constructor parameter function. Else, an IllegalArgumentException(\"No schema specified\") is thrown unless it is for text provider (as providerName constructor parameter) where the default schema with a single value column of type StringType is assumed. NOTE: text as the value of providerName constructor parameter denotes text file stream provider . === [[getOffset]] getOffset Method","title":"Refer to spark-sql-streaming-logging.md[Logging]."},{"location":"spark-sql-streaming-FileStreamSource/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSource/#getoffset-optionoffset","text":"getOffset ...FIXME The maximum offset ( getOffset ) is calculated by fetching all the files in path excluding files that start with _ (underscore). When computing the maximum offset using getOffset , you should see the following DEBUG message in the logs: Listed ${files.size} in ${(endTime.toDouble - startTime) / 1000000}ms When computing the maximum offset using getOffset , it also filters out the files that were already seen (tracked in seenFiles internal registry). You should see the following DEBUG message in the logs (depending on the status of a file): new file: $file // or old file: $file getOffset is part of the Source abstraction. === [[getBatch]] Generating DataFrame for Streaming Batch -- getBatch Method FileStreamSource.getBatch asks < > for the batch. You should see the following INFO and DEBUG messages in the logs: INFO Processing ${files.length} files from ${startId + 1}:$endId DEBUG Streaming ${files.mkString(\", \")} The method to create a result batch is given at instantiation time (as dataFrameBuilder constructor parameter). === [[metadataLog]] metadataLog metadataLog is a metadata storage using metadataPath path (which is a constructor parameter). NOTE: It extends HDFSMetadataLog[Seq[String]] . CAUTION: FIXME Review HDFSMetadataLog === [[fetchMaxOffset]] fetchMaxOffset Internal Method","title":"getOffset: Option[Offset]"},{"location":"spark-sql-streaming-FileStreamSource/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSource/#fetchmaxoffset-filestreamsourceoffset","text":"fetchMaxOffset ...FIXME NOTE: fetchMaxOffset is used exclusively when FileStreamSource is requested to < >. === [[fetchAllFiles]] fetchAllFiles Internal Method","title":"fetchMaxOffset(): FileStreamSourceOffset"},{"location":"spark-sql-streaming-FileStreamSource/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSource/#fetchallfiles-seqstring-long","text":"fetchAllFiles ...FIXME NOTE: fetchAllFiles is used exclusively when FileStreamSource is requested to < >. === [[allFilesUsingMetadataLogFileIndex]] allFilesUsingMetadataLogFileIndex Internal Method","title":"fetchAllFiles(): Seq[(String, Long)]"},{"location":"spark-sql-streaming-FileStreamSource/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSource/#allfilesusingmetadatalogfileindex-seqfilestatus","text":"allFilesUsingMetadataLogFileIndex simply creates a new < > and requests it to allFiles . NOTE: allFilesUsingMetadataLogFileIndex is used exclusively when FileStreamSource is requested to < > (when requested for < > when FileStreamSource is requested to < >).","title":"allFilesUsingMetadataLogFileIndex(): Seq[FileStatus]"},{"location":"spark-sql-streaming-FileStreamSourceLog/","text":"== [[FileStreamSourceLog]] FileStreamSourceLog FileStreamSourceLog is a concrete < > (of FileEntry metadata) of < >. FileStreamSourceLog uses a fixed-size < > of metadata of compaction batches. [[defaultCompactInterval]] FileStreamSourceLog uses < > configuration property (default: 10 ) for the < >. [[fileCleanupDelayMs]] FileStreamSourceLog uses < > configuration property (default: 10 minutes) for the < >. [[isDeletingExpiredLog]] FileStreamSourceLog uses < > configuration property (default: true ) for the < >. === [[creating-instance]] Creating FileStreamSourceLog Instance FileStreamSourceLog (like the parent < >) takes the following to be created: [[metadataLogVersion]] Metadata version [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[add]] Storing (Adding) Metadata of Streaming Batch -- add Method [source, scala] \u00b6 add( batchId: Long, logs: Array[FileEntry]): Boolean NOTE: add is part of the < > to store ( add ) metadata of a streaming batch. add requests the parent CompactibleFileStreamLog to < > (possibly < > if the batch is < >). If so (and this is a compation batch), add adds the batch and the logs to < > internal registry (and possibly removing the eldest entry if the size is above the < >). === [[get]][[get-range]] get Method [source, scala] \u00b6 get( startId: Option[Long], endId: Option[Long]): Array[(Long, Array[FileEntry])] NOTE: get is part of the < > to...FIXME. get ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | cacheSize a| [[cacheSize]] Size of the < > that is exactly the < > Used when the < > is requested to add a new entry in < > and < > a compaction batch | fileEntryCache a| [[fileEntryCache]] Metadata of a streaming batch ( FileEntry ) per batch ID ( LinkedHashMap[Long, Array[FileEntry]] ) of size configured using the < > New entry added for a < > when < > Used when < > (for a < >) |===","title":"FileStreamSourceLog"},{"location":"spark-sql-streaming-FileStreamSourceLog/#source-scala","text":"add( batchId: Long, logs: Array[FileEntry]): Boolean NOTE: add is part of the < > to store ( add ) metadata of a streaming batch. add requests the parent CompactibleFileStreamLog to < > (possibly < > if the batch is < >). If so (and this is a compation batch), add adds the batch and the logs to < > internal registry (and possibly removing the eldest entry if the size is above the < >). === [[get]][[get-range]] get Method","title":"[source, scala]"},{"location":"spark-sql-streaming-FileStreamSourceLog/#source-scala_1","text":"get( startId: Option[Long], endId: Option[Long]): Array[(Long, Array[FileEntry])] NOTE: get is part of the < > to...FIXME. get ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | cacheSize a| [[cacheSize]] Size of the < > that is exactly the < > Used when the < > is requested to add a new entry in < > and < > a compaction batch | fileEntryCache a| [[fileEntryCache]] Metadata of a streaming batch ( FileEntry ) per batch ID ( LinkedHashMap[Long, Array[FileEntry]] ) of size configured using the < > New entry added for a < > when < > Used when < > (for a < >) |===","title":"[source, scala]"},{"location":"spark-sql-streaming-FileSystemBasedCheckpointFileManager/","text":"== [[FileSystemBasedCheckpointFileManager]] FileSystemBasedCheckpointFileManager -- CheckpointFileManager on Hadoop's FileSystem API [[CheckpointFileManager]] FileSystemBasedCheckpointFileManager is a < > that uses Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] API for managing checkpoint files: < > uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#listStatus(org.apache.hadoop.fs.Path[],%20org.apache.hadoop.fs.PathFilter)++[FileSystem.listStatus ] < > uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#mkdirs(org.apache.hadoop.fs.Path,%20org.apache.hadoop.fs.permission.FsPermission)++[FileSystem.mkdirs ] < > uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#create(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.create ] (with overwrite enabled) [[createAtomic]] < > uses RenameBasedFSDataOutputStream < > uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#open(org.apache.hadoop.fs.Path)++[FileSystem.open ] < > uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#getFileStatus(org.apache.hadoop.fs.Path)++[FileSystem.getFileStatus ] < > uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#rename(org.apache.hadoop.fs.Path,%20org.apache.hadoop.fs.Path)++[FileSystem.rename ] < > uses ++ https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html#delete(org.apache.hadoop.fs.Path,%20boolean)++[FileSystem.delete ] (with recursive enabled) < > is true for the < > being LocalFileSystem or RawLocalFileSystem FileSystemBasedCheckpointFileManager is < > exclusively when CheckpointFileManager helper object is requested for a < > (for < >, < > and < >). [[RenameHelperMethods]] FileSystemBasedCheckpointFileManager is a RenameHelperMethods for < > by \"write-to-temp-file-and-rename\". === [[creating-instance]] Creating FileSystemBasedCheckpointFileManager Instance FileSystemBasedCheckpointFileManager takes the following to be created: [[path]] Checkpoint directory (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ]) [[hadoopConf]] Configuration (Hadoop's http://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ]) FileSystemBasedCheckpointFileManager initializes the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | fs a| [[fs]] Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem ] of the < > |===","title":"FileSystemBasedCheckpointFileManager"},{"location":"spark-sql-streaming-FlatMapGroupsWithStateExecHelper/","text":"== [[FlatMapGroupsWithStateExecHelper]] FlatMapGroupsWithStateExecHelper FlatMapGroupsWithStateExecHelper is a utility with the main purpose of < > for FlatMapGroupsWithStateExec physical operator. === [[createStateManager]] Creating StateManager -- createStateManager Method [source, scala] \u00b6 createStateManager( stateEncoder: ExpressionEncoder[Any], shouldStoreTimestamp: Boolean, stateFormatVersion: Int): StateManager createStateManager simply creates a < > (with the stateEncoder and shouldStoreTimestamp flag) based on stateFormatVersion : < > for 1 < > for 2 createStateManager throws an IllegalArgumentException for stateFormatVersion not 1 or 2 : Version [stateFormatVersion] is invalid createStateManager is used for the StateManager for FlatMapGroupsWithStateExec physical operator.","title":"FlatMapGroupsWithStateExecHelper Helper Class"},{"location":"spark-sql-streaming-FlatMapGroupsWithStateExecHelper/#source-scala","text":"createStateManager( stateEncoder: ExpressionEncoder[Any], shouldStoreTimestamp: Boolean, stateFormatVersion: Int): StateManager createStateManager simply creates a < > (with the stateEncoder and shouldStoreTimestamp flag) based on stateFormatVersion : < > for 1 < > for 2 createStateManager throws an IllegalArgumentException for stateFormatVersion not 1 or 2 : Version [stateFormatVersion] is invalid createStateManager is used for the StateManager for FlatMapGroupsWithStateExec physical operator.","title":"[source, scala]"},{"location":"spark-sql-streaming-FlatMapGroupsWithStateStrategy/","text":"== [[FlatMapGroupsWithStateStrategy]] FlatMapGroupsWithStateStrategy Execution Planning Strategy for FlatMapGroupsWithState Logical Operator [[apply]] FlatMapGroupsWithStateStrategy is an execution planning strategy that can plan streaming queries with FlatMapGroupsWithState unary logical operators to FlatMapGroupsWithStateExec physical operator (with undefined StatefulOperatorStateInfo , batchTimestampMs , and eventTimeWatermark ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. FlatMapGroupsWithStateStrategy is used exclusively when < > is requested to plan a streaming query. === [[demo]] Demo: Using FlatMapGroupsWithStateStrategy [source, scala] \u00b6 import org.apache.spark.sql.streaming.GroupState val stateFunc = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Long]) => { Iterator((key, values.size)) } import java.sql.Timestamp import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val numGroups = spark. readStream. format(\"rate\"). load. as[(Timestamp, Long)]. groupByKey { case (time, value) => value % 2 }. flatMapGroupsWithState(OutputMode.Update, GroupStateTimeout.NoTimeout)(stateFunc) scala> numGroups.explain(true) == Parsed Logical Plan == 'SerializeFromObject [assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#267L, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2 AS _2#268] +- 'FlatMapGroupsWithState , unresolveddeserializer(upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\"), value#262L), unresolveddeserializer(newInstance(class scala.Tuple2), timestamp#253, value#254L), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, class[value[0]: bigint], Update, false, NoTimeout +- AppendColumns , class scala.Tuple2, [StructField(_1,TimestampType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L] +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38bcac50,rate,List(),None,List(),None,Map(),None), rate, [timestamp#253, value#254L] ... == Physical Plan == *SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#267L, assertnotnull(input[0, scala.Tuple2, true])._2 AS _2#268] +- FlatMapGroupsWithState , value#262: bigint, newInstance(class scala.Tuple2), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, StatefulOperatorStateInfo( ,84b5dccb-3fa6-4343-a99c-6fa5490c9b33,0,0), class[value[0]: bigint], Update, NoTimeout, 0, 0 +- *Sort [value#262L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#262L, 200) +- AppendColumns , newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L] +- StreamingRelation rate, [timestamp#253, value#254L]","title":"FlatMapGroupsWithStateStrategy"},{"location":"spark-sql-streaming-FlatMapGroupsWithStateStrategy/#source-scala","text":"import org.apache.spark.sql.streaming.GroupState val stateFunc = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Long]) => { Iterator((key, values.size)) } import java.sql.Timestamp import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val numGroups = spark. readStream. format(\"rate\"). load. as[(Timestamp, Long)]. groupByKey { case (time, value) => value % 2 }. flatMapGroupsWithState(OutputMode.Update, GroupStateTimeout.NoTimeout)(stateFunc) scala> numGroups.explain(true) == Parsed Logical Plan == 'SerializeFromObject [assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#267L, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2 AS _2#268] +- 'FlatMapGroupsWithState , unresolveddeserializer(upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\"), value#262L), unresolveddeserializer(newInstance(class scala.Tuple2), timestamp#253, value#254L), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, class[value[0]: bigint], Update, false, NoTimeout +- AppendColumns , class scala.Tuple2, [StructField(_1,TimestampType,true), StructField(_2,LongType,false)], newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L] +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38bcac50,rate,List(),None,List(),None,Map(),None), rate, [timestamp#253, value#254L] ... == Physical Plan == *SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#267L, assertnotnull(input[0, scala.Tuple2, true])._2 AS _2#268] +- FlatMapGroupsWithState , value#262: bigint, newInstance(class scala.Tuple2), [value#262L], [timestamp#253, value#254L], obj#266: scala.Tuple2, StatefulOperatorStateInfo( ,84b5dccb-3fa6-4343-a99c-6fa5490c9b33,0,0), class[value[0]: bigint], Update, NoTimeout, 0, 0 +- *Sort [value#262L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#262L, 200) +- AppendColumns , newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#262L] +- StreamingRelation rate, [timestamp#253, value#254L]","title":"[source, scala]"},{"location":"spark-sql-streaming-ForeachBatchSink/","text":"ForeachBatchSink \u00b6 ForeachBatchSink is a streaming sink that is used for DataStreamWriter.foreachBatch streaming operator. ForeachBatchSink is < > exclusively when DataStreamWriter is requested to start execution of the streaming query (with the foreachBatch source). [[toString]] ForeachBatchSink uses ForeachBatchSink name. [source, scala] \u00b6 import org.apache.spark.sql.Dataset val q = spark.readStream .format(\"rate\") .load .writeStream .foreachBatch { (output: Dataset[_], batchId: Long) => // \u2190 creates a ForeachBatchSink println(s\"Batch ID: $batchId\") output.show } .start // q.stop scala> println(q.lastProgress.sink.description) ForeachBatchSink NOTE: ForeachBatchSink was added in Spark 2.4.0 as part of https://issues.apache.org/jira/browse/SPARK-24565[SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame]. === [[creating-instance]] Creating ForeachBatchSink Instance ForeachBatchSink takes the following when created: [[batchWriter]] Batch writer ( (Dataset[T], Long) => Unit ) [[encoder]] Encoder ( ExpressionEncoder[T] ) === [[addBatch]] Adding Batch -- addBatch Method [source, scala] \u00b6 addBatch(batchId: Long, data: DataFrame): Unit \u00b6 NOTE: addBatch is a part of < > to \"add\" a batch of data to the sink. addBatch ...FIXME","title":"ForeachBatchSink"},{"location":"spark-sql-streaming-ForeachBatchSink/#foreachbatchsink","text":"ForeachBatchSink is a streaming sink that is used for DataStreamWriter.foreachBatch streaming operator. ForeachBatchSink is < > exclusively when DataStreamWriter is requested to start execution of the streaming query (with the foreachBatch source). [[toString]] ForeachBatchSink uses ForeachBatchSink name.","title":"ForeachBatchSink"},{"location":"spark-sql-streaming-ForeachBatchSink/#source-scala","text":"import org.apache.spark.sql.Dataset val q = spark.readStream .format(\"rate\") .load .writeStream .foreachBatch { (output: Dataset[_], batchId: Long) => // \u2190 creates a ForeachBatchSink println(s\"Batch ID: $batchId\") output.show } .start // q.stop scala> println(q.lastProgress.sink.description) ForeachBatchSink NOTE: ForeachBatchSink was added in Spark 2.4.0 as part of https://issues.apache.org/jira/browse/SPARK-24565[SPARK-24565 Add API for in Structured Streaming for exposing output rows of each microbatch as a DataFrame]. === [[creating-instance]] Creating ForeachBatchSink Instance ForeachBatchSink takes the following when created: [[batchWriter]] Batch writer ( (Dataset[T], Long) => Unit ) [[encoder]] Encoder ( ExpressionEncoder[T] ) === [[addBatch]] Adding Batch -- addBatch Method","title":"[source, scala]"},{"location":"spark-sql-streaming-ForeachBatchSink/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-ForeachBatchSink/#addbatchbatchid-long-data-dataframe-unit","text":"NOTE: addBatch is a part of < > to \"add\" a batch of data to the sink. addBatch ...FIXME","title":"addBatch(batchId: Long, data: DataFrame): Unit"},{"location":"spark-sql-streaming-ForeachSink/","text":"ForeachSink \u00b6 ForeachSink is a typed streaming sink that passes rows (of the type T ) to ForeachWriter (one record at a time per partition). Note ForeachSink is assigned a ForeachWriter when DataStreamWriter is started . ForeachSink is used exclusively in foreach operator. [source, scala] \u00b6 val records = spark. readStream format(\"text\"). load(\"server-logs/*.out\"). as[String] import org.apache.spark.sql.ForeachWriter val writer = new ForeachWriter[String] { override def open(partitionId: Long, version: Long) = true override def process(value: String) = println(value) override def close(errorOrNull: Throwable) = {} } records.writeStream .queryName(\"server-logs processor\") .foreach(writer) .start Internally, addBatch (the only method from the < >) takes records from the input spark-sql-dataframe.md[DataFrame] (as data ), transforms them to expected type T (of this ForeachSink ) and (now as a spark-sql-dataset.md[Dataset]) spark-sql-dataset.md#foreachPartition[processes each partition]. [source, scala] \u00b6 addBatch(batchId: Long, data: DataFrame): Unit \u00b6 addBatch then opens the constructor's spark-sql-streaming-ForeachWriter.md[ForeachWriter] (for the spark-taskscheduler-taskcontext.md#getPartitionId[current partition] and the input batch) and passes the records to process (one at a time per partition). CAUTION: FIXME Why does Spark track whether the writer failed or not? Why couldn't it finally and do close ? CAUTION: FIXME Can we have a constant for \"foreach\" for source in DataStreamWriter ?","title":"ForeachSink"},{"location":"spark-sql-streaming-ForeachSink/#foreachsink","text":"ForeachSink is a typed streaming sink that passes rows (of the type T ) to ForeachWriter (one record at a time per partition). Note ForeachSink is assigned a ForeachWriter when DataStreamWriter is started . ForeachSink is used exclusively in foreach operator.","title":"ForeachSink"},{"location":"spark-sql-streaming-ForeachSink/#source-scala","text":"val records = spark. readStream format(\"text\"). load(\"server-logs/*.out\"). as[String] import org.apache.spark.sql.ForeachWriter val writer = new ForeachWriter[String] { override def open(partitionId: Long, version: Long) = true override def process(value: String) = println(value) override def close(errorOrNull: Throwable) = {} } records.writeStream .queryName(\"server-logs processor\") .foreach(writer) .start Internally, addBatch (the only method from the < >) takes records from the input spark-sql-dataframe.md[DataFrame] (as data ), transforms them to expected type T (of this ForeachSink ) and (now as a spark-sql-dataset.md[Dataset]) spark-sql-dataset.md#foreachPartition[processes each partition].","title":"[source, scala]"},{"location":"spark-sql-streaming-ForeachSink/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-ForeachSink/#addbatchbatchid-long-data-dataframe-unit","text":"addBatch then opens the constructor's spark-sql-streaming-ForeachWriter.md[ForeachWriter] (for the spark-taskscheduler-taskcontext.md#getPartitionId[current partition] and the input batch) and passes the records to process (one at a time per partition). CAUTION: FIXME Why does Spark track whether the writer failed or not? Why couldn't it finally and do close ? CAUTION: FIXME Can we have a constant for \"foreach\" for source in DataStreamWriter ?","title":"addBatch(batchId: Long, data: DataFrame): Unit"},{"location":"spark-sql-streaming-ForeachWriter/","text":"ForeachWriter \u00b6 ForeachWriter is the < > for a foreach writer that is a streaming format that controls streaming writes. Note ForeachWriter is set using DataStreamWriter.foreach operator. val foreachWriter = new ForeachWriter [ String ] { ... } streamingQuery . writeStream . foreach ( foreachWriter ). start === [[contract]] ForeachWriter Contract [source, scala] \u00b6 package org.apache.spark.sql abstract class ForeachWriter[T] { def open(partitionId: Long, version: Long): Boolean def process(value: T): Unit def close(errorOrNull: Throwable): Unit } .ForeachWriter Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[open]] open | Used when... | [[process]] process | Used when... | [[close]] close | Used when... |===","title":"ForeachWriter"},{"location":"spark-sql-streaming-ForeachWriter/#foreachwriter","text":"ForeachWriter is the < > for a foreach writer that is a streaming format that controls streaming writes. Note ForeachWriter is set using DataStreamWriter.foreach operator. val foreachWriter = new ForeachWriter [ String ] { ... } streamingQuery . writeStream . foreach ( foreachWriter ). start === [[contract]] ForeachWriter Contract","title":"ForeachWriter"},{"location":"spark-sql-streaming-ForeachWriter/#source-scala","text":"package org.apache.spark.sql abstract class ForeachWriter[T] { def open(partitionId: Long, version: Long): Boolean def process(value: T): Unit def close(errorOrNull: Throwable): Unit } .ForeachWriter Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[open]] open | Used when... | [[process]] process | Used when... | [[close]] close | Used when... |===","title":"[source, scala]"},{"location":"spark-sql-streaming-ForeachWriterProvider/","text":"== [[ForeachWriterProvider]] ForeachWriterProvider ForeachWriterProvider is...FIXME","title":"ForeachWriterProvider"},{"location":"spark-sql-streaming-GroupStateTimeout/","text":"GroupStateTimeout \u2014 Group State Timeout in Arbitrary Stateful Streaming Aggregation \u00b6 GroupStateTimeout represents an aggregation state timeout that defines when a GroupState can be considered timed-out ( expired ) in Arbitrary Stateful Streaming Aggregation . GroupStateTimeout is used with the following KeyValueGroupedDataset operations: mapGroupsWithState flatMapGroupsWithState [[extensions]] .GroupStateTimeouts [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | GroupStateTimeout | Description | EventTimeTimeout | [[EventTimeTimeout]] Timeout based on event time Used when...FIXME | NoTimeout | [[NoTimeout]] No timeout Used when...FIXME | ProcessingTimeTimeout a| [[ProcessingTimeTimeout]] Timeout based on processing time FlatMapGroupsWithStateExec physical operator requires that batchTimestampMs is specified when ProcessingTimeTimeout is used. batchTimestampMs is defined when < > is created (with the < >). IncrementalExecution is given OffsetSeqMetadata when StreamExecution is requested to < >. |===","title":"GroupStateTimeout"},{"location":"spark-sql-streaming-GroupStateTimeout/#groupstatetimeout-group-state-timeout-in-arbitrary-stateful-streaming-aggregation","text":"GroupStateTimeout represents an aggregation state timeout that defines when a GroupState can be considered timed-out ( expired ) in Arbitrary Stateful Streaming Aggregation . GroupStateTimeout is used with the following KeyValueGroupedDataset operations: mapGroupsWithState flatMapGroupsWithState [[extensions]] .GroupStateTimeouts [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | GroupStateTimeout | Description | EventTimeTimeout | [[EventTimeTimeout]] Timeout based on event time Used when...FIXME | NoTimeout | [[NoTimeout]] No timeout Used when...FIXME | ProcessingTimeTimeout a| [[ProcessingTimeTimeout]] Timeout based on processing time FlatMapGroupsWithStateExec physical operator requires that batchTimestampMs is specified when ProcessingTimeTimeout is used. batchTimestampMs is defined when < > is created (with the < >). IncrementalExecution is given OffsetSeqMetadata when StreamExecution is requested to < >. |===","title":"GroupStateTimeout &mdash; Group State Timeout in Arbitrary Stateful Streaming Aggregation"},{"location":"spark-sql-streaming-HDFSBackedStateStore/","text":"== [[HDFSBackedStateStore]] HDFSBackedStateStore -- State Store on HDFS-Compatible File System HDFSBackedStateStore is a concrete < > that uses a Hadoop DFS-compatible file system for versioned state persistence. HDFSBackedStateStore is < > exclusively when HDFSBackedStateStoreProvider is requested for the < > (when StateStore utility is requested to < >). [[id]] HDFSBackedStateStore uses the < > of the owning < >. [[toString]] When requested for the textual representation, HDFSBackedStateStore gives HDFSStateStore[id=(op=[operatorId],part=[partitionId]),dir=[baseDir]] . [[logging]] [TIP] ==== HDFSBackedStateStore is an internal class of < > and uses its < >. ==== === [[creating-instance]] Creating HDFSBackedStateStore Instance HDFSBackedStateStore takes the following to be created: [[version]] Version [[mapToUpdate]] State Map ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ) HDFSBackedStateStore initializes the < >. === [[state]] Internal State -- state Internal Property [source, scala] \u00b6 state: STATE \u00b6 state is the current state of HDFSBackedStateStore and can be in one of the three possible states: < >, < >, and < >. State changes (to the internal < > registry) are allowed as long as HDFSBackedStateStore is in the default < > state. Right after a HDFSBackedStateStore transitions to either < > or < > state, no further state changes are allowed. NOTE: Don't get confused with the term \"state\" as there are two states: the internal < > of HDFSBackedStateStore and the state of a streaming query (that HDFSBackedStateStore is responsible for). [[states]] .Internal States [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | ABORTED a| [[ABORTED]] After < > | COMMITTED a| [[COMMITTED]] After < > < > flag indicates whether HDFSBackedStateStore is in this state or not. | UPDATING a| [[UPDATING]] (default) Initial state after the HDFSBackedStateStore was < > Allows for state changes (e.g. < >, < >, < >) and eventually < > or < > them |=== === [[writeUpdateToDeltaFile]] writeUpdateToDeltaFile Internal Method [source, scala] \u00b6 writeUpdateToDeltaFile( output: DataOutputStream, key: UnsafeRow, value: UnsafeRow): Unit CAUTION: FIXME === [[put]] put Method [source, scala] \u00b6 put( key: UnsafeRow, value: UnsafeRow): Unit NOTE: put is a part of spark-sql-streaming-StateStore.md#put[StateStore Contract] to...FIXME put stores the copies of the key and value in < > internal registry followed by < > (using < >). put reports an IllegalStateException when HDFSBackedStateStore is not in < > state: Cannot put after already committed or aborted === [[commit]] Committing State Changes -- commit Method [source, scala] \u00b6 commit(): Long \u00b6 NOTE: commit is part of the < > to commit state changes. commit requests the parent HDFSBackedStateStoreProvider to < > (with the < >, the < > and the < >). commit transitions HDFSBackedStateStore to < > state. commit prints out the following INFO message to the logs: Committed version [newVersion] for [this] to file [finalDeltaFile] commit returns a < >. commit throws an IllegalStateException when HDFSBackedStateStore is not in < > state: Cannot commit after already committed or aborted commit throws an IllegalStateException for any NonFatal exception: Error committing version [newVersion] into [this] === [[abort]] Aborting State Changes -- abort Method [source, scala] \u00b6 abort(): Unit \u00b6 NOTE: abort is part of the < > to abort the state changes. abort ...FIXME === [[metrics]] Performance Metrics -- metrics Method [source, scala] \u00b6 metrics: StateStoreMetrics \u00b6 NOTE: metrics is part of the < > to get the < >. metrics requests the < > of the parent HDFSBackedStateStoreProvider . The performance metrics of the provider used are only the ones listed in < >. In the end, metrics returns a new < > with the following: < > as the size of < > < > as the < > metric (of the parent provider) < > as the < > and the < > metric of the parent provider === [[hasCommitted]] Are State Changes Committed? -- hasCommitted Method [source, scala] \u00b6 hasCommitted: Boolean \u00b6 NOTE: hasCommitted is part of the < > to indicate whether state changes have been committed or not. hasCommitted returns true when HDFSBackedStateStore is in < > state and false otherwise. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | compressedStream a| [[compressedStream]] [source, scala] \u00b6 compressedStream: DataOutputStream \u00b6 The compressed https://docs.oracle.com/javase/8/docs/api/java/io/DataOutputStream.html[java.io.DataOutputStream ] for the < > | deltaFileStream a| [[deltaFileStream]] [source, scala] \u00b6 deltaFileStream: CheckpointFileManager.CancellableFSDataOutputStream \u00b6 | finalDeltaFile a| [[finalDeltaFile]] [source, scala] \u00b6 finalDeltaFile: Path \u00b6 The Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the < > for the < > | newVersion a| [[newVersion]] [source, scala] \u00b6 newVersion: Long \u00b6 Used exclusively when HDFSBackedStateStore is requested for the < >, to < > and < > |===","title":"HDFSBackedStateStore"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#state-state","text":"state is the current state of HDFSBackedStateStore and can be in one of the three possible states: < >, < >, and < >. State changes (to the internal < > registry) are allowed as long as HDFSBackedStateStore is in the default < > state. Right after a HDFSBackedStateStore transitions to either < > or < > state, no further state changes are allowed. NOTE: Don't get confused with the term \"state\" as there are two states: the internal < > of HDFSBackedStateStore and the state of a streaming query (that HDFSBackedStateStore is responsible for). [[states]] .Internal States [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | ABORTED a| [[ABORTED]] After < > | COMMITTED a| [[COMMITTED]] After < > < > flag indicates whether HDFSBackedStateStore is in this state or not. | UPDATING a| [[UPDATING]] (default) Initial state after the HDFSBackedStateStore was < > Allows for state changes (e.g. < >, < >, < >) and eventually < > or < > them |=== === [[writeUpdateToDeltaFile]] writeUpdateToDeltaFile Internal Method","title":"state: STATE"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_1","text":"writeUpdateToDeltaFile( output: DataOutputStream, key: UnsafeRow, value: UnsafeRow): Unit CAUTION: FIXME === [[put]] put Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_2","text":"put( key: UnsafeRow, value: UnsafeRow): Unit NOTE: put is a part of spark-sql-streaming-StateStore.md#put[StateStore Contract] to...FIXME put stores the copies of the key and value in < > internal registry followed by < > (using < >). put reports an IllegalStateException when HDFSBackedStateStore is not in < > state: Cannot put after already committed or aborted === [[commit]] Committing State Changes -- commit Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#commit-long","text":"NOTE: commit is part of the < > to commit state changes. commit requests the parent HDFSBackedStateStoreProvider to < > (with the < >, the < > and the < >). commit transitions HDFSBackedStateStore to < > state. commit prints out the following INFO message to the logs: Committed version [newVersion] for [this] to file [finalDeltaFile] commit returns a < >. commit throws an IllegalStateException when HDFSBackedStateStore is not in < > state: Cannot commit after already committed or aborted commit throws an IllegalStateException for any NonFatal exception: Error committing version [newVersion] into [this] === [[abort]] Aborting State Changes -- abort Method","title":"commit(): Long"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#abort-unit","text":"NOTE: abort is part of the < > to abort the state changes. abort ...FIXME === [[metrics]] Performance Metrics -- metrics Method","title":"abort(): Unit"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#metrics-statestoremetrics","text":"NOTE: metrics is part of the < > to get the < >. metrics requests the < > of the parent HDFSBackedStateStoreProvider . The performance metrics of the provider used are only the ones listed in < >. In the end, metrics returns a new < > with the following: < > as the size of < > < > as the < > metric (of the parent provider) < > as the < > and the < > metric of the parent provider === [[hasCommitted]] Are State Changes Committed? -- hasCommitted Method","title":"metrics: StateStoreMetrics"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#hascommitted-boolean","text":"NOTE: hasCommitted is part of the < > to indicate whether state changes have been committed or not. hasCommitted returns true when HDFSBackedStateStore is in < > state and false otherwise. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | compressedStream a| [[compressedStream]]","title":"hasCommitted: Boolean"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#compressedstream-dataoutputstream","text":"The compressed https://docs.oracle.com/javase/8/docs/api/java/io/DataOutputStream.html[java.io.DataOutputStream ] for the < > | deltaFileStream a| [[deltaFileStream]]","title":"compressedStream: DataOutputStream"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#deltafilestream-checkpointfilemanagercancellablefsdataoutputstream","text":"| finalDeltaFile a| [[finalDeltaFile]]","title":"deltaFileStream: CheckpointFileManager.CancellableFSDataOutputStream"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#finaldeltafile-path","text":"The Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the < > for the < > | newVersion a| [[newVersion]]","title":"finalDeltaFile: Path"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#source-scala_10","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStore/#newversion-long","text":"Used exclusively when HDFSBackedStateStore is requested for the < >, to < > and < > |===","title":"newVersion: Long"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/","text":"== [[HDFSBackedStateStoreProvider]] HDFSBackedStateStoreProvider -- Hadoop DFS-based StateStoreProvider HDFSBackedStateStoreProvider is a < > that uses a Hadoop DFS-compatible file system for < >. HDFSBackedStateStoreProvider is the default StateStoreProvider per the < > internal configuration property. HDFSBackedStateStoreProvider is < > and immediately requested to < > when StateStoreProvider utility is requested to < >. That is when HDFSBackedStateStoreProvider is given the < > that uniquely identifies the < > to use for a stateful operator and a partition. HDFSStateStoreProvider uses < > to manage state (< >). HDFSBackedStateStoreProvider manages versioned state in delta and snapshot files (and uses a < > internally for faster access to state versions). [[creating-instance]] HDFSBackedStateStoreProvider takes no arguments to be created. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider=ALL Refer to < >. \u00b6 === [[metrics]] Performance Metrics [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | memoryUsedBytes a| [[memoryUsedBytes]] Estimated size of the < > internal registry | count of cache hit on states cache in provider a| [[metricLoadedMapCacheHit]][[loadedMapCacheHitCount]] The number of times < > was successful and found ( hit ) the requested state version in the < > internal cache | count of cache miss on states cache in provider a| [[metricLoadedMapCacheMiss]][[loadedMapCacheMissCount]] The number of times < > could not find ( missed ) the requested state version in the < > internal cache | estimated size of state only on current version a| [[metricStateOnCurrentVersionSizeBytes]][[stateOnCurrentVersionSizeBytes]] Estimated size of the < > (of the < >) |=== === [[baseDir]] State Checkpoint Base Directory -- baseDir Lazy Internal Property [source,scala] \u00b6 baseDir: Path \u00b6 baseDir is the base directory (as Hadoop DFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ]) for < > (for < > and < > state files). baseDir is initialized lazily since it is not yet known when HDFSBackedStateStoreProvider is < >. baseDir is initialized and created based on the < > of the < > when HDFSBackedStateStoreProvider is requested to < >. === [[stateStoreId]][[stateStoreId_]] StateStoreId -- Unique Identifier of State Store As a < >, HDFSBackedStateStoreProvider is associated with a < > (which is a unique identifier of the < > for a stateful operator and a partition). HDFSBackedStateStoreProvider is given the < > at < > (as requested by the < > contract). The < > is then used for the following: HDFSBackedStateStore is requested for the < > HDFSBackedStateStoreProvider is requested for the < > and the < > === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. HDFSBackedStateStoreProvider uses the < > and the < > for the textual representation: HDFSStateStoreProvider[id = (op=[operatorId],part=[partitionId]),dir = [baseDir]] === [[getStore]] Loading Specified Version of State (Store) For Update -- getStore Method [source, scala] \u00b6 getStore( version: Long): StateStore NOTE: getStore is part of the < > for the < > for a specified version. getStore creates a new empty state ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ) and < > for versions greater than 0 . In the end, getStore creates a new < > for the specified version with the new state and prints out the following INFO message to the logs: Retrieved version [version] of [this] for update getStore throws an IllegalArgumentException when the specified version is less than 0 (negative): Version cannot be less than 0 === [[deltaFile]] deltaFile Internal Method [source, scala] \u00b6 deltaFile(version: Long): Path \u00b6 deltaFile simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the [version].delta file in the < >. [NOTE] \u00b6 deltaFile is used when: < > is created (and creates the < >) * HDFSBackedStateStoreProvider is requested to < > \u00b6 === [[snapshotFile]] snapshotFile Internal Method [source, scala] \u00b6 snapshotFile(version: Long): Path \u00b6 snapshotFile simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the [version].snapshot file in the < >. NOTE: snapshotFile is used when HDFSBackedStateStoreProvider is requested to < > or < >. === [[fetchFiles]] Listing All Delta And Snapshot Files In State Checkpoint Directory -- fetchFiles Internal Method [source, scala] \u00b6 fetchFiles(): Seq[StoreFile] \u00b6 fetchFiles requests the < > for < > in the < >. For every file, fetchFiles splits the name into two parts with . (dot) as a separator (files with more or less than two parts are simply ignored) and registers a new StoreFile for snapshot and delta files: For snapshot files, fetchFiles creates a new StoreFile with isSnapshot flag on ( true ) For delta files, fetchFiles creates a new StoreFile with isSnapshot flag off ( false ) NOTE: delta files are only registered if there was no snapshot file for the version. fetchFiles prints out the following WARN message to the logs for any other files: Could not identify file [path] for [this] In the end, fetchFiles sorts the StoreFiles based on their version, prints out the following DEBUG message to the logs, and returns the files. Current set of files for [this]: [storeFiles] NOTE: fetchFiles is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[init]] Initializing StateStoreProvider -- init Method [source, scala] \u00b6 init( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], storeConf: StateStoreConf, hadoopConf: Configuration): Unit NOTE: init is part of the < > to initialize itself. init records the values of the input arguments as the < >, < >, < >, < >, and < > internal properties. init requests the given StateStoreConf for the < > configuration property (that is then recorded in the < > internal property). In the end, init requests the < > to < > the < > directory (with parent directories). === [[filesForVersion]] Finding Snapshot File and Delta Files For Version -- filesForVersion Internal Method [source, scala] \u00b6 filesForVersion( allFiles: Seq[StoreFile], version: Long): Seq[StoreFile] filesForVersion finds the latest snapshot version among the given allFiles files up to and including the given version (it may or may not be available). If a snapshot file was found (among the given file up to and including the given version), filesForVersion takes all delta files between the version of the snapshot file (exclusive) and the given version (inclusive) from the given allFiles files. NOTE: The number of delta files should be the given version minus the snapshot version. If a snapshot file was not found, filesForVersion takes all delta files up to the given version (inclusive) from the given allFiles files. In the end, filesForVersion returns a snapshot version (if available) and all delta files up to the given version (inclusive). NOTE: filesForVersion is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[doMaintenance]] State Maintenance (Snapshotting and Cleaning Up) -- doMaintenance Method [source, scala] \u00b6 doMaintenance(): Unit \u00b6 NOTE: doMaintenance is part of the < > for optional state maintenance. doMaintenance simply does < > followed by < >. In case of any non-fatal errors, doMaintenance simply prints out the following WARN message to the logs: Error performing snapshot and cleaning up [this] ==== [[doSnapshot]] State Snapshoting (Rolling Up Delta Files into Snapshot File) -- doSnapshot Internal Method [source, scala] \u00b6 doSnapshot(): Unit \u00b6 doSnapshot < > ( files ) and prints out the following DEBUG message to the logs: fetchFiles() took [time] ms. doSnapshot returns immediately (and does nothing) when there are no delta and snapshot files. doSnapshot takes the version of the latest file ( lastVersion ). doSnapshot < > (among the files and for the last version). doSnapshot looks up the last version in the < >. When the last version was found in the cache and the number of delta files is above < > internal threshold, doSnapshot < >. In the end, doSnapshot prints out the following DEBUG message to the logs: writeSnapshotFile() took [time] ms. In case of non-fatal errors, doSnapshot simply prints out the following WARN message to the logs: Error doing snapshots for [this] NOTE: doSnapshot is used exclusively when HDFSBackedStateStoreProvider is requested to < >. ==== [[cleanup]] Cleaning Up (Removing Old State Files) -- cleanup Internal Method [source, scala] \u00b6 cleanup(): Unit \u00b6 cleanup < > ( files ) and prints out the following DEBUG message to the logs: fetchFiles() took [time] ms. cleanup returns immediately (and does nothing) when there are no delta and snapshot files. cleanup takes the version of the latest state file ( lastVersion ) and decrements it by < > configuration property (default: 100 ) that gives the earliest version to retain (and all older state files to be removed). cleanup requests the < > to < > of every old state file. cleanup prints out the following DEBUG message to the logs: deleting files took [time] ms. In the end, cleanup prints out the following INFO message to the logs: Deleted files older than [version] for [this]: [filesToDelete] In case of a non-fatal exception, cleanup prints out the following WARN message to the logs: Error cleaning up files for [this] NOTE: cleanup is used exclusively when HDFSBackedStateStoreProvider is requested for < >. === [[close]] Closing State Store Provider -- close Method [source, scala] \u00b6 close(): Unit \u00b6 NOTE: close is part of the < > to close the state store provider. close ...FIXME === [[getMetricsForProvider]] getMetricsForProvider Method [source, scala] \u00b6 getMetricsForProvider(): Map[String, Long] \u00b6 getMetricsForProvider returns the following < >: < > < > < > NOTE: getMetricsForProvider is used exclusively when HDFSBackedStateStore is requested for < >. === [[supportedCustomMetrics]] Supported StateStoreCustomMetrics -- supportedCustomMetrics Method [source, scala] \u00b6 supportedCustomMetrics: Seq[StateStoreCustomMetric] \u00b6 NOTE: supportedCustomMetrics is part of the < > for the < > of a state store provider. supportedCustomMetrics includes the following < >: < > < > < > === [[commitUpdates]] Committing State Changes (As New Version of State) -- commitUpdates Internal Method [source, scala] \u00b6 commitUpdates( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow], output: DataOutputStream): Unit commitUpdates < > (with the given DataOutputStream ) followed by < > (with the given newVersion and the map state). NOTE: commitUpdates is used exclusively when HDFSBackedStateStore is requested to < >. === [[loadMap]] Loading Specified Version of State (from Internal Cache or Snapshot and Delta Files) -- loadMap Internal Method [source, scala] \u00b6 loadMap( version: Long): ConcurrentHashMap[UnsafeRow, UnsafeRow] loadMap firstly tries to find the state version in the < > internal cache and, if found, returns it immediately and increments the < > metric. If the requested state version could not be found in the < > internal cache, loadMap prints out the following WARN message to the logs: [options=\"wrap\"] \u00b6 The state for version [version] doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query. \u00b6 loadMap increments the < > metric. loadMap < > and, if found, < > and returns it. If not found, loadMap tries to find the most recent state version by decrementing the requested version until one is found in the < > internal cache or < >. loadMap < > for all the remaining versions (from the snapshot version up to the requested one). loadMap < > (the closest snapshot and the remaining delta versions) and returns it. In the end, loadMap prints out the following DEBUG message to the logs: Loading state for [version] takes [elapsedMs] ms. NOTE: loadMap is used exclusively when HDFSBackedStateStoreProvider is requested for the < >. === [[readSnapshotFile]] Loading State Snapshot File For Specified Version -- readSnapshotFile Internal Method [source, scala] \u00b6 readSnapshotFile( version: Long): Option[ConcurrentHashMap[UnsafeRow, UnsafeRow]] readSnapshotFile < > for the given version . readSnapshotFile requests the < > to < > and < > ( input ). readSnapshotFile reads the decompressed input stream until an EOF (that is marked as the integer -1 in the stream) and inserts key and value rows in a state map ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ): First integer is the size of a key (buffer) followed by the key itself (of the size). readSnapshotFile creates an UnsafeRow for the key (with the number of fields as indicated by the number of fields of the < >). Next integer is the size of a value (buffer) followed by the value itself (of the size). readSnapshotFile creates an UnsafeRow for the value (with the number of fields as indicated by the number of fields of the < >). In the end, readSnapshotFile prints out the following INFO message to the logs and returns the key-value map. Read snapshot file for version [version] of [this] from [fileToRead] In case of FileNotFoundException readSnapshotFile simply returns None (to indicate no snapshot state file was available and so no state for the version). readSnapshotFile throws an IOException for the size of a key or a value below 0 : Error reading snapshot file [fileToRead] of [this]: [key|value] size cannot be [keySize|valueSize] NOTE: readSnapshotFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[updateFromDeltaFile]] Updating State with State Changes For Specified Version (per Delta File) -- updateFromDeltaFile Internal Method [source, scala] \u00b6 updateFromDeltaFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit [NOTE] \u00b6 updateFromDeltaFile is very similar code-wise to < > with the two main differences: updateFromDeltaFile is given the state map to update (while < > loads the state from a snapshot file) updateFromDeltaFile removes a key from the state map when the value (size) is -1 (while < > throws an IOException ) The following description is almost an exact copy of < > just for completeness. \u00b6 updateFromDeltaFile < > for the requested version . updateFromDeltaFile requests the < > to < > and < > ( input ). updateFromDeltaFile reads the decompressed input stream until an EOF (that is marked as the integer -1 in the stream) and inserts key and value rows in the given state map: First integer is the size of a key (buffer) followed by the key itself (of the size). updateFromDeltaFile creates an UnsafeRow for the key (with the number of fields as indicated by the number of fields of the < >). Next integer is the size of a value (buffer) followed by the value itself (of the size). updateFromDeltaFile creates an UnsafeRow for the value (with the number of fields as indicated by the number of fields of the < >) or removes the corresponding key from the state map (if the value size is -1 ) NOTE: updateFromDeltaFile removes the key-value entry from the state map if the value (size) is -1 . In the end, updateFromDeltaFile prints out the following INFO message to the logs and returns the key-value map. Read delta file for version [version] of [this] from [fileToRead] updateFromDeltaFile throws an IllegalStateException in case of FileNotFoundException while opening the delta file for the specified version: Error reading delta file [fileToRead] of [this]: [fileToRead] does not exist NOTE: updateFromDeltaFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[putStateIntoStateCacheMap]] Caching New Version of State -- putStateIntoStateCacheMap Internal Method [source, scala] \u00b6 putStateIntoStateCacheMap( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit putStateIntoStateCacheMap registers state for a given version, i.e. adds the map state under the newVersion key in the < > internal registry. With the < > threshold as 0 or below, putStateIntoStateCacheMap simply removes all entries from the < > internal registry and returns. putStateIntoStateCacheMap removes the oldest state version(s) in the < > internal registry until its size is at the < > threshold. With the size of the < > internal registry is at the < > threshold, putStateIntoStateCacheMap does two more optimizations per newVersion It does not add the given state when the version of the oldest state is earlier (larger) than the given newVersion It removes the oldest state when older (smaller) than the given newVersion NOTE: putStateIntoStateCacheMap is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[writeSnapshotFile]] Writing Compressed Snapshot File for Specified Version -- writeSnapshotFile Internal Method [source, scala] \u00b6 writeSnapshotFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit writeSnapshotFile < > for the given version. writeSnapshotFile requests the < > to < > (with overwriting enabled) and < >. For every key-value UnsafeRow pair in the given map, writeSnapshotFile writes the size of the key followed by the key itself (as bytes). writeSnapshotFile then writes the size of the value followed by the value itself (as bytes). In the end, writeSnapshotFile prints out the following INFO message to the logs: Written snapshot file for version [version] of [this] at [targetFile] In case of any Throwable exception, writeSnapshotFile < > and re-throws the exception. NOTE: writeSnapshotFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[compressStream]] compressStream Internal Method [source, scala] \u00b6 compressStream( outputStream: DataOutputStream): DataOutputStream compressStream creates a new LZ4CompressionCodec (based on the < >) and requests it to create a LZ4BlockOutputStream with the given DataOutputStream . In the end, compressStream creates a new DataOutputStream with the LZ4BlockOutputStream . NOTE: compressStream is used when...FIXME === [[cancelDeltaFile]] cancelDeltaFile Internal Method [source, scala] \u00b6 cancelDeltaFile( compressedStream: DataOutputStream, rawStream: CancellableFSDataOutputStream): Unit cancelDeltaFile ...FIXME NOTE: cancelDeltaFile is used when...FIXME === [[finalizeDeltaFile]] finalizeDeltaFile Internal Method [source, scala] \u00b6 finalizeDeltaFile( output: DataOutputStream): Unit finalizeDeltaFile simply writes -1 to the given DataOutputStream (to indicate end of file) and closes it. NOTE: finalizeDeltaFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[loadedMaps]] Lookup Table (Cache) of States By Version -- loadedMaps Internal Method [source, scala] \u00b6 loadedMaps: TreeMap[ Long, // version ConcurrentHashMap[UnsafeRow, UnsafeRow]] // state (as keys and values) loadedMaps is a https://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html[java.util.TreeMap ] of state versions sorted according to the reversed ordering of the versions (i.e. long numbers). A new entry (a version and the state updates) can only be added when HDFSBackedStateStoreProvider is requested to < > (and only when the < > internal configuration is above 0 ). loadedMaps is mainly used when HDFSBackedStateStoreProvider is requested to < >. Positive hits (when a version could be found in the cache) is available as the < > performance metric while misses are counted in the < > performance metric. NOTE: With no or missing versions in cache < > metric should be above 0 while < > always 0 (or smaller than the other metric). The estimated size of loadedMaps is available as the < > performance metric. The < > internal configuration is used as the threshold of the number of elements in loadedMaps . When 0 or negative, every < > removes all elements in ( clears ) loadedMaps . NOTE: It is possible to change the configuration at restart of a structured query. The state deltas (the values) in loadedMaps are cleared (all entries removed) when HDFSBackedStateStoreProvider is requested to < >. Used when HDFSBackedStateStoreProvider is requested for the following: < > < > === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | fm a| [[fm]] < > for the < > (and the < >) Used when: Creating a new < > (to create the < > for the < >) HDFSBackedStateStoreProvider is requested to < > (to create the < >), < >, < >, < >, < >, and < > | hadoopConf a| [[hadoopConf]] Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] of the < > Given when HDFSBackedStateStoreProvider is requested to < > | keySchema a| [[keySchema]] [source, scala] \u00b6 keySchema: StructType \u00b6 Schema of the state keys | valueSchema a| [[valueSchema]] [source, scala] \u00b6 valueSchema: StructType \u00b6 Schema of the state values | numberOfVersionsToRetainInMemory a| [[numberOfVersionsToRetainInMemory]] [source, scala] \u00b6 numberOfVersionsToRetainInMemory: Int \u00b6 numberOfVersionsToRetainInMemory is the maximum number of entries in the < > internal registry and is configured by the < > internal configuration. numberOfVersionsToRetainInMemory is a threshold when HDFSBackedStateStoreProvider removes the last key from the < > internal registry (per reverse ordering of state versions) when requested to < >. | sparkConf a| [[sparkConf]] SparkConf |===","title":"HDFSBackedStateStoreProvider"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#refer-to","text":"=== [[metrics]] Performance Metrics [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | memoryUsedBytes a| [[memoryUsedBytes]] Estimated size of the < > internal registry | count of cache hit on states cache in provider a| [[metricLoadedMapCacheHit]][[loadedMapCacheHitCount]] The number of times < > was successful and found ( hit ) the requested state version in the < > internal cache | count of cache miss on states cache in provider a| [[metricLoadedMapCacheMiss]][[loadedMapCacheMissCount]] The number of times < > could not find ( missed ) the requested state version in the < > internal cache | estimated size of state only on current version a| [[metricStateOnCurrentVersionSizeBytes]][[stateOnCurrentVersionSizeBytes]] Estimated size of the < > (of the < >) |=== === [[baseDir]] State Checkpoint Base Directory -- baseDir Lazy Internal Property","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#sourcescala","text":"","title":"[source,scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#basedir-path","text":"baseDir is the base directory (as Hadoop DFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ]) for < > (for < > and < > state files). baseDir is initialized lazily since it is not yet known when HDFSBackedStateStoreProvider is < >. baseDir is initialized and created based on the < > of the < > when HDFSBackedStateStoreProvider is requested to < >. === [[stateStoreId]][[stateStoreId_]] StateStoreId -- Unique Identifier of State Store As a < >, HDFSBackedStateStoreProvider is associated with a < > (which is a unique identifier of the < > for a stateful operator and a partition). HDFSBackedStateStoreProvider is given the < > at < > (as requested by the < > contract). The < > is then used for the following: HDFSBackedStateStore is requested for the < > HDFSBackedStateStoreProvider is requested for the < > and the < > === [[toString]] Textual Representation -- toString Method","title":"baseDir: Path"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. HDFSBackedStateStoreProvider uses the < > and the < > for the textual representation: HDFSStateStoreProvider[id = (op=[operatorId],part=[partitionId]),dir = [baseDir]] === [[getStore]] Loading Specified Version of State (Store) For Update -- getStore Method","title":"toString: String"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_1","text":"getStore( version: Long): StateStore NOTE: getStore is part of the < > for the < > for a specified version. getStore creates a new empty state ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ) and < > for versions greater than 0 . In the end, getStore creates a new < > for the specified version with the new state and prints out the following INFO message to the logs: Retrieved version [version] of [this] for update getStore throws an IllegalArgumentException when the specified version is less than 0 (negative): Version cannot be less than 0 === [[deltaFile]] deltaFile Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#deltafileversion-long-path","text":"deltaFile simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the [version].delta file in the < >.","title":"deltaFile(version: Long): Path"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#note","text":"deltaFile is used when: < > is created (and creates the < >)","title":"[NOTE]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#hdfsbackedstatestoreprovider-is-requested-to","text":"=== [[snapshotFile]] snapshotFile Internal Method","title":"* HDFSBackedStateStoreProvider is requested to &lt;&gt;"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#snapshotfileversion-long-path","text":"snapshotFile simply returns the Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the [version].snapshot file in the < >. NOTE: snapshotFile is used when HDFSBackedStateStoreProvider is requested to < > or < >. === [[fetchFiles]] Listing All Delta And Snapshot Files In State Checkpoint Directory -- fetchFiles Internal Method","title":"snapshotFile(version: Long): Path"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#fetchfiles-seqstorefile","text":"fetchFiles requests the < > for < > in the < >. For every file, fetchFiles splits the name into two parts with . (dot) as a separator (files with more or less than two parts are simply ignored) and registers a new StoreFile for snapshot and delta files: For snapshot files, fetchFiles creates a new StoreFile with isSnapshot flag on ( true ) For delta files, fetchFiles creates a new StoreFile with isSnapshot flag off ( false ) NOTE: delta files are only registered if there was no snapshot file for the version. fetchFiles prints out the following WARN message to the logs for any other files: Could not identify file [path] for [this] In the end, fetchFiles sorts the StoreFiles based on their version, prints out the following DEBUG message to the logs, and returns the files. Current set of files for [this]: [storeFiles] NOTE: fetchFiles is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[init]] Initializing StateStoreProvider -- init Method","title":"fetchFiles(): Seq[StoreFile]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_5","text":"init( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], storeConf: StateStoreConf, hadoopConf: Configuration): Unit NOTE: init is part of the < > to initialize itself. init records the values of the input arguments as the < >, < >, < >, < >, and < > internal properties. init requests the given StateStoreConf for the < > configuration property (that is then recorded in the < > internal property). In the end, init requests the < > to < > the < > directory (with parent directories). === [[filesForVersion]] Finding Snapshot File and Delta Files For Version -- filesForVersion Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_6","text":"filesForVersion( allFiles: Seq[StoreFile], version: Long): Seq[StoreFile] filesForVersion finds the latest snapshot version among the given allFiles files up to and including the given version (it may or may not be available). If a snapshot file was found (among the given file up to and including the given version), filesForVersion takes all delta files between the version of the snapshot file (exclusive) and the given version (inclusive) from the given allFiles files. NOTE: The number of delta files should be the given version minus the snapshot version. If a snapshot file was not found, filesForVersion takes all delta files up to the given version (inclusive) from the given allFiles files. In the end, filesForVersion returns a snapshot version (if available) and all delta files up to the given version (inclusive). NOTE: filesForVersion is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[doMaintenance]] State Maintenance (Snapshotting and Cleaning Up) -- doMaintenance Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#domaintenance-unit","text":"NOTE: doMaintenance is part of the < > for optional state maintenance. doMaintenance simply does < > followed by < >. In case of any non-fatal errors, doMaintenance simply prints out the following WARN message to the logs: Error performing snapshot and cleaning up [this] ==== [[doSnapshot]] State Snapshoting (Rolling Up Delta Files into Snapshot File) -- doSnapshot Internal Method","title":"doMaintenance(): Unit"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#dosnapshot-unit","text":"doSnapshot < > ( files ) and prints out the following DEBUG message to the logs: fetchFiles() took [time] ms. doSnapshot returns immediately (and does nothing) when there are no delta and snapshot files. doSnapshot takes the version of the latest file ( lastVersion ). doSnapshot < > (among the files and for the last version). doSnapshot looks up the last version in the < >. When the last version was found in the cache and the number of delta files is above < > internal threshold, doSnapshot < >. In the end, doSnapshot prints out the following DEBUG message to the logs: writeSnapshotFile() took [time] ms. In case of non-fatal errors, doSnapshot simply prints out the following WARN message to the logs: Error doing snapshots for [this] NOTE: doSnapshot is used exclusively when HDFSBackedStateStoreProvider is requested to < >. ==== [[cleanup]] Cleaning Up (Removing Old State Files) -- cleanup Internal Method","title":"doSnapshot(): Unit"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#cleanup-unit","text":"cleanup < > ( files ) and prints out the following DEBUG message to the logs: fetchFiles() took [time] ms. cleanup returns immediately (and does nothing) when there are no delta and snapshot files. cleanup takes the version of the latest state file ( lastVersion ) and decrements it by < > configuration property (default: 100 ) that gives the earliest version to retain (and all older state files to be removed). cleanup requests the < > to < > of every old state file. cleanup prints out the following DEBUG message to the logs: deleting files took [time] ms. In the end, cleanup prints out the following INFO message to the logs: Deleted files older than [version] for [this]: [filesToDelete] In case of a non-fatal exception, cleanup prints out the following WARN message to the logs: Error cleaning up files for [this] NOTE: cleanup is used exclusively when HDFSBackedStateStoreProvider is requested for < >. === [[close]] Closing State Store Provider -- close Method","title":"cleanup(): Unit"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_10","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#close-unit","text":"NOTE: close is part of the < > to close the state store provider. close ...FIXME === [[getMetricsForProvider]] getMetricsForProvider Method","title":"close(): Unit"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_11","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#getmetricsforprovider-mapstring-long","text":"getMetricsForProvider returns the following < >: < > < > < > NOTE: getMetricsForProvider is used exclusively when HDFSBackedStateStore is requested for < >. === [[supportedCustomMetrics]] Supported StateStoreCustomMetrics -- supportedCustomMetrics Method","title":"getMetricsForProvider(): Map[String, Long]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_12","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#supportedcustommetrics-seqstatestorecustommetric","text":"NOTE: supportedCustomMetrics is part of the < > for the < > of a state store provider. supportedCustomMetrics includes the following < >: < > < > < > === [[commitUpdates]] Committing State Changes (As New Version of State) -- commitUpdates Internal Method","title":"supportedCustomMetrics: Seq[StateStoreCustomMetric]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_13","text":"commitUpdates( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow], output: DataOutputStream): Unit commitUpdates < > (with the given DataOutputStream ) followed by < > (with the given newVersion and the map state). NOTE: commitUpdates is used exclusively when HDFSBackedStateStore is requested to < >. === [[loadMap]] Loading Specified Version of State (from Internal Cache or Snapshot and Delta Files) -- loadMap Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_14","text":"loadMap( version: Long): ConcurrentHashMap[UnsafeRow, UnsafeRow] loadMap firstly tries to find the state version in the < > internal cache and, if found, returns it immediately and increments the < > metric. If the requested state version could not be found in the < > internal cache, loadMap prints out the following WARN message to the logs:","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#the-state-for-version-version-doesnt-exist-in-loadedmaps-reading-snapshot-file-and-delta-files-if-needednote-that-this-is-normal-for-the-first-batch-of-starting-query","text":"loadMap increments the < > metric. loadMap < > and, if found, < > and returns it. If not found, loadMap tries to find the most recent state version by decrementing the requested version until one is found in the < > internal cache or < >. loadMap < > for all the remaining versions (from the snapshot version up to the requested one). loadMap < > (the closest snapshot and the remaining delta versions) and returns it. In the end, loadMap prints out the following DEBUG message to the logs: Loading state for [version] takes [elapsedMs] ms. NOTE: loadMap is used exclusively when HDFSBackedStateStoreProvider is requested for the < >. === [[readSnapshotFile]] Loading State Snapshot File For Specified Version -- readSnapshotFile Internal Method","title":"The state for version [version] doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query."},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_15","text":"readSnapshotFile( version: Long): Option[ConcurrentHashMap[UnsafeRow, UnsafeRow]] readSnapshotFile < > for the given version . readSnapshotFile requests the < > to < > and < > ( input ). readSnapshotFile reads the decompressed input stream until an EOF (that is marked as the integer -1 in the stream) and inserts key and value rows in a state map ( ConcurrentHashMap[UnsafeRow, UnsafeRow] ): First integer is the size of a key (buffer) followed by the key itself (of the size). readSnapshotFile creates an UnsafeRow for the key (with the number of fields as indicated by the number of fields of the < >). Next integer is the size of a value (buffer) followed by the value itself (of the size). readSnapshotFile creates an UnsafeRow for the value (with the number of fields as indicated by the number of fields of the < >). In the end, readSnapshotFile prints out the following INFO message to the logs and returns the key-value map. Read snapshot file for version [version] of [this] from [fileToRead] In case of FileNotFoundException readSnapshotFile simply returns None (to indicate no snapshot state file was available and so no state for the version). readSnapshotFile throws an IOException for the size of a key or a value below 0 : Error reading snapshot file [fileToRead] of [this]: [key|value] size cannot be [keySize|valueSize] NOTE: readSnapshotFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[updateFromDeltaFile]] Updating State with State Changes For Specified Version (per Delta File) -- updateFromDeltaFile Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_16","text":"updateFromDeltaFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#note_1","text":"updateFromDeltaFile is very similar code-wise to < > with the two main differences: updateFromDeltaFile is given the state map to update (while < > loads the state from a snapshot file) updateFromDeltaFile removes a key from the state map when the value (size) is -1 (while < > throws an IOException )","title":"[NOTE]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#the-following-description-is-almost-an-exact-copy-of-just-for-completeness","text":"updateFromDeltaFile < > for the requested version . updateFromDeltaFile requests the < > to < > and < > ( input ). updateFromDeltaFile reads the decompressed input stream until an EOF (that is marked as the integer -1 in the stream) and inserts key and value rows in the given state map: First integer is the size of a key (buffer) followed by the key itself (of the size). updateFromDeltaFile creates an UnsafeRow for the key (with the number of fields as indicated by the number of fields of the < >). Next integer is the size of a value (buffer) followed by the value itself (of the size). updateFromDeltaFile creates an UnsafeRow for the value (with the number of fields as indicated by the number of fields of the < >) or removes the corresponding key from the state map (if the value size is -1 ) NOTE: updateFromDeltaFile removes the key-value entry from the state map if the value (size) is -1 . In the end, updateFromDeltaFile prints out the following INFO message to the logs and returns the key-value map. Read delta file for version [version] of [this] from [fileToRead] updateFromDeltaFile throws an IllegalStateException in case of FileNotFoundException while opening the delta file for the specified version: Error reading delta file [fileToRead] of [this]: [fileToRead] does not exist NOTE: updateFromDeltaFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[putStateIntoStateCacheMap]] Caching New Version of State -- putStateIntoStateCacheMap Internal Method","title":"The following description is almost an exact copy of &lt;&gt; just for completeness."},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_17","text":"putStateIntoStateCacheMap( newVersion: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit putStateIntoStateCacheMap registers state for a given version, i.e. adds the map state under the newVersion key in the < > internal registry. With the < > threshold as 0 or below, putStateIntoStateCacheMap simply removes all entries from the < > internal registry and returns. putStateIntoStateCacheMap removes the oldest state version(s) in the < > internal registry until its size is at the < > threshold. With the size of the < > internal registry is at the < > threshold, putStateIntoStateCacheMap does two more optimizations per newVersion It does not add the given state when the version of the oldest state is earlier (larger) than the given newVersion It removes the oldest state when older (smaller) than the given newVersion NOTE: putStateIntoStateCacheMap is used when HDFSBackedStateStoreProvider is requested to < > and < >. === [[writeSnapshotFile]] Writing Compressed Snapshot File for Specified Version -- writeSnapshotFile Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_18","text":"writeSnapshotFile( version: Long, map: ConcurrentHashMap[UnsafeRow, UnsafeRow]): Unit writeSnapshotFile < > for the given version. writeSnapshotFile requests the < > to < > (with overwriting enabled) and < >. For every key-value UnsafeRow pair in the given map, writeSnapshotFile writes the size of the key followed by the key itself (as bytes). writeSnapshotFile then writes the size of the value followed by the value itself (as bytes). In the end, writeSnapshotFile prints out the following INFO message to the logs: Written snapshot file for version [version] of [this] at [targetFile] In case of any Throwable exception, writeSnapshotFile < > and re-throws the exception. NOTE: writeSnapshotFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[compressStream]] compressStream Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_19","text":"compressStream( outputStream: DataOutputStream): DataOutputStream compressStream creates a new LZ4CompressionCodec (based on the < >) and requests it to create a LZ4BlockOutputStream with the given DataOutputStream . In the end, compressStream creates a new DataOutputStream with the LZ4BlockOutputStream . NOTE: compressStream is used when...FIXME === [[cancelDeltaFile]] cancelDeltaFile Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_20","text":"cancelDeltaFile( compressedStream: DataOutputStream, rawStream: CancellableFSDataOutputStream): Unit cancelDeltaFile ...FIXME NOTE: cancelDeltaFile is used when...FIXME === [[finalizeDeltaFile]] finalizeDeltaFile Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_21","text":"finalizeDeltaFile( output: DataOutputStream): Unit finalizeDeltaFile simply writes -1 to the given DataOutputStream (to indicate end of file) and closes it. NOTE: finalizeDeltaFile is used exclusively when HDFSBackedStateStoreProvider is requested to < >. === [[loadedMaps]] Lookup Table (Cache) of States By Version -- loadedMaps Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_22","text":"loadedMaps: TreeMap[ Long, // version ConcurrentHashMap[UnsafeRow, UnsafeRow]] // state (as keys and values) loadedMaps is a https://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html[java.util.TreeMap ] of state versions sorted according to the reversed ordering of the versions (i.e. long numbers). A new entry (a version and the state updates) can only be added when HDFSBackedStateStoreProvider is requested to < > (and only when the < > internal configuration is above 0 ). loadedMaps is mainly used when HDFSBackedStateStoreProvider is requested to < >. Positive hits (when a version could be found in the cache) is available as the < > performance metric while misses are counted in the < > performance metric. NOTE: With no or missing versions in cache < > metric should be above 0 while < > always 0 (or smaller than the other metric). The estimated size of loadedMaps is available as the < > performance metric. The < > internal configuration is used as the threshold of the number of elements in loadedMaps . When 0 or negative, every < > removes all elements in ( clears ) loadedMaps . NOTE: It is possible to change the configuration at restart of a structured query. The state deltas (the values) in loadedMaps are cleared (all entries removed) when HDFSBackedStateStoreProvider is requested to < >. Used when HDFSBackedStateStoreProvider is requested for the following: < > < > === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | fm a| [[fm]] < > for the < > (and the < >) Used when: Creating a new < > (to create the < > for the < >) HDFSBackedStateStoreProvider is requested to < > (to create the < >), < >, < >, < >, < >, and < > | hadoopConf a| [[hadoopConf]] Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] of the < > Given when HDFSBackedStateStoreProvider is requested to < > | keySchema a| [[keySchema]]","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_23","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#keyschema-structtype","text":"Schema of the state keys | valueSchema a| [[valueSchema]]","title":"keySchema: StructType"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_24","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#valueschema-structtype","text":"Schema of the state values | numberOfVersionsToRetainInMemory a| [[numberOfVersionsToRetainInMemory]]","title":"valueSchema: StructType"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#source-scala_25","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSBackedStateStoreProvider/#numberofversionstoretaininmemory-int","text":"numberOfVersionsToRetainInMemory is the maximum number of entries in the < > internal registry and is configured by the < > internal configuration. numberOfVersionsToRetainInMemory is a threshold when HDFSBackedStateStoreProvider removes the last key from the < > internal registry (per reverse ordering of state versions) when requested to < >. | sparkConf a| [[sparkConf]] SparkConf |===","title":"numberOfVersionsToRetainInMemory: Int"},{"location":"spark-sql-streaming-HDFSMetadataLog/","text":"== [[HDFSMetadataLog]] HDFSMetadataLog -- Hadoop DFS-based Metadata Storage HDFSMetadataLog is a concrete < > (of type T ) that uses Hadoop DFS for fault-tolerance and reliability. [[metadataPath]] HDFSMetadataLog uses the given < > as the metadata directory with metadata logs. The path is immediately converted to a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] for file management. [[formats]] HDFSMetadataLog uses http://json4s.org/[Json4s ] with the https://github.com/FasterXML/jackson-databind[Jackson ] binding for metadata < > and < > (to and from JSON format). HDFSMetadataLog is further customized by the < >. [[extensions]] .HDFSMetadataLogs (Direct Extensions Only) [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | HDFSMetadataLog | Description | Anonymous | [[KafkaSource]] HDFSMetadataLog of < > for < > | Anonymous | [[RateStreamMicroBatchReader]] HDFSMetadataLog of < > for < > | < > | [[CommitLog]] Offset commit log of streaming query execution engines | < > | [[CompactibleFileStreamLog]] Compactible metadata logs (that compact logs at regular interval) | < > | [[KafkaSourceInitialOffsetWriter]] HDFSMetadataLog of < > for < > | < > | [[OffsetSeqLog]] Write-Ahead Log (WAL) of streaming query execution engines |=== Creating Instance \u00b6 HDFSMetadataLog takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory While being < > HDFSMetadataLog creates the < > unless exists already. === [[serialize]] Serializing Metadata (Writing Metadata in Serialized Format) -- serialize Method [source, scala] \u00b6 serialize( metadata: T, out: OutputStream): Unit serialize simply writes the log data (serialized using < > library). NOTE: serialize is used exclusively when HDFSMetadataLog is requested to < > (when < >). === [[deserialize]] Deserializing Metadata (Reading Metadata from Serialized Format) -- deserialize Method [source, scala] \u00b6 deserialize(in: InputStream): T \u00b6 deserialize deserializes a metadata (of type T ) from a given InputStream . NOTE: deserialize is used exclusively when HDFSMetadataLog is requested to < >. === [[get]][[get-batchId]] Retrieving Metadata Of Streaming Batch -- get Method [source, scala] \u00b6 get(batchId: Long): Option[T] \u00b6 NOTE: get is part of the < > to get metadata of a batch. get ...FIXME === [[get-range]] Retrieving Metadata of Range of Batches -- get Method [source, scala] \u00b6 get( startId: Option[Long], endId: Option[Long]): Array[(Long, T)] NOTE: get is part of the < > to get metadata of range of batches. get ...FIXME === [[add]] Persisting Metadata of Streaming Micro-Batch -- add Method [source, scala] \u00b6 add( batchId: Long, metadata: T): Boolean NOTE: add is part of the < > to persist metadata of a streaming batch. add return true when the metadata of the streaming batch was not available and persisted successfully. Otherwise, add returns false . Internally, add < > ( batchId ) and returns false when found. Otherwise, when not found, add < > for the given batchId and < >. add returns true if successful. === [[getLatest]] Latest Committed Batch Id with Metadata (When Available) -- getLatest Method [source, scala] \u00b6 getLatest(): Option[(Long, T)] \u00b6 NOTE: getLatest is a part of spark-sql-streaming-MetadataLog.md#getLatest[MetadataLog Contract] to retrieve the recently-committed batch id and the corresponding metadata if available in the metadata storage. getLatest requests the internal < > for the files in < > that match < >. getLatest takes the batch ids (the batch files correspond to) and sorts the ids in reverse order. getLatest gives the first batch id with the metadata which < >. NOTE: It is possible that the batch id could be in the metadata storage, but not available for retrieval. === [[purge]] Removing Expired Metadata (Purging) -- purge Method [source, scala] \u00b6 purge(thresholdBatchId: Long): Unit \u00b6 NOTE: purge is part of the < > to...FIXME. purge ...FIXME === [[batchIdToPath]] Creating Batch Metadata File -- batchIdToPath Method [source, scala] \u00b6 batchIdToPath(batchId: Long): Path \u00b6 batchIdToPath simply creates a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] for the file called by the specified batchId under the < >. [NOTE] \u00b6 batchIdToPath is used when: CompactibleFileStreamLog is requested to < > and < > * HDFSMetadataLog is requested to < >, < >, < >, and < > \u00b6 === [[isBatchFile]] isBatchFile Method [source, scala] \u00b6 isBatchFile(path: Path): Boolean \u00b6 isBatchFile ...FIXME NOTE: isBatchFile is used exclusively when HDFSMetadataLog is requested for the < >. === [[pathToBatchId]] pathToBatchId Method [source, scala] \u00b6 pathToBatchId(path: Path): Long \u00b6 pathToBatchId ...FIXME [NOTE] \u00b6 pathToBatchId is used when: CompactibleFileStreamLog is requested for the < > * HDFSMetadataLog is requested to < >, < >, < >, < >, < >, and < > \u00b6 === [[verifyBatchIds]] verifyBatchIds Object Method [source, scala] \u00b6 verifyBatchIds( batchIds: Seq[Long], startId: Option[Long], endId: Option[Long]): Unit verifyBatchIds ...FIXME [NOTE] \u00b6 verifyBatchIds is used when: FileStreamSourceLog is requested to < > * HDFSMetadataLog is requested to < > \u00b6 === [[parseVersion]] Retrieving Version (From Text Line) -- parseVersion Internal Method [source, scala] \u00b6 parseVersion( text: String, maxSupportedVersion: Int): Int parseVersion ...FIXME [NOTE] \u00b6 parseVersion is used when: KafkaSourceInitialOffsetWriter is requested to < > KafkaSource is requested for the < > CommitLog is requested to < > CompactibleFileStreamLog is requested to < > OffsetSeqLog is requested to < > * RateStreamMicroBatchReader is requested to < > \u00b6 === [[purgeAfter]] purgeAfter Method [source, scala] \u00b6 purgeAfter(thresholdBatchId: Long): Unit \u00b6 purgeAfter ...FIXME NOTE: purgeAfter seems to be used exclusively in tests. === [[writeBatchToFile]] Writing Batch Metadata to File (Metadata Log) -- writeBatchToFile Internal Method [source, scala] \u00b6 writeBatchToFile( metadata: T, path: Path): Unit writeBatchToFile requests the < > to < > (for the specified path and the overwriteIfPossible flag disabled). writeBatchToFile then < > (to the CancellableFSDataOutputStream output stream) and closes the stream. In case of an exception, writeBatchToFile simply requests the CancellableFSDataOutputStream output stream to cancel (so that the output file is not generated) and re-throws the exception. NOTE: writeBatchToFile is used exclusively when HDFSMetadataLog is requested to < >. === [[getOrderedBatchFiles]] Retrieving Ordered Batch Metadata Files -- getOrderedBatchFiles Method [source, scala] \u00b6 getOrderedBatchFiles(): Array[FileStatus] \u00b6 getOrderedBatchFiles ...FIXME NOTE: getOrderedBatchFiles does not seem to be used at all. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | batchFilesFilter a| [[batchFilesFilter]] Hadoop's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/PathFilter.html[PathFilter ] of < > (with names being long numbers) Used when: CompactibleFileStreamLog is requested for the < > HDFSMetadataLog is requested to < >, < >, < >, < >, and < > | fileManager a| [[fileManager]] < > Used when...FIXME |===","title":"HDFSMetadataLog"},{"location":"spark-sql-streaming-HDFSMetadataLog/#creating-instance","text":"HDFSMetadataLog takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory While being < > HDFSMetadataLog creates the < > unless exists already. === [[serialize]] Serializing Metadata (Writing Metadata in Serialized Format) -- serialize Method","title":"Creating Instance"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala","text":"serialize( metadata: T, out: OutputStream): Unit serialize simply writes the log data (serialized using < > library). NOTE: serialize is used exclusively when HDFSMetadataLog is requested to < > (when < >). === [[deserialize]] Deserializing Metadata (Reading Metadata from Serialized Format) -- deserialize Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#deserializein-inputstream-t","text":"deserialize deserializes a metadata (of type T ) from a given InputStream . NOTE: deserialize is used exclusively when HDFSMetadataLog is requested to < >. === [[get]][[get-batchId]] Retrieving Metadata Of Streaming Batch -- get Method","title":"deserialize(in: InputStream): T"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#getbatchid-long-optiont","text":"NOTE: get is part of the < > to get metadata of a batch. get ...FIXME === [[get-range]] Retrieving Metadata of Range of Batches -- get Method","title":"get(batchId: Long): Option[T]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_3","text":"get( startId: Option[Long], endId: Option[Long]): Array[(Long, T)] NOTE: get is part of the < > to get metadata of range of batches. get ...FIXME === [[add]] Persisting Metadata of Streaming Micro-Batch -- add Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_4","text":"add( batchId: Long, metadata: T): Boolean NOTE: add is part of the < > to persist metadata of a streaming batch. add return true when the metadata of the streaming batch was not available and persisted successfully. Otherwise, add returns false . Internally, add < > ( batchId ) and returns false when found. Otherwise, when not found, add < > for the given batchId and < >. add returns true if successful. === [[getLatest]] Latest Committed Batch Id with Metadata (When Available) -- getLatest Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#getlatest-optionlong-t","text":"NOTE: getLatest is a part of spark-sql-streaming-MetadataLog.md#getLatest[MetadataLog Contract] to retrieve the recently-committed batch id and the corresponding metadata if available in the metadata storage. getLatest requests the internal < > for the files in < > that match < >. getLatest takes the batch ids (the batch files correspond to) and sorts the ids in reverse order. getLatest gives the first batch id with the metadata which < >. NOTE: It is possible that the batch id could be in the metadata storage, but not available for retrieval. === [[purge]] Removing Expired Metadata (Purging) -- purge Method","title":"getLatest(): Option[(Long, T)]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#purgethresholdbatchid-long-unit","text":"NOTE: purge is part of the < > to...FIXME. purge ...FIXME === [[batchIdToPath]] Creating Batch Metadata File -- batchIdToPath Method","title":"purge(thresholdBatchId: Long): Unit"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#batchidtopathbatchid-long-path","text":"batchIdToPath simply creates a Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] for the file called by the specified batchId under the < >.","title":"batchIdToPath(batchId: Long): Path"},{"location":"spark-sql-streaming-HDFSMetadataLog/#note","text":"batchIdToPath is used when: CompactibleFileStreamLog is requested to < > and < >","title":"[NOTE]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#hdfsmetadatalog-is-requested-to-and","text":"=== [[isBatchFile]] isBatchFile Method","title":"* HDFSMetadataLog is requested to &lt;&gt;, &lt;&gt;, &lt;&gt;, and &lt;&gt;"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#isbatchfilepath-path-boolean","text":"isBatchFile ...FIXME NOTE: isBatchFile is used exclusively when HDFSMetadataLog is requested for the < >. === [[pathToBatchId]] pathToBatchId Method","title":"isBatchFile(path: Path): Boolean"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#pathtobatchidpath-path-long","text":"pathToBatchId ...FIXME","title":"pathToBatchId(path: Path): Long"},{"location":"spark-sql-streaming-HDFSMetadataLog/#note_1","text":"pathToBatchId is used when: CompactibleFileStreamLog is requested for the < >","title":"[NOTE]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#hdfsmetadatalog-is-requested-to-and_1","text":"=== [[verifyBatchIds]] verifyBatchIds Object Method","title":"* HDFSMetadataLog is requested to &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, and &lt;&gt;"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_10","text":"verifyBatchIds( batchIds: Seq[Long], startId: Option[Long], endId: Option[Long]): Unit verifyBatchIds ...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#note_2","text":"verifyBatchIds is used when: FileStreamSourceLog is requested to < >","title":"[NOTE]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#hdfsmetadatalog-is-requested-to","text":"=== [[parseVersion]] Retrieving Version (From Text Line) -- parseVersion Internal Method","title":"* HDFSMetadataLog is requested to &lt;&gt;"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_11","text":"parseVersion( text: String, maxSupportedVersion: Int): Int parseVersion ...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#note_3","text":"parseVersion is used when: KafkaSourceInitialOffsetWriter is requested to < > KafkaSource is requested for the < > CommitLog is requested to < > CompactibleFileStreamLog is requested to < > OffsetSeqLog is requested to < >","title":"[NOTE]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#ratestreammicrobatchreader-is-requested-to","text":"=== [[purgeAfter]] purgeAfter Method","title":"* RateStreamMicroBatchReader is requested to &lt;&gt;"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_12","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#purgeafterthresholdbatchid-long-unit","text":"purgeAfter ...FIXME NOTE: purgeAfter seems to be used exclusively in tests. === [[writeBatchToFile]] Writing Batch Metadata to File (Metadata Log) -- writeBatchToFile Internal Method","title":"purgeAfter(thresholdBatchId: Long): Unit"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_13","text":"writeBatchToFile( metadata: T, path: Path): Unit writeBatchToFile requests the < > to < > (for the specified path and the overwriteIfPossible flag disabled). writeBatchToFile then < > (to the CancellableFSDataOutputStream output stream) and closes the stream. In case of an exception, writeBatchToFile simply requests the CancellableFSDataOutputStream output stream to cancel (so that the output file is not generated) and re-throws the exception. NOTE: writeBatchToFile is used exclusively when HDFSMetadataLog is requested to < >. === [[getOrderedBatchFiles]] Retrieving Ordered Batch Metadata Files -- getOrderedBatchFiles Method","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#source-scala_14","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-HDFSMetadataLog/#getorderedbatchfiles-arrayfilestatus","text":"getOrderedBatchFiles ...FIXME NOTE: getOrderedBatchFiles does not seem to be used at all. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | batchFilesFilter a| [[batchFilesFilter]] Hadoop's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/PathFilter.html[PathFilter ] of < > (with names being long numbers) Used when: CompactibleFileStreamLog is requested for the < > HDFSMetadataLog is requested to < >, < >, < >, < >, and < > | fileManager a| [[fileManager]] < > Used when...FIXME |===","title":"getOrderedBatchFiles(): Array[FileStatus]"},{"location":"spark-sql-streaming-IncrementalExecution/","text":"IncrementalExecution \u2014 QueryExecution of Streaming Queries \u00b6 IncrementalExecution is the QueryExecution of streaming queries. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-QueryExecution.html[QueryExecution ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. IncrementalExecution is < > (and becomes the StreamExecution.lastExecution ) when: MicroBatchExecution is requested to < > (in < > phase) ContinuousExecution is requested to < > (in < > phase) Dataset.explain operator is executed (on a streaming query) [[statefulOperatorId]] IncrementalExecution uses the statefulOperatorId internal counter for the IDs of the stateful operators in the < > (while applying the < > rules) when requested to prepare the plan for execution (in < > phase). === [[preparing-for-execution]][[optimizedPlan]][[executedPlan]][[preparations]] Preparing Logical Plan (of Streaming Query) for Execution -- optimizedPlan and executedPlan Phases of Query Execution When requested for the optimized logical plan (of the < >), IncrementalExecution transforms CurrentBatchTimestamp and ExpressionWithRandomSeed expressions with the timestamp literal and new random seeds, respectively. When transforming CurrentBatchTimestamp expressions, IncrementalExecution prints out the following INFO message to the logs: Current batch timestamp = [timestamp] Once < >, IncrementalExecution is immediately executed (by the < > and < > stream execution engines in the queryPlanning phase) and so the entire query execution pipeline is executed up to and including executedPlan . That means that the < > and the < > have been applied at this point and the < > is ready for execution. === [[creating-instance]] Creating IncrementalExecution Instance IncrementalExecution takes the following to be created: [[sparkSession]] SparkSession [[logicalPlan]] Logical plan ( LogicalPlan ) [[outputMode]] < > (as specified using DataStreamWriter.outputMode method) [[checkpointLocation]] < > [[runId]] Run ID of a streaming query ( UUID ) [[currentBatchId]] Batch ID [[offsetSeqMetadata]] < > === [[state-checkpoint-location]] State Checkpoint Location (Directory) When < >, IncrementalExecution is given the < >. For the two available execution engines (< > and < >), the checkpoint location is actually state directory under the checkpoint root directory . val queryName = \"rate2memory\" val checkpointLocation = s\"file:/tmp/checkpoint-$queryName\" val query = spark .readStream .format(\"rate\") .load .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .start // Give the streaming query a moment (one micro-batch) // So lastExecution is available for the checkpointLocation import scala.concurrent.duration._ query.awaitTermination(1.second.toMillis) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val stateCheckpointDir = query .asInstanceOf[StreamingQueryWrapper] .streamingQuery .lastExecution .checkpointLocation val stateDir = s\"$checkpointLocation/state\" assert(stateCheckpointDir equals stateDir) State checkpoint location is used exclusively when IncrementalExecution is requested for the < > (when requested to optimize a streaming physical plan using the < > that creates the stateful physical operators: < >, < >, < >, FlatMapGroupsWithStateExec , < >, and < >). === [[numStateStores]] Number of State Stores (spark.sql.shuffle.partitions) -- numStateStores Internal Property [source, scala] \u00b6 numStateStores: Int \u00b6 numStateStores is the number of state stores which corresponds to spark.sql.shuffle.partitions configuration property (default: 200 ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-properties.html#spark.sql.shuffle.partitions[spark.sql.shuffle.partitions ] configuration property (and the others) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. Internally, numStateStores requests the < > for the < > configuration property (using the < >) or simply takes whatever was defined for the given < > (default: 200 ). numStateStores is initialized right when IncrementalExecution is < >. numStateStores is used exclusively when IncrementalExecution is requested for the < > (when requested to optimize a streaming physical plan using the < > that creates the stateful physical operators: < >, < >, < >, FlatMapGroupsWithStateExec , < >, and < >). === [[planner]][[extraPlanningStrategies]] Extra Planning Strategies for Streaming Queries -- planner Property IncrementalExecution uses a custom SparkPlanner with the following extra planning strategies to plan the < > for execution: < > < > < > < > < > < > TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlanner.html[SparkPlanner ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. === [[state]] State Preparation Rule For Execution-Specific Configuration -- state Property [source, scala] \u00b6 state: Rule[SparkPlan] \u00b6 state is a custom physical preparation rule ( Rule[SparkPlan] ) that can transform a streaming physical plan ( SparkPlan ) with the following physical operators: < > with any unary physical operator ( UnaryExecNode ) with a < > < > FlatMapGroupsWithStateExec < > < > state simply transforms the physical plan with the above physical operators and fills out the execution-specific configuration: < > for the state info < > < > (through the < >) for the event-time watermark < > (through the < >) for the current timestamp < > for the state watermark predicates (for < >) state rule is used (as part of the physical query optimizations) when IncrementalExecution is requested to < > (once for < > and every trigger for < > in their queryPlanning phases). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-QueryExecution.html#preparations[Physical Query Optimizations] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. === [[nextStatefulOperationStateInfo]] nextStatefulOperationStateInfo Internal Method [source, scala] \u00b6 nextStatefulOperationStateInfo(): StatefulOperatorStateInfo \u00b6 nextStatefulOperationStateInfo simply creates a new < > with the < >, the < > (of the streaming query), the next < >, the < >, and the < >. [NOTE] \u00b6 The only changing part of StatefulOperatorStateInfo across calls of the nextStatefulOperationStateInfo method is the the next < >. All the other properties (the < >, the < >, the < >, and the < >) are the same within a single IncrementalExecution instance. The only two properties that may ever change are the < > (after a streaming query is restarted from the checkpoint) and the < > (every micro-batch in < > execution engine). \u00b6 NOTE: nextStatefulOperationStateInfo is used exclusively when IncrementalExecution is requested to optimize a streaming physical plan using the < > (and creates the stateful physical operators: < >, < >, < >, FlatMapGroupsWithStateExec , < >, and < >). === [[shouldRunAnotherBatch]] Checking Out Whether Last Execution Requires Another Non-Data Micro-Batch -- shouldRunAnotherBatch Method [source, scala] \u00b6 shouldRunAnotherBatch(newMetadata: OffsetSeqMetadata): Boolean \u00b6 shouldRunAnotherBatch is positive ( true ) if there is at least one StateStoreWriter operator (in the < >) that requires another non-data batch (per the given < > with the event-time watermark and the batch timestamp). Otherwise, shouldRunAnotherBatch is negative ( false ). NOTE: shouldRunAnotherBatch is used exclusively when MicroBatchExecution is requested to < > (and checks out whether the last batch execution requires another non-data batch). === [[demo]] Demo: State Checkpoint Directory [source, scala] \u00b6 // START: Only for easier debugging // The state is then only for one partition // which should make monitoring easier import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1) assert(spark.sessionState.conf.numShufflePartitions == 1) // END: Only for easier debugging val counts = spark .readStream .format(\"rate\") .load .groupBy(window($\"timestamp\", \"5 seconds\") as \"group\") .agg(count(\"value\") as \"value_count\") // \u2190 creates an Aggregate logical operator .orderBy(\"group\") // \u2190 makes for easier checking assert(counts.isStreaming, \"This should be a streaming query\") // Search for \"checkpoint = \" in the following output // Looks for StateStoreSave and StateStoreRestore scala> counts.explain == Physical Plan == *(5) Sort [group#5 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#5 ASC NULLS FIRST, 1) +- *(4) HashAggregate(keys=[window#11], functions=[count(value#1L)]) +- StateStoreSave [window#11], state info [ checkpoint = , runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- *(3) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)]) +- StateStoreRestore [window#11], state info [ checkpoint = , runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], 2 +- *(2) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)]) +- Exchange hashpartitioning(window#11, 1) +- *(1) HashAggregate(keys=[window#11], functions=[partial_count(value#1L)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#11, value#1L] +- *(1) Filter isnotnull(timestamp#0) +- StreamingRelation rate, [timestamp#0, value#1L] // Start the query to access lastExecution that has the checkpoint resolved import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val t = Trigger.ProcessingTime(1.hour) // should be enough time for exploration val sq = counts .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", \"/tmp/spark-streams-state-checkpoint-root\") .trigger(t) .outputMode(OutputMode.Complete) .start // wait till the first batch which should happen right after start import org.apache.spark.sql.execution.streaming._ val lastExecution = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery.lastExecution scala> println(lastExecution.checkpointLocation) file:/tmp/spark-streams-state-checkpoint-root/state","title":"IncrementalExecution"},{"location":"spark-sql-streaming-IncrementalExecution/#incrementalexecution-queryexecution-of-streaming-queries","text":"IncrementalExecution is the QueryExecution of streaming queries. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-QueryExecution.html[QueryExecution ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. IncrementalExecution is < > (and becomes the StreamExecution.lastExecution ) when: MicroBatchExecution is requested to < > (in < > phase) ContinuousExecution is requested to < > (in < > phase) Dataset.explain operator is executed (on a streaming query) [[statefulOperatorId]] IncrementalExecution uses the statefulOperatorId internal counter for the IDs of the stateful operators in the < > (while applying the < > rules) when requested to prepare the plan for execution (in < > phase). === [[preparing-for-execution]][[optimizedPlan]][[executedPlan]][[preparations]] Preparing Logical Plan (of Streaming Query) for Execution -- optimizedPlan and executedPlan Phases of Query Execution When requested for the optimized logical plan (of the < >), IncrementalExecution transforms CurrentBatchTimestamp and ExpressionWithRandomSeed expressions with the timestamp literal and new random seeds, respectively. When transforming CurrentBatchTimestamp expressions, IncrementalExecution prints out the following INFO message to the logs: Current batch timestamp = [timestamp] Once < >, IncrementalExecution is immediately executed (by the < > and < > stream execution engines in the queryPlanning phase) and so the entire query execution pipeline is executed up to and including executedPlan . That means that the < > and the < > have been applied at this point and the < > is ready for execution. === [[creating-instance]] Creating IncrementalExecution Instance IncrementalExecution takes the following to be created: [[sparkSession]] SparkSession [[logicalPlan]] Logical plan ( LogicalPlan ) [[outputMode]] < > (as specified using DataStreamWriter.outputMode method) [[checkpointLocation]] < > [[runId]] Run ID of a streaming query ( UUID ) [[currentBatchId]] Batch ID [[offsetSeqMetadata]] < > === [[state-checkpoint-location]] State Checkpoint Location (Directory) When < >, IncrementalExecution is given the < >. For the two available execution engines (< > and < >), the checkpoint location is actually state directory under the checkpoint root directory . val queryName = \"rate2memory\" val checkpointLocation = s\"file:/tmp/checkpoint-$queryName\" val query = spark .readStream .format(\"rate\") .load .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .start // Give the streaming query a moment (one micro-batch) // So lastExecution is available for the checkpointLocation import scala.concurrent.duration._ query.awaitTermination(1.second.toMillis) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val stateCheckpointDir = query .asInstanceOf[StreamingQueryWrapper] .streamingQuery .lastExecution .checkpointLocation val stateDir = s\"$checkpointLocation/state\" assert(stateCheckpointDir equals stateDir) State checkpoint location is used exclusively when IncrementalExecution is requested for the < > (when requested to optimize a streaming physical plan using the < > that creates the stateful physical operators: < >, < >, < >, FlatMapGroupsWithStateExec , < >, and < >). === [[numStateStores]] Number of State Stores (spark.sql.shuffle.partitions) -- numStateStores Internal Property","title":"IncrementalExecution &mdash; QueryExecution of Streaming Queries"},{"location":"spark-sql-streaming-IncrementalExecution/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-IncrementalExecution/#numstatestores-int","text":"numStateStores is the number of state stores which corresponds to spark.sql.shuffle.partitions configuration property (default: 200 ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-properties.html#spark.sql.shuffle.partitions[spark.sql.shuffle.partitions ] configuration property (and the others) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. Internally, numStateStores requests the < > for the < > configuration property (using the < >) or simply takes whatever was defined for the given < > (default: 200 ). numStateStores is initialized right when IncrementalExecution is < >. numStateStores is used exclusively when IncrementalExecution is requested for the < > (when requested to optimize a streaming physical plan using the < > that creates the stateful physical operators: < >, < >, < >, FlatMapGroupsWithStateExec , < >, and < >). === [[planner]][[extraPlanningStrategies]] Extra Planning Strategies for Streaming Queries -- planner Property IncrementalExecution uses a custom SparkPlanner with the following extra planning strategies to plan the < > for execution: < > < > < > < > < > < > TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlanner.html[SparkPlanner ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. === [[state]] State Preparation Rule For Execution-Specific Configuration -- state Property","title":"numStateStores: Int"},{"location":"spark-sql-streaming-IncrementalExecution/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-IncrementalExecution/#state-rulesparkplan","text":"state is a custom physical preparation rule ( Rule[SparkPlan] ) that can transform a streaming physical plan ( SparkPlan ) with the following physical operators: < > with any unary physical operator ( UnaryExecNode ) with a < > < > FlatMapGroupsWithStateExec < > < > state simply transforms the physical plan with the above physical operators and fills out the execution-specific configuration: < > for the state info < > < > (through the < >) for the event-time watermark < > (through the < >) for the current timestamp < > for the state watermark predicates (for < >) state rule is used (as part of the physical query optimizations) when IncrementalExecution is requested to < > (once for < > and every trigger for < > in their queryPlanning phases). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-QueryExecution.html#preparations[Physical Query Optimizations] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. === [[nextStatefulOperationStateInfo]] nextStatefulOperationStateInfo Internal Method","title":"state: Rule[SparkPlan]"},{"location":"spark-sql-streaming-IncrementalExecution/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-IncrementalExecution/#nextstatefuloperationstateinfo-statefuloperatorstateinfo","text":"nextStatefulOperationStateInfo simply creates a new < > with the < >, the < > (of the streaming query), the next < >, the < >, and the < >.","title":"nextStatefulOperationStateInfo(): StatefulOperatorStateInfo"},{"location":"spark-sql-streaming-IncrementalExecution/#note","text":"The only changing part of StatefulOperatorStateInfo across calls of the nextStatefulOperationStateInfo method is the the next < >. All the other properties (the < >, the < >, the < >, and the < >) are the same within a single IncrementalExecution instance.","title":"[NOTE]"},{"location":"spark-sql-streaming-IncrementalExecution/#the-only-two-properties-that-may-ever-change-are-the-after-a-streaming-query-is-restarted-from-the-checkpoint-and-the-every-micro-batch-in-execution-engine","text":"NOTE: nextStatefulOperationStateInfo is used exclusively when IncrementalExecution is requested to optimize a streaming physical plan using the < > (and creates the stateful physical operators: < >, < >, < >, FlatMapGroupsWithStateExec , < >, and < >). === [[shouldRunAnotherBatch]] Checking Out Whether Last Execution Requires Another Non-Data Micro-Batch -- shouldRunAnotherBatch Method","title":"The only two properties that may ever change are the &lt;&gt; (after a streaming query is restarted from the checkpoint) and the &lt;&gt; (every micro-batch in &lt;&gt; execution engine)."},{"location":"spark-sql-streaming-IncrementalExecution/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-IncrementalExecution/#shouldrunanotherbatchnewmetadata-offsetseqmetadata-boolean","text":"shouldRunAnotherBatch is positive ( true ) if there is at least one StateStoreWriter operator (in the < >) that requires another non-data batch (per the given < > with the event-time watermark and the batch timestamp). Otherwise, shouldRunAnotherBatch is negative ( false ). NOTE: shouldRunAnotherBatch is used exclusively when MicroBatchExecution is requested to < > (and checks out whether the last batch execution requires another non-data batch). === [[demo]] Demo: State Checkpoint Directory","title":"shouldRunAnotherBatch(newMetadata: OffsetSeqMetadata): Boolean"},{"location":"spark-sql-streaming-IncrementalExecution/#source-scala_4","text":"// START: Only for easier debugging // The state is then only for one partition // which should make monitoring easier import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1) assert(spark.sessionState.conf.numShufflePartitions == 1) // END: Only for easier debugging val counts = spark .readStream .format(\"rate\") .load .groupBy(window($\"timestamp\", \"5 seconds\") as \"group\") .agg(count(\"value\") as \"value_count\") // \u2190 creates an Aggregate logical operator .orderBy(\"group\") // \u2190 makes for easier checking assert(counts.isStreaming, \"This should be a streaming query\") // Search for \"checkpoint = \" in the following output // Looks for StateStoreSave and StateStoreRestore scala> counts.explain == Physical Plan == *(5) Sort [group#5 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#5 ASC NULLS FIRST, 1) +- *(4) HashAggregate(keys=[window#11], functions=[count(value#1L)]) +- StateStoreSave [window#11], state info [ checkpoint = , runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- *(3) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)]) +- StateStoreRestore [window#11], state info [ checkpoint = , runId = 558bf725-accb-487d-97eb-f790fa4a6138, opId = 0, ver = 0, numPartitions = 1], 2 +- *(2) HashAggregate(keys=[window#11], functions=[merge_count(value#1L)]) +- Exchange hashpartitioning(window#11, 1) +- *(1) HashAggregate(keys=[window#11], functions=[partial_count(value#1L)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#11, value#1L] +- *(1) Filter isnotnull(timestamp#0) +- StreamingRelation rate, [timestamp#0, value#1L] // Start the query to access lastExecution that has the checkpoint resolved import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val t = Trigger.ProcessingTime(1.hour) // should be enough time for exploration val sq = counts .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", \"/tmp/spark-streams-state-checkpoint-root\") .trigger(t) .outputMode(OutputMode.Complete) .start // wait till the first batch which should happen right after start import org.apache.spark.sql.execution.streaming._ val lastExecution = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery.lastExecution scala> println(lastExecution.checkpointLocation) file:/tmp/spark-streams-state-checkpoint-root/state","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/","text":"== [[JoinStateWatermarkPredicate]] JoinStateWatermarkPredicate Contract (Sealed Trait) JoinStateWatermarkPredicate is the < > of < > that are described by a < > and < >. JoinStateWatermarkPredicate is created using < > utility (for < >) JoinStateWatermarkPredicate is used to create a < > (and < >). [[contract]] .JoinStateWatermarkPredicate Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | desc a| [[desc]] [source, scala] \u00b6 desc: String \u00b6 Used exclusively for the < > | expr a| [[expr]] [source, scala] \u00b6 expr: Expression \u00b6 A Catalyst Expression Used for the < > and a < > (for < > physical operator) |=== [[implementations]] .JoinStateWatermarkPredicates [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | JoinStateWatermarkPredicate | Description | JoinStateKeyWatermarkPredicate a| [[JoinStateKeyWatermarkPredicate]] Watermark predicate on state keys (i.e. when the < > is defined either on the < > or < > join keys) Created when StreamingSymmetricHashJoinHelper utility is requested for a < > for the left and right side of a stream-stream join (when IncrementalExecution is requested to optimize a query plan with a < > physical operator) Used when OneSideHashJoiner is requested for the < > and then to < > | JoinStateValueWatermarkPredicate | [[JoinStateValueWatermarkPredicate]] Watermark predicate on state values |=== NOTE: JoinStateWatermarkPredicate is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > and < > for the string representation: [desc]: [expr]","title":"JoinStateWatermarkPredicate"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#desc-string","text":"Used exclusively for the < > | expr a| [[expr]]","title":"desc: String"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#expr-expression","text":"A Catalyst Expression Used for the < > and a < > (for < > physical operator) |=== [[implementations]] .JoinStateWatermarkPredicates [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | JoinStateWatermarkPredicate | Description | JoinStateKeyWatermarkPredicate a| [[JoinStateKeyWatermarkPredicate]] Watermark predicate on state keys (i.e. when the < > is defined either on the < > or < > join keys) Created when StreamingSymmetricHashJoinHelper utility is requested for a < > for the left and right side of a stream-stream join (when IncrementalExecution is requested to optimize a query plan with a < > physical operator) Used when OneSideHashJoiner is requested for the < > and then to < > | JoinStateValueWatermarkPredicate | [[JoinStateValueWatermarkPredicate]] Watermark predicate on state values |=== NOTE: JoinStateWatermarkPredicate is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). === [[toString]] Textual Representation -- toString Method","title":"expr: Expression"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicate/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > and < > for the string representation: [desc]: [expr]","title":"toString: String"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicates/","text":"== [[JoinStateWatermarkPredicates]] JoinStateWatermarkPredicates -- Watermark Predicates for State Removal [[creating-instance]] JoinStateWatermarkPredicates contains watermark predicates for state removal of the children of a < > physical operator: [[left]] < > for the left-hand side of a join (default: None ) [[right]] < > for the right-hand side of a join (default: None ) JoinStateWatermarkPredicates is < > for the following: < > physical operator is created (with the optional properties undefined, including < >) StreamingSymmetricHashJoinHelper utility is requested for < > (for IncrementalExecution for the < > to optimize and specify the execution-specific configuration for a query plan with < > physical operators) === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > and < > predicates for the string representation: state cleanup [ left [left], right [right] ]","title":"JoinStateWatermarkPredicates"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicates/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-JoinStateWatermarkPredicates/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > and < > predicates for the string representation: state cleanup [ left [left], right [right] ]","title":"toString: String"},{"location":"spark-sql-streaming-KafkaContinuousInputPartition/","text":"== [[KafkaContinuousInputPartition]] KafkaContinuousInputPartition KafkaContinuousInputPartition is...FIXME","title":"KafkaContinuousInputPartition"},{"location":"spark-sql-streaming-KafkaContinuousReader/","text":"== [[KafkaContinuousReader]] KafkaContinuousReader -- ContinuousReader for Kafka Data Source in Continuous Stream Processing KafkaContinuousReader is a < > for < > in < >. KafkaContinuousReader is < > exclusively when KafkaSourceProvider is requested to < >. [[pollTimeoutMs]] [[kafkaConsumer.pollTimeoutMs]] KafkaContinuousReader uses kafkaConsumer.pollTimeoutMs configuration parameter (default: 512 ) for < > when requested to < >. [[logging]] [TIP] ==== Enable INFO or WARN logging levels for org.apache.spark.sql.kafka010.KafkaContinuousReader to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaContinuousReader=INFO Refer to spark-sql-streaming-logging.md[Logging]. \u00b6 === [[creating-instance]] Creating KafkaContinuousReader Instance KafkaContinuousReader takes the following to be created: [[offsetReader]] < > [[kafkaParams]] Kafka parameters (as java.util.Map[String, Object] ) [[sourceOptions]] Source options (as Map[String, String] ) [[metadataPath]] Metadata path [[initialOffsets]] < > [[failOnDataLoss]] failOnDataLoss flag === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method [source, scala] \u00b6 planInputPartitions(): java.util.List[InputPartition[InternalRow]] \u00b6 NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions ...FIXME === [[setStartOffset]] setStartOffset Method [source, java] \u00b6 setStartOffset( start: Optional[Offset]): Unit NOTE: setStartOffset is part of the < > to...FIXME. setStartOffset ...FIXME === [[deserializeOffset]] deserializeOffset Method [source, java] \u00b6 deserializeOffset( json: String): Offset NOTE: deserializeOffset is part of the < > to...FIXME. deserializeOffset ...FIXME === [[mergeOffsets]] mergeOffsets Method [source, java] \u00b6 mergeOffsets( offsets: Array[PartitionOffset]): Offset NOTE: mergeOffsets is part of the < > to...FIXME. mergeOffsets ...FIXME","title":"KafkaContinuousReader"},{"location":"spark-sql-streaming-KafkaContinuousReader/#refer-to-spark-sql-streaming-loggingmdlogging","text":"=== [[creating-instance]] Creating KafkaContinuousReader Instance KafkaContinuousReader takes the following to be created: [[offsetReader]] < > [[kafkaParams]] Kafka parameters (as java.util.Map[String, Object] ) [[sourceOptions]] Source options (as Map[String, String] ) [[metadataPath]] Metadata path [[initialOffsets]] < > [[failOnDataLoss]] failOnDataLoss flag === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method","title":"Refer to spark-sql-streaming-logging.md[Logging]."},{"location":"spark-sql-streaming-KafkaContinuousReader/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaContinuousReader/#planinputpartitions-javautillistinputpartitioninternalrow","text":"NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions ...FIXME === [[setStartOffset]] setStartOffset Method","title":"planInputPartitions(): java.util.List[InputPartition[InternalRow]]"},{"location":"spark-sql-streaming-KafkaContinuousReader/#source-java","text":"setStartOffset( start: Optional[Offset]): Unit NOTE: setStartOffset is part of the < > to...FIXME. setStartOffset ...FIXME === [[deserializeOffset]] deserializeOffset Method","title":"[source, java]"},{"location":"spark-sql-streaming-KafkaContinuousReader/#source-java_1","text":"deserializeOffset( json: String): Offset NOTE: deserializeOffset is part of the < > to...FIXME. deserializeOffset ...FIXME === [[mergeOffsets]] mergeOffsets Method","title":"[source, java]"},{"location":"spark-sql-streaming-KafkaContinuousReader/#source-java_2","text":"mergeOffsets( offsets: Array[PartitionOffset]): Offset NOTE: mergeOffsets is part of the < > to...FIXME. mergeOffsets ...FIXME","title":"[source, java]"},{"location":"spark-sql-streaming-KafkaDataConsumer/","text":"== [[KafkaDataConsumer]] KafkaDataConsumer KafkaDataConsumer is the < > of < > that use < > that can be < >. [[contract]] .KafkaDataConsumer Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | internalConsumer a| [[internalConsumer]] [source, scala] \u00b6 internalConsumer: InternalKafkaConsumer \u00b6 Used when...FIXME | release a| [[release]] [source, scala] \u00b6 release(): Unit \u00b6 Used when...FIXME |=== [[implementations]] .KafkaDataConsumers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | KafkaDataConsumer | Description | CachedKafkaDataConsumer | [[CachedKafkaDataConsumer]] | NonCachedKafkaDataConsumer | [[NonCachedKafkaDataConsumer]] |=== === [[acquire]] Acquiring Cached KafkaDataConsumer for Partition -- acquire Object Method [source, scala] \u00b6 acquire( topicPartition: TopicPartition, kafkaParams: ju.Map[String, Object], useCache: Boolean ): KafkaDataConsumer acquire ...FIXME NOTE: acquire is used when...FIXME === [[get]] Getting Kafka Record -- get Method [source, scala] \u00b6 get( offset: Long, untilOffset: Long, pollTimeoutMs: Long, failOnDataLoss: Boolean ): ConsumerRecord[Array[Byte], Array[Byte]] get ...FIXME NOTE: get is used when...FIXME","title":"KafkaDataConsumer"},{"location":"spark-sql-streaming-KafkaDataConsumer/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaDataConsumer/#internalconsumer-internalkafkaconsumer","text":"Used when...FIXME | release a| [[release]]","title":"internalConsumer: InternalKafkaConsumer"},{"location":"spark-sql-streaming-KafkaDataConsumer/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaDataConsumer/#release-unit","text":"Used when...FIXME |=== [[implementations]] .KafkaDataConsumers [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | KafkaDataConsumer | Description | CachedKafkaDataConsumer | [[CachedKafkaDataConsumer]] | NonCachedKafkaDataConsumer | [[NonCachedKafkaDataConsumer]] |=== === [[acquire]] Acquiring Cached KafkaDataConsumer for Partition -- acquire Object Method","title":"release(): Unit"},{"location":"spark-sql-streaming-KafkaDataConsumer/#source-scala_2","text":"acquire( topicPartition: TopicPartition, kafkaParams: ju.Map[String, Object], useCache: Boolean ): KafkaDataConsumer acquire ...FIXME NOTE: acquire is used when...FIXME === [[get]] Getting Kafka Record -- get Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaDataConsumer/#source-scala_3","text":"get( offset: Long, untilOffset: Long, pollTimeoutMs: Long, failOnDataLoss: Boolean ): ConsumerRecord[Array[Byte], Array[Byte]] get ...FIXME NOTE: get is used when...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchInputPartition/","text":"== [[KafkaMicroBatchInputPartition]] KafkaMicroBatchInputPartition KafkaMicroBatchInputPartition is an InputPartition (of InternalRows ) that is used (< >) exclusively when KafkaMicroBatchReader is requested for < > (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). [[creating-instance]] KafkaMicroBatchInputPartition takes the following to be created: [[offsetRange]] < > [[executorKafkaParams]] Kafka parameters used for Kafka clients on executors ( Map[String, Object] ) [[pollTimeoutMs]] Poll timeout (in ms) [[failOnDataLoss]] failOnDataLoss flag [[reuseKafkaConsumer]] reuseKafkaConsumer flag [[createPartitionReader]] KafkaMicroBatchInputPartition creates a < > when requested for a InputPartitionReader[InternalRow] (as a part of the InputPartition contract). [[preferredLocations]] KafkaMicroBatchInputPartition simply requests the given < > for the optional preferredLoc when requested for preferredLocations (as a part of the InputPartition contract).","title":"KafkaMicroBatchInputPartition"},{"location":"spark-sql-streaming-KafkaMicroBatchInputPartitionReader/","text":"== [[KafkaMicroBatchInputPartitionReader]] KafkaMicroBatchInputPartitionReader KafkaMicroBatchInputPartitionReader is an InputPartitionReader (of InternalRows ) that is < > exclusively when KafkaMicroBatchInputPartition is requested for < > (as a part of the InputPartition contract). === [[creating-instance]] Creating KafkaMicroBatchInputPartitionReader Instance KafkaMicroBatchInputPartitionReader takes the following to be created: [[offsetRange]] < > [[executorKafkaParams]] Kafka parameters used for Kafka clients on executors ( Map[String, Object] ) [[pollTimeoutMs]] Poll timeout (in ms) [[failOnDataLoss]] failOnDataLoss flag [[reuseKafkaConsumer]] reuseKafkaConsumer flag NOTE: All the input arguments to create a KafkaMicroBatchInputPartitionReader are exactly the input arguments used to create a < >. KafkaMicroBatchInputPartitionReader initializes the < >. === [[next]] next Method [source, scala] \u00b6 next(): Boolean \u00b6 NOTE: next is part of the InputPartitionReader contract to proceed to next record if available ( true ). next checks whether the < > should < > or < > (i.e. < > is smaller than the < > of the < >). ==== [[next-poll]] next Method -- KafkaDataConsumer Polls Records If so, next requests the < > to get ( poll ) records in the range of < > and the < > (of the < >) with the given < > and < >. With a new record, next requests the < > to convert ( toUnsafeRow ) the record to be the < >. next sets the < > as the offset of the record incremented. next returns true . With no new record, next simply returns false . ==== [[next-no-poll]] next Method -- No Polling If the < > is equal or larger than the < > (of the < >), next simply returns false . === [[close]] Closing (Releasing KafkaDataConsumer) -- close Method [source, scala] \u00b6 close(): Unit \u00b6 NOTE: close is part of the Java Closeable contract to release resources. close simply requests the < > to release . === [[resolveRange]] resolveRange Internal Method [source, scala] \u00b6 resolveRange( range: KafkaOffsetRange): KafkaOffsetRange resolveRange ...FIXME NOTE: resolveRange is used exclusively when KafkaMicroBatchInputPartitionReader is < > (and initializes the < > internal property). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | consumer a| [[consumer]] < > for the partition (per < >) Used in < >, < >, and < > | converter a| [[converter]] KafkaRecordToUnsafeRowConverter | nextOffset a| [[nextOffset]] Next offset | nextRow a| [[nextRow]] Next UnsafeRow | rangeToRead a| [[rangeToRead]] KafkaOffsetRange |===","title":"KafkaMicroBatchInputPartitionReader"},{"location":"spark-sql-streaming-KafkaMicroBatchInputPartitionReader/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchInputPartitionReader/#next-boolean","text":"NOTE: next is part of the InputPartitionReader contract to proceed to next record if available ( true ). next checks whether the < > should < > or < > (i.e. < > is smaller than the < > of the < >). ==== [[next-poll]] next Method -- KafkaDataConsumer Polls Records If so, next requests the < > to get ( poll ) records in the range of < > and the < > (of the < >) with the given < > and < >. With a new record, next requests the < > to convert ( toUnsafeRow ) the record to be the < >. next sets the < > as the offset of the record incremented. next returns true . With no new record, next simply returns false . ==== [[next-no-poll]] next Method -- No Polling If the < > is equal or larger than the < > (of the < >), next simply returns false . === [[close]] Closing (Releasing KafkaDataConsumer) -- close Method","title":"next(): Boolean"},{"location":"spark-sql-streaming-KafkaMicroBatchInputPartitionReader/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchInputPartitionReader/#close-unit","text":"NOTE: close is part of the Java Closeable contract to release resources. close simply requests the < > to release . === [[resolveRange]] resolveRange Internal Method","title":"close(): Unit"},{"location":"spark-sql-streaming-KafkaMicroBatchInputPartitionReader/#source-scala_2","text":"resolveRange( range: KafkaOffsetRange): KafkaOffsetRange resolveRange ...FIXME NOTE: resolveRange is used exclusively when KafkaMicroBatchInputPartitionReader is < > (and initializes the < > internal property). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | consumer a| [[consumer]] < > for the partition (per < >) Used in < >, < >, and < > | converter a| [[converter]] KafkaRecordToUnsafeRowConverter | nextOffset a| [[nextOffset]] Next offset | nextRow a| [[nextRow]] Next UnsafeRow | rangeToRead a| [[rangeToRead]] KafkaOffsetRange |===","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/","text":"== [[KafkaMicroBatchReader]] KafkaMicroBatchReader KafkaMicroBatchReader is the < > for < > for < >. KafkaMicroBatchReader is < > exclusively when KafkaSourceProvider is requested to < >. [[pollTimeoutMs]] KafkaMicroBatchReader uses the < > to access the < > option (default: spark.network.timeout or 120s ). [[maxOffsetsPerTrigger]] KafkaMicroBatchReader uses the < > to access the < > option (default: (undefined) ). KafkaMicroBatchReader uses the < > to create < > when requested to < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaMicroBatchReader to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaMicroBatchReader=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating KafkaMicroBatchReader Instance KafkaMicroBatchReader takes the following to be created: [[kafkaOffsetReader]] < > [[executorKafkaParams]] Kafka properties for executors ( Map[String, Object] ) [[options]] DataSourceOptions [[metadataPath]] Metadata Path [[startingOffsets]] Desired starting < > [[failOnDataLoss]] < > option KafkaMicroBatchReader initializes the < >. === [[readSchema]] readSchema Method [source, scala] \u00b6 readSchema(): StructType \u00b6 NOTE: readSchema is part of the DataSourceReader contract to...FIXME. readSchema simply returns the < >. === [[stop]] Stopping Streaming Reader -- stop Method [source, scala] \u00b6 stop(): Unit \u00b6 NOTE: stop is part of the < > to stop a streaming reader. stop simply requests the < > to < >. === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method [source, scala] \u00b6 planInputPartitions(): java.util.List[InputPartition[InternalRow]] \u00b6 NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions first finds the new partitions ( TopicPartitions that are in the < > but not in the < >) and requests the < > to < >. planInputPartitions prints out the following INFO message to the logs: Partitions added: [newPartitionInitialOffsets] planInputPartitions then prints out the following DEBUG message to the logs: TopicPartitions: [comma-separated list of TopicPartitions] planInputPartitions requests the < > for < > (given the < > and the newly-calculated newPartitionInitialOffsets as the fromOffsets , the < > as the untilOffsets , and the < >). In the end, planInputPartitions creates a < > for every offset range (with the < >, the < >, the < > flag and whether to reuse a Kafka consumer among Spark tasks). NOTE: < > uses a shared Kafka consumer only when all the offset ranges have distinct TopicPartitions , so concurrent tasks (of a stage in a Spark job) will not interfere and read the same TopicPartitions . planInputPartitions < > when...FIXME === [[getSortedExecutorList]] Available Executors in Spark Cluster (Sorted By Host and Executor ID in Descending Order) -- getSortedExecutorList Internal Method [source, scala] \u00b6 getSortedExecutorList(): Array[String] \u00b6 getSortedExecutorList requests the BlockManager to request the BlockManagerMaster to get the peers (the other nodes in a Spark cluster), creates a ExecutorCacheTaskLocation for every pair of host and executor ID, and in the end, sort it in descending order. NOTE: getSortedExecutorList is used exclusively when KafkaMicroBatchReader is requested to < > (and calculates offset ranges). === [[getOrCreateInitialPartitionOffsets]] getOrCreateInitialPartitionOffsets Internal Method [source, scala] \u00b6 getOrCreateInitialPartitionOffsets(): PartitionOffsetMap \u00b6 getOrCreateInitialPartitionOffsets ...FIXME NOTE: getOrCreateInitialPartitionOffsets is used exclusively for the < > internal registry. === [[getStartOffset]] getStartOffset Method [source, scala] \u00b6 getStartOffset: Offset \u00b6 NOTE: getStartOffset is part of the < > to get the start (beginning) < >. getStartOffset ...FIXME === [[getEndOffset]] getEndOffset Method [source, scala] \u00b6 getEndOffset: Offset \u00b6 NOTE: getEndOffset is part of the < > to get the end < >. getEndOffset ...FIXME === [[deserializeOffset]] deserializeOffset Method [source, scala] \u00b6 deserializeOffset(json: String): Offset \u00b6 NOTE: deserializeOffset is part of the < > to deserialize an < > (from JSON format). deserializeOffset ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | endPartitionOffsets a| [[endPartitionOffsets]] Ending offsets for the assigned partitions ( Map[TopicPartition, Long] ) Used when...FIXME | initialPartitionOffsets a| [[initialPartitionOffsets]] [source, scala] \u00b6 initialPartitionOffsets: Map[TopicPartition, Long] \u00b6 | rangeCalculator a| [[rangeCalculator]] < > (for the given < >) Used exclusively when KafkaMicroBatchReader is requested to < > (to calculate offset ranges) | startPartitionOffsets a| [[startPartitionOffsets]] Starting offsets for the assigned partitions ( Map[TopicPartition, Long] ) Used when...FIXME |===","title":"KafkaMicroBatchReader"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#refer-to","text":"=== [[creating-instance]] Creating KafkaMicroBatchReader Instance KafkaMicroBatchReader takes the following to be created: [[kafkaOffsetReader]] < > [[executorKafkaParams]] Kafka properties for executors ( Map[String, Object] ) [[options]] DataSourceOptions [[metadataPath]] Metadata Path [[startingOffsets]] Desired starting < > [[failOnDataLoss]] < > option KafkaMicroBatchReader initializes the < >. === [[readSchema]] readSchema Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#readschema-structtype","text":"NOTE: readSchema is part of the DataSourceReader contract to...FIXME. readSchema simply returns the < >. === [[stop]] Stopping Streaming Reader -- stop Method","title":"readSchema(): StructType"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#stop-unit","text":"NOTE: stop is part of the < > to stop a streaming reader. stop simply requests the < > to < >. === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method","title":"stop(): Unit"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#planinputpartitions-javautillistinputpartitioninternalrow","text":"NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions first finds the new partitions ( TopicPartitions that are in the < > but not in the < >) and requests the < > to < >. planInputPartitions prints out the following INFO message to the logs: Partitions added: [newPartitionInitialOffsets] planInputPartitions then prints out the following DEBUG message to the logs: TopicPartitions: [comma-separated list of TopicPartitions] planInputPartitions requests the < > for < > (given the < > and the newly-calculated newPartitionInitialOffsets as the fromOffsets , the < > as the untilOffsets , and the < >). In the end, planInputPartitions creates a < > for every offset range (with the < >, the < >, the < > flag and whether to reuse a Kafka consumer among Spark tasks). NOTE: < > uses a shared Kafka consumer only when all the offset ranges have distinct TopicPartitions , so concurrent tasks (of a stage in a Spark job) will not interfere and read the same TopicPartitions . planInputPartitions < > when...FIXME === [[getSortedExecutorList]] Available Executors in Spark Cluster (Sorted By Host and Executor ID in Descending Order) -- getSortedExecutorList Internal Method","title":"planInputPartitions(): java.util.List[InputPartition[InternalRow]]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#getsortedexecutorlist-arraystring","text":"getSortedExecutorList requests the BlockManager to request the BlockManagerMaster to get the peers (the other nodes in a Spark cluster), creates a ExecutorCacheTaskLocation for every pair of host and executor ID, and in the end, sort it in descending order. NOTE: getSortedExecutorList is used exclusively when KafkaMicroBatchReader is requested to < > (and calculates offset ranges). === [[getOrCreateInitialPartitionOffsets]] getOrCreateInitialPartitionOffsets Internal Method","title":"getSortedExecutorList(): Array[String]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#getorcreateinitialpartitionoffsets-partitionoffsetmap","text":"getOrCreateInitialPartitionOffsets ...FIXME NOTE: getOrCreateInitialPartitionOffsets is used exclusively for the < > internal registry. === [[getStartOffset]] getStartOffset Method","title":"getOrCreateInitialPartitionOffsets(): PartitionOffsetMap"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#getstartoffset-offset","text":"NOTE: getStartOffset is part of the < > to get the start (beginning) < >. getStartOffset ...FIXME === [[getEndOffset]] getEndOffset Method","title":"getStartOffset: Offset"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#getendoffset-offset","text":"NOTE: getEndOffset is part of the < > to get the end < >. getEndOffset ...FIXME === [[deserializeOffset]] deserializeOffset Method","title":"getEndOffset: Offset"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#deserializeoffsetjson-string-offset","text":"NOTE: deserializeOffset is part of the < > to deserialize an < > (from JSON format). deserializeOffset ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | endPartitionOffsets a| [[endPartitionOffsets]] Ending offsets for the assigned partitions ( Map[TopicPartition, Long] ) Used when...FIXME | initialPartitionOffsets a| [[initialPartitionOffsets]]","title":"deserializeOffset(json: String): Offset"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaMicroBatchReader/#initialpartitionoffsets-maptopicpartition-long","text":"| rangeCalculator a| [[rangeCalculator]] < > (for the given < >) Used exclusively when KafkaMicroBatchReader is requested to < > (to calculate offset ranges) | startPartitionOffsets a| [[startPartitionOffsets]] Starting offsets for the assigned partitions ( Map[TopicPartition, Long] ) Used when...FIXME |===","title":"initialPartitionOffsets: Map[TopicPartition, Long]"},{"location":"spark-sql-streaming-KafkaOffsetRangeCalculator/","text":"== [[KafkaOffsetRangeCalculator]] KafkaOffsetRangeCalculator KafkaOffsetRangeCalculator is < > for < > to < > (when KafkaMicroBatchReader is requested to < >). [[minPartitions]][[creating-instance]] KafkaOffsetRangeCalculator takes an optional minimum number of partitions per executor ( minPartitions ) to be created (that can either be undefined or greater than 0 ). [[apply]] When created with a DataSourceOptions , KafkaOffsetRangeCalculator uses < > option for the < >. === [[getRanges]] Offset Ranges -- getRanges Method [source, scala] \u00b6 getRanges( fromOffsets: PartitionOffsetMap, untilOffsets: PartitionOffsetMap, executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] getRanges finds the common TopicPartitions that are the keys that are used in the fromOffsets and untilOffsets collections ( intersection ). For every common TopicPartition , getRanges creates a < > with the from and until offsets from the fromOffsets and untilOffsets collections (and the < > undefined). getRanges filters out the TopicPartitions that < > (i.e. the difference between until and from offsets is not greater than 0 ). At this point, getRanges knows the TopicPartitions with records to consume. getRanges branches off based on the defined < > and the number of KafkaOffsetRanges ( TopicPartitions with records to consume). For the < > undefined or smaller than the number of KafkaOffsetRanges ( TopicPartitions to consume records from), getRanges updates every KafkaOffsetRange with the < > based on the TopicPartition and the executorLocations ). Otherwise (with the < > defined and greater than the number of KafkaOffsetRanges ), getRanges splits KafkaOffsetRanges into smaller ones. NOTE: getRanges is used exclusively when KafkaMicroBatchReader is requested to < >. === [[KafkaOffsetRange]] KafkaOffsetRange -- TopicPartition with From and Until Offsets and Optional Preferred Location KafkaOffsetRange is a case class with the following attributes: [[topicPartition]] TopicPartition [[fromOffset]] fromOffset offset [[untilOffset]] untilOffset offset [[preferredLoc]] Optional preferred location [[size]] KafkaOffsetRange knows the size, i.e. the number of records between the < > and < > offsets. === [[getLocation]] Selecting Preferred Executor for TopicPartition -- getLocation Internal Method [source, scala] \u00b6 getLocation( tp: TopicPartition, executorLocations: Seq[String]): Option[String] getLocation ...FIXME NOTE: getLocation is used exclusively when KafkaOffsetRangeCalculator is requested to < >.","title":"KafkaOffsetRangeCalculator"},{"location":"spark-sql-streaming-KafkaOffsetRangeCalculator/#source-scala","text":"getRanges( fromOffsets: PartitionOffsetMap, untilOffsets: PartitionOffsetMap, executorLocations: Seq[String] = Seq.empty): Seq[KafkaOffsetRange] getRanges finds the common TopicPartitions that are the keys that are used in the fromOffsets and untilOffsets collections ( intersection ). For every common TopicPartition , getRanges creates a < > with the from and until offsets from the fromOffsets and untilOffsets collections (and the < > undefined). getRanges filters out the TopicPartitions that < > (i.e. the difference between until and from offsets is not greater than 0 ). At this point, getRanges knows the TopicPartitions with records to consume. getRanges branches off based on the defined < > and the number of KafkaOffsetRanges ( TopicPartitions with records to consume). For the < > undefined or smaller than the number of KafkaOffsetRanges ( TopicPartitions to consume records from), getRanges updates every KafkaOffsetRange with the < > based on the TopicPartition and the executorLocations ). Otherwise (with the < > defined and greater than the number of KafkaOffsetRanges ), getRanges splits KafkaOffsetRanges into smaller ones. NOTE: getRanges is used exclusively when KafkaMicroBatchReader is requested to < >. === [[KafkaOffsetRange]] KafkaOffsetRange -- TopicPartition with From and Until Offsets and Optional Preferred Location KafkaOffsetRange is a case class with the following attributes: [[topicPartition]] TopicPartition [[fromOffset]] fromOffset offset [[untilOffset]] untilOffset offset [[preferredLoc]] Optional preferred location [[size]] KafkaOffsetRange knows the size, i.e. the number of records between the < > and < > offsets. === [[getLocation]] Selecting Preferred Executor for TopicPartition -- getLocation Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetRangeCalculator/#source-scala_1","text":"getLocation( tp: TopicPartition, executorLocations: Seq[String]): Option[String] getLocation ...FIXME NOTE: getLocation is used exclusively when KafkaOffsetRangeCalculator is requested to < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetRangeLimit/","text":"== [[KafkaOffsetRangeLimit]] KafkaOffsetRangeLimit -- Desired Offset Range Limits KafkaOffsetRangeLimit represents the desired offset range limits for starting, ending, and specific offsets in < >. [[implementations]] .KafkaOffsetRangeLimits [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | KafkaOffsetRangeLimit | Description | EarliestOffsetRangeLimit | [[EarliestOffsetRangeLimit]] Intent to bind to the earliest offset | LatestOffsetRangeLimit | [[LatestOffsetRangeLimit]] Intent to bind to the latest offset | SpecificOffsetRangeLimit a| [[SpecificOffsetRangeLimit]] Intent to bind to specific offsets with the following special offset \"magic\" numbers: [[LATEST]] -1 or KafkaOffsetRangeLimit.LATEST - the latest offset [[EARLIEST]] -2 or KafkaOffsetRangeLimit.EARLIEST - the earliest offset |=== NOTE: KafkaOffsetRangeLimit is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). KafkaOffsetRangeLimit is often used in a text-based representation and is converted to from latest , earliest or a JSON-formatted text using < > object method. NOTE: A JSON-formatted text is of the following format {\"topicName\":{\"partition\":offset},...} , e.g. {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}} . KafkaOffsetRangeLimit is used when: < > is created (with the < >) < > is created (with the < >) < > is created (with the < > and < > offsets) < > is created (with the < >) KafkaSourceProvider is requested to < >","title":"KafkaOffsetRangeLimit"},{"location":"spark-sql-streaming-KafkaOffsetReader/","text":"== [[KafkaOffsetReader]] KafkaOffsetReader KafkaOffsetReader relies on the < > to < >. KafkaOffsetReader < > with group.id ( ConsumerConfig.GROUP_ID_CONFIG ) configuration explicitly set to < > (i.e. the given < > followed by < >). KafkaOffsetReader is < > when: KafkaRelation is requested to < > KafkaSourceProvider is requested to < >, < >, and < > [[options]] .KafkaOffsetReader's Options [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | fetchOffset.numRetries a| [[fetchOffset.numRetries]] Default: 3 | fetchOffset.retryIntervalMs a| [[fetchOffset.retryIntervalMs]] How long to wait before retries Default: 1000 |=== [[kafkaSchema]] KafkaOffsetReader defines the < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaOffsetReader to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaOffsetReader=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating KafkaOffsetReader Instance KafkaOffsetReader takes the following to be created: [[consumerStrategy]] spark-sql-streaming-ConsumerStrategy.md[ConsumerStrategy] [[driverKafkaParams]] Kafka parameters (as name-value pairs that are used exclusively to < > [[readerOptions]] Options (as name-value pairs) [[driverGroupIdPrefix]] Prefix of the group ID KafkaOffsetReader initializes the < >. === [[nextGroupId]] nextGroupId Internal Method [source, scala] \u00b6 nextGroupId(): String \u00b6 nextGroupId sets the < > to be the < >, - followed by the < > (i.e. [driverGroupIdPrefix]-[nextId] ). In the end, nextGroupId increments the < > and returns the < >. NOTE: nextGroupId is used exclusively when KafkaOffsetReader is requested for a < >. === [[resetConsumer]] resetConsumer Internal Method [source, scala] \u00b6 resetConsumer(): Unit \u00b6 resetConsumer ...FIXME NOTE: resetConsumer is used when...FIXME === [[fetchTopicPartitions]] fetchTopicPartitions Method [source, scala] \u00b6 fetchTopicPartitions(): Set[TopicPartition] \u00b6 CAUTION: FIXME NOTE: fetchTopicPartitions is used when KafkaRelation spark-sql-streaming-KafkaRelation.md#getPartitionOffsets[getPartitionOffsets]. === [[fetchEarliestOffsets]] Fetching Earliest Offsets -- fetchEarliestOffsets Method [source, scala] \u00b6 fetchEarliestOffsets(): Map[TopicPartition, Long] fetchEarliestOffsets(newPartitions: Seq[TopicPartition]): Map[TopicPartition, Long] CAUTION: FIXME NOTE: fetchEarliestOffsets is used when KafkaSource spark-sql-streaming-KafkaSource.md#rateLimit[rateLimit] and spark-sql-streaming-KafkaSource.md#getBatch[generates a DataFrame for a batch] (when new partitions have been assigned). === [[fetchLatestOffsets]] Fetching Latest Offsets -- fetchLatestOffsets Method [source, scala] \u00b6 fetchLatestOffsets(): Map[TopicPartition, Long] \u00b6 CAUTION: FIXME NOTE: fetchLatestOffsets is used when KafkaSource spark-sql-streaming-KafkaSource.md#getOffset[gets offsets] or initialPartitionOffsets is spark-sql-streaming-KafkaSource.md#initialPartitionOffsets[initialized]. === [[withRetriesWithoutInterrupt]] withRetriesWithoutInterrupt Internal Method [source, scala] \u00b6 withRetriesWithoutInterrupt( body: => Map[TopicPartition, Long]): Map[TopicPartition, Long] withRetriesWithoutInterrupt ...FIXME NOTE: withRetriesWithoutInterrupt is used when...FIXME === [[fetchSpecificOffsets]] Fetching Offsets for Selected TopicPartitions -- fetchSpecificOffsets Method [source, scala] \u00b6 fetchSpecificOffsets( partitionOffsets: Map[TopicPartition, Long], reportDataLoss: String => Unit): KafkaSourceOffset .KafkaOffsetReader's fetchSpecificOffsets image::images/KafkaOffsetReader-fetchSpecificOffsets.png[align=\"center\"] fetchSpecificOffsets requests the < > to poll(0) . fetchSpecificOffsets requests the < > for assigned partitions (using Consumer.assignment() ). fetchSpecificOffsets requests the < > to pause(partitions) . You should see the following DEBUG message in the logs: DEBUG KafkaOffsetReader: Partitions assigned to consumer: [partitions]. Seeking to [partitionOffsets] For every partition offset in the input partitionOffsets , fetchSpecificOffsets requests the < > to: seekToEnd for the latest (aka -1 ) seekToBeginning for the earliest (aka -2 ) seek for other offsets In the end, fetchSpecificOffsets creates a collection of Kafka's TopicPartition and position (using the < >). NOTE: fetchSpecificOffsets is used when KafkaSource spark-sql-streaming-KafkaSource.md#fetchAndVerify[fetches and verifies initial partition offsets]. === [[createConsumer]] Creating Kafka Consumer -- createConsumer Internal Method [source, scala] \u00b6 createConsumer(): Consumer[Array[Byte], Array[Byte]] \u00b6 createConsumer requests < > to spark-sql-streaming-ConsumerStrategy.md#createConsumer[create a Kafka Consumer] with < > and < >. NOTE: createConsumer is used when KafkaOffsetReader is < > (and initializes < >) and < > === [[consumer]] Creating Kafka Consumer (Unless Already Available) -- consumer Method [source, scala] \u00b6 consumer: Consumer[Array[Byte], Array[Byte]] \u00b6 consumer gives the cached <<_consumer, Kafka Consumer>> or creates one itself. NOTE: Since consumer method is used (to access the internal <<_consumer, Kafka Consumer>>) in the fetch methods that gives the property of creating a new Kafka Consumer whenever the internal <<_consumer, Kafka Consumer>> reference become null , i.e. as in < >. consumer ...FIXME NOTE: consumer is used when KafkaOffsetReader is requested to < >, < >, < >, and < >. === [[close]] Closing -- close Method [source, scala] \u00b6 close(): Unit \u00b6 close < > (if the <<_consumer, Kafka Consumer>> is available). close requests the < > to shut down. [NOTE] \u00b6 close is used when: < >, < >, and < > are requested to stop a streaming reader or source * KafkaRelation is requested to < > \u00b6 === [[runUninterruptibly]] runUninterruptibly Internal Method [source, scala] \u00b6 runUninterruptibly T : T \u00b6 runUninterruptibly ...FIXME NOTE: runUninterruptibly is used when...FIXME === [[stopConsumer]] stopConsumer Internal Method [source, scala] \u00b6 stopConsumer(): Unit \u00b6 stopConsumer ...FIXME NOTE: stopConsumer is used when...FIXME === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | _consumer a| [[_consumer]] Kafka's https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/Consumer.html[Consumer ] ( Consumer[Array[Byte], Array[Byte]] ) < > when KafkaOffsetReader is < >. Used when KafkaOffsetReader : < > < > < > < > < > < > | execContext a| [[execContext]] https://www.scala-lang.org/api/2.12.8/scala/concurrent/ExecutionContextExecutorService.html[scala.concurrent.ExecutionContextExecutorService ] | groupId a| [[groupId]] | kafkaReaderThread a| [[kafkaReaderThread]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ExecutorService.html[java.util.concurrent.ExecutorService ] | maxOffsetFetchAttempts a| [[maxOffsetFetchAttempts]] | nextId a| [[nextId]] Initially 0 | offsetFetchAttemptIntervalMs a| [[offsetFetchAttemptIntervalMs]] |===","title":"KafkaOffsetReader"},{"location":"spark-sql-streaming-KafkaOffsetReader/#refer-to","text":"=== [[creating-instance]] Creating KafkaOffsetReader Instance KafkaOffsetReader takes the following to be created: [[consumerStrategy]] spark-sql-streaming-ConsumerStrategy.md[ConsumerStrategy] [[driverKafkaParams]] Kafka parameters (as name-value pairs that are used exclusively to < > [[readerOptions]] Options (as name-value pairs) [[driverGroupIdPrefix]] Prefix of the group ID KafkaOffsetReader initializes the < >. === [[nextGroupId]] nextGroupId Internal Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#nextgroupid-string","text":"nextGroupId sets the < > to be the < >, - followed by the < > (i.e. [driverGroupIdPrefix]-[nextId] ). In the end, nextGroupId increments the < > and returns the < >. NOTE: nextGroupId is used exclusively when KafkaOffsetReader is requested for a < >. === [[resetConsumer]] resetConsumer Internal Method","title":"nextGroupId(): String"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#resetconsumer-unit","text":"resetConsumer ...FIXME NOTE: resetConsumer is used when...FIXME === [[fetchTopicPartitions]] fetchTopicPartitions Method","title":"resetConsumer(): Unit"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#fetchtopicpartitions-settopicpartition","text":"CAUTION: FIXME NOTE: fetchTopicPartitions is used when KafkaRelation spark-sql-streaming-KafkaRelation.md#getPartitionOffsets[getPartitionOffsets]. === [[fetchEarliestOffsets]] Fetching Earliest Offsets -- fetchEarliestOffsets Method","title":"fetchTopicPartitions(): Set[TopicPartition]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_3","text":"fetchEarliestOffsets(): Map[TopicPartition, Long] fetchEarliestOffsets(newPartitions: Seq[TopicPartition]): Map[TopicPartition, Long] CAUTION: FIXME NOTE: fetchEarliestOffsets is used when KafkaSource spark-sql-streaming-KafkaSource.md#rateLimit[rateLimit] and spark-sql-streaming-KafkaSource.md#getBatch[generates a DataFrame for a batch] (when new partitions have been assigned). === [[fetchLatestOffsets]] Fetching Latest Offsets -- fetchLatestOffsets Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#fetchlatestoffsets-maptopicpartition-long","text":"CAUTION: FIXME NOTE: fetchLatestOffsets is used when KafkaSource spark-sql-streaming-KafkaSource.md#getOffset[gets offsets] or initialPartitionOffsets is spark-sql-streaming-KafkaSource.md#initialPartitionOffsets[initialized]. === [[withRetriesWithoutInterrupt]] withRetriesWithoutInterrupt Internal Method","title":"fetchLatestOffsets(): Map[TopicPartition, Long]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_5","text":"withRetriesWithoutInterrupt( body: => Map[TopicPartition, Long]): Map[TopicPartition, Long] withRetriesWithoutInterrupt ...FIXME NOTE: withRetriesWithoutInterrupt is used when...FIXME === [[fetchSpecificOffsets]] Fetching Offsets for Selected TopicPartitions -- fetchSpecificOffsets Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_6","text":"fetchSpecificOffsets( partitionOffsets: Map[TopicPartition, Long], reportDataLoss: String => Unit): KafkaSourceOffset .KafkaOffsetReader's fetchSpecificOffsets image::images/KafkaOffsetReader-fetchSpecificOffsets.png[align=\"center\"] fetchSpecificOffsets requests the < > to poll(0) . fetchSpecificOffsets requests the < > for assigned partitions (using Consumer.assignment() ). fetchSpecificOffsets requests the < > to pause(partitions) . You should see the following DEBUG message in the logs: DEBUG KafkaOffsetReader: Partitions assigned to consumer: [partitions]. Seeking to [partitionOffsets] For every partition offset in the input partitionOffsets , fetchSpecificOffsets requests the < > to: seekToEnd for the latest (aka -1 ) seekToBeginning for the earliest (aka -2 ) seek for other offsets In the end, fetchSpecificOffsets creates a collection of Kafka's TopicPartition and position (using the < >). NOTE: fetchSpecificOffsets is used when KafkaSource spark-sql-streaming-KafkaSource.md#fetchAndVerify[fetches and verifies initial partition offsets]. === [[createConsumer]] Creating Kafka Consumer -- createConsumer Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#createconsumer-consumerarraybyte-arraybyte","text":"createConsumer requests < > to spark-sql-streaming-ConsumerStrategy.md#createConsumer[create a Kafka Consumer] with < > and < >. NOTE: createConsumer is used when KafkaOffsetReader is < > (and initializes < >) and < > === [[consumer]] Creating Kafka Consumer (Unless Already Available) -- consumer Method","title":"createConsumer(): Consumer[Array[Byte], Array[Byte]]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#consumer-consumerarraybyte-arraybyte","text":"consumer gives the cached <<_consumer, Kafka Consumer>> or creates one itself. NOTE: Since consumer method is used (to access the internal <<_consumer, Kafka Consumer>>) in the fetch methods that gives the property of creating a new Kafka Consumer whenever the internal <<_consumer, Kafka Consumer>> reference become null , i.e. as in < >. consumer ...FIXME NOTE: consumer is used when KafkaOffsetReader is requested to < >, < >, < >, and < >. === [[close]] Closing -- close Method","title":"consumer: Consumer[Array[Byte], Array[Byte]]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#close-unit","text":"close < > (if the <<_consumer, Kafka Consumer>> is available). close requests the < > to shut down.","title":"close(): Unit"},{"location":"spark-sql-streaming-KafkaOffsetReader/#note","text":"close is used when: < >, < >, and < > are requested to stop a streaming reader or source","title":"[NOTE]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#kafkarelation-is-requested-to","text":"=== [[runUninterruptibly]] runUninterruptibly Internal Method","title":"* KafkaRelation is requested to &lt;&gt;"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_10","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#rununinterruptiblyt-t","text":"runUninterruptibly ...FIXME NOTE: runUninterruptibly is used when...FIXME === [[stopConsumer]] stopConsumer Internal Method","title":"runUninterruptiblyT: T"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_11","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#stopconsumer-unit","text":"stopConsumer ...FIXME NOTE: stopConsumer is used when...FIXME === [[toString]] Textual Representation -- toString Method","title":"stopConsumer(): Unit"},{"location":"spark-sql-streaming-KafkaOffsetReader/#source-scala_12","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaOffsetReader/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | _consumer a| [[_consumer]] Kafka's https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/Consumer.html[Consumer ] ( Consumer[Array[Byte], Array[Byte]] ) < > when KafkaOffsetReader is < >. Used when KafkaOffsetReader : < > < > < > < > < > < > | execContext a| [[execContext]] https://www.scala-lang.org/api/2.12.8/scala/concurrent/ExecutionContextExecutorService.html[scala.concurrent.ExecutionContextExecutorService ] | groupId a| [[groupId]] | kafkaReaderThread a| [[kafkaReaderThread]] https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ExecutorService.html[java.util.concurrent.ExecutorService ] | maxOffsetFetchAttempts a| [[maxOffsetFetchAttempts]] | nextId a| [[nextId]] Initially 0 | offsetFetchAttemptIntervalMs a| [[offsetFetchAttemptIntervalMs]] |===","title":"toString: String"},{"location":"spark-sql-streaming-KafkaRelation/","text":"== [[KafkaRelation]] KafkaRelation [[schema]] KafkaRelation represents a collection of rows with a < > ( BaseRelation ) that supports < > ( TableScan ). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-BaseRelation.html[BaseRelation ] and https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-TableScan.html[TableScan ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. KafkaRelation is < > exclusively when KafkaSourceProvider is requested to < >. [[options]] .KafkaRelation's Options [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Description | kafkaConsumer.pollTimeoutMs a| [[kafkaConsumer.pollTimeoutMs]][[pollTimeoutMs]] Default: spark.network.timeout configuration if set or 120s |=== [[logging]] [TIP] ==== Enable ALL logging levels for org.apache.spark.sql.kafka010.KafkaRelation to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaRelation=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating KafkaRelation Instance KafkaRelation takes the following when created: [[sqlContext]] SQLContext [[strategy]] spark-sql-streaming-ConsumerStrategy.md[ConsumerStrategy] [[sourceOptions]] Source options ( Map[String, String] ) [[specifiedKafkaParams]] User-defined Kafka parameters ( Map[String, String] ) [[failOnDataLoss]] failOnDataLoss flag [[startingOffsets]] < > [[endingOffsets]] < > === [[getPartitionOffsets]] getPartitionOffsets Internal Method [source, scala] \u00b6 getPartitionOffsets( kafkaReader: KafkaOffsetReader, kafkaOffsets: KafkaOffsetRangeLimit): Map[TopicPartition, Long] CAUTION: FIXME NOTE: getPartitionOffsets is used exclusively when KafkaRelation < >. === [[buildScan]] Building Distributed Data Scan with Column Pruning -- buildScan Method [source, scala] \u00b6 buildScan(): RDD[Row] \u00b6 NOTE: buildScan is part of the https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-TableScan.html[TableScan ] contract to build a distributed data scan with column pruning. buildScan generates a unique group ID of the format spark-kafka-relation-[randomUUID] (to make sure that a streaming query creates a new consumer group). buildScan creates a < > with the following: The given < > and the < > < > based on the given < > spark-kafka-relation-[randomUUID]-driver for the driverGroupIdPrefix buildScan uses the KafkaOffsetReader to < > for the starting and ending offsets (based on the given < > and the < >, respectively). buildScan requests the KafkaOffsetReader to < > afterwards. buildScan creates offset ranges (that are a collection of KafkaSourceRDDOffsetRanges with a Kafka TopicPartition , beginning and ending offsets and undefined preferred location). buildScan prints out the following INFO message to the logs: Generating RDD of offset ranges: [offsetRanges] buildScan creates a < > with the following: < > based on the given < > and the unique group ID ( spark-kafka-relation-[randomUUID] ) The offset ranges created < > configuration The given < > flag reuseKafkaConsumer flag off ( false ) buildScan requests the KafkaSourceRDD to map Kafka ConsumerRecords to InternalRows . In the end, buildScan requests the < > to create a DataFrame (with the name kafka and the predefined < >) that is immediately converted to a RDD[InternalRow] . buildScan throws a IllegalStateException when...FIXME different topic partitions for starting offsets topics[[fromTopics]] and ending offsets topics[[untilTopics]] buildScan throws a IllegalStateException when...FIXME [tp] doesn't have a from offset","title":"KafkaRelation"},{"location":"spark-sql-streaming-KafkaRelation/#refer-to","text":"=== [[creating-instance]] Creating KafkaRelation Instance KafkaRelation takes the following when created: [[sqlContext]] SQLContext [[strategy]] spark-sql-streaming-ConsumerStrategy.md[ConsumerStrategy] [[sourceOptions]] Source options ( Map[String, String] ) [[specifiedKafkaParams]] User-defined Kafka parameters ( Map[String, String] ) [[failOnDataLoss]] failOnDataLoss flag [[startingOffsets]] < > [[endingOffsets]] < > === [[getPartitionOffsets]] getPartitionOffsets Internal Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-KafkaRelation/#source-scala","text":"getPartitionOffsets( kafkaReader: KafkaOffsetReader, kafkaOffsets: KafkaOffsetRangeLimit): Map[TopicPartition, Long] CAUTION: FIXME NOTE: getPartitionOffsets is used exclusively when KafkaRelation < >. === [[buildScan]] Building Distributed Data Scan with Column Pruning -- buildScan Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaRelation/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaRelation/#buildscan-rddrow","text":"NOTE: buildScan is part of the https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-TableScan.html[TableScan ] contract to build a distributed data scan with column pruning. buildScan generates a unique group ID of the format spark-kafka-relation-[randomUUID] (to make sure that a streaming query creates a new consumer group). buildScan creates a < > with the following: The given < > and the < > < > based on the given < > spark-kafka-relation-[randomUUID]-driver for the driverGroupIdPrefix buildScan uses the KafkaOffsetReader to < > for the starting and ending offsets (based on the given < > and the < >, respectively). buildScan requests the KafkaOffsetReader to < > afterwards. buildScan creates offset ranges (that are a collection of KafkaSourceRDDOffsetRanges with a Kafka TopicPartition , beginning and ending offsets and undefined preferred location). buildScan prints out the following INFO message to the logs: Generating RDD of offset ranges: [offsetRanges] buildScan creates a < > with the following: < > based on the given < > and the unique group ID ( spark-kafka-relation-[randomUUID] ) The offset ranges created < > configuration The given < > flag reuseKafkaConsumer flag off ( false ) buildScan requests the KafkaSourceRDD to map Kafka ConsumerRecords to InternalRows . In the end, buildScan requests the < > to create a DataFrame (with the name kafka and the predefined < >) that is immediately converted to a RDD[InternalRow] . buildScan throws a IllegalStateException when...FIXME different topic partitions for starting offsets topics[[fromTopics]] and ending offsets topics[[untilTopics]] buildScan throws a IllegalStateException when...FIXME [tp] doesn't have a from offset","title":"buildScan(): RDD[Row]"},{"location":"spark-sql-streaming-KafkaSink/","text":"== [[KafkaSink]] KafkaSink KafkaSink is a spark-sql-streaming-Sink.md[streaming sink] that spark-sql-streaming-KafkaSourceProvider.md[KafkaSourceProvider] registers as the kafka format. [source, scala] \u00b6 // start spark-shell or a Spark application with spark-sql-kafka-0-10 module // spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT import org.apache.spark.sql.SparkSession val spark: SparkSession = ... spark. readStream. format(\"text\"). load(\"server-logs/*.out\"). as[String]. writeStream. queryName(\"server-logs processor\"). format(\"kafka\"). // \u2190 uses KafkaSink option(\"topic\", \"topic1\"). option(\"checkpointLocation\", \"/tmp/kafka-sink-checkpoint\"). // \u2190 mandatory start // in another terminal $ echo hello > server-logs/hello.out // in the terminal with Spark FIXME === [[creating-instance]] Creating KafkaSink Instance KafkaSink takes the following when created: [[sqlContext]] SQLContext [[executorKafkaParams]] Kafka parameters (used on executor) as a map of (String, Object) pairs [[topic]] Optional topic name === [[addBatch]] addBatch Method [source, scala] \u00b6 addBatch(batchId: Long, data: DataFrame): Unit \u00b6 Internally, addBatch requests KafkaWriter to write the input data to the < > (if defined) or a topic in < >. NOTE: addBatch is a part of spark-sql-streaming-Sink.md#addBatch[Sink Contract] to \"add\" a batch of data to the sink.","title":"KafkaSink"},{"location":"spark-sql-streaming-KafkaSink/#source-scala","text":"// start spark-shell or a Spark application with spark-sql-kafka-0-10 module // spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT import org.apache.spark.sql.SparkSession val spark: SparkSession = ... spark. readStream. format(\"text\"). load(\"server-logs/*.out\"). as[String]. writeStream. queryName(\"server-logs processor\"). format(\"kafka\"). // \u2190 uses KafkaSink option(\"topic\", \"topic1\"). option(\"checkpointLocation\", \"/tmp/kafka-sink-checkpoint\"). // \u2190 mandatory start // in another terminal $ echo hello > server-logs/hello.out // in the terminal with Spark FIXME === [[creating-instance]] Creating KafkaSink Instance KafkaSink takes the following when created: [[sqlContext]] SQLContext [[executorKafkaParams]] Kafka parameters (used on executor) as a map of (String, Object) pairs [[topic]] Optional topic name === [[addBatch]] addBatch Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSink/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSink/#addbatchbatchid-long-data-dataframe-unit","text":"Internally, addBatch requests KafkaWriter to write the input data to the < > (if defined) or a topic in < >. NOTE: addBatch is a part of spark-sql-streaming-Sink.md#addBatch[Sink Contract] to \"add\" a batch of data to the sink.","title":"addBatch(batchId: Long, data: DataFrame): Unit"},{"location":"spark-sql-streaming-KafkaSource/","text":"KafkaSource \u00b6 KafkaSource is a streaming source that < >. NOTE: Kafka topics are checked for new records every spark-sql-streaming-Trigger.md[trigger] and so there is some noticeable delay between when the records have arrived to Kafka topics and when a Spark application processes them. KafkaSource uses the < > to persist offsets. The directory is the source ID under the sources directory in the checkpointRoot (of the StreamExecution ). Note The checkpointRoot directory is one of the following: checkpointLocation option spark.sql.streaming.checkpointLocation configuration property KafkaSource < > for kafka format (that is registered by spark-sql-streaming-KafkaSourceProvider.md#shortName[KafkaSourceProvider]). .KafkaSource Is Created for kafka Format by KafkaSourceProvider image::images/KafkaSource-creating-instance.png[align=\"center\"] [[schema]] KafkaSource uses a < > (that < >). KafkaSource also supports batch Datasets. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaSource to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaSource=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating KafkaSource Instance KafkaSource takes the following to be created: [[sqlContext]] spark-sql-sqlcontext.md[SQLContext] [[kafkaReader]] spark-sql-streaming-KafkaOffsetReader.md[KafkaOffsetReader] [[executorKafkaParams]] Parameters of executors (reading from Kafka) [[sourceOptions]] Collection of key-value options [[metadataPath]] Streaming metadata log directory , i.e. the directory for streaming metadata log (where KafkaSource persists spark-sql-streaming-KafkaSourceOffset.md[KafkaSourceOffset] offsets in JSON format) [[startingOffsets]] < > (as defined using < > option) [[failOnDataLoss]] Flag used to spark-sql-streaming-KafkaSourceRDD.md#creating-instance[create KafkaSourceRDDs ] every trigger and when checking to < >. KafkaSource initializes the < >. === [[getBatch]] Generating Streaming DataFrame with Records From Kafka for Streaming Micro-Batch -- getBatch Method [source, scala] \u00b6 getBatch( start: Option[Offset], end: Offset): DataFrame getBatch is part of the Source abstraction. getBatch creates a streaming DataFrame with a query plan with LogicalRDD logical operator to scan data from a < >. Internally, getBatch initializes < > (unless initialized already). You should see the following INFO message in the logs: GetBatch called with start = [start], end = [end] getBatch requests KafkaSourceOffset for spark-sql-streaming-KafkaSourceOffset.md#getPartitionOffsets[end partition offsets] for the input end offset (known as untilPartitionOffsets ). getBatch requests KafkaSourceOffset for spark-sql-streaming-KafkaSourceOffset.md#getPartitionOffsets[start partition offsets] for the input start offset (if defined) or uses < > (known as fromPartitionOffsets ). getBatch finds the new partitions (as the difference between the topic partitions in untilPartitionOffsets and fromPartitionOffsets ) and requests < > to spark-sql-streaming-KafkaOffsetReader.md#fetchEarliestOffsets[fetch their earliest offsets]. getBatch < > if the new partitions don't match to what < > fetched. Cannot find earliest offsets of [partitions]. Some data may have been missed You should see the following INFO message in the logs: Partitions added: [newPartitionOffsets] getBatch < > if the new partitions don't have their offsets 0 . Added partition [partition] starts from [offset] instead of 0. Some data may have been missed getBatch < > if the fromPartitionOffsets partitions differ from untilPartitionOffsets partitions. [partitions] are gone. Some data may have been missed You should see the following DEBUG message in the logs: TopicPartitions: [topicPartitions] getBatch < > (sorted by executorId and host of the registered block managers). IMPORTANT: That is when getBatch goes very low-level to allow for cached KafkaConsumers in the executors to be re-used to read the same partition in every batch (aka location preference ). You should see the following DEBUG message in the logs: Sorted executors: [sortedExecutors] getBatch creates a KafkaSourceRDDOffsetRange per TopicPartition . getBatch filters out KafkaSourceRDDOffsetRanges for which until offsets are smaller than from offsets. getBatch < > if they are found. Partition [topicPartition]'s offset was changed from [fromOffset] to [untilOffset], some data may have been missed getBatch spark-sql-streaming-KafkaSourceRDD.md#creating-instance[creates a KafkaSourceRDD] (with < >, < > and reuseKafkaConsumer flag enabled) and maps it to an RDD of InternalRow . IMPORTANT: getBatch creates a KafkaSourceRDD with reuseKafkaConsumer flag enabled. You should see the following INFO message in the logs: GetBatch generating RDD of offset range: [offsetRanges] getBatch sets < > if it was empty (which is when...FIXME) In the end, getBatch creates a streaming DataFrame for the KafkaSourceRDD and the < >. === [[getOffset]] Fetching Offsets (From Metadata Log or Kafka Directly) -- getOffset Method [source, scala] \u00b6 getOffset: Option[Offset] \u00b6 NOTE: getOffset is a part of the Source.md#getOffset[Source Contract]. Internally, getOffset fetches the < > (from the metadata log or Kafka directly). .KafkaSource Initializing initialPartitionOffsets While Fetching Initial Offsets image::images/KafkaSource-initialPartitionOffsets.png[align=\"center\"] NOTE: < > is a lazy value and is initialized the very first time getOffset is called (which is when StreamExecution MicroBatchExecution.md#constructNextBatch-hasNewData[constructs a streaming micro-batch]). [source, scala] \u00b6 scala> spark.version res0: String = 2.3.0-SNAPSHOT // Case 1: Checkpoint directory undefined // initialPartitionOffsets read from Kafka directly val records = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load // Start the streaming query // dump records to the console every 10 seconds import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Note the temporary checkpoint directory 17/08/07 11:09:29 INFO StreamExecution: Starting [id = 75dd261d-6b62-40fc-a368-9d95d3cb6f5f, runId = f18a5eb5-ccab-4d9d-8a81-befed41a72bd] with file:///private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-d0055630-24e4-4d9a-8f36-7a12a0f11bc0 to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}} // Stop the streaming query q.stop // Case 2: Checkpoint directory defined // initialPartitionOffsets read from Kafka directly // since the checkpoint directory is not available yet // it will be the next time the query is started val records = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load. select($\"value\" cast \"string\", $\"topic\", $\"partition\", $\"offset\") import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records. writeStream. format(\"console\"). option(\"truncate\", false). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Note the checkpoint directory in use 17/08/07 11:21:25 INFO StreamExecution: Starting [id = b8f59854-61c1-4c2f-931d-62bbaf90ee3b, runId = 70d06a3b-f2b1-4fa8-a518-15df4cf59130] with file:///tmp/checkpoint to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}} ... INFO StreamExecution: Stored offsets for batch 0. Metadata OffsetSeqMetadata(0,1502098526848,Map(spark.sql.shuffle.partitions -> 200, spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)) // Review the checkpoint location // $ ls -ltr /tmp/checkpoint/offsets // total 8 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:21 0 // $ tail -2 /tmp/checkpoint/offsets/0 | jq // Produce messages to Kafka so the latest offset changes // And more importanly the offset gets stored to checkpoint location Batch: 1 \u00b6 +---------------------------+------+---------+------+ |value |topic |partition|offset| +---------------------------+------+---------+------+ |testing checkpoint location|topic1|0 |2 | +---------------------------+------+---------+------+ // and one more // Note the offset Batch: 2 \u00b6 +------------+------+---------+------+ |value |topic |partition|offset| +------------+------+---------+------+ |another test|topic1|0 |3 | +------------+------+---------+------+ // See what was checkpointed // $ ls -ltr /tmp/checkpoint/offsets // total 24 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:35 0 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:37 1 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:38 2 // $ tail -2 /tmp/checkpoint/offsets/2 | jq // Stop the streaming query q.stop // And start over to see what offset the query starts from // Checkpoint location should have the offsets val q = records. writeStream. format(\"console\"). option(\"truncate\", false). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Whoops...console format does not support recovery (!) // Reported as https://issues.apache.org/jira/browse/SPARK-21667 org.apache.spark.sql.AnalysisException: This query does not support recovering from checkpoint location. Delete /tmp/checkpoint/offsets to start over.; at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:222) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:278) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:284) ... 61 elided // Change the sink (= output format) to JSON val q = records. writeStream. format(\"json\"). option(\"path\", \"/tmp/json-sink\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). start // Note the checkpoint directory in use 17/08/07 12:09:02 INFO StreamExecution: Starting [id = 02e00924-5f0d-4501-bcb8-80be8a8be385, runId = 5eba2576-dad6-4f95-9031-e72514475edc] with file:///tmp/checkpoint to store the query checkpoint. ... 17/08/07 12:09:02 INFO KafkaSource: GetBatch called with start = Some({\"topic1\":{\"0\":3}}), end = {\"topic1\":{\"0\":4}} 17/08/07 12:09:02 INFO KafkaSource: Partitions added: Map() 17/08/07 12:09:02 DEBUG KafkaSource: TopicPartitions: topic1-0 17/08/07 12:09:02 DEBUG KafkaSource: Sorted executors: 17/08/07 12:09:02 INFO KafkaSource: GetBatch generating RDD of offset range: KafkaSourceRDDOffsetRange(topic1-0,3,4,None) 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Partitions assigned to consumer: [topic1-0]. Seeking to the end. 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Got latest offsets for partition : Map(topic1-0 -> 4) 17/08/07 12:09:03 DEBUG KafkaSource: GetOffset: ArrayBuffer((topic1-0,4)) 17/08/07 12:09:03 DEBUG StreamExecution: getOffset took 122 ms 17/08/07 12:09:03 DEBUG StreamExecution: Resuming at batch 3 with committed offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} and available offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} 17/08/07 12:09:03 DEBUG StreamExecution: Stream running from {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} to {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} getOffset requests < > to spark-sql-streaming-KafkaOffsetReader.md#fetchLatestOffsets[fetchLatestOffsets] (known later as latest ). NOTE: (Possible performance degradation?) It is possible that getOffset will request the latest offsets from Kafka twice, i.e. while initializing < > (when no metadata log is available and KafkaSource's < > is LatestOffsetRangeLimit ) and always as part of getOffset itself. getOffset then calculates < > based on the < > option. .getOffset's Offset Calculation per maxOffsetsPerTrigger [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | maxOffsetsPerTrigger | Offsets | Unspecified (i.e. None ) | latest | Defined (but < > is empty) | < > with limit limit, < > as from , until as latest | Defined (and < > contains partitions and offsets) | < > with limit limit, < > as from , until as latest |=== You should see the following DEBUG message in the logs: DEBUG KafkaSource: GetOffset: [offsets] In the end, getOffset creates a spark-sql-streaming-KafkaSourceOffset.md#creating-instance[KafkaSourceOffset] with offsets (as Map[TopicPartition, Long] ). === [[fetchAndVerify]] Fetching and Verifying Specific Offsets -- fetchAndVerify Internal Method [source, scala] \u00b6 fetchAndVerify(specificOffsets: Map[TopicPartition, Long]): KafkaSourceOffset \u00b6 fetchAndVerify requests < > to spark-sql-streaming-KafkaOffsetReader.md#fetchSpecificOffsets[fetchSpecificOffsets] for the given specificOffsets . fetchAndVerify makes sure that the starting offsets in specificOffsets are the same as in Kafka and < > otherwise. startingOffsets for [tp] was [off] but consumer reset to [result(tp)] In the end, fetchAndVerify creates a spark-sql-streaming-KafkaSourceOffset.md[KafkaSourceOffset] (with the result of < >). NOTE: fetchAndVerify is used exclusively when KafkaSource initializes < >. === [[initialPartitionOffsets]] Initial Partition Offsets (of 0 th Batch) -- initialPartitionOffsets Internal Lazy Property [source, scala] \u00b6 initialPartitionOffsets: Map[TopicPartition, Long] \u00b6 initialPartitionOffsets is the initial partition offsets for the batch 0 that were already persisted in the < > or persisted on demand. As the very first step, initialPartitionOffsets creates a custom < > (of < > metadata) in the < >. initialPartitionOffsets requests the HDFSMetadataLog for the < > of the 0 th batch (as KafkaSourceOffset ). If the metadata is available, initialPartitionOffsets requests the metadata for the < >. If the metadata could not be found, initialPartitionOffsets creates a new KafkaSourceOffset per < >: For EarliestOffsetRangeLimit , initialPartitionOffsets requests the < > to < > For LatestOffsetRangeLimit , initialPartitionOffsets requests the < > to < > For SpecificOffsetRangeLimit , initialPartitionOffsets requests the < > to < > (and report a data loss per the < > flag) initialPartitionOffsets requests the custom HDFSMetadataLog to < > (as the metadata of the 0 th batch). initialPartitionOffsets prints out the following INFO message to the logs: Initial offsets: [offsets] Note initialPartitionOffsets is used when KafkaSource is requested for the following: < > < > (when the start offsets are not defined, i.e. before StreamExecution commits the first streaming batch and so nothing is in committedOffsets registry for a KafkaSource data source yet) ==== [[initialPartitionOffsets-HDFSMetadataLog-serialize]] HDFSMetadataLog.serialize [source, scala] \u00b6 serialize( metadata: KafkaSourceOffset, out: OutputStream): Unit NOTE: serialize is part of the < > to...FIXME. serialize requests the OutputStream to write a zero byte (to support Spark 2.1.0 as per SPARK-19517). serialize creates a BufferedWriter over a OutputStreamWriter over the OutputStream (with UTF_8 charset encoding). serialize requests the BufferedWriter to write the v1 version indicator followed by a new line. serialize then requests the KafkaSourceOffset for a JSON-serialized representation and the BufferedWriter to write it out. In the end, serialize requests the BufferedWriter to flush (the underlying stream). === [[rateLimit]] rateLimit Internal Method [source, scala] \u00b6 rateLimit( limit: Long, from: Map[TopicPartition, Long], until: Map[TopicPartition, Long]): Map[TopicPartition, Long] rateLimit requests < > to spark-sql-streaming-KafkaOffsetReader.md#fetchEarliestOffsets[fetchEarliestOffsets]. CAUTION: FIXME NOTE: rateLimit is used exclusively when KafkaSource < > (when < > option is specified). === [[getSortedExecutorList]] getSortedExecutorList Method CAUTION: FIXME === [[reportDataLoss]] reportDataLoss Internal Method CAUTION: FIXME [NOTE] \u00b6 reportDataLoss is used when KafkaSource does the following: < > < > \u00b6 === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentPartitionOffsets | [[currentPartitionOffsets]] Current partition offsets (as Map[TopicPartition, Long] ) Initially NONE and set when KafkaSource is requested to < > or < >. | pollTimeoutMs a| [[pollTimeoutMs]] | sc a| [[sc]] Spark Core's SparkContext (of the < >) Used when: < > (and creating a < >) Initializing the < > internal property |===","title":"KafkaSource"},{"location":"spark-sql-streaming-KafkaSource/#kafkasource","text":"KafkaSource is a streaming source that < >. NOTE: Kafka topics are checked for new records every spark-sql-streaming-Trigger.md[trigger] and so there is some noticeable delay between when the records have arrived to Kafka topics and when a Spark application processes them. KafkaSource uses the < > to persist offsets. The directory is the source ID under the sources directory in the checkpointRoot (of the StreamExecution ). Note The checkpointRoot directory is one of the following: checkpointLocation option spark.sql.streaming.checkpointLocation configuration property KafkaSource < > for kafka format (that is registered by spark-sql-streaming-KafkaSourceProvider.md#shortName[KafkaSourceProvider]). .KafkaSource Is Created for kafka Format by KafkaSourceProvider image::images/KafkaSource-creating-instance.png[align=\"center\"] [[schema]] KafkaSource uses a < > (that < >). KafkaSource also supports batch Datasets. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.kafka010.KafkaSource to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaSource=ALL","title":"KafkaSource"},{"location":"spark-sql-streaming-KafkaSource/#refer-to","text":"=== [[creating-instance]] Creating KafkaSource Instance KafkaSource takes the following to be created: [[sqlContext]] spark-sql-sqlcontext.md[SQLContext] [[kafkaReader]] spark-sql-streaming-KafkaOffsetReader.md[KafkaOffsetReader] [[executorKafkaParams]] Parameters of executors (reading from Kafka) [[sourceOptions]] Collection of key-value options [[metadataPath]] Streaming metadata log directory , i.e. the directory for streaming metadata log (where KafkaSource persists spark-sql-streaming-KafkaSourceOffset.md[KafkaSourceOffset] offsets in JSON format) [[startingOffsets]] < > (as defined using < > option) [[failOnDataLoss]] Flag used to spark-sql-streaming-KafkaSourceRDD.md#creating-instance[create KafkaSourceRDDs ] every trigger and when checking to < >. KafkaSource initializes the < >. === [[getBatch]] Generating Streaming DataFrame with Records From Kafka for Streaming Micro-Batch -- getBatch Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-KafkaSource/#source-scala","text":"getBatch( start: Option[Offset], end: Offset): DataFrame getBatch is part of the Source abstraction. getBatch creates a streaming DataFrame with a query plan with LogicalRDD logical operator to scan data from a < >. Internally, getBatch initializes < > (unless initialized already). You should see the following INFO message in the logs: GetBatch called with start = [start], end = [end] getBatch requests KafkaSourceOffset for spark-sql-streaming-KafkaSourceOffset.md#getPartitionOffsets[end partition offsets] for the input end offset (known as untilPartitionOffsets ). getBatch requests KafkaSourceOffset for spark-sql-streaming-KafkaSourceOffset.md#getPartitionOffsets[start partition offsets] for the input start offset (if defined) or uses < > (known as fromPartitionOffsets ). getBatch finds the new partitions (as the difference between the topic partitions in untilPartitionOffsets and fromPartitionOffsets ) and requests < > to spark-sql-streaming-KafkaOffsetReader.md#fetchEarliestOffsets[fetch their earliest offsets]. getBatch < > if the new partitions don't match to what < > fetched. Cannot find earliest offsets of [partitions]. Some data may have been missed You should see the following INFO message in the logs: Partitions added: [newPartitionOffsets] getBatch < > if the new partitions don't have their offsets 0 . Added partition [partition] starts from [offset] instead of 0. Some data may have been missed getBatch < > if the fromPartitionOffsets partitions differ from untilPartitionOffsets partitions. [partitions] are gone. Some data may have been missed You should see the following DEBUG message in the logs: TopicPartitions: [topicPartitions] getBatch < > (sorted by executorId and host of the registered block managers). IMPORTANT: That is when getBatch goes very low-level to allow for cached KafkaConsumers in the executors to be re-used to read the same partition in every batch (aka location preference ). You should see the following DEBUG message in the logs: Sorted executors: [sortedExecutors] getBatch creates a KafkaSourceRDDOffsetRange per TopicPartition . getBatch filters out KafkaSourceRDDOffsetRanges for which until offsets are smaller than from offsets. getBatch < > if they are found. Partition [topicPartition]'s offset was changed from [fromOffset] to [untilOffset], some data may have been missed getBatch spark-sql-streaming-KafkaSourceRDD.md#creating-instance[creates a KafkaSourceRDD] (with < >, < > and reuseKafkaConsumer flag enabled) and maps it to an RDD of InternalRow . IMPORTANT: getBatch creates a KafkaSourceRDD with reuseKafkaConsumer flag enabled. You should see the following INFO message in the logs: GetBatch generating RDD of offset range: [offsetRanges] getBatch sets < > if it was empty (which is when...FIXME) In the end, getBatch creates a streaming DataFrame for the KafkaSourceRDD and the < >. === [[getOffset]] Fetching Offsets (From Metadata Log or Kafka Directly) -- getOffset Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSource/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSource/#getoffset-optionoffset","text":"NOTE: getOffset is a part of the Source.md#getOffset[Source Contract]. Internally, getOffset fetches the < > (from the metadata log or Kafka directly). .KafkaSource Initializing initialPartitionOffsets While Fetching Initial Offsets image::images/KafkaSource-initialPartitionOffsets.png[align=\"center\"] NOTE: < > is a lazy value and is initialized the very first time getOffset is called (which is when StreamExecution MicroBatchExecution.md#constructNextBatch-hasNewData[constructs a streaming micro-batch]).","title":"getOffset: Option[Offset]"},{"location":"spark-sql-streaming-KafkaSource/#source-scala_2","text":"scala> spark.version res0: String = 2.3.0-SNAPSHOT // Case 1: Checkpoint directory undefined // initialPartitionOffsets read from Kafka directly val records = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load // Start the streaming query // dump records to the console every 10 seconds import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Note the temporary checkpoint directory 17/08/07 11:09:29 INFO StreamExecution: Starting [id = 75dd261d-6b62-40fc-a368-9d95d3cb6f5f, runId = f18a5eb5-ccab-4d9d-8a81-befed41a72bd] with file:///private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-d0055630-24e4-4d9a-8f36-7a12a0f11bc0 to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}} // Stop the streaming query q.stop // Case 2: Checkpoint directory defined // initialPartitionOffsets read from Kafka directly // since the checkpoint directory is not available yet // it will be the next time the query is started val records = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load. select($\"value\" cast \"string\", $\"topic\", $\"partition\", $\"offset\") import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = records. writeStream. format(\"console\"). option(\"truncate\", false). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Note the checkpoint directory in use 17/08/07 11:21:25 INFO StreamExecution: Starting [id = b8f59854-61c1-4c2f-931d-62bbaf90ee3b, runId = 70d06a3b-f2b1-4fa8-a518-15df4cf59130] with file:///tmp/checkpoint to store the query checkpoint. ... INFO KafkaSource: Initial offsets: {\"topic1\":{\"0\":1}} ... INFO StreamExecution: Stored offsets for batch 0. Metadata OffsetSeqMetadata(0,1502098526848,Map(spark.sql.shuffle.partitions -> 200, spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)) // Review the checkpoint location // $ ls -ltr /tmp/checkpoint/offsets // total 8 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:21 0 // $ tail -2 /tmp/checkpoint/offsets/0 | jq // Produce messages to Kafka so the latest offset changes // And more importanly the offset gets stored to checkpoint location","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSource/#batch-1","text":"+---------------------------+------+---------+------+ |value |topic |partition|offset| +---------------------------+------+---------+------+ |testing checkpoint location|topic1|0 |2 | +---------------------------+------+---------+------+ // and one more // Note the offset","title":"Batch: 1"},{"location":"spark-sql-streaming-KafkaSource/#batch-2","text":"+------------+------+---------+------+ |value |topic |partition|offset| +------------+------+---------+------+ |another test|topic1|0 |3 | +------------+------+---------+------+ // See what was checkpointed // $ ls -ltr /tmp/checkpoint/offsets // total 24 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:35 0 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:37 1 // -rw-r--r-- 1 jacek wheel 248 7 sie 11:38 2 // $ tail -2 /tmp/checkpoint/offsets/2 | jq // Stop the streaming query q.stop // And start over to see what offset the query starts from // Checkpoint location should have the offsets val q = records. writeStream. format(\"console\"). option(\"truncate\", false). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // Whoops...console format does not support recovery (!) // Reported as https://issues.apache.org/jira/browse/SPARK-21667 org.apache.spark.sql.AnalysisException: This query does not support recovering from checkpoint location. Delete /tmp/checkpoint/offsets to start over.; at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:222) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:278) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:284) ... 61 elided // Change the sink (= output format) to JSON val q = records. writeStream. format(\"json\"). option(\"path\", \"/tmp/json-sink\"). option(\"checkpointLocation\", \"/tmp/checkpoint\"). // \u2190 checkpoint directory trigger(Trigger.ProcessingTime(10.seconds)). start // Note the checkpoint directory in use 17/08/07 12:09:02 INFO StreamExecution: Starting [id = 02e00924-5f0d-4501-bcb8-80be8a8be385, runId = 5eba2576-dad6-4f95-9031-e72514475edc] with file:///tmp/checkpoint to store the query checkpoint. ... 17/08/07 12:09:02 INFO KafkaSource: GetBatch called with start = Some({\"topic1\":{\"0\":3}}), end = {\"topic1\":{\"0\":4}} 17/08/07 12:09:02 INFO KafkaSource: Partitions added: Map() 17/08/07 12:09:02 DEBUG KafkaSource: TopicPartitions: topic1-0 17/08/07 12:09:02 DEBUG KafkaSource: Sorted executors: 17/08/07 12:09:02 INFO KafkaSource: GetBatch generating RDD of offset range: KafkaSourceRDDOffsetRange(topic1-0,3,4,None) 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Partitions assigned to consumer: [topic1-0]. Seeking to the end. 17/08/07 12:09:03 DEBUG KafkaOffsetReader: Got latest offsets for partition : Map(topic1-0 -> 4) 17/08/07 12:09:03 DEBUG KafkaSource: GetOffset: ArrayBuffer((topic1-0,4)) 17/08/07 12:09:03 DEBUG StreamExecution: getOffset took 122 ms 17/08/07 12:09:03 DEBUG StreamExecution: Resuming at batch 3 with committed offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} and available offsets {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} 17/08/07 12:09:03 DEBUG StreamExecution: Stream running from {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} to {KafkaSource[Subscribe[topic1]]: {\"topic1\":{\"0\":4}}} getOffset requests < > to spark-sql-streaming-KafkaOffsetReader.md#fetchLatestOffsets[fetchLatestOffsets] (known later as latest ). NOTE: (Possible performance degradation?) It is possible that getOffset will request the latest offsets from Kafka twice, i.e. while initializing < > (when no metadata log is available and KafkaSource's < > is LatestOffsetRangeLimit ) and always as part of getOffset itself. getOffset then calculates < > based on the < > option. .getOffset's Offset Calculation per maxOffsetsPerTrigger [cols=\"1,1\",options=\"header\",width=\"100%\"] |=== | maxOffsetsPerTrigger | Offsets | Unspecified (i.e. None ) | latest | Defined (but < > is empty) | < > with limit limit, < > as from , until as latest | Defined (and < > contains partitions and offsets) | < > with limit limit, < > as from , until as latest |=== You should see the following DEBUG message in the logs: DEBUG KafkaSource: GetOffset: [offsets] In the end, getOffset creates a spark-sql-streaming-KafkaSourceOffset.md#creating-instance[KafkaSourceOffset] with offsets (as Map[TopicPartition, Long] ). === [[fetchAndVerify]] Fetching and Verifying Specific Offsets -- fetchAndVerify Internal Method","title":"Batch: 2"},{"location":"spark-sql-streaming-KafkaSource/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSource/#fetchandverifyspecificoffsets-maptopicpartition-long-kafkasourceoffset","text":"fetchAndVerify requests < > to spark-sql-streaming-KafkaOffsetReader.md#fetchSpecificOffsets[fetchSpecificOffsets] for the given specificOffsets . fetchAndVerify makes sure that the starting offsets in specificOffsets are the same as in Kafka and < > otherwise. startingOffsets for [tp] was [off] but consumer reset to [result(tp)] In the end, fetchAndVerify creates a spark-sql-streaming-KafkaSourceOffset.md[KafkaSourceOffset] (with the result of < >). NOTE: fetchAndVerify is used exclusively when KafkaSource initializes < >. === [[initialPartitionOffsets]] Initial Partition Offsets (of 0 th Batch) -- initialPartitionOffsets Internal Lazy Property","title":"fetchAndVerify(specificOffsets: Map[TopicPartition, Long]): KafkaSourceOffset"},{"location":"spark-sql-streaming-KafkaSource/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSource/#initialpartitionoffsets-maptopicpartition-long","text":"initialPartitionOffsets is the initial partition offsets for the batch 0 that were already persisted in the < > or persisted on demand. As the very first step, initialPartitionOffsets creates a custom < > (of < > metadata) in the < >. initialPartitionOffsets requests the HDFSMetadataLog for the < > of the 0 th batch (as KafkaSourceOffset ). If the metadata is available, initialPartitionOffsets requests the metadata for the < >. If the metadata could not be found, initialPartitionOffsets creates a new KafkaSourceOffset per < >: For EarliestOffsetRangeLimit , initialPartitionOffsets requests the < > to < > For LatestOffsetRangeLimit , initialPartitionOffsets requests the < > to < > For SpecificOffsetRangeLimit , initialPartitionOffsets requests the < > to < > (and report a data loss per the < > flag) initialPartitionOffsets requests the custom HDFSMetadataLog to < > (as the metadata of the 0 th batch). initialPartitionOffsets prints out the following INFO message to the logs: Initial offsets: [offsets] Note initialPartitionOffsets is used when KafkaSource is requested for the following: < > < > (when the start offsets are not defined, i.e. before StreamExecution commits the first streaming batch and so nothing is in committedOffsets registry for a KafkaSource data source yet) ==== [[initialPartitionOffsets-HDFSMetadataLog-serialize]] HDFSMetadataLog.serialize","title":"initialPartitionOffsets: Map[TopicPartition, Long]"},{"location":"spark-sql-streaming-KafkaSource/#source-scala_5","text":"serialize( metadata: KafkaSourceOffset, out: OutputStream): Unit NOTE: serialize is part of the < > to...FIXME. serialize requests the OutputStream to write a zero byte (to support Spark 2.1.0 as per SPARK-19517). serialize creates a BufferedWriter over a OutputStreamWriter over the OutputStream (with UTF_8 charset encoding). serialize requests the BufferedWriter to write the v1 version indicator followed by a new line. serialize then requests the KafkaSourceOffset for a JSON-serialized representation and the BufferedWriter to write it out. In the end, serialize requests the BufferedWriter to flush (the underlying stream). === [[rateLimit]] rateLimit Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSource/#source-scala_6","text":"rateLimit( limit: Long, from: Map[TopicPartition, Long], until: Map[TopicPartition, Long]): Map[TopicPartition, Long] rateLimit requests < > to spark-sql-streaming-KafkaOffsetReader.md#fetchEarliestOffsets[fetchEarliestOffsets]. CAUTION: FIXME NOTE: rateLimit is used exclusively when KafkaSource < > (when < > option is specified). === [[getSortedExecutorList]] getSortedExecutorList Method CAUTION: FIXME === [[reportDataLoss]] reportDataLoss Internal Method CAUTION: FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSource/#note","text":"reportDataLoss is used when KafkaSource does the following: < >","title":"[NOTE]"},{"location":"spark-sql-streaming-KafkaSource/#_1","text":"=== [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentPartitionOffsets | [[currentPartitionOffsets]] Current partition offsets (as Map[TopicPartition, Long] ) Initially NONE and set when KafkaSource is requested to < > or < >. | pollTimeoutMs a| [[pollTimeoutMs]] | sc a| [[sc]] Spark Core's SparkContext (of the < >) Used when: < > (and creating a < >) Initializing the < > internal property |===","title":"&lt;&gt;"},{"location":"spark-sql-streaming-KafkaSourceInitialOffsetWriter/","text":"== [[KafkaSourceInitialOffsetWriter]] KafkaSourceInitialOffsetWriter KafkaSourceInitialOffsetWriter is a < > for < >. KafkaSourceInitialOffsetWriter is < > exclusively when KafkaMicroBatchReader is requested to < >. [[VERSION]] KafkaSourceInitialOffsetWriter uses 1 for the version. === [[creating-instance]] Creating KafkaSourceInitialOffsetWriter Instance KafkaSourceInitialOffsetWriter takes the following to be created: [[sparkSession]] SparkSession [[metadataPath]] Path of the metadata log directory === [[deserialize]] Deserializing Metadata (Reading Metadata from Serialized Format) -- deserialize Method [source, scala] \u00b6 deserialize( in: InputStream): KafkaSourceOffset NOTE: deserialize is part of the < > to deserialize metadata (reading metadata from a serialized format) deserialize ...FIXME","title":"KafkaSourceInitialOffsetWriter"},{"location":"spark-sql-streaming-KafkaSourceInitialOffsetWriter/#source-scala","text":"deserialize( in: InputStream): KafkaSourceOffset NOTE: deserialize is part of the < > to deserialize metadata (reading metadata from a serialized format) deserialize ...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceOffset/","text":"== [[KafkaSourceOffset]] KafkaSourceOffset KafkaSourceOffset is a custom < > for < >. KafkaSourceOffset is < > (directly or indirectly using < >) when: KafkaContinuousReader is requested to < >, < >, and < > KafkaMicroBatchReader is requested to < >, < >, < >, and < > KafkaOffsetReader is requested to < > KafkaSource is requested for the < > and < > KafkaSourceInitialOffsetWriter is requested to < > KafkaSourceOffset is requested for < > [[creating-instance]][[partitionToOffsets]] KafkaSourceOffset takes a collection of Kafka TopicPartitions with offsets to be created. === [[getPartitionOffsets]] Partition Offsets -- getPartitionOffsets Method [source, scala] \u00b6 getPartitionOffsets( offset: Offset): Map[TopicPartition, Long] getPartitionOffsets takes < > from offset . If offset is KafkaSourceOffset , getPartitionOffsets takes the partitions and offsets straight from it. If however offset is SerializedOffset , getPartitionOffsets deserializes the offsets from JSON. getPartitionOffsets reports an IllegalArgumentException when offset is neither KafkaSourceOffset or SerializedOffset . Invalid conversion from offset of [class] to KafkaSourceOffset [NOTE] \u00b6 getPartitionOffsets is used when: KafkaContinuousReader is requested to < > * KafkaSource is requested to < > \u00b6 === [[json]] JSON-Encoded Offset -- json Method [source, scala] \u00b6 json: String \u00b6 NOTE: json is part of the < > for a JSON-encoded offset. json ...FIXME === [[apply]] Creating KafkaSourceOffset Instance -- apply Utility Method [source, scala] \u00b6 apply( offsetTuples: (String, Int, Long)*): KafkaSourceOffset // <1> apply( offset: SerializedOffset): KafkaSourceOffset <1> Used in tests only apply ...FIXME [NOTE] \u00b6 apply is used when: KafkaSourceInitialOffsetWriter is requested to < > KafkaSource is requested for the < > * KafkaSourceOffset is requested to < > \u00b6","title":"KafkaSourceOffset"},{"location":"spark-sql-streaming-KafkaSourceOffset/#source-scala","text":"getPartitionOffsets( offset: Offset): Map[TopicPartition, Long] getPartitionOffsets takes < > from offset . If offset is KafkaSourceOffset , getPartitionOffsets takes the partitions and offsets straight from it. If however offset is SerializedOffset , getPartitionOffsets deserializes the offsets from JSON. getPartitionOffsets reports an IllegalArgumentException when offset is neither KafkaSourceOffset or SerializedOffset . Invalid conversion from offset of [class] to KafkaSourceOffset","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceOffset/#note","text":"getPartitionOffsets is used when: KafkaContinuousReader is requested to < >","title":"[NOTE]"},{"location":"spark-sql-streaming-KafkaSourceOffset/#kafkasource-is-requested-to","text":"=== [[json]] JSON-Encoded Offset -- json Method","title":"* KafkaSource is requested to &lt;&gt;"},{"location":"spark-sql-streaming-KafkaSourceOffset/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceOffset/#json-string","text":"NOTE: json is part of the < > for a JSON-encoded offset. json ...FIXME === [[apply]] Creating KafkaSourceOffset Instance -- apply Utility Method","title":"json: String"},{"location":"spark-sql-streaming-KafkaSourceOffset/#source-scala_2","text":"apply( offsetTuples: (String, Int, Long)*): KafkaSourceOffset // <1> apply( offset: SerializedOffset): KafkaSourceOffset <1> Used in tests only apply ...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceOffset/#note_1","text":"apply is used when: KafkaSourceInitialOffsetWriter is requested to < > KafkaSource is requested for the < >","title":"[NOTE]"},{"location":"spark-sql-streaming-KafkaSourceOffset/#kafkasourceoffset-is-requested-to","text":"","title":"* KafkaSourceOffset is requested to &lt;&gt;"},{"location":"spark-sql-streaming-KafkaSourceProvider/","text":"KafkaSourceProvider \u2014 Data Source Provider for Apache Kafka \u00b6 [[shortName]] KafkaSourceProvider is a DataSourceRegister and registers a developer-friendly alias for kafka data source format in Spark Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceRegister.html[DataSourceRegister ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. KafkaSourceProvider supports < > (through < > contract) and < >. KafkaSourceProvider requires the following options (that you can set using option method of < > or DataStreamWriter ): Exactly one of the following options: < >, < > or < > < > TIP: Refer to < > for the supported configuration options. Internally, KafkaSourceProvider sets the < > (that are passed on to InternalKafkaConsumer when requested to create a Kafka consumer with a single TopicPartition manually assigned). [[kafkaParamsForExecutors-properties]] .KafkaSourceProvider's Properties for Kafka Consumers on Executors [cols=\"1m,1m,2\",options=\"header\",width=\"100%\"] |=== | ConsumerConfig's Key | Value | Description | KEY_DESERIALIZER_CLASS_CONFIG | ByteArrayDeserializer a| [[KEY_DESERIALIZER_CLASS_CONFIG]] FIXME | VALUE_DESERIALIZER_CLASS_CONFIG | ByteArrayDeserializer a| [[VALUE_DESERIALIZER_CLASS_CONFIG]] FIXME | AUTO_OFFSET_RESET_CONFIG | none a| [[AUTO_OFFSET_RESET_CONFIG]] FIXME | GROUP_ID_CONFIG | < >-executor a| [[GROUP_ID_CONFIG]] FIXME | ENABLE_AUTO_COMMIT_CONFIG | false a| [[ENABLE_AUTO_COMMIT_CONFIG]] FIXME | RECEIVE_BUFFER_CONFIG | 65536 a| [[RECEIVE_BUFFER_CONFIG]] Only when not set in the < > already |=== [[logging]] [TIP] ==== Enable ALL logging levels for org.apache.spark.sql.kafka010.KafkaSourceProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaSourceProvider=ALL Refer to < >. \u00b6 === [[createSource]] Creating Streaming Source -- createSource Method [source, scala] \u00b6 createSource( sqlContext: SQLContext, metadataPath: String, schema: Option[StructType], providerName: String, parameters: Map[String, String]): Source createSource is part of the StreamSourceProvider abstraction. createSource first < >. createSource ...FIXME === [[validateGeneralOptions]] Validating General Options For Batch And Streaming Queries -- validateGeneralOptions Internal Method [source, scala] \u00b6 validateGeneralOptions(parameters: Map[String, String]): Unit \u00b6 NOTE: Parameters are case-insensitive, i.e. OptioN and option are equal. validateGeneralOptions makes sure that exactly one topic subscription strategy is used in parameters and can be: subscribe subscribepattern assign validateGeneralOptions reports an IllegalArgumentException when there is no subscription strategy in use or there are more than one strategies used. validateGeneralOptions makes sure that the value of subscription strategies meet the requirements: assign strategy starts with { (the opening curly brace) subscribe strategy has at least one topic (in a comma-separated list of topics) subscribepattern strategy has the pattern defined validateGeneralOptions makes sure that group.id has not been specified and reports an IllegalArgumentException otherwise. Kafka option 'group.id' is not supported as user-specified consumer groups are not used to track offsets. validateGeneralOptions makes sure that auto.offset.reset has not been specified and reports an IllegalArgumentException otherwise. [options=\"wrap\"] \u00b6 Kafka option 'auto.offset.reset' is not supported. Instead set the source option 'startingoffsets' to 'earliest' or 'latest' to specify where to start. Structured Streaming manages which offsets are consumed internally, rather than relying on the kafkaConsumer to do it. This will ensure that no data is missed when new topics/partitions are dynamically subscribed. Note that 'startingoffsets' only applies when a new Streaming query is started, and that resuming will always pick up from where the query left off. See the docs for more details. validateGeneralOptions makes sure that the following options have not been specified and reports an IllegalArgumentException otherwise: kafka.key.deserializer kafka.value.deserializer kafka.enable.auto.commit kafka.interceptor.classes In the end, validateGeneralOptions makes sure that kafka.bootstrap.servers option was specified and reports an IllegalArgumentException otherwise. Option 'kafka.bootstrap.servers' must be specified for configuring Kafka consumer NOTE: validateGeneralOptions is used when KafkaSourceProvider validates options for < > and < > queries. === [[strategy]] Creating ConsumerStrategy -- strategy Internal Method [source, scala] \u00b6 strategy(caseInsensitiveParams: Map[String, String]) \u00b6 Internally, strategy finds the keys in the input caseInsensitiveParams that are one of the following and creates a corresponding spark-sql-streaming-ConsumerStrategy.md[ConsumerStrategy]. .KafkaSourceProvider.strategy's Key to ConsumerStrategy Conversion [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Key | ConsumerStrategy | assign a| spark-sql-streaming-ConsumerStrategy.md#AssignStrategy[AssignStrategy] with Kafka's http://kafka.apache.org/0110/javadoc/org/apache/kafka/common/TopicPartition.html[TopicPartitions ]. strategy uses JsonUtils.partitions method to parse a JSON with topic names and partitions, e.g. {\"topicA\":[0,1],\"topicB\":[0,1]} The topic names and partitions are mapped directly to Kafka's TopicPartition objects. | subscribe a| spark-sql-streaming-ConsumerStrategy.md#SubscribeStrategy[SubscribeStrategy] with topic names strategy extracts topic names from a comma-separated string, e.g. topic1,topic2,topic3 | subscribepattern a| spark-sql-streaming-ConsumerStrategy.md#SubscribePatternStrategy[SubscribePatternStrategy] with topic subscription regex pattern (that uses Java's http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern ] for the pattern), e.g. topic\\d |=== [NOTE] \u00b6 strategy is used when: KafkaSourceProvider < >. * KafkaSourceProvider creates a KafkaRelation (using createRelation method). \u00b6 === [[sourceSchema]] Describing Streaming Source with Name and Schema -- sourceSchema Method [source, scala] \u00b6 sourceSchema( sqlContext: SQLContext, schema: Option[StructType], providerName: String, parameters: Map[String, String]): (String, StructType) sourceSchema gives the < > (i.e. kafka ) and the spark-sql-streaming-KafkaOffsetReader.md#kafkaSchema[fixed schema]. Internally, sourceSchema < > and makes sure that the optional input schema is indeed undefined. When the input schema is defined, sourceSchema reports a IllegalArgumentException . Kafka source has a fixed schema and cannot be set with a custom one sourceSchema is part of the StreamSourceProvider abstraction. === [[validateStreamOptions]] Validating Kafka Options for Streaming Queries -- validateStreamOptions Internal Method [source, scala] \u00b6 validateStreamOptions(caseInsensitiveParams: Map[String, String]): Unit \u00b6 Firstly, validateStreamOptions makes sure that endingoffsets option is not used. Otherwise, validateStreamOptions reports a IllegalArgumentException . ending offset not valid in streaming queries validateStreamOptions then < >. NOTE: validateStreamOptions is used when KafkaSourceProvider is requested the < > and to < >. === [[createContinuousReader]] Creating ContinuousReader for Continuous Stream Processing -- createContinuousReader Method [source, scala] \u00b6 createContinuousReader( schema: Optional[StructType], metadataPath: String, options: DataSourceOptions): KafkaContinuousReader NOTE: createContinuousReader is part of the < > to create a < >. createContinuousReader ...FIXME === [[getKafkaOffsetRangeLimit]] Converting Configuration Options to KafkaOffsetRangeLimit -- getKafkaOffsetRangeLimit Object Method [source, scala] \u00b6 getKafkaOffsetRangeLimit( params: Map[String, String], offsetOptionKey: String, defaultOffsets: KafkaOffsetRangeLimit): KafkaOffsetRangeLimit getKafkaOffsetRangeLimit finds the given offsetOptionKey in the params and does the following conversion: latest becomes < > earliest becomes < > A JSON-formatted text becomes < > When the given offsetOptionKey is not found, getKafkaOffsetRangeLimit returns the given defaultOffsets NOTE: getKafkaOffsetRangeLimit is used when KafkaSourceProvider is requested to < >, < >, < >, < >, and < >. === [[createMicroBatchReader]] Creating MicroBatchReader for Micro-Batch Stream Processing -- createMicroBatchReader Method [source, scala] \u00b6 createMicroBatchReader( schema: Optional[StructType], metadataPath: String, options: DataSourceOptions): KafkaMicroBatchReader NOTE: createMicroBatchReader is part of the < > to create a < > in < >. createMicroBatchReader < > (in the given DataSourceOptions ). createMicroBatchReader generates a unique group ID of the format spark-kafka-source-[randomUUID]-[metadataPath_hashCode] (to make sure that a new streaming query creates a new consumer group). createMicroBatchReader finds all the parameters (in the given DataSourceOptions ) that start with kafka. prefix, removes the prefix, and creates the current Kafka parameters. createMicroBatchReader creates a < > with the following: < > (in the given DataSourceOptions ) < > (given the current Kafka parameters, i.e. without kafka. prefix) The given DataSourceOptions spark-kafka-source-[randomUUID]-[metadataPath_hashCode]-driver for the driverGroupIdPrefix In the end, createMicroBatchReader creates a < > with the following: the KafkaOffsetReader < > (given the current Kafka parameters, i.e. without kafka. prefix) and the unique group ID ( spark-kafka-source-[randomUUID]-[metadataPath_hashCode]-driver ) The given DataSourceOptions and the metadataPath < > (< > option with the default of LatestOffsetRangeLimit offsets) < > === [[createRelation]] Creating BaseRelation -- createRelation Method [source, scala] \u00b6 createRelation( sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation NOTE: createRelation is part of the https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-RelationProvider.html[RelationProvider ] contract to create a BaseRelation . createRelation ...FIXME === [[validateBatchOptions]] Validating Configuration Options for Batch Processing -- validateBatchOptions Internal Method [source, scala] \u00b6 validateBatchOptions(caseInsensitiveParams: Map[String, String]): Unit \u00b6 validateBatchOptions ...FIXME NOTE: validateBatchOptions is used exclusively when KafkaSourceProvider is requested to < >. === [[kafkaParamsForDriver]] kafkaParamsForDriver Method [source, scala] \u00b6 kafkaParamsForDriver(specifiedKafkaParams: Map[String, String]): Map[String, Object] \u00b6 kafkaParamsForDriver ...FIXME NOTE: kafkaParamsForDriver is used when...FIXME === [[kafkaParamsForExecutors]] kafkaParamsForExecutors Method [source, scala] \u00b6 kafkaParamsForExecutors( specifiedKafkaParams: Map[String, String], uniqueGroupId: String): Map[String, Object] kafkaParamsForExecutors sets the < >. While setting the properties, kafkaParamsForExecutors prints out the following DEBUG message to the logs: executor: Set [key] to [value], earlier value: [value] [NOTE] \u00b6 kafkaParamsForExecutors is used when: KafkaSourceProvider is requested to < > (for a < >), < > (for a < >), and < > (for a < >) * KafkaRelation is requested to < > (for a KafkaSourceRDD ) \u00b6 === [[failOnDataLoss]] Looking Up failOnDataLoss Configuration Property -- failOnDataLoss Internal Method [source, scala] \u00b6 failOnDataLoss(caseInsensitiveParams: Map[String, String]): Boolean \u00b6 failOnDataLoss simply looks up the failOnDataLoss configuration property in the given caseInsensitiveParams (in case-insensitive manner) or defaults to true . NOTE: failOnDataLoss is used when KafkaSourceProvider is requested to < > (for a < >), < > (for a < >), < > (for a < >), and < > (for a < >).","title":"KafkaSourceProvider"},{"location":"spark-sql-streaming-KafkaSourceProvider/#kafkasourceprovider-data-source-provider-for-apache-kafka","text":"[[shortName]] KafkaSourceProvider is a DataSourceRegister and registers a developer-friendly alias for kafka data source format in Spark Structured Streaming. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceRegister.html[DataSourceRegister ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. KafkaSourceProvider supports < > (through < > contract) and < >. KafkaSourceProvider requires the following options (that you can set using option method of < > or DataStreamWriter ): Exactly one of the following options: < >, < > or < > < > TIP: Refer to < > for the supported configuration options. Internally, KafkaSourceProvider sets the < > (that are passed on to InternalKafkaConsumer when requested to create a Kafka consumer with a single TopicPartition manually assigned). [[kafkaParamsForExecutors-properties]] .KafkaSourceProvider's Properties for Kafka Consumers on Executors [cols=\"1m,1m,2\",options=\"header\",width=\"100%\"] |=== | ConsumerConfig's Key | Value | Description | KEY_DESERIALIZER_CLASS_CONFIG | ByteArrayDeserializer a| [[KEY_DESERIALIZER_CLASS_CONFIG]] FIXME | VALUE_DESERIALIZER_CLASS_CONFIG | ByteArrayDeserializer a| [[VALUE_DESERIALIZER_CLASS_CONFIG]] FIXME | AUTO_OFFSET_RESET_CONFIG | none a| [[AUTO_OFFSET_RESET_CONFIG]] FIXME | GROUP_ID_CONFIG | < >-executor a| [[GROUP_ID_CONFIG]] FIXME | ENABLE_AUTO_COMMIT_CONFIG | false a| [[ENABLE_AUTO_COMMIT_CONFIG]] FIXME | RECEIVE_BUFFER_CONFIG | 65536 a| [[RECEIVE_BUFFER_CONFIG]] Only when not set in the < > already |=== [[logging]] [TIP] ==== Enable ALL logging levels for org.apache.spark.sql.kafka010.KafkaSourceProvider logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.kafka010.KafkaSourceProvider=ALL","title":"KafkaSourceProvider &mdash; Data Source Provider for Apache Kafka"},{"location":"spark-sql-streaming-KafkaSourceProvider/#refer-to","text":"=== [[createSource]] Creating Streaming Source -- createSource Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala","text":"createSource( sqlContext: SQLContext, metadataPath: String, schema: Option[StructType], providerName: String, parameters: Map[String, String]): Source createSource is part of the StreamSourceProvider abstraction. createSource first < >. createSource ...FIXME === [[validateGeneralOptions]] Validating General Options For Batch And Streaming Queries -- validateGeneralOptions Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#validategeneraloptionsparameters-mapstring-string-unit","text":"NOTE: Parameters are case-insensitive, i.e. OptioN and option are equal. validateGeneralOptions makes sure that exactly one topic subscription strategy is used in parameters and can be: subscribe subscribepattern assign validateGeneralOptions reports an IllegalArgumentException when there is no subscription strategy in use or there are more than one strategies used. validateGeneralOptions makes sure that the value of subscription strategies meet the requirements: assign strategy starts with { (the opening curly brace) subscribe strategy has at least one topic (in a comma-separated list of topics) subscribepattern strategy has the pattern defined validateGeneralOptions makes sure that group.id has not been specified and reports an IllegalArgumentException otherwise. Kafka option 'group.id' is not supported as user-specified consumer groups are not used to track offsets. validateGeneralOptions makes sure that auto.offset.reset has not been specified and reports an IllegalArgumentException otherwise.","title":"validateGeneralOptions(parameters: Map[String, String]): Unit"},{"location":"spark-sql-streaming-KafkaSourceProvider/#optionswrap","text":"Kafka option 'auto.offset.reset' is not supported. Instead set the source option 'startingoffsets' to 'earliest' or 'latest' to specify where to start. Structured Streaming manages which offsets are consumed internally, rather than relying on the kafkaConsumer to do it. This will ensure that no data is missed when new topics/partitions are dynamically subscribed. Note that 'startingoffsets' only applies when a new Streaming query is started, and that resuming will always pick up from where the query left off. See the docs for more details. validateGeneralOptions makes sure that the following options have not been specified and reports an IllegalArgumentException otherwise: kafka.key.deserializer kafka.value.deserializer kafka.enable.auto.commit kafka.interceptor.classes In the end, validateGeneralOptions makes sure that kafka.bootstrap.servers option was specified and reports an IllegalArgumentException otherwise. Option 'kafka.bootstrap.servers' must be specified for configuring Kafka consumer NOTE: validateGeneralOptions is used when KafkaSourceProvider validates options for < > and < > queries. === [[strategy]] Creating ConsumerStrategy -- strategy Internal Method","title":"[options=\"wrap\"]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#strategycaseinsensitiveparams-mapstring-string","text":"Internally, strategy finds the keys in the input caseInsensitiveParams that are one of the following and creates a corresponding spark-sql-streaming-ConsumerStrategy.md[ConsumerStrategy]. .KafkaSourceProvider.strategy's Key to ConsumerStrategy Conversion [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Key | ConsumerStrategy | assign a| spark-sql-streaming-ConsumerStrategy.md#AssignStrategy[AssignStrategy] with Kafka's http://kafka.apache.org/0110/javadoc/org/apache/kafka/common/TopicPartition.html[TopicPartitions ]. strategy uses JsonUtils.partitions method to parse a JSON with topic names and partitions, e.g. {\"topicA\":[0,1],\"topicB\":[0,1]} The topic names and partitions are mapped directly to Kafka's TopicPartition objects. | subscribe a| spark-sql-streaming-ConsumerStrategy.md#SubscribeStrategy[SubscribeStrategy] with topic names strategy extracts topic names from a comma-separated string, e.g. topic1,topic2,topic3 | subscribepattern a| spark-sql-streaming-ConsumerStrategy.md#SubscribePatternStrategy[SubscribePatternStrategy] with topic subscription regex pattern (that uses Java's http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern ] for the pattern), e.g. topic\\d |===","title":"strategy(caseInsensitiveParams: Map[String, String])"},{"location":"spark-sql-streaming-KafkaSourceProvider/#note","text":"strategy is used when: KafkaSourceProvider < >.","title":"[NOTE]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#kafkasourceprovider-creates-a-kafkarelation-using-createrelation-method","text":"=== [[sourceSchema]] Describing Streaming Source with Name and Schema -- sourceSchema Method","title":"* KafkaSourceProvider creates a KafkaRelation (using createRelation method)."},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_3","text":"sourceSchema( sqlContext: SQLContext, schema: Option[StructType], providerName: String, parameters: Map[String, String]): (String, StructType) sourceSchema gives the < > (i.e. kafka ) and the spark-sql-streaming-KafkaOffsetReader.md#kafkaSchema[fixed schema]. Internally, sourceSchema < > and makes sure that the optional input schema is indeed undefined. When the input schema is defined, sourceSchema reports a IllegalArgumentException . Kafka source has a fixed schema and cannot be set with a custom one sourceSchema is part of the StreamSourceProvider abstraction. === [[validateStreamOptions]] Validating Kafka Options for Streaming Queries -- validateStreamOptions Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#validatestreamoptionscaseinsensitiveparams-mapstring-string-unit","text":"Firstly, validateStreamOptions makes sure that endingoffsets option is not used. Otherwise, validateStreamOptions reports a IllegalArgumentException . ending offset not valid in streaming queries validateStreamOptions then < >. NOTE: validateStreamOptions is used when KafkaSourceProvider is requested the < > and to < >. === [[createContinuousReader]] Creating ContinuousReader for Continuous Stream Processing -- createContinuousReader Method","title":"validateStreamOptions(caseInsensitiveParams: Map[String, String]): Unit"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_5","text":"createContinuousReader( schema: Optional[StructType], metadataPath: String, options: DataSourceOptions): KafkaContinuousReader NOTE: createContinuousReader is part of the < > to create a < >. createContinuousReader ...FIXME === [[getKafkaOffsetRangeLimit]] Converting Configuration Options to KafkaOffsetRangeLimit -- getKafkaOffsetRangeLimit Object Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_6","text":"getKafkaOffsetRangeLimit( params: Map[String, String], offsetOptionKey: String, defaultOffsets: KafkaOffsetRangeLimit): KafkaOffsetRangeLimit getKafkaOffsetRangeLimit finds the given offsetOptionKey in the params and does the following conversion: latest becomes < > earliest becomes < > A JSON-formatted text becomes < > When the given offsetOptionKey is not found, getKafkaOffsetRangeLimit returns the given defaultOffsets NOTE: getKafkaOffsetRangeLimit is used when KafkaSourceProvider is requested to < >, < >, < >, < >, and < >. === [[createMicroBatchReader]] Creating MicroBatchReader for Micro-Batch Stream Processing -- createMicroBatchReader Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_7","text":"createMicroBatchReader( schema: Optional[StructType], metadataPath: String, options: DataSourceOptions): KafkaMicroBatchReader NOTE: createMicroBatchReader is part of the < > to create a < > in < >. createMicroBatchReader < > (in the given DataSourceOptions ). createMicroBatchReader generates a unique group ID of the format spark-kafka-source-[randomUUID]-[metadataPath_hashCode] (to make sure that a new streaming query creates a new consumer group). createMicroBatchReader finds all the parameters (in the given DataSourceOptions ) that start with kafka. prefix, removes the prefix, and creates the current Kafka parameters. createMicroBatchReader creates a < > with the following: < > (in the given DataSourceOptions ) < > (given the current Kafka parameters, i.e. without kafka. prefix) The given DataSourceOptions spark-kafka-source-[randomUUID]-[metadataPath_hashCode]-driver for the driverGroupIdPrefix In the end, createMicroBatchReader creates a < > with the following: the KafkaOffsetReader < > (given the current Kafka parameters, i.e. without kafka. prefix) and the unique group ID ( spark-kafka-source-[randomUUID]-[metadataPath_hashCode]-driver ) The given DataSourceOptions and the metadataPath < > (< > option with the default of LatestOffsetRangeLimit offsets) < > === [[createRelation]] Creating BaseRelation -- createRelation Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_8","text":"createRelation( sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation NOTE: createRelation is part of the https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-RelationProvider.html[RelationProvider ] contract to create a BaseRelation . createRelation ...FIXME === [[validateBatchOptions]] Validating Configuration Options for Batch Processing -- validateBatchOptions Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#validatebatchoptionscaseinsensitiveparams-mapstring-string-unit","text":"validateBatchOptions ...FIXME NOTE: validateBatchOptions is used exclusively when KafkaSourceProvider is requested to < >. === [[kafkaParamsForDriver]] kafkaParamsForDriver Method","title":"validateBatchOptions(caseInsensitiveParams: Map[String, String]): Unit"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_10","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#kafkaparamsfordriverspecifiedkafkaparams-mapstring-string-mapstring-object","text":"kafkaParamsForDriver ...FIXME NOTE: kafkaParamsForDriver is used when...FIXME === [[kafkaParamsForExecutors]] kafkaParamsForExecutors Method","title":"kafkaParamsForDriver(specifiedKafkaParams: Map[String, String]): Map[String, Object]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_11","text":"kafkaParamsForExecutors( specifiedKafkaParams: Map[String, String], uniqueGroupId: String): Map[String, Object] kafkaParamsForExecutors sets the < >. While setting the properties, kafkaParamsForExecutors prints out the following DEBUG message to the logs: executor: Set [key] to [value], earlier value: [value]","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#note_1","text":"kafkaParamsForExecutors is used when: KafkaSourceProvider is requested to < > (for a < >), < > (for a < >), and < > (for a < >)","title":"[NOTE]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#kafkarelation-is-requested-to-for-a-kafkasourcerdd","text":"=== [[failOnDataLoss]] Looking Up failOnDataLoss Configuration Property -- failOnDataLoss Internal Method","title":"* KafkaRelation is requested to &lt;&gt; (for a KafkaSourceRDD)"},{"location":"spark-sql-streaming-KafkaSourceProvider/#source-scala_12","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceProvider/#failondatalosscaseinsensitiveparams-mapstring-string-boolean","text":"failOnDataLoss simply looks up the failOnDataLoss configuration property in the given caseInsensitiveParams (in case-insensitive manner) or defaults to true . NOTE: failOnDataLoss is used when KafkaSourceProvider is requested to < > (for a < >), < > (for a < >), < > (for a < >), and < > (for a < >).","title":"failOnDataLoss(caseInsensitiveParams: Map[String, String]): Boolean"},{"location":"spark-sql-streaming-KafkaSourceRDD/","text":"== [[KafkaSourceRDD]] KafkaSourceRDD KafkaSourceRDD is an RDD of Kafka's https://kafka.apache.org/0102/javadoc/org/apache/kafka/clients/consumer/ConsumerRecords.html[ConsumerRecords ] ( RDD[ConsumerRecord[Array[Byte], Array[Byte]]] ) and no parent RDDs. KafkaSourceRDD is < > when: KafkaRelation is requested to < > KafkaSource is requested to < > === [[creating-instance]] Creating KafkaSourceRDD Instance KafkaSourceRDD takes the following when created: [[sc]] SparkContext [[executorKafkaParams]] Collection of key-value settings for executors reading records from Kafka topics [[offsetRanges]] Collection of KafkaSourceRDDOffsetRange offsets [[pollTimeoutMs]] Timeout (in milliseconds) to poll data from Kafka + Used when KafkaSourceRDD < > (for given offsets) and in turn spark-sql-streaming-CachedKafkaConsumer.md#poll[requests CachedKafkaConsumer to poll for Kafka's ConsumerRecords ]. [[failOnDataLoss]] Flag to...FIXME [[reuseKafkaConsumer]] Flag to...FIXME === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method [source, scala] \u00b6 getPreferredLocations( split: Partition): Seq[String] NOTE: getPreferredLocations is part of the RDD contract to specify placement preferences. getPreferredLocations converts the given Partition to a KafkaSourceRDDPartition and...FIXME === [[compute]] Computing Partition -- compute Method [source, scala] \u00b6 compute( thePart: Partition, context: TaskContext ): Iterator[ConsumerRecord[Array[Byte], Array[Byte]]] NOTE: compute is part of the RDD contract to compute a given partition. compute uses KafkaDataConsumer utility to < > (for a partition). compute < > (based on the offsetRange of the given partition that is assumed a KafkaSourceRDDPartition ). compute returns a NextIterator so that getNext uses the KafkaDataConsumer to < >. When the beginning and ending offsets (of the offset range) are equal, compute prints out the following INFO message to the logs, requests the KafkaDataConsumer to < > and returns an empty iterator. Beginning offset [fromOffset] is the same as ending offset skipping [topic] [partition] compute throws an AssertionError when the beginning offset ( fromOffset ) is after the ending offset ( untilOffset ): [options=\"wrap\"] \u00b6 Beginning offset [fromOffset] is after the ending offset [untilOffset] for topic [topic] partition [partition]. You either provided an invalid fromOffset, or the Kafka topic has been damaged \u00b6 === [[getPartitions]] getPartitions Method [source, scala] \u00b6 getPartitions: Array[Partition] \u00b6 NOTE: getPartitions is part of the RDD contract to...FIXME. getPartitions ...FIXME === [[persist]] Persisting RDD -- persist Method [source, scala] \u00b6 persist: Array[Partition] \u00b6 NOTE: persist is part of the RDD contract to persist an RDD. persist ...FIXME === [[resolveRange]] resolveRange Internal Method [source, scala] \u00b6 resolveRange( consumer: KafkaDataConsumer, range: KafkaSourceRDDOffsetRange ): KafkaSourceRDDOffsetRange resolveRange ...FIXME NOTE: resolveRange is used when...FIXME","title":"KafkaSourceRDD"},{"location":"spark-sql-streaming-KafkaSourceRDD/#source-scala","text":"getPreferredLocations( split: Partition): Seq[String] NOTE: getPreferredLocations is part of the RDD contract to specify placement preferences. getPreferredLocations converts the given Partition to a KafkaSourceRDDPartition and...FIXME === [[compute]] Computing Partition -- compute Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceRDD/#source-scala_1","text":"compute( thePart: Partition, context: TaskContext ): Iterator[ConsumerRecord[Array[Byte], Array[Byte]]] NOTE: compute is part of the RDD contract to compute a given partition. compute uses KafkaDataConsumer utility to < > (for a partition). compute < > (based on the offsetRange of the given partition that is assumed a KafkaSourceRDDPartition ). compute returns a NextIterator so that getNext uses the KafkaDataConsumer to < >. When the beginning and ending offsets (of the offset range) are equal, compute prints out the following INFO message to the logs, requests the KafkaDataConsumer to < > and returns an empty iterator. Beginning offset [fromOffset] is the same as ending offset skipping [topic] [partition] compute throws an AssertionError when the beginning offset ( fromOffset ) is after the ending offset ( untilOffset ):","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceRDD/#optionswrap","text":"","title":"[options=\"wrap\"]"},{"location":"spark-sql-streaming-KafkaSourceRDD/#beginning-offset-fromoffset-is-after-the-ending-offset-untiloffset-for-topic-topic-partition-partition-you-either-provided-an-invalid-fromoffset-or-the-kafka-topic-has-been-damaged","text":"=== [[getPartitions]] getPartitions Method","title":"Beginning offset [fromOffset] is after the ending offset [untilOffset] for topic [topic] partition [partition]. You either provided an invalid fromOffset, or the Kafka topic has been damaged"},{"location":"spark-sql-streaming-KafkaSourceRDD/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceRDD/#getpartitions-arraypartition","text":"NOTE: getPartitions is part of the RDD contract to...FIXME. getPartitions ...FIXME === [[persist]] Persisting RDD -- persist Method","title":"getPartitions: Array[Partition]"},{"location":"spark-sql-streaming-KafkaSourceRDD/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KafkaSourceRDD/#persist-arraypartition","text":"NOTE: persist is part of the RDD contract to persist an RDD. persist ...FIXME === [[resolveRange]] resolveRange Internal Method","title":"persist: Array[Partition]"},{"location":"spark-sql-streaming-KafkaSourceRDD/#source-scala_4","text":"resolveRange( consumer: KafkaDataConsumer, range: KafkaSourceRDDOffsetRange ): KafkaSourceRDDOffsetRange resolveRange ...FIXME NOTE: resolveRange is used when...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/","text":"== [[KeyToNumValuesStore]] KeyToNumValuesStore -- State Store (Handler) Of Join Keys And Counts KeyToNumValuesStore is a < > (of < >) for < > to manage a < >. .KeyToNumValuesStore, KeyWithIndexToValueStore and Stream-Stream Join image::images/KeyToNumValuesStore-KeyWithIndexToValueStore.png[align=\"center\"] [[stateStore]] As a < >, KeyToNumValuesStore manages a < > (that is < >) with the join keys (per < >) and their count (per < >). [[longValueSchema]] KeyToNumValuesStore uses the schema for values in the < > with one field value (of type long ) that is the number of value rows (count). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore=ALL Refer to < >. \u00b6 === [[get]] Looking Up Number Of Value Rows For Given Key (Value Count) -- get Method [source, scala] \u00b6 get(key: UnsafeRow): Long \u00b6 get requests the < > for the < > and returns the long value at 0 th position (of the row found) or 0 . NOTE: get is used when SymmetricHashJoinStateManager is requested for the < > and < >. === [[put]] Storing Key Count For Given Key -- put Method [source, scala] \u00b6 put( key: UnsafeRow, numValues: Long): Unit put stores the numValues at the 0 th position (of the internal unsafe row) and requests the < > to < >. put requires that the numValues count is greater than 0 (or throws an IllegalArgumentException ). NOTE: put is used when SymmetricHashJoinStateManager is requested for the < > and < >. === [[iterator]] All State Keys and Values -- iterator Method [source, scala] \u00b6 iterator: Iterator[KeyAndNumValues] \u00b6 iterator simply requests the < > for < >. NOTE: iterator is used when SymmetricHashJoinStateManager is requested to < > and < >. === [[remove]] Removing State Key -- remove Method [source, scala] \u00b6 remove(key: UnsafeRow): Unit \u00b6 remove simply requests the < > to < >. NOTE: remove is used when...FIXME","title":"KeyToNumValuesStore"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#refer-to","text":"=== [[get]] Looking Up Number Of Value Rows For Given Key (Value Count) -- get Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-KeyToNumValuesStore/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#getkey-unsaferow-long","text":"get requests the < > for the < > and returns the long value at 0 th position (of the row found) or 0 . NOTE: get is used when SymmetricHashJoinStateManager is requested for the < > and < >. === [[put]] Storing Key Count For Given Key -- put Method","title":"get(key: UnsafeRow): Long"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#source-scala_1","text":"put( key: UnsafeRow, numValues: Long): Unit put stores the numValues at the 0 th position (of the internal unsafe row) and requests the < > to < >. put requires that the numValues count is greater than 0 (or throws an IllegalArgumentException ). NOTE: put is used when SymmetricHashJoinStateManager is requested for the < > and < >. === [[iterator]] All State Keys and Values -- iterator Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#iterator-iteratorkeyandnumvalues","text":"iterator simply requests the < > for < >. NOTE: iterator is used when SymmetricHashJoinStateManager is requested to < > and < >. === [[remove]] Removing State Key -- remove Method","title":"iterator: Iterator[KeyAndNumValues]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyToNumValuesStore/#removekey-unsaferow-unit","text":"remove simply requests the < > to < >. NOTE: remove is used when...FIXME","title":"remove(key: UnsafeRow): Unit"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-flatMapGroupsWithState/","text":"== [[flatMapGroupsWithState]] flatMapGroupsWithState Operator -- Arbitrary Stateful Streaming Aggregation (with Explicit State Logic) [source, scala] \u00b6 KeyValueGroupedDataset[K, V].flatMapGroupsWithState S: Encoder, U: Encoder ( func: (K, Iterator[V], GroupState[S]) => Iterator[U]): Dataset[U] flatMapGroupsWithState operator is used for Arbitrary Stateful Streaming Aggregation (with Explicit State Logic). flatMapGroupsWithState requires that the given < > is either < > or < > (and reports an IllegalArgumentException at runtime). NOTE: An OutputMode is a required argument, but does not seem to be used at all. Check out the question https://stackoverflow.com/q/56921772/1305344[What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used?] on StackOverflow. Every time the state function func is executed for a key, the state (as GroupState[S] ) is for this key only. [NOTE] \u00b6 K is the type of the keys in KeyValueGroupedDataset V is the type of the values (per key) in KeyValueGroupedDataset S is the user-defined type of the state as maintained for each group * U is the type of rows in the result Dataset \u00b6 flatMapGroupsWithState creates a new Dataset with FlatMapGroupsWithState unary logical operator.","title":"flatMapGroupsWithState Operator"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-flatMapGroupsWithState/#source-scala","text":"KeyValueGroupedDataset[K, V].flatMapGroupsWithState S: Encoder, U: Encoder ( func: (K, Iterator[V], GroupState[S]) => Iterator[U]): Dataset[U] flatMapGroupsWithState operator is used for Arbitrary Stateful Streaming Aggregation (with Explicit State Logic). flatMapGroupsWithState requires that the given < > is either < > or < > (and reports an IllegalArgumentException at runtime). NOTE: An OutputMode is a required argument, but does not seem to be used at all. Check out the question https://stackoverflow.com/q/56921772/1305344[What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used?] on StackOverflow. Every time the state function func is executed for a key, the state (as GroupState[S] ) is for this key only.","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-flatMapGroupsWithState/#note","text":"K is the type of the keys in KeyValueGroupedDataset V is the type of the values (per key) in KeyValueGroupedDataset S is the user-defined type of the state as maintained for each group","title":"[NOTE]"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-flatMapGroupsWithState/#u-is-the-type-of-rows-in-the-result-dataset","text":"flatMapGroupsWithState creates a new Dataset with FlatMapGroupsWithState unary logical operator.","title":"* U is the type of rows in the result Dataset"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState/","text":"== [[mapGroupsWithState]] mapGroupsWithState Operator -- Stateful Streaming Aggregation (with Explicit State Logic) [source, scala] \u00b6 mapGroupsWithState S: Encoder, U: Encoder : Dataset[U] // <1> mapGroupsWithState S: Encoder, U: Encoder ( func: (K, Iterator[V], GroupState[S]) => U): Dataset[U] <1> Uses GroupStateTimeout.NoTimeout for timeoutConf mapGroupsWithState operator...FIXME Note mapGroupsWithState is a special case of flatMapGroupsWithState operator with the following: func being transformed to return a single-element Iterator spark-sql-streaming-OutputMode.md#Update[Update] output mode mapGroupsWithState also creates a FlatMapGroupsWithState with isMapGroupsWithState internal flag enabled. // numGroups defined at the beginning scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] import org.apache.spark.sql.streaming.GroupState def mappingFunc(key: Long, values: Iterator[(java.sql.Timestamp, Long)], state: GroupState[Long]): Long = { println(s\">>> key: $key => state: $state\") val newState = state.getOption.map(_ + values.size).getOrElse(0L) state.update(newState) key } import org.apache.spark.sql.streaming.GroupStateTimeout val longs = numGroups.mapGroupsWithState( timeoutConf = GroupStateTimeout.ProcessingTimeTimeout)( func = mappingFunc) import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = longs. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). // <-- required for mapGroupsWithState start // Note GroupState ------------------------------------------- Batch: 1 ------------------------------------------- >>> key: 0 => state: GroupState(<undefined>) >>> key: 1 => state: GroupState(<undefined>) +-----+ |value| +-----+ | 0| | 1| +-----+ ------------------------------------------- Batch: 2 ------------------------------------------- >>> key: 0 => state: GroupState(0) >>> key: 1 => state: GroupState(0) +-----+ |value| +-----+ | 0| | 1| +-----+ ------------------------------------------- Batch: 3 ------------------------------------------- >>> key: 0 => state: GroupState(4) >>> key: 1 => state: GroupState(4) +-----+ |value| +-----+ | 0| | 1| +-----+ // in the end spark.streams.active.foreach(_.stop)","title":"mapGroupsWithState Operator"},{"location":"spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState/#source-scala","text":"mapGroupsWithState S: Encoder, U: Encoder : Dataset[U] // <1> mapGroupsWithState S: Encoder, U: Encoder ( func: (K, Iterator[V], GroupState[S]) => U): Dataset[U] <1> Uses GroupStateTimeout.NoTimeout for timeoutConf mapGroupsWithState operator...FIXME Note mapGroupsWithState is a special case of flatMapGroupsWithState operator with the following: func being transformed to return a single-element Iterator spark-sql-streaming-OutputMode.md#Update[Update] output mode mapGroupsWithState also creates a FlatMapGroupsWithState with isMapGroupsWithState internal flag enabled. // numGroups defined at the beginning scala> :type numGroups org.apache.spark.sql.KeyValueGroupedDataset[Long,(java.sql.Timestamp, Long)] import org.apache.spark.sql.streaming.GroupState def mappingFunc(key: Long, values: Iterator[(java.sql.Timestamp, Long)], state: GroupState[Long]): Long = { println(s\">>> key: $key => state: $state\") val newState = state.getOption.map(_ + values.size).getOrElse(0L) state.update(newState) key } import org.apache.spark.sql.streaming.GroupStateTimeout val longs = numGroups.mapGroupsWithState( timeoutConf = GroupStateTimeout.ProcessingTimeTimeout)( func = mappingFunc) import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = longs. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). // <-- required for mapGroupsWithState start // Note GroupState ------------------------------------------- Batch: 1 ------------------------------------------- >>> key: 0 => state: GroupState(<undefined>) >>> key: 1 => state: GroupState(<undefined>) +-----+ |value| +-----+ | 0| | 1| +-----+ ------------------------------------------- Batch: 2 ------------------------------------------- >>> key: 0 => state: GroupState(0) >>> key: 1 => state: GroupState(0) +-----+ |value| +-----+ | 0| | 1| +-----+ ------------------------------------------- Batch: 3 ------------------------------------------- >>> key: 0 => state: GroupState(4) >>> key: 1 => state: GroupState(4) +-----+ |value| +-----+ | 0| | 1| +-----+ // in the end spark.streams.active.foreach(_.stop)","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/","text":"== [[KeyWithIndexToValueStore]] KeyWithIndexToValueStore -- State Store (Handler) Of Join Keys With Index Of Values KeyWithIndexToValueStore is a < > (of < >) for < > to manage a < >. .KeyToNumValuesStore, KeyWithIndexToValueStore and Stream-Stream Join image::images/KeyToNumValuesStore-KeyWithIndexToValueStore.png[align=\"center\"] [[stateStore]] As a < >, KeyWithIndexToValueStore manages a < > (that is < >) for keys and values per the < > and < > schemas, respectively. [[keyWithIndexSchema]] KeyWithIndexToValueStore uses a schema (for the < >) that is the < > (of the parent SymmetricHashJoinStateManager ) with an extra field index of type long . [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyWithIndexToValueStore to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyWithIndexToValueStore=ALL Refer to < >. \u00b6 === [[get]] Looking Up State Row For Given Key and Index -- get Method [source, scala] \u00b6 get( key: UnsafeRow, valueIndex: Long): UnsafeRow get simply requests the internal < > to < > the value for the given < >. NOTE: get is used exclusively when SymmetricHashJoinStateManager is requested to < > === [[getAll]] Retrieving (Given Number of) Values for Key -- getAll Method [source, scala] \u00b6 getAll( key: UnsafeRow, numValues: Long): Iterator[KeyWithIndexAndValue] getAll ...FIXME NOTE: getAll is used when SymmetricHashJoinStateManager is requested to < > and < >. === [[put]] Storing State Row For Given Key and Index -- put Method [source, scala] \u00b6 put( key: UnsafeRow, valueIndex: Long, value: UnsafeRow): Unit put ...FIXME NOTE: put is used when SymmetricHashJoinStateManager is requested to < > and < >. === [[remove]] remove Method [source, scala] \u00b6 remove( key: UnsafeRow, valueIndex: Long): Unit remove ...FIXME NOTE: remove is used when SymmetricHashJoinStateManager is requested to < > and < >. === [[keyWithIndexRow]] keyWithIndexRow Internal Method [source, scala] \u00b6 keyWithIndexRow( key: UnsafeRow, valueIndex: Long): UnsafeRow keyWithIndexRow uses the < > to generate an UnsafeRow for the key and sets the valueIndex at the < > position. NOTE: keyWithIndexRow is used when KeyWithIndexToValueStore is requested to < >, < >, < >, < > and < >. === [[removeAllValues]] removeAllValues Method [source, scala] \u00b6 removeAllValues( key: UnsafeRow, numValues: Long): Unit removeAllValues ...FIXME NOTE: removeAllValues does not seem to be used at all. === [[iterator]] iterator Method [source, scala] \u00b6 iterator: Iterator[KeyWithIndexAndValue] \u00b6 iterator ...FIXME NOTE: iterator does not seem to be used at all. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | indexOrdinalInKeyWithIndexRow a| [[indexOrdinalInKeyWithIndexRow]] Position of the index in the key row (which corresponds to the number of the < >) Used exclusively in the < > | keyWithIndexExprs a| [[keyWithIndexExprs]] < > with Literal(1L) expression appended Used exclusively for the < > projection | keyWithIndexRowGenerator a| [[keyWithIndexRowGenerator]] UnsafeProjection for the < > bound to the < > Used exclusively in < > |===","title":"KeyWithIndexToValueStore"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#refer-to","text":"=== [[get]] Looking Up State Row For Given Key and Index -- get Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala","text":"get( key: UnsafeRow, valueIndex: Long): UnsafeRow get simply requests the internal < > to < > the value for the given < >. NOTE: get is used exclusively when SymmetricHashJoinStateManager is requested to < > === [[getAll]] Retrieving (Given Number of) Values for Key -- getAll Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_1","text":"getAll( key: UnsafeRow, numValues: Long): Iterator[KeyWithIndexAndValue] getAll ...FIXME NOTE: getAll is used when SymmetricHashJoinStateManager is requested to < > and < >. === [[put]] Storing State Row For Given Key and Index -- put Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_2","text":"put( key: UnsafeRow, valueIndex: Long, value: UnsafeRow): Unit put ...FIXME NOTE: put is used when SymmetricHashJoinStateManager is requested to < > and < >. === [[remove]] remove Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_3","text":"remove( key: UnsafeRow, valueIndex: Long): Unit remove ...FIXME NOTE: remove is used when SymmetricHashJoinStateManager is requested to < > and < >. === [[keyWithIndexRow]] keyWithIndexRow Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_4","text":"keyWithIndexRow( key: UnsafeRow, valueIndex: Long): UnsafeRow keyWithIndexRow uses the < > to generate an UnsafeRow for the key and sets the valueIndex at the < > position. NOTE: keyWithIndexRow is used when KeyWithIndexToValueStore is requested to < >, < >, < >, < > and < >. === [[removeAllValues]] removeAllValues Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_5","text":"removeAllValues( key: UnsafeRow, numValues: Long): Unit removeAllValues ...FIXME NOTE: removeAllValues does not seem to be used at all. === [[iterator]] iterator Method","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-KeyWithIndexToValueStore/#iterator-iteratorkeywithindexandvalue","text":"iterator ...FIXME NOTE: iterator does not seem to be used at all. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | indexOrdinalInKeyWithIndexRow a| [[indexOrdinalInKeyWithIndexRow]] Position of the index in the key row (which corresponds to the number of the < >) Used exclusively in the < > | keyWithIndexExprs a| [[keyWithIndexExprs]] < > with Literal(1L) expression appended Used exclusively for the < > projection | keyWithIndexRowGenerator a| [[keyWithIndexRowGenerator]] UnsafeProjection for the < > bound to the < > Used exclusively in < > |===","title":"iterator: Iterator[KeyWithIndexAndValue]"},{"location":"spark-sql-streaming-ManifestFileCommitProtocol/","text":"== [[ManifestFileCommitProtocol]] ManifestFileCommitProtocol ManifestFileCommitProtocol is...FIXME === [[commitJob]] commitJob Method [source, scala] \u00b6 commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit NOTE: commitJob is part of the FileCommitProtocol contract to...FIXME. commitJob ...FIXME === [[commitTask]] commitTask Method [source, scala] \u00b6 commitTask( taskContext: TaskAttemptContext): TaskCommitMessage NOTE: commitTask is part of the FileCommitProtocol contract to...FIXME. commitTask ...FIXME","title":"ManifestFileCommitProtocol"},{"location":"spark-sql-streaming-ManifestFileCommitProtocol/#source-scala","text":"commitJob( jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit NOTE: commitJob is part of the FileCommitProtocol contract to...FIXME. commitJob ...FIXME === [[commitTask]] commitTask Method","title":"[source, scala]"},{"location":"spark-sql-streaming-ManifestFileCommitProtocol/#source-scala_1","text":"commitTask( taskContext: TaskAttemptContext): TaskCommitMessage NOTE: commitTask is part of the FileCommitProtocol contract to...FIXME. commitTask ...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryPlan/","text":"MemoryPlan Logical Operator \u00b6 MemoryPlan is a leaf logical operator (i.e. LogicalPlan ) that is used to query the data that has been written into a spark-sql-streaming-MemorySink.md[MemorySink]. MemoryPlan is created when starting continuous writing (to a MemorySink ). TIP: See the example in spark-sql-streaming-MemoryStream.md[MemoryStream]. scala> intsOut.explain(true) == Parsed Logical Plan == SubqueryAlias memstream +- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Analyzed Logical Plan == value: int SubqueryAlias memstream +- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Optimized Logical Plan == MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Physical Plan == LocalTableScan [value#21] When executed, MemoryPlan is translated to LocalTableScanExec physical operator (similar to LocalRelation logical operator) in BasicOperators execution planning strategy.","title":"MemoryPlan Logical Query Plan"},{"location":"spark-sql-streaming-MemoryPlan/#memoryplan-logical-operator","text":"MemoryPlan is a leaf logical operator (i.e. LogicalPlan ) that is used to query the data that has been written into a spark-sql-streaming-MemorySink.md[MemorySink]. MemoryPlan is created when starting continuous writing (to a MemorySink ). TIP: See the example in spark-sql-streaming-MemoryStream.md[MemoryStream]. scala> intsOut.explain(true) == Parsed Logical Plan == SubqueryAlias memstream +- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Analyzed Logical Plan == value: int SubqueryAlias memstream +- MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Optimized Logical Plan == MemoryPlan org.apache.spark.sql.execution.streaming.MemorySink@481bf251, [value#21] == Physical Plan == LocalTableScan [value#21] When executed, MemoryPlan is translated to LocalTableScanExec physical operator (similar to LocalRelation logical operator) in BasicOperators execution planning strategy.","title":"MemoryPlan Logical Operator"},{"location":"spark-sql-streaming-MemorySink/","text":"== [[MemorySink]] MemorySink MemorySink is a < > that < >. MemorySink is intended only for testing or demos. MemorySink is used for memory format and requires a query name (by queryName method or queryName option). NOTE: MemorySink was introduced in the https://github.com/apache/spark/pull/12119[pull request for [SPARK-14288][SQL] Memory Sink for streaming]. Use toDebugString to see the batches. Its aim is to allow users to test streaming applications in the Spark shell or other local tests. You can set checkpointLocation using option method or it will be set to spark-sql-streaming-properties.md#spark-sql-streaming-properties.md[spark.sql.streaming.checkpointLocation] property. If spark.sql.streaming.checkpointLocation is set, the code uses $location/$queryName directory. Finally, when no spark.sql.streaming.checkpointLocation is set, a temporary directory memory.stream under java.io.tmpdir is used with offsets subdirectory inside. NOTE: The directory is cleaned up at shutdown using ShutdownHookManager.registerShutdownDeleteDir . It creates MemorySink instance based on the schema of the DataFrame it operates on. It creates a new DataFrame using MemoryPlan with MemorySink instance created earlier and registers it as a temporary table (using spark-sql-dataframe.md#registerTempTable[DataFrame.registerTempTable] method). NOTE: At this point you can query the table as if it were a regular non-streaming table using spark-sql-sqlcontext.md#sql[sql] method. A new StreamingQuery.md[StreamingQuery] is started (using StreamingQueryManager.startQuery ) and returned. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.MemorySink logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MemorySink=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating MemorySink Instance MemorySink takes the following to be created: [[schema]] Output schema [[outputMode]] < > MemorySink initializes the < > internal property. === [[batches]] In-Memory Buffer of Streaming Batches -- batches Internal Property [source, scala] \u00b6 batches: ArrayBuffer[AddedData] \u00b6 batches holds data from streaming batches that have been < > ( written ) to this sink. For < > and < > output modes, batches holds rows from all batches. For < > output mode, batches holds rows from the last batch only. batches can be cleared ( emptied ) using < >. === [[addBatch]] Adding Batch of Data to Sink -- addBatch Method [source, scala] \u00b6 addBatch( batchId: Long, data: DataFrame): Unit NOTE: addBatch is part of the < > to \"add\" a batch of data to the sink. addBatch branches off based on whether the given batchId has already been < > or < >. A batch ID is considered committed when the given batch ID is greater than the < > (if available). ==== [[addBatch-not-committed]] Batch Not Committed With the batchId not committed, addBatch prints out the following DEBUG message to the logs: Committing batch [batchId] to [this] addBatch collects records from the given data . NOTE: addBatch uses Dataset.collect operator to collect records. For < > and < > output modes, addBatch adds the data (as a AddedData ) to the < > internal registry. For < > output mode, addBatch clears the < > internal registry first before adding the data (as a AddedData ). For any other output mode, addBatch reports an IllegalArgumentException : Output mode [outputMode] is not supported by MemorySink ==== [[addBatch-committed]] Batch Committed With the batchId committed, addBatch simply prints out the following DEBUG message to the logs and returns. Skipping already committed batch: [batchId] === [[clear]] Clearing Up Internal Batch Buffer -- clear Method [source, scala] \u00b6 clear(): Unit \u00b6 clear simply removes ( clears ) all data from the < > internal registry. NOTE: clear is used exclusively in tests.","title":"MemorySink"},{"location":"spark-sql-streaming-MemorySink/#refer-to","text":"=== [[creating-instance]] Creating MemorySink Instance MemorySink takes the following to be created: [[schema]] Output schema [[outputMode]] < > MemorySink initializes the < > internal property. === [[batches]] In-Memory Buffer of Streaming Batches -- batches Internal Property","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-MemorySink/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemorySink/#batches-arraybufferaddeddata","text":"batches holds data from streaming batches that have been < > ( written ) to this sink. For < > and < > output modes, batches holds rows from all batches. For < > output mode, batches holds rows from the last batch only. batches can be cleared ( emptied ) using < >. === [[addBatch]] Adding Batch of Data to Sink -- addBatch Method","title":"batches: ArrayBuffer[AddedData]"},{"location":"spark-sql-streaming-MemorySink/#source-scala_1","text":"addBatch( batchId: Long, data: DataFrame): Unit NOTE: addBatch is part of the < > to \"add\" a batch of data to the sink. addBatch branches off based on whether the given batchId has already been < > or < >. A batch ID is considered committed when the given batch ID is greater than the < > (if available). ==== [[addBatch-not-committed]] Batch Not Committed With the batchId not committed, addBatch prints out the following DEBUG message to the logs: Committing batch [batchId] to [this] addBatch collects records from the given data . NOTE: addBatch uses Dataset.collect operator to collect records. For < > and < > output modes, addBatch adds the data (as a AddedData ) to the < > internal registry. For < > output mode, addBatch clears the < > internal registry first before adding the data (as a AddedData ). For any other output mode, addBatch reports an IllegalArgumentException : Output mode [outputMode] is not supported by MemorySink ==== [[addBatch-committed]] Batch Committed With the batchId committed, addBatch simply prints out the following DEBUG message to the logs and returns. Skipping already committed batch: [batchId] === [[clear]] Clearing Up Internal Batch Buffer -- clear Method","title":"[source, scala]"},{"location":"spark-sql-streaming-MemorySink/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemorySink/#clear-unit","text":"clear simply removes ( clears ) all data from the < > internal registry. NOTE: clear is used exclusively in tests.","title":"clear(): Unit"},{"location":"spark-sql-streaming-MemorySinkBase/","text":"== [[MemorySinkBase]] MemorySinkBase Contract -- Base Contract for Memory Sinks MemorySinkBase is the < > of the < > for < > that manage < > in memory. [[contract]] .MemorySinkBase Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | allData a| [[allData]] [source, scala] \u00b6 allData: Seq[Row] \u00b6 | dataSinceBatch a| [[dataSinceBatch]] [source, scala] \u00b6 dataSinceBatch( sinceBatchId: Long): Seq[Row] | latestBatchData a| [[latestBatchData]] [source, scala] \u00b6 latestBatchData: Seq[Row] \u00b6 | latestBatchId a| [[latestBatchId]] [source, scala] \u00b6 latestBatchId: Option[Long] \u00b6 |=== [[implementations]] .MemorySinkBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemorySinkBase | Description | < > | [[MemorySink]] < > for < > (based on Data Source API V1) | < > | [[MemorySinkV2]] < > for < > (based on Data Source API V2) |===","title":"MemorySinkBase"},{"location":"spark-sql-streaming-MemorySinkBase/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemorySinkBase/#alldata-seqrow","text":"| dataSinceBatch a| [[dataSinceBatch]]","title":"allData: Seq[Row]"},{"location":"spark-sql-streaming-MemorySinkBase/#source-scala_1","text":"dataSinceBatch( sinceBatchId: Long): Seq[Row] | latestBatchData a| [[latestBatchData]]","title":"[source, scala]"},{"location":"spark-sql-streaming-MemorySinkBase/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemorySinkBase/#latestbatchdata-seqrow","text":"| latestBatchId a| [[latestBatchId]]","title":"latestBatchData: Seq[Row]"},{"location":"spark-sql-streaming-MemorySinkBase/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemorySinkBase/#latestbatchid-optionlong","text":"|=== [[implementations]] .MemorySinkBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemorySinkBase | Description | < > | [[MemorySink]] < > for < > (based on Data Source API V1) | < > | [[MemorySinkV2]] < > for < > (based on Data Source API V2) |===","title":"latestBatchId: Option[Long]"},{"location":"spark-sql-streaming-MemorySinkV2/","text":"== [[MemorySinkV2]] MemorySinkV2 -- Writable Streaming Sink for Continuous Stream Processing MemorySinkV2 is a DataSourceV2 with < > for memory data source format in < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceV2.html[DataSourceV2 Contract] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. MemorySinkV2 is a custom < >. [[createStreamWriter]] When requested for a < >, MemorySinkV2 simply creates a < >.","title":"MemorySinkV2"},{"location":"spark-sql-streaming-MemoryStream/","text":"== [[MemoryStream]] MemoryStream -- Streaming Reader for Micro-Batch Stream Processing MemoryStream is a concrete < > of < > that supports < > in < >. [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.MemoryStream logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MemoryStream=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating MemoryStream Instance MemoryStream takes the following to be created: [[id]] ID [[sqlContext]] SQLContext MemoryStream initializes the < >. === [[apply]] Creating MemoryStream Instance -- apply Object Factory [source, scala] \u00b6 apply A : Encoder : MemoryStream[A] apply uses an memoryStreamId internal counter to < > with a unique < > and the implicit SQLContext . === [[addData]] Adding Data to Source -- addData Method [source, scala] \u00b6 addData( data: TraversableOnce[A]): Offset addData adds the given data to the < > internal registry. Internally, addData prints out the following DEBUG message to the logs: Adding: [data] In the end, addData increments the < > and adds the data to the < > internal registry. === [[getBatch]] Generating Next Streaming Batch -- getBatch Method getBatch is a part of the Source abstraction. When executed, getBatch uses the internal < > collection to return requested offsets. You should see the following DEBUG message in the logs: DEBUG MemoryStream: MemoryBatch [[startOrdinal], [endOrdinal]]: [newBlocks] === [[logicalPlan]] Logical Plan -- logicalPlan Internal Property [source, scala] \u00b6 logicalPlan: LogicalPlan \u00b6 NOTE: logicalPlan is part of the < > for the logical query plan of the memory stream. logicalPlan is simply a < > (for this memory source and the < >). MemoryStream uses spark-sql-streaming-StreamingExecutionRelation.md[StreamingExecutionRelation] logical plan to build spark-sql-dataset.md[Datasets] or spark-sql-dataset.md#ofRows[DataFrames] when requested. [source, scala] \u00b6 scala> val ints = MemoryStream[Int] ints: org.apache.spark.sql.execution.streaming.MemoryStream[Int] = MemoryStream[value#13] scala> ints.toDS.queryExecution.logical.isStreaming res14: Boolean = true scala> ints.toDS.queryExecution.logical res15: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = MemoryStream[value#13] === [[schema]] Schema (schema method) MemoryStream works with the data of the spark-sql-schema.md[schema] as described by the spark-sql-Encoder.md[Encoder] (of the Dataset ). === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > to return the following textual representation: MemoryStream[[output]] === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method [source, scala] \u00b6 planInputPartitions(): java.util.List[InputPartition[InternalRow]] \u00b6 NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions ...FIXME planInputPartitions prints out a DEBUG message to the logs with the < > (with the batches after the < >). planInputPartitions ...FIXME === [[generateDebugString]] generateDebugString Internal Method [source, scala] \u00b6 generateDebugString( rows: Seq[UnsafeRow], startOrdinal: Int, endOrdinal: Int): String generateDebugString resolves and binds the < > for the data. In the end, generateDebugString returns the following string: MemoryBatch [[startOrdinal], [endOrdinal]]: [rows] NOTE: generateDebugString is used exclusively when MemoryStream is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | batches a| [[batches]] Batch data ( ListBuffer[Array[UnsafeRow]] ) | currentOffset a| [[currentOffset]] Current < > (as < >) | lastOffsetCommitted a| [[lastOffsetCommitted]] Last committed < > (as < >) | output a| [[output]] Output schema ( Seq[Attribute] ) of the < > Used exclusively for < > |===","title":"MemoryStream"},{"location":"spark-sql-streaming-MemoryStream/#refer-to","text":"=== [[creating-instance]] Creating MemoryStream Instance MemoryStream takes the following to be created: [[id]] ID [[sqlContext]] SQLContext MemoryStream initializes the < >. === [[apply]] Creating MemoryStream Instance -- apply Object Factory","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-MemoryStream/#source-scala","text":"apply A : Encoder : MemoryStream[A] apply uses an memoryStreamId internal counter to < > with a unique < > and the implicit SQLContext . === [[addData]] Adding Data to Source -- addData Method","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStream/#source-scala_1","text":"addData( data: TraversableOnce[A]): Offset addData adds the given data to the < > internal registry. Internally, addData prints out the following DEBUG message to the logs: Adding: [data] In the end, addData increments the < > and adds the data to the < > internal registry. === [[getBatch]] Generating Next Streaming Batch -- getBatch Method getBatch is a part of the Source abstraction. When executed, getBatch uses the internal < > collection to return requested offsets. You should see the following DEBUG message in the logs: DEBUG MemoryStream: MemoryBatch [[startOrdinal], [endOrdinal]]: [newBlocks] === [[logicalPlan]] Logical Plan -- logicalPlan Internal Property","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStream/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStream/#logicalplan-logicalplan","text":"NOTE: logicalPlan is part of the < > for the logical query plan of the memory stream. logicalPlan is simply a < > (for this memory source and the < >). MemoryStream uses spark-sql-streaming-StreamingExecutionRelation.md[StreamingExecutionRelation] logical plan to build spark-sql-dataset.md[Datasets] or spark-sql-dataset.md#ofRows[DataFrames] when requested.","title":"logicalPlan: LogicalPlan"},{"location":"spark-sql-streaming-MemoryStream/#source-scala_3","text":"scala> val ints = MemoryStream[Int] ints: org.apache.spark.sql.execution.streaming.MemoryStream[Int] = MemoryStream[value#13] scala> ints.toDS.queryExecution.logical.isStreaming res14: Boolean = true scala> ints.toDS.queryExecution.logical res15: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan = MemoryStream[value#13] === [[schema]] Schema (schema method) MemoryStream works with the data of the spark-sql-schema.md[schema] as described by the spark-sql-Encoder.md[Encoder] (of the Dataset ). === [[toString]] Textual Representation -- toString Method","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStream/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStream/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString uses the < > to return the following textual representation: MemoryStream[[output]] === [[planInputPartitions]] Plan Input Partitions -- planInputPartitions Method","title":"toString: String"},{"location":"spark-sql-streaming-MemoryStream/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStream/#planinputpartitions-javautillistinputpartitioninternalrow","text":"NOTE: planInputPartitions is part of the DataSourceReader contract in Spark SQL for the number of InputPartitions to use as RDD partitions (when DataSourceV2ScanExec physical operator is requested for the partitions of the input RDD). planInputPartitions ...FIXME planInputPartitions prints out a DEBUG message to the logs with the < > (with the batches after the < >). planInputPartitions ...FIXME === [[generateDebugString]] generateDebugString Internal Method","title":"planInputPartitions(): java.util.List[InputPartition[InternalRow]]"},{"location":"spark-sql-streaming-MemoryStream/#source-scala_6","text":"generateDebugString( rows: Seq[UnsafeRow], startOrdinal: Int, endOrdinal: Int): String generateDebugString resolves and binds the < > for the data. In the end, generateDebugString returns the following string: MemoryBatch [[startOrdinal], [endOrdinal]]: [rows] NOTE: generateDebugString is used exclusively when MemoryStream is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | batches a| [[batches]] Batch data ( ListBuffer[Array[UnsafeRow]] ) | currentOffset a| [[currentOffset]] Current < > (as < >) | lastOffsetCommitted a| [[lastOffsetCommitted]] Last committed < > (as < >) | output a| [[output]] Output schema ( Seq[Attribute] ) of the < > Used exclusively for < > |===","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStreamBase/","text":"== [[MemoryStreamBase]] MemoryStreamBase Contract -- Base Contract for Memory Sources MemoryStreamBase is the < > of the < > for < > that can < >. [[contract]] .MemoryStreamBase Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | addData a| [[addData]] [source, scala] \u00b6 addData( data: TraversableOnce[A]): Offset | logicalPlan a| [[logicalPlan]] [source, scala] \u00b6 logicalPlan: LogicalPlan \u00b6 |=== [[implementations]] .MemoryStreamBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemoryStreamBase | Description | < > | [[ContinuousMemoryStream]] | < > | [[MemoryStream]] < > for < > |=== === [[creating-instance]] Creating MemoryStreamBase Instance MemoryStreamBase takes the following to be created: [[sqlContext]] SQLContext NOTE: MemoryStreamBase is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. === [[toDS]] Creating Streaming Dataset -- toDS Method [source, scala] \u00b6 toDS(): Dataset[A] \u00b6 toDS simply creates a Dataset (for the < > and the < >) === [[toDF]] Creating Streaming DataFrame -- toDF Method [source, scala] \u00b6 toDF(): DataFrame \u00b6 toDF simply creates a Dataset of rows (for the < > and the < >) === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | attributes a| [[attributes]] Schema attributes of the < > ( Seq[AttributeReference] ) Used when...FIXME | encoder a| [[encoder]] Spark SQL's ExpressionEncoder for the data Used when...FIXME |===","title":"MemoryStreamBase"},{"location":"spark-sql-streaming-MemoryStreamBase/#source-scala","text":"addData( data: TraversableOnce[A]): Offset | logicalPlan a| [[logicalPlan]]","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStreamBase/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStreamBase/#logicalplan-logicalplan","text":"|=== [[implementations]] .MemoryStreamBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MemoryStreamBase | Description | < > | [[ContinuousMemoryStream]] | < > | [[MemoryStream]] < > for < > |=== === [[creating-instance]] Creating MemoryStreamBase Instance MemoryStreamBase takes the following to be created: [[sqlContext]] SQLContext NOTE: MemoryStreamBase is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. === [[toDS]] Creating Streaming Dataset -- toDS Method","title":"logicalPlan: LogicalPlan"},{"location":"spark-sql-streaming-MemoryStreamBase/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStreamBase/#tods-dataseta","text":"toDS simply creates a Dataset (for the < > and the < >) === [[toDF]] Creating Streaming DataFrame -- toDF Method","title":"toDS(): Dataset[A]"},{"location":"spark-sql-streaming-MemoryStreamBase/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MemoryStreamBase/#todf-dataframe","text":"toDF simply creates a Dataset of rows (for the < > and the < >) === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | attributes a| [[attributes]] Schema attributes of the < > ( Seq[AttributeReference] ) Used when...FIXME | encoder a| [[encoder]] Spark SQL's ExpressionEncoder for the data Used when...FIXME |===","title":"toDF(): DataFrame"},{"location":"spark-sql-streaming-MemoryStreamWriter/","text":"== [[MemoryStreamWriter]] MemoryStreamWriter MemoryStreamWriter is...FIXME","title":"MemoryStreamWriter"},{"location":"spark-sql-streaming-MetadataLog/","text":"== [[MetadataLog]] MetadataLog Contract -- Metadata Storage MetadataLog is the < > of < > that can < >, < >, and < > metadata (of type T ). [[contract]] .MetadataLog Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | add a| [[add]] [source, scala] \u00b6 add( batchId: Long, metadata: T): Boolean Persists ( adds ) metadata of a streaming batch Used when: KafkaMicroBatchReader is requested to < > KafkaSource is requested for the < > CompactibleFileStreamLog is requested for the < > and to < > FileStreamSource is requested to < > FileStreamSourceLog is requested to < > ManifestFileCommitProtocol is requested to < > MicroBatchExecution stream execution engine is requested to < > and < > ContinuousExecution stream execution engine is requested to < > and < > RateStreamMicroBatchReader is created ( creationTimeMs ) | get a| [[get]] [source, scala] \u00b6 get( batchId: Long): Option[T] get( startId: Option[Long], endId: Option[Long]): Array[(Long, T)] Retrieves ( gets ) metadata of one or more batches Used when...FIXME | getLatest a| [[getLatest]] [source, scala] \u00b6 getLatest(): Option[(Long, T)] \u00b6 Retrieves the latest-committed metadata (if available) Used when...FIXME | purge a| [[purge]] [source, scala] \u00b6 purge(thresholdBatchId: Long): Unit \u00b6 Used when...FIXME |=== [[implementations]] NOTE: < > is the only direct implementation of the < > in Spark Structured Streaming.","title":"MetadataLog"},{"location":"spark-sql-streaming-MetadataLog/#source-scala","text":"add( batchId: Long, metadata: T): Boolean Persists ( adds ) metadata of a streaming batch Used when: KafkaMicroBatchReader is requested to < > KafkaSource is requested for the < > CompactibleFileStreamLog is requested for the < > and to < > FileStreamSource is requested to < > FileStreamSourceLog is requested to < > ManifestFileCommitProtocol is requested to < > MicroBatchExecution stream execution engine is requested to < > and < > ContinuousExecution stream execution engine is requested to < > and < > RateStreamMicroBatchReader is created ( creationTimeMs ) | get a| [[get]]","title":"[source, scala]"},{"location":"spark-sql-streaming-MetadataLog/#source-scala_1","text":"get( batchId: Long): Option[T] get( startId: Option[Long], endId: Option[Long]): Array[(Long, T)] Retrieves ( gets ) metadata of one or more batches Used when...FIXME | getLatest a| [[getLatest]]","title":"[source, scala]"},{"location":"spark-sql-streaming-MetadataLog/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MetadataLog/#getlatest-optionlong-t","text":"Retrieves the latest-committed metadata (if available) Used when...FIXME | purge a| [[purge]]","title":"getLatest(): Option[(Long, T)]"},{"location":"spark-sql-streaming-MetadataLog/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-MetadataLog/#purgethresholdbatchid-long-unit","text":"Used when...FIXME |=== [[implementations]] NOTE: < > is the only direct implementation of the < > in Spark Structured Streaming.","title":"purge(thresholdBatchId: Long): Unit"},{"location":"spark-sql-streaming-MetadataLogFileIndex/","text":"== [[MetadataLogFileIndex]] MetadataLogFileIndex MetadataLogFileIndex is a PartitioningAwareFileIndex of < > (generated by < >). MetadataLogFileIndex is < > when: DataSource (Spark SQL) is requested to resolve a FileFormat relation ( resolveRelation ) and creates a HadoopFsRelation FileStreamSource is requested to < > [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.MetadataLogFileIndex to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.MetadataLogFileIndex=ALL Refer to < >. \u00b6 === [[creating-instance]] Creating MetadataLogFileIndex Instance MetadataLogFileIndex takes the following to be created: [[sparkSession]] SparkSession [[path]] Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ] [[userSpecifiedSchema]] User-defined schema ( Option[StructType] ) MetadataLogFileIndex initializes the < >. While being created, MetadataLogFileIndex prints out the following INFO message to the logs: Reading streaming file log from [metadataDirectory] === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | metadataDirectory a| [[metadataDirectory]] Metadata directory (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ] of the < > directory under the < >) Used when...FIXME | metadataLog a| [[metadataLog]] < > (with the < >) | allFilesFromLog a| [[allFilesFromLog]] Metadata log files |===","title":"MetadataLogFileIndex"},{"location":"spark-sql-streaming-MetadataLogFileIndex/#refer-to","text":"=== [[creating-instance]] Creating MetadataLogFileIndex Instance MetadataLogFileIndex takes the following to be created: [[sparkSession]] SparkSession [[path]] Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ] [[userSpecifiedSchema]] User-defined schema ( Option[StructType] ) MetadataLogFileIndex initializes the < >. While being created, MetadataLogFileIndex prints out the following INFO message to the logs: Reading streaming file log from [metadataDirectory] === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | metadataDirectory a| [[metadataDirectory]] Metadata directory (Hadoop's https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/Path.html[Path ] of the < > directory under the < >) Used when...FIXME | metadataLog a| [[metadataLog]] < > (with the < >) | allFilesFromLog a| [[allFilesFromLog]] Metadata log files |===","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-MicroBatchReadSupport/","text":"== [[MicroBatchReadSupport]] MicroBatchReadSupport Contract -- Data Sources with MicroBatchReaders MicroBatchReadSupport is the < > of the DataSourceV2 for < > with a < > for < >. [[contract]][[createMicroBatchReader]] MicroBatchReadSupport defines a single createMicroBatchReader method to create a < >. [source, java] \u00b6 MicroBatchReader createMicroBatchReader( Optional schema, String checkpointLocation, DataSourceOptions options) createMicroBatchReader is used when: MicroBatchExecution is requested for the < > (and creates a < > for a < > with a MicroBatchReadSupport data source) DataStreamReader is requested to < > [[implementations]] .MicroBatchReadSupports [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MicroBatchReadSupport | Description | < > | [[KafkaSourceProvider]] Data source provider for kafka format | < > | [[RateStreamProvider]] Data source provider for rate format | < > | [[TextSocketSourceProvider]] Data source provider for socket format |===","title":"MicroBatchReadSupport"},{"location":"spark-sql-streaming-MicroBatchReadSupport/#source-java","text":"MicroBatchReader createMicroBatchReader( Optional schema, String checkpointLocation, DataSourceOptions options) createMicroBatchReader is used when: MicroBatchExecution is requested for the < > (and creates a < > for a < > with a MicroBatchReadSupport data source) DataStreamReader is requested to < > [[implementations]] .MicroBatchReadSupports [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | MicroBatchReadSupport | Description | < > | [[KafkaSourceProvider]] Data source provider for kafka format | < > | [[RateStreamProvider]] Data source provider for rate format | < > | [[TextSocketSourceProvider]] Data source provider for socket format |===","title":"[source, java]"},{"location":"spark-sql-streaming-MicroBatchReader/","text":"== [[MicroBatchReader]] MicroBatchReader Contract -- Data Source Readers in Micro-Batch Stream Processing (Data Source API V2) MicroBatchReader is the < > of Spark SQL's DataSourceReader (and < >) contracts for < > in < >. MicroBatchReader is part of the novel Data Source API V2 in Spark SQL. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-data-source-api-v2.html[Data Source API V2] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[contract]] .MicroBatchReader Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | commit a| [[commit]] [source, java] \u00b6 void commit(Offset end) \u00b6 Used when...FIXME | deserializeOffset a| [[deserializeOffset]] [source, java] \u00b6 Offset deserializeOffset(String json) \u00b6 Deserializes < > (from JSON format) Used when...FIXME | getEndOffset a| [[getEndOffset]] [source, java] \u00b6 Offset getEndOffset() \u00b6 End < > of this reader Used when...FIXME | getStartOffset a| [[getStartOffset]] [source, java] \u00b6 Offset getStartOffset() \u00b6 Start (beginning) < > of this reader Used when...FIXME | setOffsetRange a| [[setOffsetRange]] [source, java] \u00b6 void setOffsetRange( Optional start, Optional end) Sets the desired offset range for input partitions created from this reader (for data scan) Used when...FIXME |=== [[implementations]] .MicroBatchReaders [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | MicroBatchReader | Description | < > | [[KafkaMicroBatchReader]] | < > | [[MemoryStream]] | RateStreamMicroBatchReader | [[RateStreamMicroBatchReader]] | TextSocketMicroBatchReader | [[TextSocketMicroBatchReader]] |===","title":"MicroBatchReader"},{"location":"spark-sql-streaming-MicroBatchReader/#source-java","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-MicroBatchReader/#void-commitoffset-end","text":"Used when...FIXME | deserializeOffset a| [[deserializeOffset]]","title":"void commit(Offset end)"},{"location":"spark-sql-streaming-MicroBatchReader/#source-java_1","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-MicroBatchReader/#offset-deserializeoffsetstring-json","text":"Deserializes < > (from JSON format) Used when...FIXME | getEndOffset a| [[getEndOffset]]","title":"Offset deserializeOffset(String json)"},{"location":"spark-sql-streaming-MicroBatchReader/#source-java_2","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-MicroBatchReader/#offset-getendoffset","text":"End < > of this reader Used when...FIXME | getStartOffset a| [[getStartOffset]]","title":"Offset getEndOffset()"},{"location":"spark-sql-streaming-MicroBatchReader/#source-java_3","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-MicroBatchReader/#offset-getstartoffset","text":"Start (beginning) < > of this reader Used when...FIXME | setOffsetRange a| [[setOffsetRange]]","title":"Offset getStartOffset()"},{"location":"spark-sql-streaming-MicroBatchReader/#source-java_4","text":"void setOffsetRange( Optional start, Optional end) Sets the desired offset range for input partitions created from this reader (for data scan) Used when...FIXME |=== [[implementations]] .MicroBatchReaders [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | MicroBatchReader | Description | < > | [[KafkaMicroBatchReader]] | < > | [[MemoryStream]] | RateStreamMicroBatchReader | [[RateStreamMicroBatchReader]] | TextSocketMicroBatchReader | [[TextSocketMicroBatchReader]] |===","title":"[source, java]"},{"location":"spark-sql-streaming-MicroBatchWriter/","text":"== [[MicroBatchWriter]] MicroBatchWriter -- Data Source Writer in Micro-Batch Stream Processing (Data Source API V2) [[batchId]][[writer]][[creating-instance]][[commit]][[abort]] MicroBatchWriter is a DataSourceWriter (Spark SQL) that uses the given batch ID as the epoch when requested to commit, abort and create a WriterFactory for a given < > in < >. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceWriter.html[DataSourceWriter ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. MicroBatchWriter is part of the novel Data Source API V2 in Spark SQL. MicroBatchWriter is < > exclusively when MicroBatchExecution is requested to < > (with a < > streaming sink).","title":"MicroBatchWriter"},{"location":"spark-sql-streaming-Offset/","text":"== [[Offset]] Offset -- Read Position of Streaming Query Offset is the < > of < > that represent progress of a streaming query in < > format. [[contract]] .Offset Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | json a| [[json]] [source, java] \u00b6 String json() \u00b6 Converts the offset to JSON format (JSON-encoded offset) Used when: MicroBatchExecution stream execution engine is requested to < > and < > (with < > sources) OffsetSeq is requested for the < > OffsetSeqLog is requested to < > ProgressReporter is requested to record trigger offsets ContinuousExecution stream execution engine is requested to < > and < > |=== [[extensions]] .Offsets [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Offset | Description | ContinuousMemoryStreamOffset | [[ContinuousMemoryStreamOffset]] | FileStreamSourceOffset | [[FileStreamSourceOffset]] | < > | [[KafkaSourceOffset]] | LongOffset | [[LongOffset]] | RateStreamOffset | [[RateStreamOffset]] | SerializedOffset | [[SerializedOffset]] JSON-encoded offset that is used when loading an offset from an external storage, e.g. from < > after restart | TextSocketOffset | [[TextSocketOffset]] |===","title":"Offset"},{"location":"spark-sql-streaming-Offset/#source-java","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-Offset/#string-json","text":"Converts the offset to JSON format (JSON-encoded offset) Used when: MicroBatchExecution stream execution engine is requested to < > and < > (with < > sources) OffsetSeq is requested for the < > OffsetSeqLog is requested to < > ProgressReporter is requested to record trigger offsets ContinuousExecution stream execution engine is requested to < > and < > |=== [[extensions]] .Offsets [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Offset | Description | ContinuousMemoryStreamOffset | [[ContinuousMemoryStreamOffset]] | FileStreamSourceOffset | [[FileStreamSourceOffset]] | < > | [[KafkaSourceOffset]] | LongOffset | [[LongOffset]] | RateStreamOffset | [[RateStreamOffset]] | SerializedOffset | [[SerializedOffset]] JSON-encoded offset that is used when loading an offset from an external storage, e.g. from < > after restart | TextSocketOffset | [[TextSocketOffset]] |===","title":"String json()"},{"location":"spark-sql-streaming-OffsetSeq/","text":"== [[OffsetSeq]] OffsetSeq OffsetSeq is the metadata managed by < >. OffsetSeq is < > (possibly using the < > factory methods) when: OffsetSeqLog is requested to < > (retrieve metadata from a persistent storage) StreamProgress is requested to < > (most importantly when MicroBatchExecution stream execution engine is requested to < > to < >) ContinuousExecution stream execution engine is requested to < > and < > === [[creating-instance]] Creating OffsetSeq Instance OffsetSeq takes the following when created: [[offsets]] Collection of optional < > (with None for < >) [[metadata]] Optional < > (default: None ) === [[toStreamProgress]] Converting to StreamProgress -- toStreamProgress Method [source, scala] \u00b6 toStreamProgress( sources: Seq[BaseStreamingSource]): StreamProgress toStreamProgress creates a new < > and adds the streaming sources for which there are new < > available. NOTE: < > is a collection with holes (empty elements) for streaming sources with no new data available. toStreamProgress throws an AssertionError if the number of the input sources does not match the < >: There are [[offsets.size]] sources in the checkpoint offsets and now there are [[sources.size]] sources requested by the query. Cannot continue. [NOTE] \u00b6 toStreamProgress is used when: MicroBatchExecution is requested to < > and < > * ContinuousExecution is requested for < > \u00b6 === [[toString]] Textual Representation -- toString Method [source, scala] \u00b6 toString: String \u00b6 NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString simply converts the < > to JSON (if an offset is available) or - (a dash if an offset is not available for a streaming source at that position). === [[fill]] Creating OffsetSeq Instance -- fill Factory Methods [source, scala] \u00b6 fill( offsets: Offset*): OffsetSeq // <1> fill( metadata: Option[String], offsets: Offset*): OffsetSeq <1> Uses no metadata ( None ) fill simply creates an < > for the given variable sequence of < > and the optional < > (in JSON format). [NOTE] \u00b6 fill is used when: OffsetSeqLog is requested to < > * ContinuousExecution stream execution engine is requested to < > and < > \u00b6","title":"OffsetSeq"},{"location":"spark-sql-streaming-OffsetSeq/#source-scala","text":"toStreamProgress( sources: Seq[BaseStreamingSource]): StreamProgress toStreamProgress creates a new < > and adds the streaming sources for which there are new < > available. NOTE: < > is a collection with holes (empty elements) for streaming sources with no new data available. toStreamProgress throws an AssertionError if the number of the input sources does not match the < >: There are [[offsets.size]] sources in the checkpoint offsets and now there are [[sources.size]] sources requested by the query. Cannot continue.","title":"[source, scala]"},{"location":"spark-sql-streaming-OffsetSeq/#note","text":"toStreamProgress is used when: MicroBatchExecution is requested to < > and < >","title":"[NOTE]"},{"location":"spark-sql-streaming-OffsetSeq/#continuousexecution-is-requested-for","text":"=== [[toString]] Textual Representation -- toString Method","title":"* ContinuousExecution is requested for &lt;&gt;"},{"location":"spark-sql-streaming-OffsetSeq/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OffsetSeq/#tostring-string","text":"NOTE: toString is part of the ++ https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html#toString--++[java.lang.Object ] contract for the string representation of the object. toString simply converts the < > to JSON (if an offset is available) or - (a dash if an offset is not available for a streaming source at that position). === [[fill]] Creating OffsetSeq Instance -- fill Factory Methods","title":"toString: String"},{"location":"spark-sql-streaming-OffsetSeq/#source-scala_2","text":"fill( offsets: Offset*): OffsetSeq // <1> fill( metadata: Option[String], offsets: Offset*): OffsetSeq <1> Uses no metadata ( None ) fill simply creates an < > for the given variable sequence of < > and the optional < > (in JSON format).","title":"[source, scala]"},{"location":"spark-sql-streaming-OffsetSeq/#note_1","text":"fill is used when: OffsetSeqLog is requested to < >","title":"[NOTE]"},{"location":"spark-sql-streaming-OffsetSeq/#continuousexecution-stream-execution-engine-is-requested-to-and","text":"","title":"* ContinuousExecution stream execution engine is requested to &lt;&gt; and &lt;&gt;"},{"location":"spark-sql-streaming-OffsetSeqLog/","text":"OffsetSeqLog \u2014 Hadoop DFS-based Metadata Storage of OffsetSeqs \u00b6 OffsetSeqLog is a < > for < > metadata. [[OffsetSeq]][[offsets]][[metadata]] OffsetSeqLog uses < > for metadata which holds an ordered collection of offsets and optional metadata (as < > for < >). OffsetSeqLog is < > exclusively for the write-ahead log (WAL) of offsets of stream execution engines . [[VERSION]] OffsetSeqLog uses 1 for the version when < > and < > metadata. Creating Instance \u00b6 OffsetSeqLog takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[serialize]] Serializing Metadata (Writing Metadata in Serialized Format) -- serialize Method [source, scala] \u00b6 serialize( offsetSeq: OffsetSeq, out: OutputStream): Unit NOTE: serialize is part of < > to serialize metadata (write metadata in serialized format). serialize firstly writes out the < > prefixed with v on a single line (e.g. v1 ) followed by the < > in JSON format. serialize then writes out the < > in JSON format, one per line. NOTE: No offsets to write in offsetSeq for a streaming source is marked as - (a dash) in the log. $ ls -tr [checkpoint-directory]/offsets 0 1 2 3 4 5 6 $ cat [checkpoint-directory]/offsets/6 v1 {\"batchWatermarkMs\":0,\"batchTimestampMs\":1502872590006,\"conf\":{\"spark.sql.shuffle.partitions\":\"200\",\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\"}} 51 === [[deserialize]] Deserializing Metadata (Reading OffsetSeq from Serialized Format) -- deserialize Method [source, scala] \u00b6 deserialize(in: InputStream): OffsetSeq \u00b6 NOTE: deserialize is part of < > to deserialize metadata (read metadata from serialized format). deserialize firstly parses the < > on the first line. deserialize reads the optional metadata (with an empty line for metadata not available). deserialize creates a < > for every line left. In the end, deserialize creates a < > for the optional metadata and the SerializedOffsets . When there are no lines in the InputStream , deserialize throws an IllegalStateException : Incomplete log file","title":"OffsetSeqLog"},{"location":"spark-sql-streaming-OffsetSeqLog/#offsetseqlog-hadoop-dfs-based-metadata-storage-of-offsetseqs","text":"OffsetSeqLog is a < > for < > metadata. [[OffsetSeq]][[offsets]][[metadata]] OffsetSeqLog uses < > for metadata which holds an ordered collection of offsets and optional metadata (as < > for < >). OffsetSeqLog is < > exclusively for the write-ahead log (WAL) of offsets of stream execution engines . [[VERSION]] OffsetSeqLog uses 1 for the version when < > and < > metadata.","title":"OffsetSeqLog &mdash; Hadoop DFS-based Metadata Storage of OffsetSeqs"},{"location":"spark-sql-streaming-OffsetSeqLog/#creating-instance","text":"OffsetSeqLog takes the following to be created: [[sparkSession]] SparkSession [[path]] Path of the metadata log directory === [[serialize]] Serializing Metadata (Writing Metadata in Serialized Format) -- serialize Method","title":"Creating Instance"},{"location":"spark-sql-streaming-OffsetSeqLog/#source-scala","text":"serialize( offsetSeq: OffsetSeq, out: OutputStream): Unit NOTE: serialize is part of < > to serialize metadata (write metadata in serialized format). serialize firstly writes out the < > prefixed with v on a single line (e.g. v1 ) followed by the < > in JSON format. serialize then writes out the < > in JSON format, one per line. NOTE: No offsets to write in offsetSeq for a streaming source is marked as - (a dash) in the log. $ ls -tr [checkpoint-directory]/offsets 0 1 2 3 4 5 6 $ cat [checkpoint-directory]/offsets/6 v1 {\"batchWatermarkMs\":0,\"batchTimestampMs\":1502872590006,\"conf\":{\"spark.sql.shuffle.partitions\":\"200\",\"spark.sql.streaming.stateStore.providerClass\":\"org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider\"}} 51 === [[deserialize]] Deserializing Metadata (Reading OffsetSeq from Serialized Format) -- deserialize Method","title":"[source, scala]"},{"location":"spark-sql-streaming-OffsetSeqLog/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OffsetSeqLog/#deserializein-inputstream-offsetseq","text":"NOTE: deserialize is part of < > to deserialize metadata (read metadata from serialized format). deserialize firstly parses the < > on the first line. deserialize reads the optional metadata (with an empty line for metadata not available). deserialize creates a < > for every line left. In the end, deserialize creates a < > for the optional metadata and the SerializedOffsets . When there are no lines in the InputStream , deserialize throws an IllegalStateException : Incomplete log file","title":"deserialize(in: InputStream): OffsetSeq"},{"location":"spark-sql-streaming-OffsetSeqMetadata/","text":"== [[OffsetSeqMetadata]] OffsetSeqMetadata -- Metadata of Streaming Batch OffsetSeqMetadata holds the metadata for the current streaming batch: [[batchWatermarkMs]] < > threshold [[batchTimestampMs]] < > (in millis) [[conf]] Streaming configuration with spark.sql.shuffle.partitions and spark-sql-streaming-properties.md#spark.sql.streaming.stateStore.providerClass[spark.sql.streaming.stateStore.providerClass] Spark properties NOTE: OffsetSeqMetadata is used mainly when IncrementalExecution is spark-sql-streaming-IncrementalExecution.md#creating-instance[created]. [[relevantSQLConfs]] OffsetSeqMetadata considers some configuration properties as relevantSQLConfs : < > < > < > < > < > relevantSQLConfs are used when OffsetSeqMetadata is < > and is requested to < >. === [[apply]] Creating OffsetSeqMetadata -- apply Factory Method [source, scala] \u00b6 apply( batchWatermarkMs: Long, batchTimestampMs: Long, sessionConf: RuntimeConfig): OffsetSeqMetadata apply ...FIXME NOTE: apply is used when...FIXME === [[setSessionConf]] setSessionConf Method [source, scala] \u00b6 setSessionConf(metadata: OffsetSeqMetadata, sessionConf: RuntimeConfig): Unit \u00b6 setSessionConf ...FIXME NOTE: setSessionConf is used when...FIXME","title":"OffsetSeqMetadata"},{"location":"spark-sql-streaming-OffsetSeqMetadata/#source-scala","text":"apply( batchWatermarkMs: Long, batchTimestampMs: Long, sessionConf: RuntimeConfig): OffsetSeqMetadata apply ...FIXME NOTE: apply is used when...FIXME === [[setSessionConf]] setSessionConf Method","title":"[source, scala]"},{"location":"spark-sql-streaming-OffsetSeqMetadata/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OffsetSeqMetadata/#setsessionconfmetadata-offsetseqmetadata-sessionconf-runtimeconfig-unit","text":"setSessionConf ...FIXME NOTE: setSessionConf is used when...FIXME","title":"setSessionConf(metadata: OffsetSeqMetadata, sessionConf: RuntimeConfig): Unit"},{"location":"spark-sql-streaming-OneSideHashJoiner/","text":"== [[OneSideHashJoiner]] OneSideHashJoiner OneSideHashJoiner manages join state of one side of a < > (using < >). OneSideHashJoiner is < > exclusively for < > physical operator (when requested to < >). .OneSideHashJoiner and StreamingSymmetricHashJoinExec image::images/OneSideHashJoiner.png[align=\"center\"] StreamingSymmetricHashJoinExec physical operator uses two OneSideHashJoiners per side of the stream-stream join (< > and < > sides). OneSideHashJoiner uses an < > to < >. NOTE: OneSideHashJoiner is a Scala private internal class of < > and so has full access to StreamingSymmetricHashJoinExec properties. === [[creating-instance]] Creating OneSideHashJoiner Instance OneSideHashJoiner takes the following to be created: [[joinSide]] < > [[inputAttributes]] Input attributes ( Seq[Attribute] ) [[joinKeys]] Join keys ( Seq[Expression] ) [[inputIter]] Input rows ( Iterator[InternalRow] ) [[preJoinFilterExpr]] Optional pre-join filter Catalyst expression [[postJoinFilter]] Post-join filter ( (InternalRow) => Boolean ) < > OneSideHashJoiner initializes the < >. === [[joinStateManager]] SymmetricHashJoinStateManager -- joinStateManager Internal Property [source, scala] \u00b6 joinStateManager: SymmetricHashJoinStateManager \u00b6 joinStateManager is a < > that is created for a OneSideHashJoiner (with the < >, the < >, the < >, and the < > of the owning < >). joinStateManager is used when OneSideHashJoiner is requested for the following: < > < > < > < > === [[updatedStateRowsCount]] Number of Updated State Rows -- updatedStateRowsCount Internal Counter updatedStateRowsCount is the number the join keys and associated rows that were persisted as a join state, i.e. how many times < > requested the < > to < > the join key and the input row (to a join state). updatedStateRowsCount is then used (via < > method) for the < > performance metric. updatedStateRowsCount is available via numUpdatedStateRows method. [[numUpdatedStateRows]] [source, scala] numUpdatedStateRows: Long \u00b6 NOTE: numUpdatedStateRows is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (and < >). === [[stateWatermarkPredicate]] Optional Join State Watermark Predicate -- stateWatermarkPredicate Internal Property [source, scala] \u00b6 stateWatermarkPredicate: Option[JoinStateWatermarkPredicate] \u00b6 When < >, OneSideHashJoiner is given a < >. stateWatermarkPredicate is used for the < > (when a < >) and the < > (when a < >) that are both used when OneSideHashJoiner is requested to < >. === [[storeAndJoinWithOtherSide]] storeAndJoinWithOtherSide Method [source, scala] \u00b6 storeAndJoinWithOtherSide( otherSideJoiner: OneSideHashJoiner)( generateJoinedRow: (InternalRow, InternalRow) => JoinedRow): Iterator[InternalRow] storeAndJoinWithOtherSide tries to find the < > among the < >. storeAndJoinWithOtherSide creates a < > (for the watermark attribute and the current < >). [[storeAndJoinWithOtherSide-nonLateRows]] With the watermark attribute found, storeAndJoinWithOtherSide generates a new predicate for the watermark expression and the < > that is then used to filter out ( exclude ) late rows from the < >. Otherwise, the input rows are left unchanged (i.e. no rows are considered late and excluded). [[storeAndJoinWithOtherSide-nonLateRows-flatMap]] For every < > (possibly < >), storeAndJoinWithOtherSide applies the < > predicate and branches off per result (< > or < >). NOTE: storeAndJoinWithOtherSide is used when StreamingSymmetricHashJoinExec physical operator is requested to < >. ==== [[preJoinFilter-true]] preJoinFilter Predicate Positive ( true ) When the < > predicate succeeds on an input row, storeAndJoinWithOtherSide extracts the join key (using the < >) and requests the given OneSideHashJoiner ( otherSideJoiner ) for the < > that is in turn requested for the state values for the extracted join key. The values are then processed ( mapped over ) using the given generateJoinedRow function and then filtered by the < >. storeAndJoinWithOtherSide uses the < > (on the extracted join key) and the < > (on the current input row) to determine whether to request the < > to < > the key and the input row (to a join state). If so, storeAndJoinWithOtherSide increments the < > counter. ==== [[preJoinFilter-false]] preJoinFilter Predicate Negative ( false ) When the < > predicate fails on an input row, storeAndJoinWithOtherSide creates a new Iterator[InternalRow] of joined rows per < > and < >: For < > and LeftOuter , the join row is the current row with the values of the right side all null ( nullRight ) For < > and RightOuter , the join row is the current row with the values of the left side all null ( nullLeft ) For all other combinations, the iterator is simply empty (that will be removed from the output by the outer < >). === [[removeOldState]] Removing Old State -- removeOldState Method [source, scala] \u00b6 removeOldState(): Iterator[UnsafeRowPair] \u00b6 removeOldState branches off per the < >: For < >, removeOldState requests the < > to < > (with the < >) For < >, removeOldState requests the < > to < > (with the < >) For any other predicates, removeOldState returns an empty iterator (no rows to process) NOTE: removeOldState is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[get]] Retrieving Value Rows For Key -- get Method [source, scala] \u00b6 get(key: UnsafeRow): Iterator[UnsafeRow] \u00b6 get simply requests the < > to < >. NOTE: get is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[commitStateAndGetMetrics]] Committing State (Changes) and Requesting Performance Metrics -- commitStateAndGetMetrics Method [source, scala] \u00b6 commitStateAndGetMetrics(): StateStoreMetrics \u00b6 commitStateAndGetMetrics simply requests the < > to < > followed by requesting for the < >. NOTE: commitStateAndGetMetrics is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keyGenerator a| [[keyGenerator]] [source, scala] \u00b6 keyGenerator: UnsafeProjection \u00b6 Function to project ( extract ) join keys from an input row Used when...FIXME | preJoinFilter a| [[preJoinFilter]] [source, scala] \u00b6 preJoinFilter: InternalRow => Boolean \u00b6 Used when...FIXME | stateKeyWatermarkPredicateFunc a| [[stateKeyWatermarkPredicateFunc]] [source, scala] \u00b6 stateKeyWatermarkPredicateFunc: InternalRow => Boolean \u00b6 Predicate for late rows based on the < > Used for the following: < > (and check out whether to < > to the < >) < > | stateValueWatermarkPredicateFunc a| [[stateValueWatermarkPredicateFunc]] [source, scala] \u00b6 stateValueWatermarkPredicateFunc: InternalRow => Boolean \u00b6 Predicate for late rows based on the < > Used for the following: < > (and check out whether to < > to the < >) < > |===","title":"OneSideHashJoiner"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#joinstatemanager-symmetrichashjoinstatemanager","text":"joinStateManager is a < > that is created for a OneSideHashJoiner (with the < >, the < >, the < >, and the < > of the owning < >). joinStateManager is used when OneSideHashJoiner is requested for the following: < > < > < > < > === [[updatedStateRowsCount]] Number of Updated State Rows -- updatedStateRowsCount Internal Counter updatedStateRowsCount is the number the join keys and associated rows that were persisted as a join state, i.e. how many times < > requested the < > to < > the join key and the input row (to a join state). updatedStateRowsCount is then used (via < > method) for the < > performance metric. updatedStateRowsCount is available via numUpdatedStateRows method. [[numUpdatedStateRows]] [source, scala]","title":"joinStateManager: SymmetricHashJoinStateManager"},{"location":"spark-sql-streaming-OneSideHashJoiner/#numupdatedstaterows-long","text":"NOTE: numUpdatedStateRows is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (and < >). === [[stateWatermarkPredicate]] Optional Join State Watermark Predicate -- stateWatermarkPredicate Internal Property","title":"numUpdatedStateRows: Long"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#statewatermarkpredicate-optionjoinstatewatermarkpredicate","text":"When < >, OneSideHashJoiner is given a < >. stateWatermarkPredicate is used for the < > (when a < >) and the < > (when a < >) that are both used when OneSideHashJoiner is requested to < >. === [[storeAndJoinWithOtherSide]] storeAndJoinWithOtherSide Method","title":"stateWatermarkPredicate: Option[JoinStateWatermarkPredicate]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala_2","text":"storeAndJoinWithOtherSide( otherSideJoiner: OneSideHashJoiner)( generateJoinedRow: (InternalRow, InternalRow) => JoinedRow): Iterator[InternalRow] storeAndJoinWithOtherSide tries to find the < > among the < >. storeAndJoinWithOtherSide creates a < > (for the watermark attribute and the current < >). [[storeAndJoinWithOtherSide-nonLateRows]] With the watermark attribute found, storeAndJoinWithOtherSide generates a new predicate for the watermark expression and the < > that is then used to filter out ( exclude ) late rows from the < >. Otherwise, the input rows are left unchanged (i.e. no rows are considered late and excluded). [[storeAndJoinWithOtherSide-nonLateRows-flatMap]] For every < > (possibly < >), storeAndJoinWithOtherSide applies the < > predicate and branches off per result (< > or < >). NOTE: storeAndJoinWithOtherSide is used when StreamingSymmetricHashJoinExec physical operator is requested to < >. ==== [[preJoinFilter-true]] preJoinFilter Predicate Positive ( true ) When the < > predicate succeeds on an input row, storeAndJoinWithOtherSide extracts the join key (using the < >) and requests the given OneSideHashJoiner ( otherSideJoiner ) for the < > that is in turn requested for the state values for the extracted join key. The values are then processed ( mapped over ) using the given generateJoinedRow function and then filtered by the < >. storeAndJoinWithOtherSide uses the < > (on the extracted join key) and the < > (on the current input row) to determine whether to request the < > to < > the key and the input row (to a join state). If so, storeAndJoinWithOtherSide increments the < > counter. ==== [[preJoinFilter-false]] preJoinFilter Predicate Negative ( false ) When the < > predicate fails on an input row, storeAndJoinWithOtherSide creates a new Iterator[InternalRow] of joined rows per < > and < >: For < > and LeftOuter , the join row is the current row with the values of the right side all null ( nullRight ) For < > and RightOuter , the join row is the current row with the values of the left side all null ( nullLeft ) For all other combinations, the iterator is simply empty (that will be removed from the output by the outer < >). === [[removeOldState]] Removing Old State -- removeOldState Method","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#removeoldstate-iteratorunsaferowpair","text":"removeOldState branches off per the < >: For < >, removeOldState requests the < > to < > (with the < >) For < >, removeOldState requests the < > to < > (with the < >) For any other predicates, removeOldState returns an empty iterator (no rows to process) NOTE: removeOldState is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[get]] Retrieving Value Rows For Key -- get Method","title":"removeOldState(): Iterator[UnsafeRowPair]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#getkey-unsaferow-iteratorunsaferow","text":"get simply requests the < > to < >. NOTE: get is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[commitStateAndGetMetrics]] Committing State (Changes) and Requesting Performance Metrics -- commitStateAndGetMetrics Method","title":"get(key: UnsafeRow): Iterator[UnsafeRow]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#commitstateandgetmetrics-statestoremetrics","text":"commitStateAndGetMetrics simply requests the < > to < > followed by requesting for the < >. NOTE: commitStateAndGetMetrics is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keyGenerator a| [[keyGenerator]]","title":"commitStateAndGetMetrics(): StateStoreMetrics"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#keygenerator-unsafeprojection","text":"Function to project ( extract ) join keys from an input row Used when...FIXME | preJoinFilter a| [[preJoinFilter]]","title":"keyGenerator: UnsafeProjection"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#prejoinfilter-internalrow-boolean","text":"Used when...FIXME | stateKeyWatermarkPredicateFunc a| [[stateKeyWatermarkPredicateFunc]]","title":"preJoinFilter: InternalRow =&gt; Boolean"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#statekeywatermarkpredicatefunc-internalrow-boolean","text":"Predicate for late rows based on the < > Used for the following: < > (and check out whether to < > to the < >) < > | stateValueWatermarkPredicateFunc a| [[stateValueWatermarkPredicateFunc]]","title":"stateKeyWatermarkPredicateFunc: InternalRow =&gt; Boolean"},{"location":"spark-sql-streaming-OneSideHashJoiner/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-OneSideHashJoiner/#statevaluewatermarkpredicatefunc-internalrow-boolean","text":"Predicate for late rows based on the < > Used for the following: < > (and check out whether to < > to the < >) < > |===","title":"stateValueWatermarkPredicateFunc: InternalRow =&gt; Boolean"},{"location":"spark-sql-streaming-OutputMode/","text":"OutputMode \u00b6 Output mode ( OutputMode ) of a streaming query describes what data is written to a < >. [[available-output-modes]] There are three available output modes: < > < > < > The output mode is specified on the writing side of a streaming query using DataStreamWriter.outputMode method (by alias or a value of org.apache.spark.sql.streaming.OutputMode object). import org.apache.spark.sql.streaming.OutputMode.Update val inputStream = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .outputMode(Update) // <-- update output mode .start === [[Append]] Append Output Mode Append (alias: append ) is the default output mode that writes \"new\" rows only. In < >, a \"new\" row is when the intermediate state becomes final, i.e. when new events for the grouping key can only be considered late which is when watermark moves past the event time of the key. Append output mode requires that a streaming query defines event-time watermark (using withWatermark operator) on the event time column that is used in aggregation (directly or using window standard function). Required for datasets with FileFormat format (to create spark-sql-streaming-FileStreamSink.md[FileStreamSink]) Append is spark-sql-streaming-UnsupportedOperationChecker.md#multiple-flatMapGroupsWithState[mandatory] when multiple flatMapGroupsWithState operators are used in a structured query. === [[Complete]] Complete Output Mode Complete (alias: complete ) writes all the rows of a Result Table (and corresponds to a traditional batch structured query). Complete mode does not drop old aggregation state and preserves all data in the Result Table. Supported only for < > (as asserted by spark-sql-streaming-UnsupportedOperationChecker.md#checkForStreaming[UnsupportedOperationChecker]). === [[Update]] Update Output Mode Update (alias: update ) writes only the rows that were updated (every time there are updates). For queries that are not < >, Update is equivalent to the < > output mode.","title":"OutputMode"},{"location":"spark-sql-streaming-OutputMode/#outputmode","text":"Output mode ( OutputMode ) of a streaming query describes what data is written to a < >. [[available-output-modes]] There are three available output modes: < > < > < > The output mode is specified on the writing side of a streaming query using DataStreamWriter.outputMode method (by alias or a value of org.apache.spark.sql.streaming.OutputMode object). import org.apache.spark.sql.streaming.OutputMode.Update val inputStream = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .outputMode(Update) // <-- update output mode .start === [[Append]] Append Output Mode Append (alias: append ) is the default output mode that writes \"new\" rows only. In < >, a \"new\" row is when the intermediate state becomes final, i.e. when new events for the grouping key can only be considered late which is when watermark moves past the event time of the key. Append output mode requires that a streaming query defines event-time watermark (using withWatermark operator) on the event time column that is used in aggregation (directly or using window standard function). Required for datasets with FileFormat format (to create spark-sql-streaming-FileStreamSink.md[FileStreamSink]) Append is spark-sql-streaming-UnsupportedOperationChecker.md#multiple-flatMapGroupsWithState[mandatory] when multiple flatMapGroupsWithState operators are used in a structured query. === [[Complete]] Complete Output Mode Complete (alias: complete ) writes all the rows of a Result Table (and corresponds to a traditional batch structured query). Complete mode does not drop old aggregation state and preserves all data in the Result Table. Supported only for < > (as asserted by spark-sql-streaming-UnsupportedOperationChecker.md#checkForStreaming[UnsupportedOperationChecker]). === [[Update]] Update Output Mode Update (alias: update ) writes only the rows that were updated (every time there are updates). For queries that are not < >, Update is equivalent to the < > output mode.","title":"OutputMode"},{"location":"spark-sql-streaming-PartitionOffset/","text":"== [[PartitionOffset]] PartitionOffset PartitionOffset is...FIXME","title":"PartitionOffset"},{"location":"spark-sql-streaming-RateSourceProvider/","text":"RateSourceProvider \u00b6 RateSourceProvider is a StreamSourceProvider for RateStreamSource (that acts as the source for rate format). RateSourceProvider is also a DataSourceRegister . [[shortName]] The short name of the data source is rate . [[sourceSchema]]","title":"RateSourceProvider"},{"location":"spark-sql-streaming-RateSourceProvider/#ratesourceprovider","text":"RateSourceProvider is a StreamSourceProvider for RateStreamSource (that acts as the source for rate format). RateSourceProvider is also a DataSourceRegister . [[shortName]] The short name of the data source is rate . [[sourceSchema]]","title":"RateSourceProvider"},{"location":"spark-sql-streaming-RateStreamContinuousReader/","text":"== [[RateStreamContinuousReader]] RateStreamContinuousReader RateStreamContinuousReader is a < > that...FIXME","title":"RateStreamContinuousReader"},{"location":"spark-sql-streaming-RateStreamMicroBatchReader/","text":"== [[RateStreamMicroBatchReader]] RateStreamMicroBatchReader RateStreamMicroBatchReader is...FIXME","title":"RateStreamMicroBatchReader"},{"location":"spark-sql-streaming-RateStreamProvider/","text":"== [[RateStreamProvider]] RateStreamProvider RateStreamProvider is...FIXME","title":"RateStreamProvider"},{"location":"spark-sql-streaming-RateStreamSource/","text":"RateStreamSource \u00b6 RateStreamSource is a streaming source that generates < > that can be useful for testing and PoCs. RateStreamSource < > for rate format (that is registered by spark-sql-streaming-RateSourceProvider.md[RateSourceProvider]). [source, scala] \u00b6 val rates = spark .readStream .format(\"rate\") // \u2190 use RateStreamSource .option(\"rowsPerSecond\", 1) .load [[options]] .RateStreamSource's Options [cols=\"1m,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default Value | Description | numPartitions | (default parallelism) | [[numPartitions]] Number of partitions to use | rampUpTime | 0 (seconds) | [[rampUpTime]] | rowsPerSecond | 1 | [[rowsPerSecond]] Number of rows to generate per second (has to be greater than 0 ) |=== [[schema]] RateStreamSource uses a predefined schema that cannot be changed. [source, scala] \u00b6 val schema = rates.schema scala> println(schema.treeString) root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) .RateStreamSource's Dataset Schema (in the positional order) [cols=\"1m,1m\",options=\"header\",width=\"100%\"] |=== | Name | Type | timestamp | TimestampType | value | LongType |=== [[internal-registries]] .RateStreamSource's Internal Registries and Counters [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | clock | [[clock]] | lastTimeMs | [[lastTimeMs]] | maxSeconds | [[maxSeconds]] | startTimeMs | [[startTimeMs]] |=== [[logging]] [TIP] ==== Enable INFO or DEBUG logging levels for org.apache.spark.sql.execution.streaming.RateStreamSource to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.RateStreamSource=DEBUG Refer to spark-sql-streaming-logging.md[Logging]. \u00b6 === [[getBatch]] Generating DataFrame for Streaming Batch -- getBatch Method [source, scala] \u00b6 getBatch(start: Option[Offset], end: Offset): DataFrame \u00b6 getBatch is a part of the Source abstraction. Internally, getBatch calculates the seconds to start from and end at (from the input start and end offsets) or assumes 0 . getBatch then calculates the values to generate for the start and end seconds. You should see the following DEBUG message in the logs: DEBUG RateStreamSource: startSeconds: [startSeconds], endSeconds: [endSeconds], rangeStart: [rangeStart], rangeEnd: [rangeEnd] If the start and end ranges are equal, getBatch creates an empty DataFrame (with the < >) and returns. Otherwise, when the ranges are different, getBatch creates a DataFrame using SparkContext.range operator (for the start and end ranges and < > partitions). === [[creating-instance]] Creating RateStreamSource Instance RateStreamSource takes the following when created: [[sqlContext]] SQLContext [[metadataPath]] Path to the metadata [[rowsPerSecond]] Rows per second [[rampUpTimeSeconds]] RampUp time in seconds [[numPartitions]] Number of partitions [[useManualClock]] Flag to whether to use ManualClock ( true ) or SystemClock ( false ) RateStreamSource initializes the < >.","title":"RateStreamSource"},{"location":"spark-sql-streaming-RateStreamSource/#ratestreamsource","text":"RateStreamSource is a streaming source that generates < > that can be useful for testing and PoCs. RateStreamSource < > for rate format (that is registered by spark-sql-streaming-RateSourceProvider.md[RateSourceProvider]).","title":"RateStreamSource"},{"location":"spark-sql-streaming-RateStreamSource/#source-scala","text":"val rates = spark .readStream .format(\"rate\") // \u2190 use RateStreamSource .option(\"rowsPerSecond\", 1) .load [[options]] .RateStreamSource's Options [cols=\"1m,1,2\",options=\"header\",width=\"100%\"] |=== | Name | Default Value | Description | numPartitions | (default parallelism) | [[numPartitions]] Number of partitions to use | rampUpTime | 0 (seconds) | [[rampUpTime]] | rowsPerSecond | 1 | [[rowsPerSecond]] Number of rows to generate per second (has to be greater than 0 ) |=== [[schema]] RateStreamSource uses a predefined schema that cannot be changed.","title":"[source, scala]"},{"location":"spark-sql-streaming-RateStreamSource/#source-scala_1","text":"val schema = rates.schema scala> println(schema.treeString) root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) .RateStreamSource's Dataset Schema (in the positional order) [cols=\"1m,1m\",options=\"header\",width=\"100%\"] |=== | Name | Type | timestamp | TimestampType | value | LongType |=== [[internal-registries]] .RateStreamSource's Internal Registries and Counters [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | clock | [[clock]] | lastTimeMs | [[lastTimeMs]] | maxSeconds | [[maxSeconds]] | startTimeMs | [[startTimeMs]] |=== [[logging]] [TIP] ==== Enable INFO or DEBUG logging levels for org.apache.spark.sql.execution.streaming.RateStreamSource to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.RateStreamSource=DEBUG","title":"[source, scala]"},{"location":"spark-sql-streaming-RateStreamSource/#refer-to-spark-sql-streaming-loggingmdlogging","text":"=== [[getBatch]] Generating DataFrame for Streaming Batch -- getBatch Method","title":"Refer to spark-sql-streaming-logging.md[Logging]."},{"location":"spark-sql-streaming-RateStreamSource/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-RateStreamSource/#getbatchstart-optionoffset-end-offset-dataframe","text":"getBatch is a part of the Source abstraction. Internally, getBatch calculates the seconds to start from and end at (from the input start and end offsets) or assumes 0 . getBatch then calculates the values to generate for the start and end seconds. You should see the following DEBUG message in the logs: DEBUG RateStreamSource: startSeconds: [startSeconds], endSeconds: [endSeconds], rangeStart: [rangeStart], rangeEnd: [rangeEnd] If the start and end ranges are equal, getBatch creates an empty DataFrame (with the < >) and returns. Otherwise, when the ranges are different, getBatch creates a DataFrame using SparkContext.range operator (for the start and end ranges and < > partitions). === [[creating-instance]] Creating RateStreamSource Instance RateStreamSource takes the following when created: [[sqlContext]] SQLContext [[metadataPath]] Path to the metadata [[rowsPerSecond]] Rows per second [[rampUpTimeSeconds]] RampUp time in seconds [[numPartitions]] Number of partitions [[useManualClock]] Flag to whether to use ManualClock ( true ) or SystemClock ( false ) RateStreamSource initializes the < >.","title":"getBatch(start: Option[Offset], end: Offset): DataFrame"},{"location":"spark-sql-streaming-Sink/","text":"== [[Sink]] Sink Contract -- Streaming Sinks for Micro-Batch Stream Processing Sink is the < > of the < > for < > that can < >. Sink is part of Data Source API V1 and used in < > only. [[contract]] .Sink Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | addBatch a| [[addBatch]] [source, scala] \u00b6 addBatch( batchId: Long, data: DataFrame): Unit Adds a batch of data to the sink Used exclusively when < > stream execution engine (< >) is requested to < > while < >. |=== [[implementations]] .Sinks [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Sink | Description | FileStreamSink | [[FileStreamSink]] Used in file-based data sources ( FileFormat ) | < > | [[ForeachBatchSink]] Used for DataStreamWriter.foreachBatch operator | < > | [[KafkaSink]] Used for < > output format | < > | [[MemorySink]] Used for memory output format |===","title":"Sink"},{"location":"spark-sql-streaming-Sink/#source-scala","text":"addBatch( batchId: Long, data: DataFrame): Unit Adds a batch of data to the sink Used exclusively when < > stream execution engine (< >) is requested to < > while < >. |=== [[implementations]] .Sinks [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Sink | Description | FileStreamSink | [[FileStreamSink]] Used in file-based data sources ( FileFormat ) | < > | [[ForeachBatchSink]] Used for DataStreamWriter.foreachBatch operator | < > | [[KafkaSink]] Used for < > output format | < > | [[MemorySink]] Used for memory output format |===","title":"[source, scala]"},{"location":"spark-sql-streaming-SinkFileStatus/","text":"== [[SinkFileStatus]] SinkFileStatus [[creating-instance]] SinkFileStatus represents the status of files of < > (and the type of the metadata of < >): [[path]] Path [[size]] Size [[isDir]] isDir flag [[modificationTime]] Modification time [[blockReplication]] Block replication [[blockSize]] Block size [[action]] Action (either < > or < >) === [[toFileStatus]] toFileStatus Method [source, scala] \u00b6 toFileStatus: FileStatus \u00b6 toFileStatus simply creates a new Hadoop https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileStatus.html[FileStatus ]. NOTE: toFileStatus is used exclusively when MetadataLogFileIndex is < >. === [[apply]] Creating SinkFileStatus Instance -- apply Object Method [source, scala] \u00b6 apply(f: FileStatus): SinkFileStatus \u00b6 apply simply creates a new < > (with < > action). NOTE: apply is used exclusively when ManifestFileCommitProtocol is requested to < >.","title":"SinkFileStatus"},{"location":"spark-sql-streaming-SinkFileStatus/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SinkFileStatus/#tofilestatus-filestatus","text":"toFileStatus simply creates a new Hadoop https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/fs/FileStatus.html[FileStatus ]. NOTE: toFileStatus is used exclusively when MetadataLogFileIndex is < >. === [[apply]] Creating SinkFileStatus Instance -- apply Object Method","title":"toFileStatus: FileStatus"},{"location":"spark-sql-streaming-SinkFileStatus/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SinkFileStatus/#applyf-filestatus-sinkfilestatus","text":"apply simply creates a new < > (with < > action). NOTE: apply is used exclusively when ManifestFileCommitProtocol is requested to < >.","title":"apply(f: FileStatus): SinkFileStatus"},{"location":"spark-sql-streaming-StateManager/","text":"== [[StateManager]] StateManager Contract -- State Managers for Arbitrary Stateful Streaming Aggregation StateManager is the < > of < > that act as middlemen between < > and the FlatMapGroupsWithStateExec physical operator used in Arbitrary Stateful Streaming Aggregation . [[contract]] .StateManager Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | getAllState a| [[getAllState]] [source, scala] \u00b6 getAllState(store: StateStore): Iterator[StateData] \u00b6 Retrieves all state data (for all keys) from the < > Used when InputProcessor is requested to processTimedOutState | getState a| [[getState]] [source, scala] \u00b6 getState( store: StateStore, keyRow: UnsafeRow): StateData Gets the state data for the key from the < > Used exclusively when InputProcessor is requested to processNewData | putState a| [[putState]] [source, scala] \u00b6 putState( store: StateStore, keyRow: UnsafeRow, state: Any, timeoutTimestamp: Long): Unit Persists ( puts ) the state value for the key in the < > Used exclusively when InputProcessor is requested to callFunctionAndUpdateState ( right after all rows have been processed ) | removeState a| [[removeState]] [source, scala] \u00b6 removeState( store: StateStore, keyRow: UnsafeRow): Unit Removes the state for the key from the < > Used exclusively when InputProcessor is requested to callFunctionAndUpdateState ( right after all rows have been processed ) | stateSchema a| [[stateSchema]] [source, scala] \u00b6 stateSchema: StructType \u00b6 State schema NOTE: < > (in < > of the FlatMapGroupsWithStateExec physical operator) stateSchema is used for the schema of state value objects (not state keys as they are described by the grouping attributes instead). Used when: FlatMapGroupsWithStateExec physical operator is executed StateManagerImplBase is requested for the < > |=== [[implementations]] NOTE: < > is the one and only known direct implementation of the < > in Spark Structured Streaming. NOTE: StateManager is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"StateManager"},{"location":"spark-sql-streaming-StateManager/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#getallstatestore-statestore-iteratorstatedata","text":"Retrieves all state data (for all keys) from the < > Used when InputProcessor is requested to processTimedOutState | getState a| [[getState]]","title":"getAllState(store: StateStore): Iterator[StateData]"},{"location":"spark-sql-streaming-StateManager/#source-scala_1","text":"getState( store: StateStore, keyRow: UnsafeRow): StateData Gets the state data for the key from the < > Used exclusively when InputProcessor is requested to processNewData | putState a| [[putState]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#source-scala_2","text":"putState( store: StateStore, keyRow: UnsafeRow, state: Any, timeoutTimestamp: Long): Unit Persists ( puts ) the state value for the key in the < > Used exclusively when InputProcessor is requested to callFunctionAndUpdateState ( right after all rows have been processed ) | removeState a| [[removeState]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#source-scala_3","text":"removeState( store: StateStore, keyRow: UnsafeRow): Unit Removes the state for the key from the < > Used exclusively when InputProcessor is requested to callFunctionAndUpdateState ( right after all rows have been processed ) | stateSchema a| [[stateSchema]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManager/#stateschema-structtype","text":"State schema NOTE: < > (in < > of the FlatMapGroupsWithStateExec physical operator) stateSchema is used for the schema of state value objects (not state keys as they are described by the grouping attributes instead). Used when: FlatMapGroupsWithStateExec physical operator is executed StateManagerImplBase is requested for the < > |=== [[implementations]] NOTE: < > is the one and only known direct implementation of the < > in Spark Structured Streaming. NOTE: StateManager is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"stateSchema: StructType"},{"location":"spark-sql-streaming-StateManagerImplBase/","text":"== [[StateManagerImplBase]] StateManagerImplBase StateManagerImplBase is the < > of the < > for < > of FlatMapGroupsWithStateExec physical operator with the following features: Use Catalyst expressions for < > and < > Use < > when < > with the < > flag on [[contract]] .StateManagerImplBase Contract (Abstract Methods Only) [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | stateDeserializerExpr a| [[stateDeserializerExpr]] [source, scala] \u00b6 stateDeserializerExpr: Expression \u00b6 State deserializer , i.e. a Catalyst expression to deserialize a state object from a row ( UnsafeRow ) Used exclusively for the < > | stateSerializerExprs a| [[stateSerializerExprs]] [source, scala] \u00b6 stateSerializerExprs: Seq[Expression] \u00b6 State serializer , i.e. Catalyst expressions to serialize a state object to a row ( UnsafeRow ) Used exclusively for the < > | timeoutTimestampOrdinalInRow a| [[timeoutTimestampOrdinalInRow]] [source, scala] \u00b6 timeoutTimestampOrdinalInRow: Int \u00b6 Position of the timeout timestamp in a state row Used when StateManagerImplBase is requested to < > and < > |=== [[implementations]] .StateManagerImplBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateManagerImplBase | Description | < > | [[StateManagerImplV1]] Legacy < > | < > | [[StateManagerImplV2]] Default < > |=== === [[creating-instance]][[shouldStoreTimestamp]] Creating StateManagerImplBase Instance StateManagerImplBase takes a single shouldStoreTimestamp flag to be created (that is set when the < > are created). NOTE: StateManagerImplBase is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. StateManagerImplBase initializes the < >. === [[getState]] Getting State Data for Key from StateStore -- getState Method [source, scala] \u00b6 getState( store: StateStore, keyRow: UnsafeRow): StateData NOTE: getState is part of the < > to get the state data for the key from the < >. getState ...FIXME === [[putState]] Persisting State Value for Key in StateStore -- putState Method [source, scala] \u00b6 putState( store: StateStore, key: UnsafeRow, state: Any, timestamp: Long): Unit NOTE: putState is part of the < > to persist ( put ) the state value for the key in the < >. putState ...FIXME === [[removeState]] Removing State for Key from StateStore -- removeState Method [source, scala] \u00b6 removeState( store: StateStore, keyRow: UnsafeRow): Unit NOTE: removeState is part of the < > to remove the state for the key from the < >. removeState ...FIXME === [[getAllState]] Getting All State Data (for All Keys) from StateStore -- getAllState Method [source, scala] \u00b6 getAllState(store: StateStore): Iterator[StateData] \u00b6 NOTE: getAllState is part of the < > to retrieve all state data (for all keys) from the < >. getAllState ...FIXME === [[getStateObject]] getStateObject Internal Method [source, scala] \u00b6 getStateObject(row: UnsafeRow): Any \u00b6 getStateObject ...FIXME NOTE: getStateObject is used when...FIXME === [[getStateRow]] getStateRow Internal Method [source, scala] \u00b6 getStateRow(obj: Any): UnsafeRow \u00b6 getStateRow ...FIXME NOTE: getStateRow is used when...FIXME === [[getTimestamp]] Getting Timeout Timestamp (from State Row) -- getTimestamp Internal Method [source, scala] \u00b6 getTimestamp(stateRow: UnsafeRow): Long \u00b6 getTimestamp ...FIXME NOTE: getTimestamp is used when...FIXME === [[setTimestamp]] Setting Timeout Timestamp (to State Row) -- setTimestamp Internal Method [source, scala] \u00b6 setTimestamp( stateRow: UnsafeRow, timeoutTimestamps: Long): Unit setTimestamp ...FIXME NOTE: setTimestamp is used when...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | stateSerializerFunc a| [[stateSerializerFunc]] State object serializer (of type Any => UnsafeRow ) to serialize a state object (for a per-group state key) to a row ( UnsafeRow ) The serialization expression (incl. the type) is specified as the < > Used exclusively in < > | stateDeserializerFunc a| [[stateDeserializerFunc]] State object deserializer (of type InternalRow => Any ) to deserialize a row (for a per-group state value) to a Scala value The deserialization expression (incl. the type) is specified as the < > Used exclusively in < > | stateDataForGets a| [[stateDataForGets]] Empty StateData to share ( reuse ) between < > calls (to avoid high use of memory with many StateData objects) |===","title":"StateManagerImplBase"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#statedeserializerexpr-expression","text":"State deserializer , i.e. a Catalyst expression to deserialize a state object from a row ( UnsafeRow ) Used exclusively for the < > | stateSerializerExprs a| [[stateSerializerExprs]]","title":"stateDeserializerExpr: Expression"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#stateserializerexprs-seqexpression","text":"State serializer , i.e. Catalyst expressions to serialize a state object to a row ( UnsafeRow ) Used exclusively for the < > | timeoutTimestampOrdinalInRow a| [[timeoutTimestampOrdinalInRow]]","title":"stateSerializerExprs: Seq[Expression]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#timeouttimestampordinalinrow-int","text":"Position of the timeout timestamp in a state row Used when StateManagerImplBase is requested to < > and < > |=== [[implementations]] .StateManagerImplBases [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateManagerImplBase | Description | < > | [[StateManagerImplV1]] Legacy < > | < > | [[StateManagerImplV2]] Default < > |=== === [[creating-instance]][[shouldStoreTimestamp]] Creating StateManagerImplBase Instance StateManagerImplBase takes a single shouldStoreTimestamp flag to be created (that is set when the < > are created). NOTE: StateManagerImplBase is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. StateManagerImplBase initializes the < >. === [[getState]] Getting State Data for Key from StateStore -- getState Method","title":"timeoutTimestampOrdinalInRow: Int"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_3","text":"getState( store: StateStore, keyRow: UnsafeRow): StateData NOTE: getState is part of the < > to get the state data for the key from the < >. getState ...FIXME === [[putState]] Persisting State Value for Key in StateStore -- putState Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_4","text":"putState( store: StateStore, key: UnsafeRow, state: Any, timestamp: Long): Unit NOTE: putState is part of the < > to persist ( put ) the state value for the key in the < >. putState ...FIXME === [[removeState]] Removing State for Key from StateStore -- removeState Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_5","text":"removeState( store: StateStore, keyRow: UnsafeRow): Unit NOTE: removeState is part of the < > to remove the state for the key from the < >. removeState ...FIXME === [[getAllState]] Getting All State Data (for All Keys) from StateStore -- getAllState Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#getallstatestore-statestore-iteratorstatedata","text":"NOTE: getAllState is part of the < > to retrieve all state data (for all keys) from the < >. getAllState ...FIXME === [[getStateObject]] getStateObject Internal Method","title":"getAllState(store: StateStore): Iterator[StateData]"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#getstateobjectrow-unsaferow-any","text":"getStateObject ...FIXME NOTE: getStateObject is used when...FIXME === [[getStateRow]] getStateRow Internal Method","title":"getStateObject(row: UnsafeRow): Any"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#getstaterowobj-any-unsaferow","text":"getStateRow ...FIXME NOTE: getStateRow is used when...FIXME === [[getTimestamp]] Getting Timeout Timestamp (from State Row) -- getTimestamp Internal Method","title":"getStateRow(obj: Any): UnsafeRow"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplBase/#gettimestampstaterow-unsaferow-long","text":"getTimestamp ...FIXME NOTE: getTimestamp is used when...FIXME === [[setTimestamp]] Setting Timeout Timestamp (to State Row) -- setTimestamp Internal Method","title":"getTimestamp(stateRow: UnsafeRow): Long"},{"location":"spark-sql-streaming-StateManagerImplBase/#source-scala_10","text":"setTimestamp( stateRow: UnsafeRow, timeoutTimestamps: Long): Unit setTimestamp ...FIXME NOTE: setTimestamp is used when...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | stateSerializerFunc a| [[stateSerializerFunc]] State object serializer (of type Any => UnsafeRow ) to serialize a state object (for a per-group state key) to a row ( UnsafeRow ) The serialization expression (incl. the type) is specified as the < > Used exclusively in < > | stateDeserializerFunc a| [[stateDeserializerFunc]] State object deserializer (of type InternalRow => Any ) to deserialize a row (for a per-group state value) to a Scala value The deserialization expression (incl. the type) is specified as the < > Used exclusively in < > | stateDataForGets a| [[stateDataForGets]] Empty StateData to share ( reuse ) between < > calls (to avoid high use of memory with many StateData objects) |===","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplV1/","text":"== [[StateManagerImplV1]] StateManagerImplV1 StateManagerImplV1 is...FIXME","title":"StateManagerImplV1"},{"location":"spark-sql-streaming-StateManagerImplV2/","text":"== [[StateManagerImplV2]] StateManagerImplV2 -- Default StateManager of FlatMapGroupsWithStateExec Physical Operator StateManagerImplV2 is a concrete < > (as a < >) that is used by default in FlatMapGroupsWithStateExec physical operator (per < > internal configuration property). StateManagerImplV2 is < > exclusively when FlatMapGroupsWithStateExecHelper utility is requested for a < > (when the stateFormatVersion is 2 ). === [[creating-instance]] Creating StateManagerImplV2 Instance StateManagerImplV2 takes the following to be created: [[stateEncoder]] State encoder ( ExpressionEncoder[Any] ) [[shouldStoreTimestamp]] shouldStoreTimestamp flag StateManagerImplV2 initializes the < >. === [[stateSchema]] State Schema -- stateSchema Value [source, scala] \u00b6 stateSchema: StructType \u00b6 NOTE: stateSchema is part of the < > for the schema of the state. stateSchema ...FIXME === [[stateSerializerExprs]] State Serializer -- stateSerializerExprs Value [source, scala] \u00b6 stateSerializerExprs: Seq[Expression] \u00b6 NOTE: stateSerializerExprs is part of the < > for the state serializer, i.e. Catalyst expressions to serialize a state object to a row ( UnsafeRow ). stateSerializerExprs ...FIXME === [[stateDeserializerExpr]] State Deserializer -- stateDeserializerExpr Value [source, scala] \u00b6 stateDeserializerExpr: Expression \u00b6 NOTE: stateDeserializerExpr is part of the < > for the state deserializer, i.e. a Catalyst expression to deserialize a state object from a row ( UnsafeRow ). stateDeserializerExpr ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | nestedStateOrdinal a| [[nestedStateOrdinal]] Position of the state in a state row ( 0 ) Used when...FIXME | timeoutTimestampOrdinalInRow a| [[timeoutTimestampOrdinalInRow]] Position of the timeout timestamp in a state row ( 1 ) Used when...FIXME |===","title":"StateManagerImplV2"},{"location":"spark-sql-streaming-StateManagerImplV2/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplV2/#stateschema-structtype","text":"NOTE: stateSchema is part of the < > for the schema of the state. stateSchema ...FIXME === [[stateSerializerExprs]] State Serializer -- stateSerializerExprs Value","title":"stateSchema: StructType"},{"location":"spark-sql-streaming-StateManagerImplV2/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplV2/#stateserializerexprs-seqexpression","text":"NOTE: stateSerializerExprs is part of the < > for the state serializer, i.e. Catalyst expressions to serialize a state object to a row ( UnsafeRow ). stateSerializerExprs ...FIXME === [[stateDeserializerExpr]] State Deserializer -- stateDeserializerExpr Value","title":"stateSerializerExprs: Seq[Expression]"},{"location":"spark-sql-streaming-StateManagerImplV2/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateManagerImplV2/#statedeserializerexpr-expression","text":"NOTE: stateDeserializerExpr is part of the < > for the state deserializer, i.e. a Catalyst expression to deserialize a state object from a row ( UnsafeRow ). stateDeserializerExpr ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | nestedStateOrdinal a| [[nestedStateOrdinal]] Position of the state in a state row ( 0 ) Used when...FIXME | timeoutTimestampOrdinalInRow a| [[timeoutTimestampOrdinalInRow]] Position of the timeout timestamp in a state row ( 1 ) Used when...FIXME |===","title":"stateDeserializerExpr: Expression"},{"location":"spark-sql-streaming-StateStore/","text":"StateStore \u2014 Kay-Value Store for Streaming State Data \u00b6 StateStore is the < > of < > for managing state in < > (e.g. for persisting running aggregates in < >). StateStore supports incremental checkpointing in which only the key-value \"Row\" pairs that changed are < > or < > (without touching other key-value pairs). StateStore is identified with the < > (among other properties for identification). [[implementations]] NOTE: < > is the default and only known implementation of the < > in Spark Structured Streaming. [[contract]] .StateStore Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | abort a| [[abort]] [source, scala] \u00b6 abort(): Unit \u00b6 Aborts ( discards ) changes to the state store Used when: StateStoreOps implicit class is requested to < > (when the state store has not been < > for a task that finishes, possibly with an error) StateStoreHandler (of < >) is requested to < > (when the state store has not been < > for a task that finishes, possibly with an error) | commit a| [[commit]] [source, scala] \u00b6 commit(): Long \u00b6 Commits the changes to the state store (and returns the current version) Used when: FlatMapGroupsWithStateExec , < > and < > physical operators are executed (right after all rows in a partition have been processed) StreamingAggregationStateManagerBaseImpl is requested to < > (exclusively when < > physical operator is executed) StateStoreHandler (of < >) is requested to < > | get a| [[get]] [source, scala] \u00b6 get(key: UnsafeRow): UnsafeRow \u00b6 Looks up ( gets ) the value of the given non- null key Used when: < > and < > physical operators are executed StateManagerImplBase (of FlatMapGroupsWithStateExecHelper ) is requested to getState < > and < > are requested to get the value of a non-null key KeyToNumValuesStore is requested to < > KeyWithIndexToValueStore` is requested to < > and < > | getRange a| [[getRange]] [source, scala] \u00b6 getRange( start: Option[UnsafeRow], end: Option[UnsafeRow]): Iterator[UnsafeRowPair] Gets the key-value pairs of UnsafeRows for the specified range (with optional approximate start and end extents) Used when: WatermarkSupport is requested to < > StateManagerImplBase is requested to getAllState StreamingAggregationStateManagerBaseImpl is requested to < > < > and < > are requested to iterator NOTE: All the uses above assume the start and end as None that basically is < >. | hasCommitted a| [[hasCommitted]] [source, scala] \u00b6 hasCommitted: Boolean \u00b6 Flag to indicate whether state changes have been committed ( true ) or not ( false ) Used when: RDD (via StateStoreOps implicit class) is requested to < > (and a task finishes and may need to < >) SymmetricHashJoinStateManager is requested to < > (when a task finishes and may need to < >)) | id a| [[id]] [source, scala] \u00b6 id: StateStoreId \u00b6 The < > of the state store Used when: HDFSBackedStateStore state store is requested for the < > StateStoreHandler (of < >) is requested to < > and < > | iterator a| [[iterator]] [source, scala] \u00b6 iterator(): Iterator[UnsafeRowPair] \u00b6 Returns an iterator with all the kay-value pairs in the state store Used when: < > physical operator is requested to execute < > state store in particular and any < > in general are requested to getRange StreamingAggregationStateManagerImplV1 state manager is requested for the < > and < > StreamingAggregationStateManagerImplV2 state manager is requested to < > and < > | metrics a| [[metrics]] [source, scala] \u00b6 metrics: StateStoreMetrics \u00b6 StateStoreMetrics of the state store Used when: StateStoreWriter stateful physical operator is requested to setStoreMetrics StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to commit and for the metrics | put a| [[put]] [source, scala] \u00b6 put( key: UnsafeRow, value: UnsafeRow): Unit Stores ( puts ) the value for the (non-null) key Used when: < > and < > physical operators are executed StateManagerImplBase is requested to putState < > and < > are requested to store a row in a state store < > and < > are requested to store a new value for a given key | remove a| [[remove]] [source, scala] \u00b6 remove(key: UnsafeRow): Unit \u00b6 Removes the (non-null) key from the state store Used when: Physical operators with WatermarkSupport are requested to < > StateManagerImplBase is requested to removeState StreamingAggregationStateManagerBaseImpl is requested to < > KeyToNumValuesStore is requested to < > KeyWithIndexToValueStore is requested to < > and < > | version a| [[version]] [source, scala] \u00b6 version: Long \u00b6 Version of the state store Used exclusively when HDFSBackedStateStore state store is requested for a < > (that simply the current version incremented) |=== [NOTE] \u00b6 StateStore was introduced in https://github.com/apache/spark/commit/8c826880f5eaa3221c4e9e7d3fece54e821a0b98[[SPARK-13809 ][SQL] State store for streaming aggregations]. Read the motivation and design in https://docs.google.com/document/d/1-ncawFx8JS5Zyfq1HAEGBx56RDet9wfVp_hDM8ZL254/edit[State Store for Streaming Aggregations]. \u00b6 [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.StateStore$ logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.StateStore$=ALL Refer to < >. \u00b6 === [[coordinatorRef]] Creating (and Caching) RPC Endpoint Reference to StateStoreCoordinator for Executors -- coordinatorRef Internal Object Method [source, scala] \u00b6 coordinatorRef: Option[StateStoreCoordinatorRef] \u00b6 coordinatorRef requests the SparkEnv helper object for the current SparkEnv . If the SparkEnv is available and the <<_coordRef, _coordRef>> is not assigned yet, coordinatorRef prints out the following DEBUG message to the logs followed by requesting the StateStoreCoordinatorRef for the < >. Getting StateStoreCoordinatorRef If the SparkEnv is available, coordinatorRef prints out the following INFO message to the logs: Retrieved reference to StateStoreCoordinator: [_coordRef] NOTE: coordinatorRef is used when StateStore helper object is requested to < > (when StateStore object helper is requested to < >) and < > (when StateStore object helper is requested to < >). === [[unload]] Unloading State Store Provider -- unload Method [source, scala] \u00b6 unload(storeProviderId: StateStoreProviderId): Unit \u00b6 unload ...FIXME NOTE: unload is used when StateStore helper object is requested to < > and < >. === [[stop]] stop Object Method [source, scala] \u00b6 stop(): Unit \u00b6 stop ...FIXME NOTE: stop seems only be used in tests. === [[reportActiveStoreInstance]] Announcing New StateStoreProvider -- reportActiveStoreInstance Internal Object Method [source, scala] \u00b6 reportActiveStoreInstance( storeProviderId: StateStoreProviderId): Unit reportActiveStoreInstance takes the current host and executorId (from the BlockManager on the Spark executor) and requests the < > to < >. NOTE: reportActiveStoreInstance uses SparkEnv to access the BlockManager . In the end, reportActiveStoreInstance prints out the following INFO message to the logs: Reported that the loaded instance [storeProviderId] is active NOTE: reportActiveStoreInstance is used exclusively when StateStore utility is requested to < >. === [[MaintenanceTask]] MaintenanceTask Daemon Thread MaintenanceTask is a daemon thread that < >. When an error occurs, MaintenanceTask clears < > internal registry. MaintenanceTask is scheduled on state-store-maintenance-task thread pool that runs periodically every < > (default: 60s ). === [[get-StateStore]] Looking Up StateStore by Provider ID -- get Object Method [source, scala] \u00b6 get( storeProviderId: StateStoreProviderId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], version: Long, storeConf: StateStoreConf, hadoopConf: Configuration): StateStore get finds StateStore for the specified < > and version. NOTE: The version is either the < > (in < >) or the < > (in < >). Internally, get looks up the < > (by storeProviderId ) in the < > internal cache. If unavailable, get uses the StateStoreProvider utility to < >. get will also < > (unless already started) and < >. In the end, get requests the StateStoreProvider to < >. [NOTE] \u00b6 get is used when: StateStoreRDD is requested to < > * StateStoreHandler (of < >) is requested to < > \u00b6 ==== [[startMaintenanceIfNeeded]] Starting Periodic Maintenance Task (Unless Already Started) -- startMaintenanceIfNeeded Internal Object Method [source, scala] \u00b6 startMaintenanceIfNeeded(): Unit \u00b6 startMaintenanceIfNeeded schedules < > to start after and every spark-sql-streaming-properties.md#spark.sql.streaming.stateStore.maintenanceInterval[spark.sql.streaming.stateStore.maintenanceInterval] (defaults to 60s ). NOTE: startMaintenanceIfNeeded does nothing when the maintenance task has already been started and is still running. NOTE: startMaintenanceIfNeeded is used exclusively when StateStore is requested to < >. ==== [[doMaintenance]] Doing State Maintenance of Registered State Store Providers -- doMaintenance Internal Object Method [source, scala] \u00b6 doMaintenance(): Unit \u00b6 Internally, doMaintenance prints the following DEBUG message to the logs: Doing maintenance doMaintenance then requests every spark-sql-streaming-StateStoreProvider.md[StateStoreProvider] (registered in < >) to spark-sql-streaming-StateStoreProvider.md#doMaintenance[do its own internal maintenance] (only when a StateStoreProvider < >). When a StateStoreProvider is < >, doMaintenance < > and prints the following INFO message to the logs: Unloaded [provider] NOTE: doMaintenance is used exclusively in < >. ==== [[verifyIfStoreInstanceActive]] verifyIfStoreInstanceActive Internal Object Method [source, scala] \u00b6 verifyIfStoreInstanceActive(storeProviderId: StateStoreProviderId): Boolean \u00b6 verifyIfStoreInstanceActive ...FIXME NOTE: verifyIfStoreInstanceActive is used exclusively when StateStore helper object is requested to < > (from a running < >). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | loadedProviders | [[loadedProviders]] Loaded providers internal cache, i.e. < > per < > Used in...FIXME | _coordRef | [[_coordRef]] < > (a RpcEndpointRef to < >) Used in...FIXME |===","title":"StateStore"},{"location":"spark-sql-streaming-StateStore/#statestore-kay-value-store-for-streaming-state-data","text":"StateStore is the < > of < > for managing state in < > (e.g. for persisting running aggregates in < >). StateStore supports incremental checkpointing in which only the key-value \"Row\" pairs that changed are < > or < > (without touching other key-value pairs). StateStore is identified with the < > (among other properties for identification). [[implementations]] NOTE: < > is the default and only known implementation of the < > in Spark Structured Streaming. [[contract]] .StateStore Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | abort a| [[abort]]","title":"StateStore &mdash; Kay-Value Store for Streaming State Data"},{"location":"spark-sql-streaming-StateStore/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#abort-unit","text":"Aborts ( discards ) changes to the state store Used when: StateStoreOps implicit class is requested to < > (when the state store has not been < > for a task that finishes, possibly with an error) StateStoreHandler (of < >) is requested to < > (when the state store has not been < > for a task that finishes, possibly with an error) | commit a| [[commit]]","title":"abort(): Unit"},{"location":"spark-sql-streaming-StateStore/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#commit-long","text":"Commits the changes to the state store (and returns the current version) Used when: FlatMapGroupsWithStateExec , < > and < > physical operators are executed (right after all rows in a partition have been processed) StreamingAggregationStateManagerBaseImpl is requested to < > (exclusively when < > physical operator is executed) StateStoreHandler (of < >) is requested to < > | get a| [[get]]","title":"commit(): Long"},{"location":"spark-sql-streaming-StateStore/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#getkey-unsaferow-unsaferow","text":"Looks up ( gets ) the value of the given non- null key Used when: < > and < > physical operators are executed StateManagerImplBase (of FlatMapGroupsWithStateExecHelper ) is requested to getState < > and < > are requested to get the value of a non-null key KeyToNumValuesStore is requested to < > KeyWithIndexToValueStore` is requested to < > and < > | getRange a| [[getRange]]","title":"get(key: UnsafeRow): UnsafeRow"},{"location":"spark-sql-streaming-StateStore/#source-scala_3","text":"getRange( start: Option[UnsafeRow], end: Option[UnsafeRow]): Iterator[UnsafeRowPair] Gets the key-value pairs of UnsafeRows for the specified range (with optional approximate start and end extents) Used when: WatermarkSupport is requested to < > StateManagerImplBase is requested to getAllState StreamingAggregationStateManagerBaseImpl is requested to < > < > and < > are requested to iterator NOTE: All the uses above assume the start and end as None that basically is < >. | hasCommitted a| [[hasCommitted]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#hascommitted-boolean","text":"Flag to indicate whether state changes have been committed ( true ) or not ( false ) Used when: RDD (via StateStoreOps implicit class) is requested to < > (and a task finishes and may need to < >) SymmetricHashJoinStateManager is requested to < > (when a task finishes and may need to < >)) | id a| [[id]]","title":"hasCommitted: Boolean"},{"location":"spark-sql-streaming-StateStore/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#id-statestoreid","text":"The < > of the state store Used when: HDFSBackedStateStore state store is requested for the < > StateStoreHandler (of < >) is requested to < > and < > | iterator a| [[iterator]]","title":"id: StateStoreId"},{"location":"spark-sql-streaming-StateStore/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#iterator-iteratorunsaferowpair","text":"Returns an iterator with all the kay-value pairs in the state store Used when: < > physical operator is requested to execute < > state store in particular and any < > in general are requested to getRange StreamingAggregationStateManagerImplV1 state manager is requested for the < > and < > StreamingAggregationStateManagerImplV2 state manager is requested to < > and < > | metrics a| [[metrics]]","title":"iterator(): Iterator[UnsafeRowPair]"},{"location":"spark-sql-streaming-StateStore/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#metrics-statestoremetrics","text":"StateStoreMetrics of the state store Used when: StateStoreWriter stateful physical operator is requested to setStoreMetrics StateStoreHandler (of SymmetricHashJoinStateManager ) is requested to commit and for the metrics | put a| [[put]]","title":"metrics: StateStoreMetrics"},{"location":"spark-sql-streaming-StateStore/#source-scala_8","text":"put( key: UnsafeRow, value: UnsafeRow): Unit Stores ( puts ) the value for the (non-null) key Used when: < > and < > physical operators are executed StateManagerImplBase is requested to putState < > and < > are requested to store a row in a state store < > and < > are requested to store a new value for a given key | remove a| [[remove]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#removekey-unsaferow-unit","text":"Removes the (non-null) key from the state store Used when: Physical operators with WatermarkSupport are requested to < > StateManagerImplBase is requested to removeState StreamingAggregationStateManagerBaseImpl is requested to < > KeyToNumValuesStore is requested to < > KeyWithIndexToValueStore is requested to < > and < > | version a| [[version]]","title":"remove(key: UnsafeRow): Unit"},{"location":"spark-sql-streaming-StateStore/#source-scala_10","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#version-long","text":"Version of the state store Used exclusively when HDFSBackedStateStore state store is requested for a < > (that simply the current version incremented) |===","title":"version: Long"},{"location":"spark-sql-streaming-StateStore/#note","text":"StateStore was introduced in https://github.com/apache/spark/commit/8c826880f5eaa3221c4e9e7d3fece54e821a0b98[[SPARK-13809 ][SQL] State store for streaming aggregations].","title":"[NOTE]"},{"location":"spark-sql-streaming-StateStore/#read-the-motivation-and-design-in-httpsdocsgooglecomdocumentd1-ncawfx8js5zyfq1haegbx56rdet9wfvp_hdm8zl254editstate-store-for-streaming-aggregations","text":"[[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.StateStore$ logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.StateStore$=ALL","title":"Read the motivation and design in https://docs.google.com/document/d/1-ncawFx8JS5Zyfq1HAEGBx56RDet9wfVp_hDM8ZL254/edit[State Store for Streaming Aggregations]."},{"location":"spark-sql-streaming-StateStore/#refer-to","text":"=== [[coordinatorRef]] Creating (and Caching) RPC Endpoint Reference to StateStoreCoordinator for Executors -- coordinatorRef Internal Object Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-StateStore/#source-scala_11","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#coordinatorref-optionstatestorecoordinatorref","text":"coordinatorRef requests the SparkEnv helper object for the current SparkEnv . If the SparkEnv is available and the <<_coordRef, _coordRef>> is not assigned yet, coordinatorRef prints out the following DEBUG message to the logs followed by requesting the StateStoreCoordinatorRef for the < >. Getting StateStoreCoordinatorRef If the SparkEnv is available, coordinatorRef prints out the following INFO message to the logs: Retrieved reference to StateStoreCoordinator: [_coordRef] NOTE: coordinatorRef is used when StateStore helper object is requested to < > (when StateStore object helper is requested to < >) and < > (when StateStore object helper is requested to < >). === [[unload]] Unloading State Store Provider -- unload Method","title":"coordinatorRef: Option[StateStoreCoordinatorRef]"},{"location":"spark-sql-streaming-StateStore/#source-scala_12","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#unloadstoreproviderid-statestoreproviderid-unit","text":"unload ...FIXME NOTE: unload is used when StateStore helper object is requested to < > and < >. === [[stop]] stop Object Method","title":"unload(storeProviderId: StateStoreProviderId): Unit"},{"location":"spark-sql-streaming-StateStore/#source-scala_13","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#stop-unit","text":"stop ...FIXME NOTE: stop seems only be used in tests. === [[reportActiveStoreInstance]] Announcing New StateStoreProvider -- reportActiveStoreInstance Internal Object Method","title":"stop(): Unit"},{"location":"spark-sql-streaming-StateStore/#source-scala_14","text":"reportActiveStoreInstance( storeProviderId: StateStoreProviderId): Unit reportActiveStoreInstance takes the current host and executorId (from the BlockManager on the Spark executor) and requests the < > to < >. NOTE: reportActiveStoreInstance uses SparkEnv to access the BlockManager . In the end, reportActiveStoreInstance prints out the following INFO message to the logs: Reported that the loaded instance [storeProviderId] is active NOTE: reportActiveStoreInstance is used exclusively when StateStore utility is requested to < >. === [[MaintenanceTask]] MaintenanceTask Daemon Thread MaintenanceTask is a daemon thread that < >. When an error occurs, MaintenanceTask clears < > internal registry. MaintenanceTask is scheduled on state-store-maintenance-task thread pool that runs periodically every < > (default: 60s ). === [[get-StateStore]] Looking Up StateStore by Provider ID -- get Object Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#source-scala_15","text":"get( storeProviderId: StateStoreProviderId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], version: Long, storeConf: StateStoreConf, hadoopConf: Configuration): StateStore get finds StateStore for the specified < > and version. NOTE: The version is either the < > (in < >) or the < > (in < >). Internally, get looks up the < > (by storeProviderId ) in the < > internal cache. If unavailable, get uses the StateStoreProvider utility to < >. get will also < > (unless already started) and < >. In the end, get requests the StateStoreProvider to < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#note_1","text":"get is used when: StateStoreRDD is requested to < >","title":"[NOTE]"},{"location":"spark-sql-streaming-StateStore/#statestorehandler-of-is-requested-to","text":"==== [[startMaintenanceIfNeeded]] Starting Periodic Maintenance Task (Unless Already Started) -- startMaintenanceIfNeeded Internal Object Method","title":"* StateStoreHandler (of &lt;&gt;) is requested to &lt;&gt;"},{"location":"spark-sql-streaming-StateStore/#source-scala_16","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#startmaintenanceifneeded-unit","text":"startMaintenanceIfNeeded schedules < > to start after and every spark-sql-streaming-properties.md#spark.sql.streaming.stateStore.maintenanceInterval[spark.sql.streaming.stateStore.maintenanceInterval] (defaults to 60s ). NOTE: startMaintenanceIfNeeded does nothing when the maintenance task has already been started and is still running. NOTE: startMaintenanceIfNeeded is used exclusively when StateStore is requested to < >. ==== [[doMaintenance]] Doing State Maintenance of Registered State Store Providers -- doMaintenance Internal Object Method","title":"startMaintenanceIfNeeded(): Unit"},{"location":"spark-sql-streaming-StateStore/#source-scala_17","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#domaintenance-unit","text":"Internally, doMaintenance prints the following DEBUG message to the logs: Doing maintenance doMaintenance then requests every spark-sql-streaming-StateStoreProvider.md[StateStoreProvider] (registered in < >) to spark-sql-streaming-StateStoreProvider.md#doMaintenance[do its own internal maintenance] (only when a StateStoreProvider < >). When a StateStoreProvider is < >, doMaintenance < > and prints the following INFO message to the logs: Unloaded [provider] NOTE: doMaintenance is used exclusively in < >. ==== [[verifyIfStoreInstanceActive]] verifyIfStoreInstanceActive Internal Object Method","title":"doMaintenance(): Unit"},{"location":"spark-sql-streaming-StateStore/#source-scala_18","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStore/#verifyifstoreinstanceactivestoreproviderid-statestoreproviderid-boolean","text":"verifyIfStoreInstanceActive ...FIXME NOTE: verifyIfStoreInstanceActive is used exclusively when StateStore helper object is requested to < > (from a running < >). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | loadedProviders | [[loadedProviders]] Loaded providers internal cache, i.e. < > per < > Used in...FIXME | _coordRef | [[_coordRef]] < > (a RpcEndpointRef to < >) Used in...FIXME |===","title":"verifyIfStoreInstanceActive(storeProviderId: StateStoreProviderId): Boolean"},{"location":"spark-sql-streaming-StateStoreAwareZipPartitionsHelper/","text":"== [[StateStoreAwareZipPartitionsHelper]] StateStoreAwareZipPartitionsHelper -- Extension Methods for Creating StateStoreAwareZipPartitionsRDD [[dataRDD]] StateStoreAwareZipPartitionsHelper is a Scala implicit class of a data RDD (of type RDD[T] ) to < > for < > physical operator. NOTE: http://docs.scala-lang.org/overviews/core/implicit-classes.html[Implicit Classes] are a language feature in Scala for implicit conversions with extension methods for existing types. === [[stateStoreAwareZipPartitions]] Creating StateStoreAwareZipPartitionsRDD -- stateStoreAwareZipPartitions Method [source, scala] \u00b6 stateStoreAwareZipPartitions U: ClassTag, V: ClassTag (f: (Iterator[T], Iterator[U]) => Iterator[V]): RDD[V] stateStoreAwareZipPartitions simply creates a new < >. NOTE: stateStoreAwareZipPartitions is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >.","title":"StateStoreAwareZipPartitionsHelper"},{"location":"spark-sql-streaming-StateStoreAwareZipPartitionsHelper/#source-scala","text":"stateStoreAwareZipPartitions U: ClassTag, V: ClassTag (f: (Iterator[T], Iterator[U]) => Iterator[V]): RDD[V] stateStoreAwareZipPartitions simply creates a new < >. NOTE: stateStoreAwareZipPartitions is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreAwareZipPartitionsRDD/","text":"== [[StateStoreAwareZipPartitionsRDD]] StateStoreAwareZipPartitionsRDD StateStoreAwareZipPartitionsRDD is a ZippedPartitionsRDD2 with the < > and < > parent RDDs. StateStoreAwareZipPartitionsRDD is < > exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (and requests < > for one). === [[creating-instance]] Creating StateStoreAwareZipPartitionsRDD Instance StateStoreAwareZipPartitionsRDD takes the following to be created: [[sc]] SparkContext [[f]] Function ( (Iterator[A], Iterator[B]) => Iterator[V] , e.g. < >) [[rdd1]] Left RDD - the RDD of the left side of a join ( RDD[A] ) [[rdd2]] Right RDD - the RDD of the right side of a join ( RDD[B] ) [[stateInfo]] < > [[stateStoreNames]] Names of the < > [[storeCoordinator]] < > === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method [source, scala] \u00b6 getPreferredLocations(partition: Partition): Seq[String] \u00b6 NOTE: getPreferredLocations is a part of the RDD Contract to specify placement preferences (aka preferred task locations ), i.e. where tasks should be executed to be as close to the data as possible. getPreferredLocations simply requests the < > for < > of every < > (with the < > and the partition ID) and returns unique executor IDs (so that processing a partition happens on the executor with the proper state store for the operator and the partition).","title":"StateStoreAwareZipPartitionsRDD"},{"location":"spark-sql-streaming-StateStoreAwareZipPartitionsRDD/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreAwareZipPartitionsRDD/#getpreferredlocationspartition-partition-seqstring","text":"NOTE: getPreferredLocations is a part of the RDD Contract to specify placement preferences (aka preferred task locations ), i.e. where tasks should be executed to be as close to the data as possible. getPreferredLocations simply requests the < > for < > of every < > (with the < > and the partition ID) and returns unique executor IDs (so that processing a partition happens on the executor with the proper state store for the operator and the partition).","title":"getPreferredLocations(partition: Partition): Seq[String]"},{"location":"spark-sql-streaming-StateStoreConf/","text":"== [[StateStoreConf]] StateStoreConf StateStoreConf is...FIXME [[properties]] .StateStoreConf's Properties [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Name | Configuration Property | minDeltasForSnapshot | [[minDeltasForSnapshot]] < > | maxVersionsToRetainInMemory | [[maxVersionsToRetainInMemory]] < > | minVersionsToRetain | [[minVersionsToRetain]] < > Used exclusively when HDFSBackedStateStoreProvider is requested for < >. | providerClass a| [[providerClass]] < > Used exclusively when StateStoreProvider helper object is requested to < >. |===","title":"StateStoreConf"},{"location":"spark-sql-streaming-StateStoreCoordinator/","text":"== [[StateStoreCoordinator]] StateStoreCoordinator RPC Endpoint -- Tracking Locations of StateStores for StateStoreRDD StateStoreCoordinator keeps track of < > on Spark executors (per host and executor ID). StateStoreCoordinator is used by StateStoreRDD when requested to spark-sql-streaming-StateStoreRDD.md#getPreferredLocations[get the location preferences of partitions] (based on the location of the stores). StateStoreCoordinator is a ThreadSafeRpcEndpoint RPC endpoint that manipulates < > registry through < >. [[messages]] .StateStoreCoordinator RPC Endpoint's Messages and Message Handlers [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Message | Message Handler | DeactivateInstances a| [[DeactivateInstances]] Removes < > of a streaming query (given runId ) Internally, StateStoreCoordinator finds the StateStoreProviderIds of the streaming query per queryRunId and the given runId and removes them from the < > internal registry. StateStoreCoordinator prints out the following DEBUG message to the logs: Deactivating instances related to checkpoint location [runId]: [storeIdsToRemove] | GetLocation a| [[GetLocation]] Gives the location of < > (from < >) with the host and an executor id on that host. You should see the following DEBUG message in the logs: Got location of the state store [id]: [executorId] | ReportActiveInstance a| [[ReportActiveInstance]] One-way asynchronous (fire-and-forget) message to register a new < > on an executor (given host and executorId ). Sent out exclusively when < > RPC endpoint reference is requested to < > (when StateStore utility is requested to < > when the StateStore and a corresponding StateStoreProvider were just created and initialized). Internally, StateStoreCoordinator prints out the following DEBUG message to the logs: Reported state store [id] is active at [executorId] In the end, StateStoreCoordinator adds the StateStoreProviderId to the < > internal registry. | StopCoordinator a| [[StopCoordinator]] Stops StateStoreCoordinator RPC Endpoint You should see the following DEBUG message in the logs: StateStoreCoordinator stopped | VerifyIfInstanceActive a| [[VerifyIfInstanceActive]] Verifies if a given < > is registered (in < >) on executorId You should see the following DEBUG message in the logs: Verified that state store [id] is active: [response] |=== [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator=ALL Refer to < >. \u00b6 === [[instances]] instances Internal Registry [source,scala] \u00b6 instances: HashMap[StateStoreProviderId, ExecutorCacheTaskLocation] \u00b6 instances is an internal registry of < > by their < > and ExecutorCacheTaskLocations (with a host and a executorId ). A new StateStoreProviderId added when StateStoreCoordinator is requested to < > All StateStoreProviderIds of a streaming query are removed when StateStoreCoordinator is requested to < >","title":"StateStoreCoordinator"},{"location":"spark-sql-streaming-StateStoreCoordinator/#refer-to","text":"=== [[instances]] instances Internal Registry","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-StateStoreCoordinator/#sourcescala","text":"","title":"[source,scala]"},{"location":"spark-sql-streaming-StateStoreCoordinator/#instances-hashmapstatestoreproviderid-executorcachetasklocation","text":"instances is an internal registry of < > by their < > and ExecutorCacheTaskLocations (with a host and a executorId ). A new StateStoreProviderId added when StateStoreCoordinator is requested to < > All StateStoreProviderIds of a streaming query are removed when StateStoreCoordinator is requested to < >","title":"instances: HashMap[StateStoreProviderId, ExecutorCacheTaskLocation]"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/","text":"StateStoreCoordinatorRef \u2014 RPC Endpoint Reference to StateStoreCoordinator \u00b6 StateStoreCoordinatorRef is used to (let the tasks on Spark executors to) send < > to the < > (that lives on the driver). [[creating-instance]] [[rpcEndpointRef]] StateStoreCoordinatorRef is given the RpcEndpointRef to the < > RPC endpoint when created. StateStoreCoordinatorRef is < > through StateStoreCoordinatorRef helper object when requested to create one for the < > (when StreamingQueryManager is created) or an < > (when StateStore helper object is requested for the < >). [[messages]] .StateStoreCoordinatorRef's Methods and Underlying RPC Messages [width=\"100%\",cols=\"1m,3\",options=\"header\"] |=== | Method | Description | deactivateInstances a| [[deactivateInstances]] [source, scala] \u00b6 deactivateInstances(runId: UUID): Unit \u00b6 Requests the < > to send a < > synchronous message with the given runId and waits for a true / false response Used exclusively when StreamingQueryManager is requested to handle termination of a streaming query (when StreamExecution is requested to run a streaming query and the query has finished (running streaming batches) ). | getLocation a| [[getLocation]] [source, scala] \u00b6 getLocation( stateStoreProviderId: StateStoreProviderId): Option[String] Requests the < > to send a < > synchronous message with the given < > and waits for the location Used when: StateStoreAwareZipPartitionsRDD is requested for the < > (when StreamingSymmetricHashJoinExec physical operator is requested to < >) StateStoreRDD is requested for < > | reportActiveInstance a| [[reportActiveInstance]] [source, scala] \u00b6 reportActiveInstance( stateStoreProviderId: StateStoreProviderId, host: String, executorId: String): Unit Requests the < > to send a < > one-way asynchronous (fire-and-forget) message with the given < >, host and executorId Used exclusively when StateStore utility is requested for < > (when StateStore utility is requested to < >) | stop a| [[stop]] [source, scala] \u00b6 stop(): Unit \u00b6 Requests the < > to send a < > synchronous message Used exclusively for unit testing | verifyIfInstanceActive a| [[verifyIfInstanceActive]] [source, scala] \u00b6 verifyIfInstanceActive( stateStoreProviderId: StateStoreProviderId, executorId: String): Boolean Requests the < > to send a < > synchronous message with the given < > and executorId , and waits for a true / false response Used exclusively when StateStore helper object is requested for < > (when requested to < > from a running < >) |=== === [[forDriver]] Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Driver -- forDriver Factory Method [source, scala] \u00b6 forDriver(env: SparkEnv): StateStoreCoordinatorRef \u00b6 forDriver ...FIXME forDriver is used when StreamingQueryManager is created . === [[forExecutor]] Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Executor -- forExecutor Factory Method [source, scala] \u00b6 forExecutor(env: SparkEnv): StateStoreCoordinatorRef \u00b6 forExecutor ...FIXME NOTE: forExecutor is used exclusively when StateStore helper object is requested for the < >.","title":"StateStoreCoordinatorRef"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#statestorecoordinatorref-rpc-endpoint-reference-to-statestorecoordinator","text":"StateStoreCoordinatorRef is used to (let the tasks on Spark executors to) send < > to the < > (that lives on the driver). [[creating-instance]] [[rpcEndpointRef]] StateStoreCoordinatorRef is given the RpcEndpointRef to the < > RPC endpoint when created. StateStoreCoordinatorRef is < > through StateStoreCoordinatorRef helper object when requested to create one for the < > (when StreamingQueryManager is created) or an < > (when StateStore helper object is requested for the < >). [[messages]] .StateStoreCoordinatorRef's Methods and Underlying RPC Messages [width=\"100%\",cols=\"1m,3\",options=\"header\"] |=== | Method | Description | deactivateInstances a| [[deactivateInstances]]","title":"StateStoreCoordinatorRef &mdash; RPC Endpoint Reference to StateStoreCoordinator"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#deactivateinstancesrunid-uuid-unit","text":"Requests the < > to send a < > synchronous message with the given runId and waits for a true / false response Used exclusively when StreamingQueryManager is requested to handle termination of a streaming query (when StreamExecution is requested to run a streaming query and the query has finished (running streaming batches) ). | getLocation a| [[getLocation]]","title":"deactivateInstances(runId: UUID): Unit"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#source-scala_1","text":"getLocation( stateStoreProviderId: StateStoreProviderId): Option[String] Requests the < > to send a < > synchronous message with the given < > and waits for the location Used when: StateStoreAwareZipPartitionsRDD is requested for the < > (when StreamingSymmetricHashJoinExec physical operator is requested to < >) StateStoreRDD is requested for < > | reportActiveInstance a| [[reportActiveInstance]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#source-scala_2","text":"reportActiveInstance( stateStoreProviderId: StateStoreProviderId, host: String, executorId: String): Unit Requests the < > to send a < > one-way asynchronous (fire-and-forget) message with the given < >, host and executorId Used exclusively when StateStore utility is requested for < > (when StateStore utility is requested to < >) | stop a| [[stop]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#stop-unit","text":"Requests the < > to send a < > synchronous message Used exclusively for unit testing | verifyIfInstanceActive a| [[verifyIfInstanceActive]]","title":"stop(): Unit"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#source-scala_4","text":"verifyIfInstanceActive( stateStoreProviderId: StateStoreProviderId, executorId: String): Boolean Requests the < > to send a < > synchronous message with the given < > and executorId , and waits for a true / false response Used exclusively when StateStore helper object is requested for < > (when requested to < > from a running < >) |=== === [[forDriver]] Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Driver -- forDriver Factory Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#fordriverenv-sparkenv-statestorecoordinatorref","text":"forDriver ...FIXME forDriver is used when StreamingQueryManager is created . === [[forExecutor]] Creating StateStoreCoordinatorRef to StateStoreCoordinator RPC Endpoint for Executor -- forExecutor Factory Method","title":"forDriver(env: SparkEnv): StateStoreCoordinatorRef"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCoordinatorRef/#forexecutorenv-sparkenv-statestorecoordinatorref","text":"forExecutor ...FIXME NOTE: forExecutor is used exclusively when StateStore helper object is requested for the < >.","title":"forExecutor(env: SparkEnv): StateStoreCoordinatorRef"},{"location":"spark-sql-streaming-StateStoreCustomMetric/","text":"== [[StateStoreCustomMetric]] StateStoreCustomMetric Contract StateStoreCustomMetric is the < > of < > that a state store may wish to expose (as < > or < >). StateStoreCustomMetric is used when: StateStoreProvider is requested for the < > StateStoreMetrics is < > [[contract]] .StateStoreCustomMetric Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | desc a| [[desc]] [source, scala] \u00b6 desc: String \u00b6 Description of the custom metrics | name a| [[name]] [source, scala] \u00b6 name: String \u00b6 Name of the custom metrics |=== [[implementations]] .StateStoreCustomMetrics [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreCustomMetric | Description | StateStoreCustomSizeMetric | [[StateStoreCustomSizeMetric]] | StateStoreCustomSumMetric | [[StateStoreCustomSumMetric]] | StateStoreCustomTimingMetric | [[StateStoreCustomTimingMetric]] |===","title":"StateStoreCustomMetric"},{"location":"spark-sql-streaming-StateStoreCustomMetric/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCustomMetric/#desc-string","text":"Description of the custom metrics | name a| [[name]]","title":"desc: String"},{"location":"spark-sql-streaming-StateStoreCustomMetric/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreCustomMetric/#name-string","text":"Name of the custom metrics |=== [[implementations]] .StateStoreCustomMetrics [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreCustomMetric | Description | StateStoreCustomSizeMetric | [[StateStoreCustomSizeMetric]] | StateStoreCustomSumMetric | [[StateStoreCustomSumMetric]] | StateStoreCustomTimingMetric | [[StateStoreCustomTimingMetric]] |===","title":"name: String"},{"location":"spark-sql-streaming-StateStoreHandler/","text":"== [[StateStoreHandler]] StateStoreHandler Internal Contract StateStoreHandler is the internal < > of < > that manage a < > (i.e. < >, < > and < >). [[stateStoreType]] StateStoreHandler takes a single StateStoreType to be created: [[KeyToNumValuesType]] KeyToNumValuesType for < > (alias: keyToNumValues ) [[KeyWithIndexToValueType]] KeyWithIndexToValueType for < > (alias: keyWithIndexToValue ) NOTE: StateStoreHandler is a Scala private abstract class and cannot be < > directly. It is created indirectly for the < >. [[contract]] .StateStoreHandler Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | stateStore a| [[stateStore]] [source, scala] \u00b6 stateStore: StateStore \u00b6 < > |=== [[extensions]] .StateStoreHandlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StateStoreHandler | Description | < > | [[KeyToNumValuesStore]] StateStoreHandler of < > | < > | [[KeyWithIndexToValueStore]] |=== [[logging]] [TIP] ==== Enable ALL logging levels for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler=ALL Refer to < >. \u00b6 === [[metrics]] Performance Metrics -- metrics Method [source, scala] \u00b6 metrics: StateStoreMetrics \u00b6 metrics simply requests the < > for the < >. NOTE: metrics is used exclusively when SymmetricHashJoinStateManager is requested for the < >. === [[commit]] Committing State (Changes to State Store) -- commit Method [source, scala] \u00b6 commit(): Unit \u00b6 commit ...FIXME NOTE: commit is used when...FIXME === [[abortIfNeeded]] abortIfNeeded Method [source, scala] \u00b6 abortIfNeeded(): Unit \u00b6 abortIfNeeded ...FIXME NOTE: abortIfNeeded is used when...FIXME === [[getStateStore]] Loading State Store (By Key and Value Schemas) -- getStateStore Method [source, scala] \u00b6 getStateStore( keySchema: StructType, valueSchema: StructType): StateStore getStateStore creates a new < > (for the < > of the owning SymmetricHashJoinStateManager , the partition ID from the execution context, and the < > for the < > and < >). getStateStore uses the StateStore utility to < >. In the end, getStateStore prints out the following INFO message to the logs: Loaded store [storeId] NOTE: getStateStore is used when < > and < > state store handlers are created (for < >). === [[StateStoreType]] StateStoreType Contract (Sealed Trait) StateStoreType is required to create a < >. [[StateStoreType-implementations]] .StateStoreTypes [cols=\"1m,1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreType | toString | Description | KeyToNumValuesType | keyToNumValues | [[KeyToNumValuesType]] | KeyWithIndexToValueType | keyWithIndexToValue | [[KeyWithIndexToValueType]] |=== NOTE: StateStoreType is a Scala private sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"StateStoreHandler"},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreHandler/#statestore-statestore","text":"< > |=== [[extensions]] .StateStoreHandlers [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StateStoreHandler | Description | < > | [[KeyToNumValuesStore]] StateStoreHandler of < > | < > | [[KeyWithIndexToValueStore]] |=== [[logging]] [TIP] ==== Enable ALL logging levels for org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.StateStoreHandler=ALL","title":"stateStore: StateStore"},{"location":"spark-sql-streaming-StateStoreHandler/#refer-to","text":"=== [[metrics]] Performance Metrics -- metrics Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreHandler/#metrics-statestoremetrics","text":"metrics simply requests the < > for the < >. NOTE: metrics is used exclusively when SymmetricHashJoinStateManager is requested for the < >. === [[commit]] Committing State (Changes to State Store) -- commit Method","title":"metrics: StateStoreMetrics"},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreHandler/#commit-unit","text":"commit ...FIXME NOTE: commit is used when...FIXME === [[abortIfNeeded]] abortIfNeeded Method","title":"commit(): Unit"},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreHandler/#abortifneeded-unit","text":"abortIfNeeded ...FIXME NOTE: abortIfNeeded is used when...FIXME === [[getStateStore]] Loading State Store (By Key and Value Schemas) -- getStateStore Method","title":"abortIfNeeded(): Unit"},{"location":"spark-sql-streaming-StateStoreHandler/#source-scala_4","text":"getStateStore( keySchema: StructType, valueSchema: StructType): StateStore getStateStore creates a new < > (for the < > of the owning SymmetricHashJoinStateManager , the partition ID from the execution context, and the < > for the < > and < >). getStateStore uses the StateStore utility to < >. In the end, getStateStore prints out the following INFO message to the logs: Loaded store [storeId] NOTE: getStateStore is used when < > and < > state store handlers are created (for < >). === [[StateStoreType]] StateStoreType Contract (Sealed Trait) StateStoreType is required to create a < >. [[StateStoreType-implementations]] .StateStoreTypes [cols=\"1m,1m,2\",options=\"header\",width=\"100%\"] |=== | StateStoreType | toString | Description | KeyToNumValuesType | keyToNumValues | [[KeyToNumValuesType]] | KeyWithIndexToValueType | keyWithIndexToValue | [[KeyWithIndexToValueType]] |=== NOTE: StateStoreType is a Scala private sealed trait which means that all the < > are in the same compilation unit (a single file).","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreId/","text":"== [[StateStoreId]] StateStoreId -- Unique Identifier of State Store [[creating-instance]] StateStoreId is a unique identifier of a < > with the following attributes: [[checkpointRootLocation]] Checkpoint Root Location - the root directory for state checkpointing [[operatorId]] Operator ID - a unique ID of the stateful operator [[partitionId]] Partition ID - the index of the partition [[storeName]] Store Name - the name of the < > (default: < >) StateStoreId is < > when: StateStoreRDD is requested for the < > (executed on the driver) and to < > (later on an executor) StateStoreProviderId helper object is requested to create a < > (with a < > and the run ID of a streaming query) that is then used for the < > of a StateStoreAwareZipPartitionsRDD (executed on the driver) and to...FIXME [[DEFAULT_STORE_NAME]] The name of the default state store (for reading state store data that was generated before store names were used, i.e. in Spark 2.2 and earlier) is default . === [[storeCheckpointLocation]] State Checkpoint Base Directory of Stateful Operator -- storeCheckpointLocation Method [source, scala] \u00b6 storeCheckpointLocation(): Path \u00b6 storeCheckpointLocation is Hadoop DFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the checkpoint location (for the stateful operator by < >, the partition by the < > in the < >). If the < > is used (for Spark 2.2 and earlier), the < > is not included in the path. NOTE: storeCheckpointLocation is used exclusively when HDFSBackedStateStoreProvider is requested for the < >.","title":"StateStoreId"},{"location":"spark-sql-streaming-StateStoreId/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreId/#storecheckpointlocation-path","text":"storeCheckpointLocation is Hadoop DFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path ] of the checkpoint location (for the stateful operator by < >, the partition by the < > in the < >). If the < > is used (for Spark 2.2 and earlier), the < > is not included in the path. NOTE: storeCheckpointLocation is used exclusively when HDFSBackedStateStoreProvider is requested for the < >.","title":"storeCheckpointLocation(): Path"},{"location":"spark-sql-streaming-StateStoreMetrics/","text":"== [[StateStoreMetrics]] StateStoreMetrics [[creating-instance]] StateStoreMetrics holds the performance metrics of a < >: [[numKeys]] Number of keys [[memoryUsedBytes]] Memory used (in bytes) [[customMetrics]] < > with their current values ( Map[StateStoreCustomMetric, Long] ) StateStoreMetrics is used (and < >) when the following are requested for the performance metrics: < > < > < >","title":"StateStoreMetrics"},{"location":"spark-sql-streaming-StateStoreOps/","text":"== [[StateStoreOps]] StateStoreOps -- Extension Methods for Creating StateStoreRDD [[dataRDD]] StateStoreOps is a Scala implicit class of a data RDD (of type RDD[T] ) to < > for the following physical operators: FlatMapGroupsWithStateExec < > < > < > NOTE: http://docs.scala-lang.org/overviews/core/implicit-classes.html[Implicit Classes] are a language feature in Scala for implicit conversions with extension methods for existing types. === [[mapPartitionsWithStateStore]] Creating StateStoreRDD (with storeUpdateFunction Aborting StateStore When Task Fails) -- mapPartitionsWithStateStore Method [source, scala] \u00b6 mapPartitionsWithStateStore U ( storeUpdateFunction: (StateStore, Iterator[T]) => Iterator[U]): StateStoreRDD[T, U] // Used for testing only mapPartitionsWithStateStore U ( storeUpdateFunction: (StateStore, Iterator[T]) => Iterator[U]): StateStoreRDD[T, U] // <1> <1> Uses sqlContext.streams.stateStoreCoordinator to access StateStoreCoordinator Internally, mapPartitionsWithStateStore requests SparkContext to clean storeUpdateFunction function. NOTE: mapPartitionsWithStateStore uses the < > to access the current SparkContext . NOTE: Function Cleaning is to clean a closure from unreferenced variables before it is serialized and sent to tasks. SparkContext reports a SparkException when the closure is not serializable. mapPartitionsWithStateStore then creates a (wrapper) function to spark-sql-streaming-StateStore.md#abort[abort] the StateStore if spark-sql-streaming-StateStore.md#hasCommitted[state updates had not been committed] before a task finished (which is to make sure that the StateStore has been spark-sql-streaming-StateStore.md#commit[committed] or spark-sql-streaming-StateStore.md##abort[aborted] in the end to follow the contract of StateStore ). NOTE: mapPartitionsWithStateStore uses TaskCompletionListener to be notified when a task has finished. In the end, mapPartitionsWithStateStore creates a spark-sql-streaming-StateStoreRDD.md[StateStoreRDD] (with the wrapper function, SessionState and spark-sql-streaming-StateStoreCoordinatorRef.md[StateStoreCoordinatorRef]). [NOTE] \u00b6 mapPartitionsWithStateStore is used when the following physical operators are executed: FlatMapGroupsWithStateExec < > < > < > < > \u00b6","title":"StateStoreOps"},{"location":"spark-sql-streaming-StateStoreOps/#source-scala","text":"mapPartitionsWithStateStore U ( storeUpdateFunction: (StateStore, Iterator[T]) => Iterator[U]): StateStoreRDD[T, U] // Used for testing only mapPartitionsWithStateStore U ( storeUpdateFunction: (StateStore, Iterator[T]) => Iterator[U]): StateStoreRDD[T, U] // <1> <1> Uses sqlContext.streams.stateStoreCoordinator to access StateStoreCoordinator Internally, mapPartitionsWithStateStore requests SparkContext to clean storeUpdateFunction function. NOTE: mapPartitionsWithStateStore uses the < > to access the current SparkContext . NOTE: Function Cleaning is to clean a closure from unreferenced variables before it is serialized and sent to tasks. SparkContext reports a SparkException when the closure is not serializable. mapPartitionsWithStateStore then creates a (wrapper) function to spark-sql-streaming-StateStore.md#abort[abort] the StateStore if spark-sql-streaming-StateStore.md#hasCommitted[state updates had not been committed] before a task finished (which is to make sure that the StateStore has been spark-sql-streaming-StateStore.md#commit[committed] or spark-sql-streaming-StateStore.md##abort[aborted] in the end to follow the contract of StateStore ). NOTE: mapPartitionsWithStateStore uses TaskCompletionListener to be notified when a task has finished. In the end, mapPartitionsWithStateStore creates a spark-sql-streaming-StateStoreRDD.md[StateStoreRDD] (with the wrapper function, SessionState and spark-sql-streaming-StateStoreCoordinatorRef.md[StateStoreCoordinatorRef]).","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreOps/#note","text":"mapPartitionsWithStateStore is used when the following physical operators are executed: FlatMapGroupsWithStateExec < > < > < >","title":"[NOTE]"},{"location":"spark-sql-streaming-StateStoreOps/#_1","text":"","title":"&lt;&gt;"},{"location":"spark-sql-streaming-StateStoreProvider/","text":"StateStoreProvider \u2014 State Store Providers \u00b6 StateStoreProvider is the < > of < > that manage < > in < > (e.g. for persisting running aggregates in < >) in stateful streaming queries. NOTE: StateStoreProvider utility uses < > internal configuration property for the name of the class of the default < >. [[implementations]] NOTE: < > is the default and only known StateStoreProvider in Spark Structured Streaming. [[contract]] .StateStoreProvider Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | close a| [[close]] [source, scala] \u00b6 close(): Unit \u00b6 Closes the state store provider Used exclusively when StateStore helper object is requested to < > | doMaintenance a| [[doMaintenance]] [source, scala] \u00b6 doMaintenance(): Unit = {} \u00b6 Optional state maintenance Used exclusively when StateStore utility is requested to < > (on a separate < >) | getStore a| [[getStore]] [source, scala] \u00b6 getStore( version: Long): StateStore Finds the < > for the specified version Used exclusively when StateStore utility is requested to < > | init a| [[init]] [source, scala] \u00b6 init( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, keyIndexOrdinal: Option[Int], storeConfs: StateStoreConf, hadoopConf: Configuration): Unit Initializes the state store provider Used exclusively when StateStoreProvider helper object is requested to < > for a given < > (when StateStore helper object is requested to < >) | stateStoreId a| [[stateStoreId]] [source, scala] \u00b6 stateStoreId: StateStoreId \u00b6 < > associated with the provider (at < >) Used when: HDFSBackedStateStore is requested for the < > HDFSBackedStateStoreProvider is < > and requested for the < > | supportedCustomMetrics a| [[supportedCustomMetrics]] [source, scala] \u00b6 supportedCustomMetrics: Seq[StateStoreCustomMetric] \u00b6 < > of the state store provider Used when: StateStoreWriter stateful physical operators are requested for the stateStoreCustomMetrics (when requested for the metrics and getProgress ) HDFSBackedStateStore is requested for the < > |=== === [[createAndInit]] Creating and Initializing StateStoreProvider -- createAndInit Object Method [source, scala] \u00b6 createAndInit( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], storeConf: StateStoreConf, hadoopConf: Configuration): StateStoreProvider createAndInit creates a new < > (per < > internal configuration property). createAndInit requests the StateStoreProvider to < >. NOTE: createAndInit is used exclusively when StateStore utility is requested for the < >.","title":"StateStoreProvider"},{"location":"spark-sql-streaming-StateStoreProvider/#statestoreprovider-state-store-providers","text":"StateStoreProvider is the < > of < > that manage < > in < > (e.g. for persisting running aggregates in < >) in stateful streaming queries. NOTE: StateStoreProvider utility uses < > internal configuration property for the name of the class of the default < >. [[implementations]] NOTE: < > is the default and only known StateStoreProvider in Spark Structured Streaming. [[contract]] .StateStoreProvider Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | close a| [[close]]","title":"StateStoreProvider &mdash; State Store Providers"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#close-unit","text":"Closes the state store provider Used exclusively when StateStore helper object is requested to < > | doMaintenance a| [[doMaintenance]]","title":"close(): Unit"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#domaintenance-unit","text":"Optional state maintenance Used exclusively when StateStore utility is requested to < > (on a separate < >) | getStore a| [[getStore]]","title":"doMaintenance(): Unit = {}"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_2","text":"getStore( version: Long): StateStore Finds the < > for the specified version Used exclusively when StateStore utility is requested to < > | init a| [[init]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_3","text":"init( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, keyIndexOrdinal: Option[Int], storeConfs: StateStoreConf, hadoopConf: Configuration): Unit Initializes the state store provider Used exclusively when StateStoreProvider helper object is requested to < > for a given < > (when StateStore helper object is requested to < >) | stateStoreId a| [[stateStoreId]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#statestoreid-statestoreid","text":"< > associated with the provider (at < >) Used when: HDFSBackedStateStore is requested for the < > HDFSBackedStateStoreProvider is < > and requested for the < > | supportedCustomMetrics a| [[supportedCustomMetrics]]","title":"stateStoreId: StateStoreId"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProvider/#supportedcustommetrics-seqstatestorecustommetric","text":"< > of the state store provider Used when: StateStoreWriter stateful physical operators are requested for the stateStoreCustomMetrics (when requested for the metrics and getProgress ) HDFSBackedStateStore is requested for the < > |=== === [[createAndInit]] Creating and Initializing StateStoreProvider -- createAndInit Object Method","title":"supportedCustomMetrics: Seq[StateStoreCustomMetric]"},{"location":"spark-sql-streaming-StateStoreProvider/#source-scala_6","text":"createAndInit( stateStoreId: StateStoreId, keySchema: StructType, valueSchema: StructType, indexOrdinal: Option[Int], storeConf: StateStoreConf, hadoopConf: Configuration): StateStoreProvider createAndInit creates a new < > (per < > internal configuration property). createAndInit requests the StateStoreProvider to < >. NOTE: createAndInit is used exclusively when StateStore utility is requested for the < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProviderId/","text":"== [[StateStoreProviderId]] StateStoreProviderId -- Unique Identifier of State Store Provider [[creating-instance]] StateStoreProviderId is a unique identifier of a < > with the following properties: [[storeId]] < > [[queryRunId]] Run ID of a streaming query ( https://docs.oracle.com/javase/8/docs/api/java/util/UUID.html[java.util.UUID ]) In other words, StateStoreProviderId is a < > with the < > that is different every restart. StateStoreProviderId is used by the following execution components: StateStoreCoordinator to track the < > (on the driver) StateStore object to manage < > (on executors) StateStoreProviderId is < > (directly or using < > factory method) when: StateStoreRDD is requested for the < > and to < > StateStoreAwareZipPartitionsRDD is requested for the < > StateStoreHandler is requested to < > === [[apply]] Creating StateStoreProviderId -- apply Factory Method [source, scala] \u00b6 apply( stateInfo: StatefulOperatorStateInfo, partitionIndex: Int, storeName: String): StateStoreProviderId apply simply creates a < > for the < >, the partition and the store name. Internally, apply requests the StatefulOperatorStateInfo for the < > (aka checkpointLocation ) and the < > and creates a new < > (with the partitionIndex and storeName ). In the end, apply requests the StatefulOperatorStateInfo for the < > and creates a < > (together with the run ID). [NOTE] \u00b6 apply is used when: StateStoreAwareZipPartitionsRDD is requested for the < > * StateStoreHandler is requested to < > \u00b6","title":"StateStoreProviderId"},{"location":"spark-sql-streaming-StateStoreProviderId/#source-scala","text":"apply( stateInfo: StatefulOperatorStateInfo, partitionIndex: Int, storeName: String): StateStoreProviderId apply simply creates a < > for the < >, the partition and the store name. Internally, apply requests the StatefulOperatorStateInfo for the < > (aka checkpointLocation ) and the < > and creates a new < > (with the partitionIndex and storeName ). In the end, apply requests the StatefulOperatorStateInfo for the < > and creates a < > (together with the run ID).","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreProviderId/#note","text":"apply is used when: StateStoreAwareZipPartitionsRDD is requested for the < >","title":"[NOTE]"},{"location":"spark-sql-streaming-StateStoreProviderId/#statestorehandler-is-requested-to","text":"","title":"* StateStoreHandler is requested to &lt;&gt;"},{"location":"spark-sql-streaming-StateStoreRDD/","text":"== [[StateStoreRDD]] StateStoreRDD -- RDD for Updating State (in StateStores Across Spark Cluster) StateStoreRDD is an RDD for < > with spark-sql-streaming-StateStore.md[StateStore] (and data from partitions of the < >). StateStoreRDD is < > for the following stateful physical operators (using < >): FlatMapGroupsWithStateExec < > < > < > < > .StateStoreRDD, Physical and Logical Plans, and operators image::images/StateStoreRDD-SparkPlans-LogicalPlans-operators.png[align=\"center\"] StateStoreRDD uses StateStoreCoordinator for the < > for job scheduling. .StateStoreRDD and StateStoreCoordinator image::images/StateStoreRDD-StateStoreCoordinator.png[align=\"center\"] [[getPartitions]] getPartitions is exactly the partitions of the < >. === [[compute]] Computing Partition -- compute Method [source, scala] \u00b6 compute( partition: Partition, ctxt: TaskContext): Iterator[U] NOTE: compute is part of the RDD Contract to compute a given partition. compute computes < > passing the result on to < > (with a configured spark-sql-streaming-StateStore.md[StateStore]). Internally, (and similarly to < >) compute creates a < > with StateStoreId (using < >, < > and the index of the input partition ) and < >. compute then requests StateStore for spark-sql-streaming-StateStore.md#get[the store for the StateStoreProviderId]. In the end, compute computes < > (using the input partition and ctxt ) followed by executing < > (with the store and the result). === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method [source, scala] \u00b6 getPreferredLocations(partition: Partition): Seq[String] \u00b6 NOTE: getPreferredLocations is a part of the RDD Contract to specify placement preferences (aka preferred task locations ), i.e. where tasks should be executed to be as close to the data as possible. getPreferredLocations creates a < > with StateStoreId (using < >, < > and the index of the input partition ) and < >. NOTE: < > and < > are shared across different partitions and so the only difference in < > is the partition index. In the end, getPreferredLocations requests < > for the spark-sql-streaming-StateStoreCoordinatorRef.md#getLocation[location of the state store] for the < >. NOTE: spark-sql-streaming-StateStoreCoordinator.md[StateStoreCoordinator] coordinates instances of StateStores across Spark executors in the cluster, and tracks their locations for job scheduling. === [[creating-instance]] Creating StateStoreRDD Instance StateStoreRDD takes the following to be created: [[dataRDD]] Data RDD ( RDD[T] to update the aggregates in a state store) [[storeUpdateFunction]] Store update function ( (StateStore, Iterator[T]) => Iterator[U] where T is the type of rows in the < >) [[checkpointLocation]] Checkpoint directory [[queryRunId]] Run ID of the streaming query [[operatorId]] Operator ID [[storeVersion]] Version of the store [[keySchema]] Key schema - schema of the keys [[valueSchema]] Value schema - schema of the values [[indexOrdinal]] Index [[sessionState]] SessionState [[storeCoordinator]] Optional < > StateStoreRDD initializes the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | hadoopConfBroadcast | [[hadoopConfBroadcast]] | storeConf | [[storeConf]] Configuration parameters (as StateStoreConf ) using the current SQLConf (from SessionState ) |===","title":"StateStoreRDD"},{"location":"spark-sql-streaming-StateStoreRDD/#source-scala","text":"compute( partition: Partition, ctxt: TaskContext): Iterator[U] NOTE: compute is part of the RDD Contract to compute a given partition. compute computes < > passing the result on to < > (with a configured spark-sql-streaming-StateStore.md[StateStore]). Internally, (and similarly to < >) compute creates a < > with StateStoreId (using < >, < > and the index of the input partition ) and < >. compute then requests StateStore for spark-sql-streaming-StateStore.md#get[the store for the StateStoreProviderId]. In the end, compute computes < > (using the input partition and ctxt ) followed by executing < > (with the store and the result). === [[getPreferredLocations]] Placement Preferences of Partition (Preferred Locations) -- getPreferredLocations Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreRDD/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StateStoreRDD/#getpreferredlocationspartition-partition-seqstring","text":"NOTE: getPreferredLocations is a part of the RDD Contract to specify placement preferences (aka preferred task locations ), i.e. where tasks should be executed to be as close to the data as possible. getPreferredLocations creates a < > with StateStoreId (using < >, < > and the index of the input partition ) and < >. NOTE: < > and < > are shared across different partitions and so the only difference in < > is the partition index. In the end, getPreferredLocations requests < > for the spark-sql-streaming-StateStoreCoordinatorRef.md#getLocation[location of the state store] for the < >. NOTE: spark-sql-streaming-StateStoreCoordinator.md[StateStoreCoordinator] coordinates instances of StateStores across Spark executors in the cluster, and tracks their locations for job scheduling. === [[creating-instance]] Creating StateStoreRDD Instance StateStoreRDD takes the following to be created: [[dataRDD]] Data RDD ( RDD[T] to update the aggregates in a state store) [[storeUpdateFunction]] Store update function ( (StateStore, Iterator[T]) => Iterator[U] where T is the type of rows in the < >) [[checkpointLocation]] Checkpoint directory [[queryRunId]] Run ID of the streaming query [[operatorId]] Operator ID [[storeVersion]] Version of the store [[keySchema]] Key schema - schema of the keys [[valueSchema]] Value schema - schema of the values [[indexOrdinal]] Index [[sessionState]] SessionState [[storeCoordinator]] Optional < > StateStoreRDD initializes the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | hadoopConfBroadcast | [[hadoopConfBroadcast]] | storeConf | [[storeConf]] Configuration parameters (as StateStoreConf ) using the current SQLConf (from SessionState ) |===","title":"getPreferredLocations(partition: Partition): Seq[String]"},{"location":"spark-sql-streaming-StateStoreUpdater/","text":"== [[StateStoreUpdater]] StateStoreUpdater StateStoreUpdater is...FIXME === [[updateStateForKeysWithData]] updateStateForKeysWithData Method CAUTION: FIXME === [[updateStateForTimedOutKeys]] updateStateForTimedOutKeys Method CAUTION: FIXME","title":"StateStoreUpdater"},{"location":"spark-sql-streaming-StatefulAggregationStrategy/","text":"StatefulAggregationStrategy Execution Planning Strategy \u00b6 StatefulAggregationStrategy is an execution planning strategy that is used to < > with the two logical operators: EventTimeWatermark logical operator ( Dataset.withWatermark operator) Aggregate logical operator (for Dataset.groupBy and Dataset.groupByKey operators, and GROUP BY SQL clause) TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. StatefulAggregationStrategy is used exclusively when < > is requested to plan a streaming query. StatefulAggregationStrategy is available using SessionState . [source, scala] \u00b6 spark.sessionState.planner.StatefulAggregationStrategy \u00b6 [[apply]] [[selection-requirements]] .StatefulAggregationStrategy's Logical to Physical Operator Conversions [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Logical Operator | Physical Operator | EventTimeWatermark.md[EventTimeWatermark] a| [[EventTimeWatermark]] EventTimeWatermarkExec | Aggregate a| [[Aggregate]] In the order of preference: HashAggregateExec ObjectHashAggregateExec SortAggregateExec TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy-Aggregation.html[Aggregation Execution Planning Strategy for Aggregate Physical Operators] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. |=== [source, scala] \u00b6 val counts = spark. readStream. format(\"rate\"). load. groupBy(window($\"timestamp\", \"5 seconds\") as \"group\"). agg(count(\"value\") as \"count\"). orderBy(\"group\") scala> counts.explain == Physical Plan == *Sort [group#6 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#6 ASC NULLS FIRST, 200) +- *HashAggregate(keys=[window#13], functions=[count(value#1L)]) +- StateStoreSave [window#13], StatefulOperatorStateInfo( ,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0), Append, 0 +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)]) +- StateStoreRestore [window#13], StatefulOperatorStateInfo( ,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0) +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)]) +- Exchange hashpartitioning(window#13, 200) +- *HashAggregate(keys=[window#13], functions=[partial_count(value#1L)]) +- *Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13, value#1L] +- *Filter isnotnull(timestamp#0) +- StreamingRelation rate, [timestamp#0, value#1L] import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val consoleOutput = counts. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). queryName(\"counts\"). outputMode(OutputMode.Complete). // \u2190 required for groupBy start // Eventually... consoleOutput.stop === [[planStreamingAggregation]][[AggUtils-planStreamingAggregation]] Selecting Aggregate Physical Operator Given Aggregate Expressions\u2009\u2014\u2009 AggUtils.planStreamingAggregation Internal Method [source, scala] \u00b6 planStreamingAggregation( groupingExpressions: Seq[NamedExpression], functionsWithoutDistinct: Seq[AggregateExpression], resultExpressions: Seq[NamedExpression], child: SparkPlan): Seq[SparkPlan] planStreamingAggregation takes the grouping attributes (from groupingExpressions ). NOTE: groupingExpressions corresponds to the grouping function in groupBy operator. [[partialAggregate]] planStreamingAggregation creates an aggregate physical operator (called partialAggregate ) with: requiredChildDistributionExpressions undefined (i.e. None ) initialInputBufferOffset as 0 functionsWithoutDistinct in Partial mode child operator as the input child [NOTE] \u00b6 planStreamingAggregation creates one of the following aggregate physical operators (in the order of preference): HashAggregateExec ObjectHashAggregateExec SortAggregateExec planStreamingAggregation uses AggUtils.createAggregate method to select an aggregate physical operator that you can read about in https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-SparkStrategy-Aggregation.html#AggUtils-createAggregate[Selecting Aggregate Physical Operator Given Aggregate Expressions -- AggUtils.createAggregate Internal Method] in Mastering Apache Spark 2 gitbook. \u00b6 [[partialMerged1]] planStreamingAggregation creates an aggregate physical operator (called partialMerged1 ) with: requiredChildDistributionExpressions based on the input groupingExpressions initialInputBufferOffset as the length of groupingExpressions functionsWithoutDistinct in PartialMerge mode child operator as < > aggregate physical operator created above [[restored]] planStreamingAggregation creates spark-sql-streaming-StateStoreRestoreExec.md#creating-instance[StateStoreRestoreExec] with the grouping attributes, undefined StatefulOperatorStateInfo , and < > aggregate physical operator created above. [[partialMerged2]] planStreamingAggregation creates an aggregate physical operator (called partialMerged2 ) with: child operator as < > physical operator created above NOTE: The only difference between < > and < > steps is the child physical operator. [[saved]] planStreamingAggregation creates StateStoreSaveExec.md#creating-instance[StateStoreSaveExec] with: the grouping attributes based on the input groupingExpressions No stateInfo , outputMode and eventTimeWatermark child operator as < > aggregate physical operator created above [[finalAndCompleteAggregate]] In the end, planStreamingAggregation creates the final aggregate physical operator (called finalAndCompleteAggregate ) with: requiredChildDistributionExpressions based on the input groupingExpressions initialInputBufferOffset as the length of groupingExpressions functionsWithoutDistinct in Final mode child operator as < > physical operator created above NOTE: planStreamingAggregation is used exclusively when StatefulAggregationStrategy spark-sql-streaming-StatefulAggregationStrategy.md#apply[plans a streaming aggregation].","title":"StatefulAggregationStrategy"},{"location":"spark-sql-streaming-StatefulAggregationStrategy/#statefulaggregationstrategy-execution-planning-strategy","text":"StatefulAggregationStrategy is an execution planning strategy that is used to < > with the two logical operators: EventTimeWatermark logical operator ( Dataset.withWatermark operator) Aggregate logical operator (for Dataset.groupBy and Dataset.groupByKey operators, and GROUP BY SQL clause) TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. StatefulAggregationStrategy is used exclusively when < > is requested to plan a streaming query. StatefulAggregationStrategy is available using SessionState .","title":"StatefulAggregationStrategy Execution Planning Strategy"},{"location":"spark-sql-streaming-StatefulAggregationStrategy/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StatefulAggregationStrategy/#sparksessionstateplannerstatefulaggregationstrategy","text":"[[apply]] [[selection-requirements]] .StatefulAggregationStrategy's Logical to Physical Operator Conversions [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Logical Operator | Physical Operator | EventTimeWatermark.md[EventTimeWatermark] a| [[EventTimeWatermark]] EventTimeWatermarkExec | Aggregate a| [[Aggregate]] In the order of preference: HashAggregateExec ObjectHashAggregateExec SortAggregateExec TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy-Aggregation.html[Aggregation Execution Planning Strategy for Aggregate Physical Operators] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. |===","title":"spark.sessionState.planner.StatefulAggregationStrategy"},{"location":"spark-sql-streaming-StatefulAggregationStrategy/#source-scala_1","text":"val counts = spark. readStream. format(\"rate\"). load. groupBy(window($\"timestamp\", \"5 seconds\") as \"group\"). agg(count(\"value\") as \"count\"). orderBy(\"group\") scala> counts.explain == Physical Plan == *Sort [group#6 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#6 ASC NULLS FIRST, 200) +- *HashAggregate(keys=[window#13], functions=[count(value#1L)]) +- StateStoreSave [window#13], StatefulOperatorStateInfo( ,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0), Append, 0 +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)]) +- StateStoreRestore [window#13], StatefulOperatorStateInfo( ,736d67c2-6daa-4c4c-9c4b-c12b15af20f4,0,0) +- *HashAggregate(keys=[window#13], functions=[merge_count(value#1L)]) +- Exchange hashpartitioning(window#13, 200) +- *HashAggregate(keys=[window#13], functions=[partial_count(value#1L)]) +- *Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#0, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13, value#1L] +- *Filter isnotnull(timestamp#0) +- StreamingRelation rate, [timestamp#0, value#1L] import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val consoleOutput = counts. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). queryName(\"counts\"). outputMode(OutputMode.Complete). // \u2190 required for groupBy start // Eventually... consoleOutput.stop === [[planStreamingAggregation]][[AggUtils-planStreamingAggregation]] Selecting Aggregate Physical Operator Given Aggregate Expressions\u2009\u2014\u2009 AggUtils.planStreamingAggregation Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StatefulAggregationStrategy/#source-scala_2","text":"planStreamingAggregation( groupingExpressions: Seq[NamedExpression], functionsWithoutDistinct: Seq[AggregateExpression], resultExpressions: Seq[NamedExpression], child: SparkPlan): Seq[SparkPlan] planStreamingAggregation takes the grouping attributes (from groupingExpressions ). NOTE: groupingExpressions corresponds to the grouping function in groupBy operator. [[partialAggregate]] planStreamingAggregation creates an aggregate physical operator (called partialAggregate ) with: requiredChildDistributionExpressions undefined (i.e. None ) initialInputBufferOffset as 0 functionsWithoutDistinct in Partial mode child operator as the input child","title":"[source, scala]"},{"location":"spark-sql-streaming-StatefulAggregationStrategy/#note","text":"planStreamingAggregation creates one of the following aggregate physical operators (in the order of preference): HashAggregateExec ObjectHashAggregateExec SortAggregateExec","title":"[NOTE]"},{"location":"spark-sql-streaming-StatefulAggregationStrategy/#planstreamingaggregation-uses-aggutilscreateaggregate-method-to-select-an-aggregate-physical-operator-that-you-can-read-about-in-httpsjaceklaskowskigitbooksiomastering-apache-sparkspark-sql-sparkstrategy-aggregationhtmlaggutils-createaggregateselecting-aggregate-physical-operator-given-aggregate-expressions-aggutilscreateaggregate-internal-method-in-mastering-apache-spark-2-gitbook","text":"[[partialMerged1]] planStreamingAggregation creates an aggregate physical operator (called partialMerged1 ) with: requiredChildDistributionExpressions based on the input groupingExpressions initialInputBufferOffset as the length of groupingExpressions functionsWithoutDistinct in PartialMerge mode child operator as < > aggregate physical operator created above [[restored]] planStreamingAggregation creates spark-sql-streaming-StateStoreRestoreExec.md#creating-instance[StateStoreRestoreExec] with the grouping attributes, undefined StatefulOperatorStateInfo , and < > aggregate physical operator created above. [[partialMerged2]] planStreamingAggregation creates an aggregate physical operator (called partialMerged2 ) with: child operator as < > physical operator created above NOTE: The only difference between < > and < > steps is the child physical operator. [[saved]] planStreamingAggregation creates StateStoreSaveExec.md#creating-instance[StateStoreSaveExec] with: the grouping attributes based on the input groupingExpressions No stateInfo , outputMode and eventTimeWatermark child operator as < > aggregate physical operator created above [[finalAndCompleteAggregate]] In the end, planStreamingAggregation creates the final aggregate physical operator (called finalAndCompleteAggregate ) with: requiredChildDistributionExpressions based on the input groupingExpressions initialInputBufferOffset as the length of groupingExpressions functionsWithoutDistinct in Final mode child operator as < > physical operator created above NOTE: planStreamingAggregation is used exclusively when StatefulAggregationStrategy spark-sql-streaming-StatefulAggregationStrategy.md#apply[plans a streaming aggregation].","title":"planStreamingAggregation uses AggUtils.createAggregate method to select an aggregate physical operator that you can read about in https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-SparkStrategy-Aggregation.html#AggUtils-createAggregate[Selecting Aggregate Physical Operator Given Aggregate Expressions -- AggUtils.createAggregate Internal Method] in Mastering Apache Spark 2 gitbook."},{"location":"spark-sql-streaming-StatefulOperatorStateInfo/","text":"== [[StatefulOperatorStateInfo]] StatefulOperatorStateInfo [[creating-instance]] StatefulOperatorStateInfo identifies the state store for a given stateful physical operator: [[checkpointLocation]] Checkpoint directory ( checkpointLocation ) [[queryRunId]] < > of a streaming query ( queryRunId ) [[operatorId]] Stateful operator ID ( operatorId ) [[storeVersion]] < > ( storeVersion ) [[numPartitions]] Number of partitions StatefulOperatorStateInfo is < > exclusively when IncrementalExecution is requested for < >. [[toString]] When requested for a textual representation ( toString ), StatefulOperatorStateInfo returns the following: state info [ checkpoint = [checkpointLocation], runId = [queryRunId], opId = [operatorId], ver = [storeVersion], numPartitions = [numPartitions]] === [[state-version]] State Version and Batch ID When < > (when IncrementalExecution is requested for the < >), a StatefulOperatorStateInfo is given a < >. The < > is exactly the < > of the < >.","title":"StatefulOperatorStateInfo"},{"location":"spark-sql-streaming-StreamMetadata/","text":"StreamMetadata \u00b6 StreamMetadata is a metadata associated with a < > (indirectly through StreamExecution ). [[creating-instance]] [[id]] StreamMetadata takes an ID to be created. StreamMetadata is < > exclusively when StreamExecution is created (with a randomly-generated 128-bit universally unique identifier (UUID)). StreamMetadata can be < > to and < > from a JSON file. StreamMetadata uses http://json4s.org/[json4s-jackson ] library for JSON persistence. import org.apache.spark.sql.execution.streaming.StreamMetadata import org.apache.hadoop.fs.Path val metadataPath = new Path(\"metadata\") scala> :type spark org.apache.spark.sql.SparkSession val hadoopConf = spark.sessionState.newHadoopConf() val sm = StreamMetadata.read(metadataPath, hadoopConf) scala> :type sm Option[org.apache.spark.sql.execution.streaming.StreamMetadata] === [[read]] Unpersisting StreamMetadata (from JSON File) -- read Object Method [source, scala] \u00b6 read( metadataFile: Path, hadoopConf: Configuration): Option[StreamMetadata] read unpersists StreamMetadata from the given metadataFile file if available. read returns a StreamMetadata if the metadata file was available and the content could be read in JSON format. Otherwise, read returns None . NOTE: read uses org.json4s.jackson.Serialization.read for JSON deserialization. NOTE: read is used exclusively when StreamExecution is created> (and tries to read the metadata checkpoint file). === [[write]] Persisting Metadata -- write Object Method [source, scala] \u00b6 write( metadata: StreamMetadata, metadataFile: Path, hadoopConf: Configuration): Unit write persists the given StreamMetadata to the given metadataFile file in JSON format. NOTE: write uses org.json4s.jackson.Serialization.write for JSON serialization. write is used when StreamExecution is created (and the metadata checkpoint file is not available).","title":"StreamMetadata"},{"location":"spark-sql-streaming-StreamMetadata/#streammetadata","text":"StreamMetadata is a metadata associated with a < > (indirectly through StreamExecution ). [[creating-instance]] [[id]] StreamMetadata takes an ID to be created. StreamMetadata is < > exclusively when StreamExecution is created (with a randomly-generated 128-bit universally unique identifier (UUID)). StreamMetadata can be < > to and < > from a JSON file. StreamMetadata uses http://json4s.org/[json4s-jackson ] library for JSON persistence. import org.apache.spark.sql.execution.streaming.StreamMetadata import org.apache.hadoop.fs.Path val metadataPath = new Path(\"metadata\") scala> :type spark org.apache.spark.sql.SparkSession val hadoopConf = spark.sessionState.newHadoopConf() val sm = StreamMetadata.read(metadataPath, hadoopConf) scala> :type sm Option[org.apache.spark.sql.execution.streaming.StreamMetadata] === [[read]] Unpersisting StreamMetadata (from JSON File) -- read Object Method","title":"StreamMetadata"},{"location":"spark-sql-streaming-StreamMetadata/#source-scala","text":"read( metadataFile: Path, hadoopConf: Configuration): Option[StreamMetadata] read unpersists StreamMetadata from the given metadataFile file if available. read returns a StreamMetadata if the metadata file was available and the content could be read in JSON format. Otherwise, read returns None . NOTE: read uses org.json4s.jackson.Serialization.read for JSON deserialization. NOTE: read is used exclusively when StreamExecution is created> (and tries to read the metadata checkpoint file). === [[write]] Persisting Metadata -- write Object Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamMetadata/#source-scala_1","text":"write( metadata: StreamMetadata, metadataFile: Path, hadoopConf: Configuration): Unit write persists the given StreamMetadata to the given metadataFile file in JSON format. NOTE: write uses org.json4s.jackson.Serialization.write for JSON serialization. write is used when StreamExecution is created (and the metadata checkpoint file is not available).","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamProgress/","text":"StreamProgress \u2014 Collection of Offsets per Streaming Source \u00b6 StreamProgress is a collection of < > per < >. StreamProgress is < > when: StreamExecution is created (and creates committed and available offsets) OffsetSeq is requested to < > StreamProgress is an extension of Scala's https://www.scala-lang.org/api/2.11.11/index.html#scala.collection.immutable.Map[scala.collection.immutable.Map ] with < > as keys and their < > as values. === [[creating-instance]] Creating StreamProgress Instance StreamProgress takes the following to be created: [[baseMap]] Optional collection of < > per < > ( Map[BaseStreamingSource, Offset] ) (default: empty) === [[get]] Looking Up Offset by Streaming Source -- get Method [source, scala] \u00b6 get(key: BaseStreamingSource): Option[Offset] \u00b6 NOTE: get is part of the Scala's scala.collection.MapLike to...FIXME. get simply looks up an < > for the given < > in the < >. === [[plusplus]] ++ Method [source, scala] \u00b6 ++( updates: GenTraversableOnce[(BaseStreamingSource, Offset)]): StreamProgress ++ simply creates a new < > with the < > and the given updates. NOTE: ++ is used exclusively when OffsetSeq is requested to < >. === [[toOffsetSeq]] Converting to OffsetSeq -- toOffsetSeq Method [source, scala] \u00b6 toOffsetSeq( sources: Seq[BaseStreamingSource], metadata: OffsetSeqMetadata): OffsetSeq toOffsetSeq creates a < > with offsets that are < > for every < >. toOffsetSeq is used when: MicroBatchExecution stream execution engine is requested to construct the next streaming micro-batch (to commit available offsets for a batch to the write-ahead log ) StreamExecution is requested to run stream processing (that failed with a Throwable )","title":"StreamProgress"},{"location":"spark-sql-streaming-StreamProgress/#streamprogress-collection-of-offsets-per-streaming-source","text":"StreamProgress is a collection of < > per < >. StreamProgress is < > when: StreamExecution is created (and creates committed and available offsets) OffsetSeq is requested to < > StreamProgress is an extension of Scala's https://www.scala-lang.org/api/2.11.11/index.html#scala.collection.immutable.Map[scala.collection.immutable.Map ] with < > as keys and their < > as values. === [[creating-instance]] Creating StreamProgress Instance StreamProgress takes the following to be created: [[baseMap]] Optional collection of < > per < > ( Map[BaseStreamingSource, Offset] ) (default: empty) === [[get]] Looking Up Offset by Streaming Source -- get Method","title":"StreamProgress &mdash; Collection of Offsets per Streaming Source"},{"location":"spark-sql-streaming-StreamProgress/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamProgress/#getkey-basestreamingsource-optionoffset","text":"NOTE: get is part of the Scala's scala.collection.MapLike to...FIXME. get simply looks up an < > for the given < > in the < >. === [[plusplus]] ++ Method","title":"get(key: BaseStreamingSource): Option[Offset]"},{"location":"spark-sql-streaming-StreamProgress/#source-scala_1","text":"++( updates: GenTraversableOnce[(BaseStreamingSource, Offset)]): StreamProgress ++ simply creates a new < > with the < > and the given updates. NOTE: ++ is used exclusively when OffsetSeq is requested to < >. === [[toOffsetSeq]] Converting to OffsetSeq -- toOffsetSeq Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamProgress/#source-scala_2","text":"toOffsetSeq( sources: Seq[BaseStreamingSource], metadata: OffsetSeqMetadata): OffsetSeq toOffsetSeq creates a < > with offsets that are < > for every < >. toOffsetSeq is used when: MicroBatchExecution stream execution engine is requested to construct the next streaming micro-batch (to commit available offsets for a batch to the write-ahead log ) StreamExecution is requested to run stream processing (that failed with a Throwable )","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamSinkProvider/","text":"== [[StreamSinkProvider]] StreamSinkProvider Contract StreamSinkProvider is the < > of < > that can < > for a file format (e.g. parquet ) or system (e.g. kafka ). IMPORTANT: < > is a newer version of StreamSinkProvider (aka DataSource API V2 ) and new data sources should use the contract instead. [[contract]] .StreamSinkProvider Contract [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Method | Description | createSink a| [[createSink]] [source, scala] \u00b6 createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink Creates a streaming sink Used when DataSource is requested for a streaming sink (when DataStreamWriter is requested to start a streaming query ) |=== [[implementations]] NOTE: < > is the only known StreamSinkProvider in Spark Structured Streaming.","title":"StreamSinkProvider"},{"location":"spark-sql-streaming-StreamSinkProvider/#source-scala","text":"createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink Creates a streaming sink Used when DataSource is requested for a streaming sink (when DataStreamWriter is requested to start a streaming query ) |=== [[implementations]] NOTE: < > is the only known StreamSinkProvider in Spark Structured Streaming.","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamWriteSupport/","text":"== [[StreamWriteSupport]] StreamWriteSupport Contract -- Writable Streaming Data Sources StreamWriteSupport is the < > of < > that < > for streaming write (when used in streaming queries in < > and < >). [[contract]][[createStreamWriter]] [source, java] StreamWriter createStreamWriter( String queryId, StructType schema, OutputMode mode, DataSourceOptions options) createStreamWriter creates a < > for streaming write and is used when the stream execution thread for a streaming query is started and requests the stream execution engines to start, i.e. ContinuousExecution is requested to < > MicroBatchExecution is requested to < > [[implementations]] .StreamWriteSupports [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StreamWriteSupport | Description | < > | [[ConsoleSinkProvider]] Streaming sink for console data source format | < > | [[ForeachWriterProvider]] | < > | [[KafkaSourceProvider]] | < > | [[MemorySinkV2]] |===","title":"StreamWriteSupport"},{"location":"spark-sql-streaming-StreamWriter/","text":"== [[StreamWriter]] StreamWriter Contract StreamWriter is the < > of the DataSourceWriter contract to support epochs, i.e. < > that can < > and < > writing jobs for a specified epoch. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceWriter.html[DataSourceWriter ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[contract]] .StreamWriter Contract [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Method | Description | abort a| [[abort]] [source, java] \u00b6 void abort( long epochId, WriterCommitMessage[] messages) Aborts the writing job for a specified epochId and WriterCommitMessages Used exclusively when MicroBatchWriter is requested to < > | commit a| [[commit]] [source, java] \u00b6 void commit( long epochId, WriterCommitMessage[] messages) Commits the writing job for a specified epochId and WriterCommitMessages Used when: EpochCoordinator is requested to < > MicroBatchWriter is requested to < > |=== [[implementations]] .StreamWriters [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | StreamWriter | Description | < > | [[ForeachWriterProvider]] foreachWriter data source | < > | [[ConsoleWriter]] console data source | KafkaStreamWriter | [[KafkaStreamWriter]] kafka data source | MemoryStreamWriter | [[MemoryStreamWriter]] memory data source |===","title":"StreamWriter"},{"location":"spark-sql-streaming-StreamWriter/#source-java","text":"void abort( long epochId, WriterCommitMessage[] messages) Aborts the writing job for a specified epochId and WriterCommitMessages Used exclusively when MicroBatchWriter is requested to < > | commit a| [[commit]]","title":"[source, java]"},{"location":"spark-sql-streaming-StreamWriter/#source-java_1","text":"void commit( long epochId, WriterCommitMessage[] messages) Commits the writing job for a specified epochId and WriterCommitMessages Used when: EpochCoordinator is requested to < > MicroBatchWriter is requested to < > |=== [[implementations]] .StreamWriters [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | StreamWriter | Description | < > | [[ForeachWriterProvider]] foreachWriter data source | < > | [[ConsoleWriter]] console data source | KafkaStreamWriter | [[KafkaStreamWriter]] kafka data source | MemoryStreamWriter | [[MemoryStreamWriter]] memory data source |===","title":"[source, java]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/","text":"== [[StreamingAggregationStateManager]] StreamingAggregationStateManager Contract -- State Managers for Streaming Aggregation StreamingAggregationStateManager is the < > of < > that act as middlemen between < > and the physical operators used in < > (e.g. < > and < >). [[contract]] .StreamingAggregationStateManager Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | commit a| [[commit]] [source, scala] \u00b6 commit( store: StateStore): Long Commits all updates ( changes ) to the given < > and returns the new version Used exclusively when < > physical operator is executed. | get a| [[get]] [source, scala] \u00b6 get(store: StateStore, key: UnsafeRow): UnsafeRow \u00b6 Looks up the value of the key from the < > (the key is non- null ) Used exclusively when < > physical operator is executed. | getKey a| [[getKey]] [source, scala] \u00b6 getKey(row: UnsafeRow): UnsafeRow \u00b6 Extracts the columns for the key from the input row Used when: < > physical operator is executed StreamingAggregationStateManagerImplV1 legacy state manager is requested to < > | getStateValueSchema a| [[getStateValueSchema]] [source, scala] \u00b6 getStateValueSchema: StructType \u00b6 Gets the schema of the values in a < > Used when < > and < > physical operators are executed | iterator a| [[iterator]] [source, scala] \u00b6 iterator( store: StateStore): Iterator[UnsafeRowPair] Returns all UnsafeRow key-value pairs in the given < > Used exclusively when < > physical operator is executed. | keys a| [[keys]] [source, scala] \u00b6 keys(store: StateStore): Iterator[UnsafeRow] \u00b6 Returns all the keys in the < > Used exclusively when physical operators with WatermarkSupport are requested to < > (i.e. exclusively when < > physical operator is executed). | put a| [[put]] [source, scala] \u00b6 put( store: StateStore, row: UnsafeRow): Unit Stores ( puts ) the given row in the given < > Used exclusively when < > physical operator is executed. | remove a| [[remove]] [source, scala] \u00b6 remove( store: StateStore, key: UnsafeRow): Unit Removes the key-value pair from the given < > per key Used exclusively when < > physical operator is executed (directly or indirectly as a < >) | values a| [[values]] [source, scala] \u00b6 values( store: StateStore): Iterator[UnsafeRow] All values in the < > Used exclusively when < > physical operator is executed. |=== [[supportedVersions]] StreamingAggregationStateManager supports < > (per the < > internal configuration property): [[legacyVersion]] 1 (for the legacy < >) [[default]] 2 (for the default < >) [[implementations]] NOTE: < > is the one and only known direct implementation of the < > in Spark Structured Streaming. NOTE: StreamingAggregationStateManager is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). === [[createStateManager]] Creating StreamingAggregationStateManager Instance -- createStateManager Factory Method [source, scala] \u00b6 createStateManager( keyExpressions: Seq[Attribute], inputRowAttributes: Seq[Attribute], stateFormatVersion: Int): StreamingAggregationStateManager createStateManager creates a new StreamingAggregationStateManager for a given stateFormatVersion : < > for stateFormatVersion being 1 < > for stateFormatVersion being 2 createStateManager throws a IllegalArgumentException for any other stateFormatVersion : Version [stateFormatVersion] is invalid NOTE: createStateManager is used when < > and < > physical operators are created.","title":"StreamingAggregationStateManager"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala","text":"commit( store: StateStore): Long Commits all updates ( changes ) to the given < > and returns the new version Used exclusively when < > physical operator is executed. | get a| [[get]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#getstore-statestore-key-unsaferow-unsaferow","text":"Looks up the value of the key from the < > (the key is non- null ) Used exclusively when < > physical operator is executed. | getKey a| [[getKey]]","title":"get(store: StateStore, key: UnsafeRow): UnsafeRow"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#getkeyrow-unsaferow-unsaferow","text":"Extracts the columns for the key from the input row Used when: < > physical operator is executed StreamingAggregationStateManagerImplV1 legacy state manager is requested to < > | getStateValueSchema a| [[getStateValueSchema]]","title":"getKey(row: UnsafeRow): UnsafeRow"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#getstatevalueschema-structtype","text":"Gets the schema of the values in a < > Used when < > and < > physical operators are executed | iterator a| [[iterator]]","title":"getStateValueSchema: StructType"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala_4","text":"iterator( store: StateStore): Iterator[UnsafeRowPair] Returns all UnsafeRow key-value pairs in the given < > Used exclusively when < > physical operator is executed. | keys a| [[keys]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#keysstore-statestore-iteratorunsaferow","text":"Returns all the keys in the < > Used exclusively when physical operators with WatermarkSupport are requested to < > (i.e. exclusively when < > physical operator is executed). | put a| [[put]]","title":"keys(store: StateStore): Iterator[UnsafeRow]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala_6","text":"put( store: StateStore, row: UnsafeRow): Unit Stores ( puts ) the given row in the given < > Used exclusively when < > physical operator is executed. | remove a| [[remove]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala_7","text":"remove( store: StateStore, key: UnsafeRow): Unit Removes the key-value pair from the given < > per key Used exclusively when < > physical operator is executed (directly or indirectly as a < >) | values a| [[values]]","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala_8","text":"values( store: StateStore): Iterator[UnsafeRow] All values in the < > Used exclusively when < > physical operator is executed. |=== [[supportedVersions]] StreamingAggregationStateManager supports < > (per the < > internal configuration property): [[legacyVersion]] 1 (for the legacy < >) [[default]] 2 (for the default < >) [[implementations]] NOTE: < > is the one and only known direct implementation of the < > in Spark Structured Streaming. NOTE: StreamingAggregationStateManager is a Scala sealed trait which means that all the < > are in the same compilation unit (a single file). === [[createStateManager]] Creating StreamingAggregationStateManager Instance -- createStateManager Factory Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManager/#source-scala_9","text":"createStateManager( keyExpressions: Seq[Attribute], inputRowAttributes: Seq[Attribute], stateFormatVersion: Int): StreamingAggregationStateManager createStateManager creates a new StreamingAggregationStateManager for a given stateFormatVersion : < > for stateFormatVersion being 1 < > for stateFormatVersion being 2 createStateManager throws a IllegalArgumentException for any other stateFormatVersion : Version [stateFormatVersion] is invalid NOTE: createStateManager is used when < > and < > physical operators are created.","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/","text":"== [[StreamingAggregationStateManagerBaseImpl]] StreamingAggregationStateManagerBaseImpl -- Base State Manager for Streaming Aggregation StreamingAggregationStateManagerBaseImpl is the base implementation of the < > for < > that < >. [[keyProjector]] StreamingAggregationStateManagerBaseImpl uses UnsafeProjection to < >. [[implementations]] .StreamingAggregationStateManagerBaseImpls [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StreamingAggregationStateManagerBaseImpl | Description | < > | [[StreamingAggregationStateManagerImplV1]] Legacy < > (used when < > configuration property is 1 ) | < > | [[StreamingAggregationStateManagerImplV2]] Default < > (used when < > configuration property is 2 ) |=== [[creating-instance]] StreamingAggregationStateManagerBaseImpl takes the following to be created: [[keyExpressions]] Catalyst expressions for the keys ( Seq[Attribute] ) [[inputRowAttributes]] Catalyst expressions for the input rows ( Seq[Attribute] ) NOTE: StreamingAggregationStateManagerBaseImpl is a Scala abstract class and cannot be < > directly. It is created indirectly for the < >. === [[commit]] Committing (Changes to) State Store -- commit Method [source, scala] \u00b6 commit( store: StateStore): Long NOTE: commit is part of the < > to commit changes to a < >. commit simply requests the < > to < >. === [[remove]] Removing Key From State Store -- remove Method [source, scala] \u00b6 remove(store: StateStore, key: UnsafeRow): Unit \u00b6 NOTE: remove is part of the < > to remove a key from a state store. remove ...FIXME === [[getKey]] getKey Method [source, scala] \u00b6 getKey(row: UnsafeRow): UnsafeRow \u00b6 NOTE: getKey is part of the < > to...FIXME getKey ...FIXME === [[keys]] Getting All Keys in State Store -- keys Method [source, scala] \u00b6 keys(store: StateStore): Iterator[UnsafeRow] \u00b6 NOTE: keys is part of the < > to get all keys in a state store (as an iterator). keys ...FIXME","title":"StreamingAggregationStateManagerBaseImpl"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/#source-scala","text":"commit( store: StateStore): Long NOTE: commit is part of the < > to commit changes to a < >. commit simply requests the < > to < >. === [[remove]] Removing Key From State Store -- remove Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/#removestore-statestore-key-unsaferow-unit","text":"NOTE: remove is part of the < > to remove a key from a state store. remove ...FIXME === [[getKey]] getKey Method","title":"remove(store: StateStore, key: UnsafeRow): Unit"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/#getkeyrow-unsaferow-unsaferow","text":"NOTE: getKey is part of the < > to...FIXME getKey ...FIXME === [[keys]] Getting All Keys in State Store -- keys Method","title":"getKey(row: UnsafeRow): UnsafeRow"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/#keysstore-statestore-iteratorunsaferow","text":"NOTE: keys is part of the < > to get all keys in a state store (as an iterator). keys ...FIXME","title":"keys(store: StateStore): Iterator[UnsafeRow]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV1/","text":"== [[StreamingAggregationStateManagerImplV1]] StreamingAggregationStateManagerImplV1 -- Legacy State Manager for Streaming Aggregation StreamingAggregationStateManagerImplV1 is the legacy < >. NOTE: The version of a state manager is controlled using < > internal configuration property. StreamingAggregationStateManagerImplV1 is < > exclusively when StreamingAggregationStateManager is requested for a < >. === [[put]] Storing Row in State Store -- put Method [source, scala] \u00b6 put(store: StateStore, row: UnsafeRow): Unit \u00b6 NOTE: put is part of the < > to store a row in a state store. put ...FIXME === [[creating-instance]] Creating StreamingAggregationStateManagerImplV1 Instance StreamingAggregationStateManagerImplV1 takes the following when created: [[keyExpressions]] Attribute expressions for keys ( Seq[Attribute] ) [[inputRowAttributes]] Attribute expressions of input rows ( Seq[Attribute] )","title":"StreamingAggregationStateManagerImplV1"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV1/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV1/#putstore-statestore-row-unsaferow-unit","text":"NOTE: put is part of the < > to store a row in a state store. put ...FIXME === [[creating-instance]] Creating StreamingAggregationStateManagerImplV1 Instance StreamingAggregationStateManagerImplV1 takes the following when created: [[keyExpressions]] Attribute expressions for keys ( Seq[Attribute] ) [[inputRowAttributes]] Attribute expressions of input rows ( Seq[Attribute] )","title":"put(store: StateStore, row: UnsafeRow): Unit"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/","text":"== [[StreamingAggregationStateManagerImplV2]] StreamingAggregationStateManagerImplV2 -- Default State Manager for Streaming Aggregation StreamingAggregationStateManagerImplV2 is the default < >. NOTE: The version of a state manager is controlled using < > internal configuration property. StreamingAggregationStateManagerImplV2 is < > exclusively when StreamingAggregationStateManager is requested for a < >. [[creating-instance]] StreamingAggregationStateManagerImplV2 (like the parent < >) takes the following to be created: [[keyExpressions]] Catalyst expressions for the keys ( Seq[Attribute] ) [[inputRowAttributes]] Catalyst expressions for the input rows ( Seq[Attribute] ) === [[put]] Storing Row in State Store -- put Method [source, scala] \u00b6 put(store: StateStore, row: UnsafeRow): Unit \u00b6 NOTE: put is part of the < > to store a row in a state store. put ...FIXME === [[get]] Getting Saved State for Non-Null Key from State Store -- get Method [source, scala] \u00b6 get(store: StateStore, key: UnsafeRow): UnsafeRow \u00b6 NOTE: get is part of the < > to get the saved state for a given non-null key from a given < >. get requests the given < > for the current state value for the given key. get returns null if the key could not be found in the state store. Otherwise, get < > (for the key and the saved state). === [[restoreOriginalRow]] restoreOriginalRow Internal Method [source, scala] \u00b6 restoreOriginalRow(key: UnsafeRow, value: UnsafeRow): UnsafeRow restoreOriginalRow(rowPair: UnsafeRowPair): UnsafeRow restoreOriginalRow ...FIXME NOTE: restoreOriginalRow is used when StreamingAggregationStateManagerImplV2 is requested to < >, < > and < >. === [[getStateValueSchema]] getStateValueSchema Method [source, scala] \u00b6 getStateValueSchema: StructType \u00b6 NOTE: getStateValueSchema is part of the < > to...FIXME. getStateValueSchema simply requests the < > for the schema. === [[iterator]] iterator Method [source, scala] \u00b6 iterator: iterator(store: StateStore): Iterator[UnsafeRowPair] \u00b6 NOTE: iterator is part of the < > to...FIXME. iterator simply requests the input < > for the < > that is mapped to an iterator of UnsafeRowPairs with the key (of the input UnsafeRowPair ) and the value as a < >. NOTE: https://www.scala-lang.org/api/current/scala/collection/Iterator.html[scala.collection.Iterator ] is a data structure that allows to iterate over a sequence of elements that are usually fetched lazily (i.e. no elements are fetched from the underlying store until processed). === [[values]] values Method [source, scala] \u00b6 values(store: StateStore): Iterator[UnsafeRow] \u00b6 NOTE: values is part of the < > to...FIXME. values ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | joiner | [[joiner]] | keyValueJoinedExpressions a| [[keyValueJoinedExpressions]] | needToProjectToRestoreValue a| [[needToProjectToRestoreValue]] | restoreValueProjector a| [[restoreValueProjector]] | valueExpressions a| [[valueExpressions]] | valueProjector a| [[valueProjector]] |===","title":"StreamingAggregationStateManagerImplV2"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#putstore-statestore-row-unsaferow-unit","text":"NOTE: put is part of the < > to store a row in a state store. put ...FIXME === [[get]] Getting Saved State for Non-Null Key from State Store -- get Method","title":"put(store: StateStore, row: UnsafeRow): Unit"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#getstore-statestore-key-unsaferow-unsaferow","text":"NOTE: get is part of the < > to get the saved state for a given non-null key from a given < >. get requests the given < > for the current state value for the given key. get returns null if the key could not be found in the state store. Otherwise, get < > (for the key and the saved state). === [[restoreOriginalRow]] restoreOriginalRow Internal Method","title":"get(store: StateStore, key: UnsafeRow): UnsafeRow"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#source-scala_2","text":"restoreOriginalRow(key: UnsafeRow, value: UnsafeRow): UnsafeRow restoreOriginalRow(rowPair: UnsafeRowPair): UnsafeRow restoreOriginalRow ...FIXME NOTE: restoreOriginalRow is used when StreamingAggregationStateManagerImplV2 is requested to < >, < > and < >. === [[getStateValueSchema]] getStateValueSchema Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#getstatevalueschema-structtype","text":"NOTE: getStateValueSchema is part of the < > to...FIXME. getStateValueSchema simply requests the < > for the schema. === [[iterator]] iterator Method","title":"getStateValueSchema: StructType"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#iterator-iteratorstore-statestore-iteratorunsaferowpair","text":"NOTE: iterator is part of the < > to...FIXME. iterator simply requests the input < > for the < > that is mapped to an iterator of UnsafeRowPairs with the key (of the input UnsafeRowPair ) and the value as a < >. NOTE: https://www.scala-lang.org/api/current/scala/collection/Iterator.html[scala.collection.Iterator ] is a data structure that allows to iterate over a sequence of elements that are usually fetched lazily (i.e. no elements are fetched from the underlying store until processed). === [[values]] values Method","title":"iterator: iterator(store: StateStore): Iterator[UnsafeRowPair]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#source-scala_5","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingAggregationStateManagerImplV2/#valuesstore-statestore-iteratorunsaferow","text":"NOTE: values is part of the < > to...FIXME. values ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | joiner | [[joiner]] | keyValueJoinedExpressions a| [[keyValueJoinedExpressions]] | needToProjectToRestoreValue a| [[needToProjectToRestoreValue]] | restoreValueProjector a| [[restoreValueProjector]] | valueExpressions a| [[valueExpressions]] | valueProjector a| [[valueProjector]] |===","title":"values(store: StateStore): Iterator[UnsafeRow]"},{"location":"spark-sql-streaming-StreamingDeduplicationStrategy/","text":"StreamingDeduplicationStrategy Execution Planning Strategy \u00b6 [[apply]] StreamingDeduplicationStrategy is an execution planning strategy that can plan streaming queries with Deduplicate logical operators (over streaming queries) to StreamingDeduplicateExec physical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. NOTE: < > logical operator represents Dataset.dropDuplicates operator in a logical query plan. StreamingDeduplicationStrategy is available using SessionState . spark . sessionState . planner . StreamingDeduplicationStrategy","title":"StreamingDeduplicationStrategy"},{"location":"spark-sql-streaming-StreamingDeduplicationStrategy/#streamingdeduplicationstrategy-execution-planning-strategy","text":"[[apply]] StreamingDeduplicationStrategy is an execution planning strategy that can plan streaming queries with Deduplicate logical operators (over streaming queries) to StreamingDeduplicateExec physical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. NOTE: < > logical operator represents Dataset.dropDuplicates operator in a logical query plan. StreamingDeduplicationStrategy is available using SessionState . spark . sessionState . planner . StreamingDeduplicationStrategy","title":"StreamingDeduplicationStrategy Execution Planning Strategy"},{"location":"spark-sql-streaming-StreamingExecutionRelation/","text":"StreamingExecutionRelation Leaf Logical Operator \u00b6 StreamingExecutionRelation is a leaf logical operator (i.e. LogicalPlan ) that represents a streaming source in the logical query plan of a streaming Dataset . The main use of StreamingExecutionRelation logical operator is to be a \"placeholder\" in a logical query plan that will be replaced with the real relation (with new data that has arrived since the last batch) or an empty LocalRelation when StreamExecution is requested to < >. StreamingExecutionRelation is < > for a spark-sql-streaming-StreamingRelation.md[StreamingRelation] in analyzed logical query plan (that is the execution representation of a streaming Dataset). Note Right after StreamExecution MicroBatchExecution.md#runStream-initializing-sources[has started running streaming batches] it initializes the streaming sources by transforming the analyzed logical plan of the streaming Dataset so that every spark-sql-streaming-StreamingRelation.md[StreamingRelation] logical operator is replaced by the corresponding StreamingExecutionRelation . NOTE: StreamingExecutionRelation is also resolved (aka planned ) to a physical-operators/StreamingRelationExec.md[StreamingRelationExec] physical operator in spark-sql-streaming-StreamingRelationStrategy.md[StreamingRelationStrategy] execution planning strategy only when explaining a streaming Dataset . === [[creating-instance]] Creating StreamingExecutionRelation Instance StreamingExecutionRelation takes the following when created: [[source]] Streaming source [[output]] Output attributes === [[apply]] Creating StreamingExecutionRelation (based on a Source) -- apply Object Method [source, scala] \u00b6 apply(source: Source): StreamingExecutionRelation \u00b6 apply creates a StreamingExecutionRelation for the input source and with the attributes of the schema of the source . NOTE: apply seems to be used for tests only.","title":"StreamingExecutionRelation Leaf Logical Operator for Streaming Source At Execution"},{"location":"spark-sql-streaming-StreamingExecutionRelation/#streamingexecutionrelation-leaf-logical-operator","text":"StreamingExecutionRelation is a leaf logical operator (i.e. LogicalPlan ) that represents a streaming source in the logical query plan of a streaming Dataset . The main use of StreamingExecutionRelation logical operator is to be a \"placeholder\" in a logical query plan that will be replaced with the real relation (with new data that has arrived since the last batch) or an empty LocalRelation when StreamExecution is requested to < >. StreamingExecutionRelation is < > for a spark-sql-streaming-StreamingRelation.md[StreamingRelation] in analyzed logical query plan (that is the execution representation of a streaming Dataset). Note Right after StreamExecution MicroBatchExecution.md#runStream-initializing-sources[has started running streaming batches] it initializes the streaming sources by transforming the analyzed logical plan of the streaming Dataset so that every spark-sql-streaming-StreamingRelation.md[StreamingRelation] logical operator is replaced by the corresponding StreamingExecutionRelation . NOTE: StreamingExecutionRelation is also resolved (aka planned ) to a physical-operators/StreamingRelationExec.md[StreamingRelationExec] physical operator in spark-sql-streaming-StreamingRelationStrategy.md[StreamingRelationStrategy] execution planning strategy only when explaining a streaming Dataset . === [[creating-instance]] Creating StreamingExecutionRelation Instance StreamingExecutionRelation takes the following when created: [[source]] Streaming source [[output]] Output attributes === [[apply]] Creating StreamingExecutionRelation (based on a Source) -- apply Object Method","title":"StreamingExecutionRelation Leaf Logical Operator"},{"location":"spark-sql-streaming-StreamingExecutionRelation/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingExecutionRelation/#applysource-source-streamingexecutionrelation","text":"apply creates a StreamingExecutionRelation for the input source and with the attributes of the schema of the source . NOTE: apply seems to be used for tests only.","title":"apply(source: Source): StreamingExecutionRelation"},{"location":"spark-sql-streaming-StreamingGlobalLimitStrategy/","text":"== [[StreamingGlobalLimitStrategy]] StreamingGlobalLimitStrategy Execution Planning Strategy StreamingGlobalLimitStrategy is an execution planning strategy that can plan streaming queries with ReturnAnswer and Limit logical operators (over streaming queries) with the < > output mode to < > physical operator. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. StreamingGlobalLimitStrategy is used (and < >) exclusively when < > is requested to plan a streaming query. [[creating-instance]][[outputMode]] StreamingGlobalLimitStrategy takes a single < > to be created (which is the < > of the < >). === [[demo]] Demo: Using StreamingGlobalLimitStrategy [source, scala] \u00b6 FIXME \u00b6","title":"StreamingGlobalLimitStrategy"},{"location":"spark-sql-streaming-StreamingGlobalLimitStrategy/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingGlobalLimitStrategy/#fixme","text":"","title":"FIXME"},{"location":"spark-sql-streaming-StreamingJoinHelper/","text":"== [[StreamingJoinHelper]] StreamingJoinHelper Utility StreamingJoinHelper is a Scala object with the following utility methods: < > [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.catalyst.analysis.StreamingJoinHelper to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.catalyst.analysis.StreamingJoinHelper=ALL Refer to < >. \u00b6 === [[getStateValueWatermark]] State Value Watermark -- getStateValueWatermark Object Method [source, scala] \u00b6 getStateValueWatermark( attributesToFindStateWatermarkFor: AttributeSet, attributesWithEventWatermark: AttributeSet, joinCondition: Option[Expression], eventWatermark: Option[Long]): Option[Long] getStateValueWatermark ...FIXME [NOTE] \u00b6 getStateValueWatermark is used when: UnsupportedOperationChecker utility is used to < > * StreamingSymmetricHashJoinHelper utility is used to < > \u00b6","title":"StreamingJoinHelper"},{"location":"spark-sql-streaming-StreamingJoinHelper/#refer-to","text":"=== [[getStateValueWatermark]] State Value Watermark -- getStateValueWatermark Object Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-StreamingJoinHelper/#source-scala","text":"getStateValueWatermark( attributesToFindStateWatermarkFor: AttributeSet, attributesWithEventWatermark: AttributeSet, joinCondition: Option[Expression], eventWatermark: Option[Long]): Option[Long] getStateValueWatermark ...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingJoinHelper/#note","text":"getStateValueWatermark is used when: UnsupportedOperationChecker utility is used to < >","title":"[NOTE]"},{"location":"spark-sql-streaming-StreamingJoinHelper/#streamingsymmetrichashjoinhelper-utility-is-used-to","text":"","title":"* StreamingSymmetricHashJoinHelper utility is used to &lt;&gt;"},{"location":"spark-sql-streaming-StreamingJoinStrategy/","text":"== [[StreamingJoinStrategy]] StreamingJoinStrategy Execution Planning Strategy -- Stream-Stream Equi-Joins [[apply]] StreamingJoinStrategy is an execution planning strategy that can plan streaming queries with Join logical operators of two streaming queries to a < > physical operator. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. StreamingJoinStrategy throws an AnalysisException when applied to a Join logical operator with no equality predicate: Stream-stream join without equality predicate is not supported StreamingJoinStrategy is used exclusively when < > is requested to plan a streaming query. [[logging]] [TIP] ==== StreamingJoinStrategy does not print out any messages to the logs. StreamingJoinStrategy however uses ExtractEquiJoinKeys Scala extractor for destructuring Join logical operators that does print out DEBUG messages to the logs. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-ExtractEquiJoinKeys.html[ExtractEquiJoinKeys ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. Enable ALL logging level for org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys=ALL Refer to < >. \u00b6","title":"StreamingJoinStrategy"},{"location":"spark-sql-streaming-StreamingJoinStrategy/#refer-to","text":"","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-StreamingQueryWrapper/","text":"StreamingQueryWrapper \u2014 Serializable StreamExecution \u00b6 StreamingQueryWrapper is a serializable interface of a StreamExecution . StreamingQueryWrapper has the same StreamExecution API and simply passes all the method calls along to the underlying StreamExecution . StreamingQueryWrapper is created when StreamingQueryManager is requested to create a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ). Demo: Any Streaming Query is StreamingQueryWrapper \u00b6 import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val query = spark . readStream . format ( \"rate\" ) . load . writeStream . format ( \"memory\" ) . queryName ( \"rate2memory\" ) . start assert ( query . isInstanceOf [ StreamingQueryWrapper ])","title":"StreamingQueryWrapper"},{"location":"spark-sql-streaming-StreamingQueryWrapper/#streamingquerywrapper-serializable-streamexecution","text":"StreamingQueryWrapper is a serializable interface of a StreamExecution . StreamingQueryWrapper has the same StreamExecution API and simply passes all the method calls along to the underlying StreamExecution . StreamingQueryWrapper is created when StreamingQueryManager is requested to create a streaming query (when DataStreamWriter is requested to start an execution of the streaming query ).","title":"StreamingQueryWrapper &mdash; Serializable StreamExecution"},{"location":"spark-sql-streaming-StreamingQueryWrapper/#demo-any-streaming-query-is-streamingquerywrapper","text":"import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val query = spark . readStream . format ( \"rate\" ) . load . writeStream . format ( \"memory\" ) . queryName ( \"rate2memory\" ) . start assert ( query . isInstanceOf [ StreamingQueryWrapper ])","title":"Demo: Any Streaming Query is StreamingQueryWrapper"},{"location":"spark-sql-streaming-StreamingRelation/","text":"StreamingRelation Leaf Logical Operator for Streaming Source \u00b6 StreamingRelation is a leaf logical operator (i.e. LogicalPlan ) that represents a streaming source in a logical plan. StreamingRelation is < > when DataStreamReader is requested to spark-sql-streaming-DataStreamReader.md#load[load data from a streaming source] and creates a streaming Dataset . .StreamingRelation Represents Streaming Source image::images/StreamingRelation.png[align=\"center\"] [source, scala] \u00b6 val rate = spark. readStream. // \u2190 creates a DataStreamReader format(\"rate\"). load(\"hello\") // \u2190 creates a StreamingRelation scala> println(rate.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4e5dcc50,rate,List(),None,List(),None,Map(path -> hello),None), rate, [timestamp#0, value#1L] [[isStreaming]] isStreaming flag is always enabled (i.e. true ). [source, scala] \u00b6 import org.apache.spark.sql.execution.streaming.StreamingRelation val relation = rate.queryExecution.logical.asInstanceOf[StreamingRelation] scala> relation.isStreaming res1: Boolean = true [[toString]] toString gives the < >. [source, scala] \u00b6 scala> println(relation) rate NOTE: StreamingRelation is resolved (aka planned ) to spark-sql-streaming-StreamingExecutionRelation.md[StreamingExecutionRelation] (right after StreamExecution starts running batches ). === [[apply]] Creating StreamingRelation for DataSource -- apply Object Method [source, scala] \u00b6 apply(dataSource: DataSource): StreamingRelation \u00b6 apply creates a StreamingRelation for the given < > (that represents a streaming source). NOTE: apply is used exclusively when DataStreamReader is requested for a < >. === [[creating-instance]] Creating StreamingRelation Instance StreamingRelation takes the following when created: [[dataSource]] spark-sql-streaming-DataSource.md[DataSource] [[sourceName]] Short name of the streaming source [[output]] Output attributes of the schema of the streaming source","title":"StreamingRelation Leaf Logical Operator for Streaming Source"},{"location":"spark-sql-streaming-StreamingRelation/#streamingrelation-leaf-logical-operator-for-streaming-source","text":"StreamingRelation is a leaf logical operator (i.e. LogicalPlan ) that represents a streaming source in a logical plan. StreamingRelation is < > when DataStreamReader is requested to spark-sql-streaming-DataStreamReader.md#load[load data from a streaming source] and creates a streaming Dataset . .StreamingRelation Represents Streaming Source image::images/StreamingRelation.png[align=\"center\"]","title":"StreamingRelation Leaf Logical Operator for Streaming Source"},{"location":"spark-sql-streaming-StreamingRelation/#source-scala","text":"val rate = spark. readStream. // \u2190 creates a DataStreamReader format(\"rate\"). load(\"hello\") // \u2190 creates a StreamingRelation scala> println(rate.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4e5dcc50,rate,List(),None,List(),None,Map(path -> hello),None), rate, [timestamp#0, value#1L] [[isStreaming]] isStreaming flag is always enabled (i.e. true ).","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingRelation/#source-scala_1","text":"import org.apache.spark.sql.execution.streaming.StreamingRelation val relation = rate.queryExecution.logical.asInstanceOf[StreamingRelation] scala> relation.isStreaming res1: Boolean = true [[toString]] toString gives the < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingRelation/#source-scala_2","text":"scala> println(relation) rate NOTE: StreamingRelation is resolved (aka planned ) to spark-sql-streaming-StreamingExecutionRelation.md[StreamingExecutionRelation] (right after StreamExecution starts running batches ). === [[apply]] Creating StreamingRelation for DataSource -- apply Object Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingRelation/#source-scala_3","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingRelation/#applydatasource-datasource-streamingrelation","text":"apply creates a StreamingRelation for the given < > (that represents a streaming source). NOTE: apply is used exclusively when DataStreamReader is requested for a < >. === [[creating-instance]] Creating StreamingRelation Instance StreamingRelation takes the following when created: [[dataSource]] spark-sql-streaming-DataSource.md[DataSource] [[sourceName]] Short name of the streaming source [[output]] Output attributes of the schema of the streaming source","title":"apply(dataSource: DataSource): StreamingRelation"},{"location":"spark-sql-streaming-StreamingRelationStrategy/","text":"== [[StreamingRelationStrategy]] StreamingRelationStrategy Execution Planning Strategy for StreamingRelation and StreamingExecutionRelation Logical Operators [[apply]] StreamingRelationStrategy is an execution planning strategy that can plan streaming queries with < >, < >, and < > logical operators to < > physical operators. .StreamingRelationStrategy, StreamingRelation, StreamingExecutionRelation and StreamingRelationExec Operators image::images/StreamingRelationStrategy-apply.png[align=\"center\"] TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy.html[Execution Planning Strategies] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. StreamingRelationStrategy is used exclusively when < > is requested to plan a streaming query. StreamingRelationStrategy is available using SessionState (of a SparkSession ). [source, scala] \u00b6 spark.sessionState.planner.StreamingRelationStrategy \u00b6 === [[demo]] Demo: Using StreamingRelationStrategy [source, scala] \u00b6 val rates = spark. readStream. format(\"rate\"). load // \u2190 gives a streaming Dataset with a logical plan with StreamingRelation logical operator // StreamingRelation logical operator for the rate streaming source scala> println(rates.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] // StreamingRelationExec physical operator (shown without \"Exec\" suffix) scala> rates.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] // Let's do the planning manually import spark.sessionState.planner.StreamingRelationStrategy val physicalPlan = StreamingRelationStrategy.apply(rates.queryExecution.logical).head scala> println(physicalPlan.numberedTreeString) 00 StreamingRelation rate, [timestamp#0, value#1L]","title":"StreamingRelationStrategy"},{"location":"spark-sql-streaming-StreamingRelationStrategy/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingRelationStrategy/#sparksessionstateplannerstreamingrelationstrategy","text":"=== [[demo]] Demo: Using StreamingRelationStrategy","title":"spark.sessionState.planner.StreamingRelationStrategy"},{"location":"spark-sql-streaming-StreamingRelationStrategy/#source-scala_1","text":"val rates = spark. readStream. format(\"rate\"). load // \u2190 gives a streaming Dataset with a logical plan with StreamingRelation logical operator // StreamingRelation logical operator for the rate streaming source scala> println(rates.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] // StreamingRelationExec physical operator (shown without \"Exec\" suffix) scala> rates.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] // Let's do the planning manually import spark.sessionState.planner.StreamingRelationStrategy val physicalPlan = StreamingRelationStrategy.apply(rates.queryExecution.logical).head scala> println(physicalPlan.numberedTreeString) 00 StreamingRelation rate, [timestamp#0, value#1L]","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingRelationV2/","text":"== [[StreamingRelationV2]] StreamingRelationV2 Leaf Logical Operator StreamingRelationV2 is a MultiInstanceRelation leaf logical operator that represents < > or < > streaming data sources in a logical plan of a streaming query. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-LeafNode.html[Leaf logical operators] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. StreamingRelationV2 is < > when: DataStreamReader is requested to < > for < > and < > streaming data sources < > is created [[isStreaming]] isStreaming flag is always enabled (i.e. true ). [source, scala] \u00b6 scala> :type sq org.apache.spark.sql.DataFrame import org.apache.spark.sql.execution.streaming.StreamingRelationV2 val relation = sq.queryExecution.logical.asInstanceOf[StreamingRelationV2] assert(relation.isStreaming) StreamingRelationV2 is resolved ( replaced ) to the following leaf logical operators: < > when ContinuousExecution stream execution engine is requested for the < > < > when MicroBatchExecution stream execution engine is requested for the < > === [[creating-instance]] Creating StreamingRelationV2 Instance StreamingRelationV2 takes the following to be created: [[dataSource]] DataSourceV2 [[sourceName]] Name of the data source [[extraOptions]] Options ( Map[String, String] ) [[output]] Output attributes ( Seq[Attribute] ) [[v1Relation]] Optional < > [[session]] SparkSession","title":"StreamingRelationV2 Leaf Logical Operator"},{"location":"spark-sql-streaming-StreamingRelationV2/#source-scala","text":"scala> :type sq org.apache.spark.sql.DataFrame import org.apache.spark.sql.execution.streaming.StreamingRelationV2 val relation = sq.queryExecution.logical.asInstanceOf[StreamingRelationV2] assert(relation.isStreaming) StreamingRelationV2 is resolved ( replaced ) to the following leaf logical operators: < > when ContinuousExecution stream execution engine is requested for the < > < > when MicroBatchExecution stream execution engine is requested for the < > === [[creating-instance]] Creating StreamingRelationV2 Instance StreamingRelationV2 takes the following to be created: [[dataSource]] DataSourceV2 [[sourceName]] Name of the data source [[extraOptions]] Options ( Map[String, String] ) [[output]] Output attributes ( Seq[Attribute] ) [[v1Relation]] Optional < > [[session]] SparkSession","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingSymmetricHashJoinHelper/","text":"== [[StreamingSymmetricHashJoinHelper]] StreamingSymmetricHashJoinHelper Utility StreamingSymmetricHashJoinHelper is a Scala object with the following utility methods: < > === [[getStateWatermarkPredicates]] Creating JoinStateWatermarkPredicates -- getStateWatermarkPredicates Object Method [source, scala] \u00b6 getStateWatermarkPredicates( leftAttributes: Seq[Attribute], rightAttributes: Seq[Attribute], leftKeys: Seq[Expression], rightKeys: Seq[Expression], condition: Option[Expression], eventTimeWatermark: Option[Long]): JoinStateWatermarkPredicates [[getStateWatermarkPredicates-joinKeyOrdinalForWatermark]] getStateWatermarkPredicates tries to find the index of the < > among the left keys first, and if not found, the right keys. NOTE: The < > is defined using Dataset.withWatermark operator. getStateWatermarkPredicates < > for the left side of a join (for the given leftAttributes , the leftKeys and the rightAttributes ). getStateWatermarkPredicates < > for the right side of a join (for the given rightAttributes , the rightKeys and the leftAttributes ). In the end, getStateWatermarkPredicates creates a < > with the left- and right-side state watermark predicates. NOTE: getStateWatermarkPredicates is used exclusively when IncrementalExecution is requested to < > (while optimizing query plans with < > physical operators). ==== [[getOneSideStateWatermarkPredicate]] Join State Watermark Predicate (for One Side of Join) -- getOneSideStateWatermarkPredicate Internal Method [source, scala] \u00b6 getOneSideStateWatermarkPredicate( oneSideInputAttributes: Seq[Attribute], oneSideJoinKeys: Seq[Expression], otherSideInputAttributes: Seq[Attribute]): Option[JoinStateWatermarkPredicate] getOneSideStateWatermarkPredicate finds what attributes were used to define the < > (the oneSideInputAttributes attributes, the < >) and creates a < > as follows: < > if the watermark was defined on a join key (with the watermark expression for the index of the join key expression) < > if the watermark was defined among the oneSideInputAttributes (with the < > based on the given oneSideInputAttributes and otherSideInputAttributes ) NOTE: getOneSideStateWatermarkPredicate creates no < > ( None ) for no watermark found. NOTE: getStateWatermarkPredicates is used exclusively to < >.","title":"StreamingSymmetricHashJoinHelper"},{"location":"spark-sql-streaming-StreamingSymmetricHashJoinHelper/#source-scala","text":"getStateWatermarkPredicates( leftAttributes: Seq[Attribute], rightAttributes: Seq[Attribute], leftKeys: Seq[Expression], rightKeys: Seq[Expression], condition: Option[Expression], eventTimeWatermark: Option[Long]): JoinStateWatermarkPredicates [[getStateWatermarkPredicates-joinKeyOrdinalForWatermark]] getStateWatermarkPredicates tries to find the index of the < > among the left keys first, and if not found, the right keys. NOTE: The < > is defined using Dataset.withWatermark operator. getStateWatermarkPredicates < > for the left side of a join (for the given leftAttributes , the leftKeys and the rightAttributes ). getStateWatermarkPredicates < > for the right side of a join (for the given rightAttributes , the rightKeys and the leftAttributes ). In the end, getStateWatermarkPredicates creates a < > with the left- and right-side state watermark predicates. NOTE: getStateWatermarkPredicates is used exclusively when IncrementalExecution is requested to < > (while optimizing query plans with < > physical operators). ==== [[getOneSideStateWatermarkPredicate]] Join State Watermark Predicate (for One Side of Join) -- getOneSideStateWatermarkPredicate Internal Method","title":"[source, scala]"},{"location":"spark-sql-streaming-StreamingSymmetricHashJoinHelper/#source-scala_1","text":"getOneSideStateWatermarkPredicate( oneSideInputAttributes: Seq[Attribute], oneSideJoinKeys: Seq[Expression], otherSideInputAttributes: Seq[Attribute]): Option[JoinStateWatermarkPredicate] getOneSideStateWatermarkPredicate finds what attributes were used to define the < > (the oneSideInputAttributes attributes, the < >) and creates a < > as follows: < > if the watermark was defined on a join key (with the watermark expression for the index of the join key expression) < > if the watermark was defined among the oneSideInputAttributes (with the < > based on the given oneSideInputAttributes and otherSideInputAttributes ) NOTE: getOneSideStateWatermarkPredicate creates no < > ( None ) for no watermark found. NOTE: getStateWatermarkPredicates is used exclusively to < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/","text":"== [[SymmetricHashJoinStateManager]] SymmetricHashJoinStateManager SymmetricHashJoinStateManager is < > for the left and right < > of a < > physical operator (one for each side when StreamingSymmetricHashJoinExec is requested to < >). .SymmetricHashJoinStateManager and Stream-Stream Join image::images/SymmetricHashJoinStateManager.png[align=\"center\"] SymmetricHashJoinStateManager manages join state using the < > and the < > state store handlers (and simply acts like their facade). === [[creating-instance]] Creating SymmetricHashJoinStateManager Instance SymmetricHashJoinStateManager takes the following to be created: [[joinSide]] < > [[inputValueAttributes]] Attributes of input values [[joinKeys]] Join keys ( Seq[Expression] ) [[stateInfo]] < > [[storeConf]] < > [[hadoopConf]] Hadoop https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/conf/Configuration.html[Configuration ] SymmetricHashJoinStateManager initializes the < >. === [[keyToNumValues]][[keyWithIndexToValue]] KeyToNumValuesStore and KeyWithIndexToValueStore State Store Handlers -- keyToNumValues and keyWithIndexToValue Internal Properties SymmetricHashJoinStateManager uses a < > ( keyToNumValues ) and a < > ( keyWithIndexToValue ) internally that are created immediately when SymmetricHashJoinStateManager is < > (for a < >). keyToNumValues and keyWithIndexToValue are used when SymmetricHashJoinStateManager is requested for the following: < > < > < > < > < > < > < > === [[joinSide-internals]] Join Side Marker -- JoinSide Internal Enum JoinSide can be one of the two possible values: [[LeftSide]][[left]] LeftSide (alias: left ) [[RightSide]][[right]] RightSide (alias: right ) They are both used exclusively when StreamingSymmetricHashJoinExec binary physical operator is requested to < > (and < > with an < >). === [[metrics]] Performance Metrics -- metrics Method [source, scala] \u00b6 metrics: StateStoreMetrics \u00b6 metrics returns the combined < > of the < > and the < > state store handlers. NOTE: metrics is used exclusively when OneSideHashJoiner is requested to < >. === [[removeByKeyCondition]] removeByKeyCondition Method [source, scala] \u00b6 removeByKeyCondition( removalCondition: UnsafeRow => Boolean): Iterator[UnsafeRowPair] removeByKeyCondition creates an Iterator of UnsafeRowPairs that < > for which the given removalCondition predicate holds. [[removeByKeyCondition-allKeyToNumValues]] removeByKeyCondition uses the < > for < >. NOTE: removeByKeyCondition is used exclusively when OneSideHashJoiner is requested to < > (for < >). ==== [[removeByKeyCondition-getNext]] getNext Internal Method (of removeByKeyCondition Method) [source, scala] \u00b6 getNext(): UnsafeRowPair \u00b6 getNext goes over the keys and values in the < > sequence and < > (from the < >) and the < > (from the < >) for which the given removalCondition predicate holds. === [[removeByValueCondition]] removeByValueCondition Method [source, scala] \u00b6 removeByValueCondition( removalCondition: UnsafeRow => Boolean): Iterator[UnsafeRowPair] removeByValueCondition creates an Iterator of UnsafeRowPairs that < > for which the given removalCondition predicate holds. NOTE: removeByValueCondition is used exclusively when OneSideHashJoiner is requested to < > (when < > is used). ==== [[removeByValueCondition-getNext]] getNext Internal Method (of removeByValueCondition Method) [source, scala] \u00b6 getNext(): UnsafeRowPair \u00b6 getNext ...FIXME === [[append]] Appending New Value Row to Key -- append Method [source, scala] \u00b6 append( key: UnsafeRow, value: UnsafeRow): Unit append requests the < > for the < >. In the end, append requests the stores for the following: < > to < > < > to < >. NOTE: append is used exclusively when OneSideHashJoiner is requested to < >. === [[get]] Retrieving Value Rows By Key -- get Method [source, scala] \u00b6 get(key: UnsafeRow): Iterator[UnsafeRow] \u00b6 get requests the < > for the < >. In the end, get requests the < > to < > and leaves value rows only. NOTE: get is used when OneSideHashJoiner is requested to < > and < >. === [[commit]] Committing State (Changes) -- commit Method [source, scala] \u00b6 commit(): Unit \u00b6 commit simply requests the < > and < > state store handlers to < >. NOTE: commit is used exclusively when OneSideHashJoiner is requested to < >. === [[abortIfNeeded]] Aborting State (Changes) -- abortIfNeeded Method [source, scala] \u00b6 abortIfNeeded(): Unit \u00b6 abortIfNeeded ...FIXME NOTE: abortIfNeeded is used when...FIXME === [[allStateStoreNames]] allStateStoreNames Object Method [source, scala] \u00b6 allStateStoreNames(joinSides: JoinSide*): Seq[String] \u00b6 allStateStoreNames simply returns the < > for all possible combinations of the given JoinSides and the two possible store types (e.g. < > and < >). NOTE: allStateStoreNames is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (as a RDD[InternalRow] ). === [[getStateStoreName]] getStateStoreName Object Method [source, scala] \u00b6 getStateStoreName( joinSide: JoinSide, storeType: StateStoreType): String getStateStoreName simply returns a string of the following format: [joinSide]-[storeType] [NOTE] \u00b6 getStateStoreName is used when: StateStoreHandler is requested to < > * SymmetricHashJoinStateManager utility is requested for < > (for StreamingSymmetricHashJoinExec physical operator to < >) \u00b6 === [[updateNumValueForCurrentKey]] updateNumValueForCurrentKey Internal Method [source, scala] \u00b6 updateNumValueForCurrentKey(): Unit \u00b6 updateNumValueForCurrentKey ...FIXME NOTE: updateNumValueForCurrentKey is used exclusively when SymmetricHashJoinStateManager is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keyAttributes a| [[keyAttributes]] Key attributes, i.e. AttributeReferences of the < > Used exclusively in KeyWithIndexToValueStore when requested for the < >, < >, < > and < > | keySchema a| [[keySchema]] Key schema ( StructType ) based on the < > with the names in the format of field and their ordinals (index) Used when: SymmetricHashJoinStateManager is requested for the < > (for < >) KeyToNumValuesStore is requested for the < > KeyWithIndexToValueStore is requested for the < > (for the internal < >) |===","title":"SymmetricHashJoinStateManager"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#metrics-statestoremetrics","text":"metrics returns the combined < > of the < > and the < > state store handlers. NOTE: metrics is used exclusively when OneSideHashJoiner is requested to < >. === [[removeByKeyCondition]] removeByKeyCondition Method","title":"metrics: StateStoreMetrics"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_1","text":"removeByKeyCondition( removalCondition: UnsafeRow => Boolean): Iterator[UnsafeRowPair] removeByKeyCondition creates an Iterator of UnsafeRowPairs that < > for which the given removalCondition predicate holds. [[removeByKeyCondition-allKeyToNumValues]] removeByKeyCondition uses the < > for < >. NOTE: removeByKeyCondition is used exclusively when OneSideHashJoiner is requested to < > (for < >). ==== [[removeByKeyCondition-getNext]] getNext Internal Method (of removeByKeyCondition Method)","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#getnext-unsaferowpair","text":"getNext goes over the keys and values in the < > sequence and < > (from the < >) and the < > (from the < >) for which the given removalCondition predicate holds. === [[removeByValueCondition]] removeByValueCondition Method","title":"getNext(): UnsafeRowPair"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_3","text":"removeByValueCondition( removalCondition: UnsafeRow => Boolean): Iterator[UnsafeRowPair] removeByValueCondition creates an Iterator of UnsafeRowPairs that < > for which the given removalCondition predicate holds. NOTE: removeByValueCondition is used exclusively when OneSideHashJoiner is requested to < > (when < > is used). ==== [[removeByValueCondition-getNext]] getNext Internal Method (of removeByValueCondition Method)","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_4","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#getnext-unsaferowpair_1","text":"getNext ...FIXME === [[append]] Appending New Value Row to Key -- append Method","title":"getNext(): UnsafeRowPair"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_5","text":"append( key: UnsafeRow, value: UnsafeRow): Unit append requests the < > for the < >. In the end, append requests the stores for the following: < > to < > < > to < >. NOTE: append is used exclusively when OneSideHashJoiner is requested to < >. === [[get]] Retrieving Value Rows By Key -- get Method","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_6","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#getkey-unsaferow-iteratorunsaferow","text":"get requests the < > for the < >. In the end, get requests the < > to < > and leaves value rows only. NOTE: get is used when OneSideHashJoiner is requested to < > and < >. === [[commit]] Committing State (Changes) -- commit Method","title":"get(key: UnsafeRow): Iterator[UnsafeRow]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#commit-unit","text":"commit simply requests the < > and < > state store handlers to < >. NOTE: commit is used exclusively when OneSideHashJoiner is requested to < >. === [[abortIfNeeded]] Aborting State (Changes) -- abortIfNeeded Method","title":"commit(): Unit"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_8","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#abortifneeded-unit","text":"abortIfNeeded ...FIXME NOTE: abortIfNeeded is used when...FIXME === [[allStateStoreNames]] allStateStoreNames Object Method","title":"abortIfNeeded(): Unit"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_9","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#allstatestorenamesjoinsides-joinside-seqstring","text":"allStateStoreNames simply returns the < > for all possible combinations of the given JoinSides and the two possible store types (e.g. < > and < >). NOTE: allStateStoreNames is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < > (as a RDD[InternalRow] ). === [[getStateStoreName]] getStateStoreName Object Method","title":"allStateStoreNames(joinSides: JoinSide*): Seq[String]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_10","text":"getStateStoreName( joinSide: JoinSide, storeType: StateStoreType): String getStateStoreName simply returns a string of the following format: [joinSide]-[storeType]","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#note","text":"getStateStoreName is used when: StateStoreHandler is requested to < >","title":"[NOTE]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#symmetrichashjoinstatemanager-utility-is-requested-for-for-streamingsymmetrichashjoinexec-physical-operator-to","text":"=== [[updateNumValueForCurrentKey]] updateNumValueForCurrentKey Internal Method","title":"* SymmetricHashJoinStateManager utility is requested for &lt;&gt; (for StreamingSymmetricHashJoinExec physical operator to &lt;&gt;)"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#source-scala_11","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-SymmetricHashJoinStateManager/#updatenumvalueforcurrentkey-unit","text":"updateNumValueForCurrentKey ...FIXME NOTE: updateNumValueForCurrentKey is used exclusively when SymmetricHashJoinStateManager is requested to < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keyAttributes a| [[keyAttributes]] Key attributes, i.e. AttributeReferences of the < > Used exclusively in KeyWithIndexToValueStore when requested for the < >, < >, < > and < > | keySchema a| [[keySchema]] Key schema ( StructType ) based on the < > with the names in the format of field and their ordinals (index) Used when: SymmetricHashJoinStateManager is requested for the < > (for < >) KeyToNumValuesStore is requested for the < > KeyWithIndexToValueStore is requested for the < > (for the internal < >) |===","title":"updateNumValueForCurrentKey(): Unit"},{"location":"spark-sql-streaming-TextSocketSource/","text":"TextSocketSource \u00b6 TextSocketSource is a streaming source that reads lines from a socket at the host and port (defined by parameters). It uses < > internal in-memory buffer to keep all of the lines that were read from a socket forever. [CAUTION] \u00b6 This source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery. It is designed only for tutorials and debugging. \u00b6 [source, scala] \u00b6 import org.apache.spark.sql.SparkSession val spark: SparkSession = SparkSession.builder.getOrCreate() // Connect to localhost:9999 // You can use \"nc -lk 9999\" for demos val textSocket = spark. readStream. format(\"socket\"). option(\"host\", \"localhost\"). option(\"port\", 9999). load import org.apache.spark.sql.Dataset val lines: Dataset[String] = textSocket.as[String].map(_.toUpperCase) val query = lines.writeStream.format(\"console\").start // Start typing the lines in nc session // They will appear UPPERCASE in the terminal Batch: 0 \u00b6 +---------+ | value| +---------+ |UPPERCASE| +---------+ scala> query.explain == Physical Plan == *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#21] +- *MapElements , obj#20: java.lang.String +- *DeserializeToObject value#43.toString, obj#19: java.lang.String +- LocalTableScan [value#43] scala> query.stop \u00b6 === [[lines]] lines Internal Buffer [source, scala] \u00b6 lines: ArrayBuffer[(String, Timestamp)] \u00b6 lines is the internal buffer of all the lines TextSocketSource read from the socket. === [[getOffset]] Maximum Available Offset (getOffset method) TextSocketSource 's offset can either be none or LongOffset of the number of lines in the internal < > buffer. getOffset is a part of the Source abstraction. === [[schema]] Schema (schema method) TextSocketSource supports two spark-sql-schema.md[schemas]: A single value field of String type. value field of StringType type and timestamp field of spark-sql-DataType.md#TimestampType[TimestampType] type of format yyyy-MM-dd HH:mm:ss . TIP: Refer to spark-sql-streaming-TextSocketSourceProvider.md#sourceSchema[sourceSchema] for TextSocketSourceProvider . === [[creating-instance]] Creating TextSocketSource Instance [source, scala] \u00b6 TextSocketSource( host: String, port: Int, includeTimestamp: Boolean, sqlContext: SQLContext) When TextSocketSource is created (see spark-sql-streaming-TextSocketSourceProvider.md#createSource[TextSocketSourceProvider]), it gets 4 parameters passed in: host port spark-sql-streaming-TextSocketSourceProvider.md#includeTimestamp[includeTimestamp] flag spark-sql-sqlcontext.md[SQLContext] CAUTION: It appears that the source did not get \"renewed\" to use spark-sql-sparksession.md[SparkSession] instead. It opens a socket at given host and port parameters and reads a buffering character-input stream using the default charset and the default-sized input buffer (of 8192 bytes) line by line. CAUTION: FIXME Review Java's Charset.defaultCharset() It starts a readThread daemon thread (called TextSocketSource(host, port) ) to read lines from the socket. The lines are added to the internal < > buffer. === [[stop]] Stopping TextSocketSource (stop method) When stopped, TextSocketSource closes the socket connection.","title":"TextSocketSource"},{"location":"spark-sql-streaming-TextSocketSource/#textsocketsource","text":"TextSocketSource is a streaming source that reads lines from a socket at the host and port (defined by parameters). It uses < > internal in-memory buffer to keep all of the lines that were read from a socket forever.","title":"TextSocketSource"},{"location":"spark-sql-streaming-TextSocketSource/#caution","text":"This source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery.","title":"[CAUTION]"},{"location":"spark-sql-streaming-TextSocketSource/#it-is-designed-only-for-tutorials-and-debugging","text":"","title":"It is designed only for tutorials and debugging."},{"location":"spark-sql-streaming-TextSocketSource/#source-scala","text":"import org.apache.spark.sql.SparkSession val spark: SparkSession = SparkSession.builder.getOrCreate() // Connect to localhost:9999 // You can use \"nc -lk 9999\" for demos val textSocket = spark. readStream. format(\"socket\"). option(\"host\", \"localhost\"). option(\"port\", 9999). load import org.apache.spark.sql.Dataset val lines: Dataset[String] = textSocket.as[String].map(_.toUpperCase) val query = lines.writeStream.format(\"console\").start // Start typing the lines in nc session // They will appear UPPERCASE in the terminal","title":"[source, scala]"},{"location":"spark-sql-streaming-TextSocketSource/#batch-0","text":"+---------+ | value| +---------+ |UPPERCASE| +---------+ scala> query.explain == Physical Plan == *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#21] +- *MapElements , obj#20: java.lang.String +- *DeserializeToObject value#43.toString, obj#19: java.lang.String +- LocalTableScan [value#43]","title":"Batch: 0"},{"location":"spark-sql-streaming-TextSocketSource/#scala-querystop","text":"=== [[lines]] lines Internal Buffer","title":"scala&gt; query.stop"},{"location":"spark-sql-streaming-TextSocketSource/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-TextSocketSource/#lines-arraybufferstring-timestamp","text":"lines is the internal buffer of all the lines TextSocketSource read from the socket. === [[getOffset]] Maximum Available Offset (getOffset method) TextSocketSource 's offset can either be none or LongOffset of the number of lines in the internal < > buffer. getOffset is a part of the Source abstraction. === [[schema]] Schema (schema method) TextSocketSource supports two spark-sql-schema.md[schemas]: A single value field of String type. value field of StringType type and timestamp field of spark-sql-DataType.md#TimestampType[TimestampType] type of format yyyy-MM-dd HH:mm:ss . TIP: Refer to spark-sql-streaming-TextSocketSourceProvider.md#sourceSchema[sourceSchema] for TextSocketSourceProvider . === [[creating-instance]] Creating TextSocketSource Instance","title":"lines: ArrayBuffer[(String, Timestamp)]"},{"location":"spark-sql-streaming-TextSocketSource/#source-scala_2","text":"TextSocketSource( host: String, port: Int, includeTimestamp: Boolean, sqlContext: SQLContext) When TextSocketSource is created (see spark-sql-streaming-TextSocketSourceProvider.md#createSource[TextSocketSourceProvider]), it gets 4 parameters passed in: host port spark-sql-streaming-TextSocketSourceProvider.md#includeTimestamp[includeTimestamp] flag spark-sql-sqlcontext.md[SQLContext] CAUTION: It appears that the source did not get \"renewed\" to use spark-sql-sparksession.md[SparkSession] instead. It opens a socket at given host and port parameters and reads a buffering character-input stream using the default charset and the default-sized input buffer (of 8192 bytes) line by line. CAUTION: FIXME Review Java's Charset.defaultCharset() It starts a readThread daemon thread (called TextSocketSource(host, port) ) to read lines from the socket. The lines are added to the internal < > buffer. === [[stop]] Stopping TextSocketSource (stop method) When stopped, TextSocketSource closes the socket connection.","title":"[source, scala]"},{"location":"spark-sql-streaming-TextSocketSourceProvider/","text":"TextSocketSourceProvider \u00b6 TextSocketSourceProvider is a StreamSourceProvider for TextSocketSource that read records from host and port . TextSocketSourceProvider is also a DataSourceRegister . The short name of the data source is socket . It requires two mandatory options (that you can set using option method): host which is the host name. port which is the port number. It must be an integer. TextSocketSourceProvider also supports < > option that is a boolean flag that you can use to include timestamps in the schema. === [[includeTimestamp]] includeTimestamp Option CAUTION: FIXME === [[createSource]] createSource createSource grabs the two mandatory options -- host and port -- and returns an spark-sql-streaming-TextSocketSource.md[TextSocketSource]. === [[sourceSchema]] sourceSchema sourceSchema returns textSocket as the name of the source and the schema that can be one of the two available schemas: SCHEMA_REGULAR (default) which is a schema with a single value field of String type. SCHEMA_TIMESTAMP when <<includeTimestamp, includeTimestamp>> flag option is set. It is not, i.e. false , by default. The schema are value field of StringType type and timestamp field of spark-sql-DataType.md#TimestampType[TimestampType] type of format yyyy-MM-dd HH:mm:ss . TIP: Read about spark-sql-schema.md[schema]. Internally, it starts by printing out the following WARN message to the logs: WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery and stores state indefinitely. It then checks whether host and port parameters are defined and if not it throws a AnalysisException : Set a host to read from with option(\"host\", ...).","title":"TextSocketSourceProvider"},{"location":"spark-sql-streaming-TextSocketSourceProvider/#textsocketsourceprovider","text":"TextSocketSourceProvider is a StreamSourceProvider for TextSocketSource that read records from host and port . TextSocketSourceProvider is also a DataSourceRegister . The short name of the data source is socket . It requires two mandatory options (that you can set using option method): host which is the host name. port which is the port number. It must be an integer. TextSocketSourceProvider also supports < > option that is a boolean flag that you can use to include timestamps in the schema. === [[includeTimestamp]] includeTimestamp Option CAUTION: FIXME === [[createSource]] createSource createSource grabs the two mandatory options -- host and port -- and returns an spark-sql-streaming-TextSocketSource.md[TextSocketSource]. === [[sourceSchema]] sourceSchema sourceSchema returns textSocket as the name of the source and the schema that can be one of the two available schemas: SCHEMA_REGULAR (default) which is a schema with a single value field of String type. SCHEMA_TIMESTAMP when <<includeTimestamp, includeTimestamp>> flag option is set. It is not, i.e. false , by default. The schema are value field of StringType type and timestamp field of spark-sql-DataType.md#TimestampType[TimestampType] type of format yyyy-MM-dd HH:mm:ss . TIP: Read about spark-sql-schema.md[schema]. Internally, it starts by printing out the following WARN message to the logs: WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery and stores state indefinitely. It then checks whether host and port parameters are defined and if not it throws a AnalysisException : Set a host to read from with option(\"host\", ...).","title":"TextSocketSourceProvider"},{"location":"spark-sql-streaming-Trigger/","text":"== [[Trigger]] Trigger -- How Frequently to Check Sources For New Data Trigger defines how often a StreamingQuery.md[streaming query] should be executed ( triggered ) and emit a new data (which StreamExecution uses to resolve a TriggerExecutor ). [[available-implementations]] [[available-triggers]] [[triggers]] .Trigger's Factory Methods [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Trigger | Creating Instance | ContinuousTrigger a| [[ContinuousTrigger]][[Continuous]] [source, java] \u00b6 Trigger Continuous(long intervalMs) Trigger Continuous(long interval, TimeUnit timeUnit) Trigger Continuous(Duration interval) Trigger Continuous(String interval) | OneTimeTrigger a| [[OneTimeTrigger]][[Once]] [source, java] \u00b6 Trigger Once() \u00b6 | ProcessingTime a| [[ProcessingTime]] [source, java] \u00b6 Trigger ProcessingTime(Duration interval) Trigger ProcessingTime(long intervalMs) Trigger ProcessingTime(long interval, TimeUnit timeUnit) Trigger ProcessingTime(String interval) < > |=== NOTE: You specify the trigger for a streaming query using DataStreamWriter 's trigger method. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger val query = spark. readStream. format(\"rate\"). load. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.Once). // \u2190 execute once and stop queryName(\"rate-once\"). start assert(query.isActive == false) scala> println(query.lastProgress) { \"id\" : \"2ae4b0a4-434f-4ca7-a523-4e859c07175b\", \"runId\" : \"24039ce5-906c-4f90-b6e7-bbb3ec38a1f5\", \"name\" : \"rate-once\", \"timestamp\" : \"2017-07-04T18:39:35.998Z\", \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"addBatch\" : 1365, \"getBatch\" : 29, \"getOffset\" : 0, \"queryPlanning\" : 285, \"triggerExecution\" : 1742, \"walCommit\" : 40 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : null, \"endOffset\" : 0, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@7dbf277\" } } NOTE: Although Trigger allows for custom implementations, StreamExecution StreamExecution.md#triggerExecutor[refuses such attempts] and reports an IllegalStateException . [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger case object MyTrigger extends Trigger scala> val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .trigger(MyTrigger) // \u2190 use custom trigger .queryName(\"rate-custom-trigger\") .start java.lang.IllegalStateException: Unknown type of trigger: MyTrigger at org.apache.spark.sql.execution.streaming.MicroBatchExecution. (MicroBatchExecution.scala:60) at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:275) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:316) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325) ... 57 elided NOTE: Trigger was introduced in https://github.com/apache/spark/commit/855ed44ed31210d2001d7ce67c8fa99f8416edd3[the commit for [SPARK-14176][SQL] Add DataFrameWriter.trigger to set the stream batch period]. === [[ProcessingTime-examples]] Examples of ProcessingTime ProcessingTime is a Trigger that assumes that milliseconds is the minimum time unit. You can create an instance of ProcessingTime using the following constructors: ProcessingTime(Long) that accepts non-negative values that represent milliseconds. + ProcessingTime(10) ProcessingTime(interval: String) or ProcessingTime.create(interval: String) that accept CalendarInterval instances with or without leading interval string. + ProcessingTime(\"10 milliseconds\") ProcessingTime(\"interval 10 milliseconds\") ProcessingTime(Duration) that accepts scala.concurrent.duration.Duration instances. + ProcessingTime(10.seconds) ProcessingTime.create(interval: Long, unit: TimeUnit) for Long and java.util.concurrent.TimeUnit instances. + ProcessingTime.create(10, TimeUnit.SECONDS)","title":"Trigger"},{"location":"spark-sql-streaming-Trigger/#source-java","text":"Trigger Continuous(long intervalMs) Trigger Continuous(long interval, TimeUnit timeUnit) Trigger Continuous(Duration interval) Trigger Continuous(String interval) | OneTimeTrigger a| [[OneTimeTrigger]][[Once]]","title":"[source, java]"},{"location":"spark-sql-streaming-Trigger/#source-java_1","text":"","title":"[source, java]"},{"location":"spark-sql-streaming-Trigger/#trigger-once","text":"| ProcessingTime a| [[ProcessingTime]]","title":"Trigger Once()"},{"location":"spark-sql-streaming-Trigger/#source-java_2","text":"Trigger ProcessingTime(Duration interval) Trigger ProcessingTime(long intervalMs) Trigger ProcessingTime(long interval, TimeUnit timeUnit) Trigger ProcessingTime(String interval) < > |=== NOTE: You specify the trigger for a streaming query using DataStreamWriter 's trigger method.","title":"[source, java]"},{"location":"spark-sql-streaming-Trigger/#source-scala","text":"import org.apache.spark.sql.streaming.Trigger val query = spark. readStream. format(\"rate\"). load. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.Once). // \u2190 execute once and stop queryName(\"rate-once\"). start assert(query.isActive == false) scala> println(query.lastProgress) { \"id\" : \"2ae4b0a4-434f-4ca7-a523-4e859c07175b\", \"runId\" : \"24039ce5-906c-4f90-b6e7-bbb3ec38a1f5\", \"name\" : \"rate-once\", \"timestamp\" : \"2017-07-04T18:39:35.998Z\", \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0, \"durationMs\" : { \"addBatch\" : 1365, \"getBatch\" : 29, \"getOffset\" : 0, \"queryPlanning\" : 285, \"triggerExecution\" : 1742, \"walCommit\" : 40 }, \"stateOperators\" : [ ], \"sources\" : [ { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : null, \"endOffset\" : 0, \"numInputRows\" : 0, \"processedRowsPerSecond\" : 0.0 } ], \"sink\" : { \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleSink@7dbf277\" } } NOTE: Although Trigger allows for custom implementations, StreamExecution StreamExecution.md#triggerExecutor[refuses such attempts] and reports an IllegalStateException .","title":"[source, scala]"},{"location":"spark-sql-streaming-Trigger/#source-scala_1","text":"import org.apache.spark.sql.streaming.Trigger case object MyTrigger extends Trigger scala> val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .trigger(MyTrigger) // \u2190 use custom trigger .queryName(\"rate-custom-trigger\") .start java.lang.IllegalStateException: Unknown type of trigger: MyTrigger at org.apache.spark.sql.execution.streaming.MicroBatchExecution. (MicroBatchExecution.scala:60) at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:275) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:316) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325) ... 57 elided NOTE: Trigger was introduced in https://github.com/apache/spark/commit/855ed44ed31210d2001d7ce67c8fa99f8416edd3[the commit for [SPARK-14176][SQL] Add DataFrameWriter.trigger to set the stream batch period]. === [[ProcessingTime-examples]] Examples of ProcessingTime ProcessingTime is a Trigger that assumes that milliseconds is the minimum time unit. You can create an instance of ProcessingTime using the following constructors: ProcessingTime(Long) that accepts non-negative values that represent milliseconds. + ProcessingTime(10) ProcessingTime(interval: String) or ProcessingTime.create(interval: String) that accept CalendarInterval instances with or without leading interval string. + ProcessingTime(\"10 milliseconds\") ProcessingTime(\"interval 10 milliseconds\") ProcessingTime(Duration) that accepts scala.concurrent.duration.Duration instances. + ProcessingTime(10.seconds) ProcessingTime.create(interval: Long, unit: TimeUnit) for Long and java.util.concurrent.TimeUnit instances. + ProcessingTime.create(10, TimeUnit.SECONDS)","title":"[source, scala]"},{"location":"spark-sql-streaming-TriggerExecutor/","text":"== [[TriggerExecutor]] TriggerExecutor TriggerExecutor is the < > for trigger executors that StreamExecution StreamExecution.md#runStream[uses to execute a batch runner]. [[batchRunner]] NOTE: Batch runner is an executable code that is executed at regular intervals. It is also called a trigger handler . [[contract]] [[execute]] [source, scala] package org.apache.spark.sql.execution.streaming trait TriggerExecutor { def execute(batchRunner: () => Boolean): Unit } NOTE: StreamExecution reports a IllegalStateException when StreamExecution.md#triggerExecutor[TriggerExecutor] is different from the < >: OneTimeExecutor or ProcessingTimeExecutor . [[available-implementations]] .TriggerExecutor's Available Implementations [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | TriggerExecutor | Description | [[OneTimeExecutor]] OneTimeExecutor | Executes batchRunner exactly once. | [[ProcessingTimeExecutor]] ProcessingTimeExecutor a| Executes batchRunner at regular intervals (as defined using ProcessingTime and DataStreamWriter.trigger method). [source, scala] \u00b6 ProcessingTimeExecutor( processingTime: ProcessingTime, clock: Clock = new SystemClock()) NOTE: Processing terminates when batchRunner returns false . |=== === [[notifyBatchFallingBehind]] notifyBatchFallingBehind Method CAUTION: FIXME","title":"TriggerExecutor"},{"location":"spark-sql-streaming-TriggerExecutor/#source-scala","text":"ProcessingTimeExecutor( processingTime: ProcessingTime, clock: Clock = new SystemClock()) NOTE: Processing terminates when batchRunner returns false . |=== === [[notifyBatchFallingBehind]] notifyBatchFallingBehind Method CAUTION: FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-UnsupportedOperationChecker/","text":"UnsupportedOperationChecker \u00b6 UnsupportedOperationChecker checks whether the logical plan of a streaming query uses supported operations only . UnsupportedOperationChecker is used when the internal spark.sql.streaming.unsupportedOperationCheck Spark property is enabled. Note UnsupportedOperationChecker comes actually with two methods, i.e. checkForBatch and < >, whose names reveal the different flavours of Spark SQL (as of 2.0), i.e. batch and streaming, respectively. The Spark Structured Streaming gitbook is solely focused on < > method. checkForStreaming Method \u00b6 checkForStreaming ( plan : LogicalPlan , outputMode : OutputMode ) : Unit checkForStreaming asserts that the following requirements hold: < > < > (on the grouping expressions) < > checkForStreaming ...FIXME checkForStreaming finds all streaming aggregates (i.e. Aggregate logical operators with streaming sources). Note Aggregate logical operator represents Dataset.groupBy and Dataset.groupByKey operators (and SQL's GROUP BY clause) in a logical query plan. [[only-one-streaming-aggregation-allowed]] checkForStreaming asserts that there is exactly one streaming aggregation in a streaming query. Otherwise, checkForStreaming reports a AnalysisException : Multiple streaming aggregations are not supported with streaming DataFrames/Datasets [[streaming-aggregation-append-mode-requires-watermark]] checkForStreaming asserts that watermark was defined for a streaming aggregation with Append output mode (on at least one of the grouping expressions). Otherwise, checkForStreaming reports a AnalysisException : Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark CAUTION: FIXME checkForStreaming counts all FlatMapGroupsWithState logical operators (on streaming Datasets with isMapGroupsWithState flag disabled). Note FlatMapGroupsWithState.isMapGroupsWithState flag is disabled when...FIXME [[multiple-flatMapGroupsWithState]] checkForStreaming asserts that multiple FlatMapGroupsWithState logical operators are only used when: outputMode is Append output mode outputMode of the FlatMapGroupsWithState logical operators is also Append output mode CAUTION: FIXME Reference to an example in flatMapGroupsWithState Otherwise, checkForStreaming reports a AnalysisException : Multiple flatMapGroupsWithStates are not supported when they are not all in append mode or the output mode is not append on a streaming DataFrames/Datasets CAUTION: FIXME checkForStreaming is used when StreamingQueryManager is requested to create a StreamingQueryWrapper (for starting a streaming query), but only when the internal spark.sql.streaming.unsupportedOperationCheck configuration property is enabled.","title":"UnsupportedOperationChecker"},{"location":"spark-sql-streaming-UnsupportedOperationChecker/#unsupportedoperationchecker","text":"UnsupportedOperationChecker checks whether the logical plan of a streaming query uses supported operations only . UnsupportedOperationChecker is used when the internal spark.sql.streaming.unsupportedOperationCheck Spark property is enabled. Note UnsupportedOperationChecker comes actually with two methods, i.e. checkForBatch and < >, whose names reveal the different flavours of Spark SQL (as of 2.0), i.e. batch and streaming, respectively. The Spark Structured Streaming gitbook is solely focused on < > method.","title":"UnsupportedOperationChecker"},{"location":"spark-sql-streaming-UnsupportedOperationChecker/#checkforstreaming-method","text":"checkForStreaming ( plan : LogicalPlan , outputMode : OutputMode ) : Unit checkForStreaming asserts that the following requirements hold: < > < > (on the grouping expressions) < > checkForStreaming ...FIXME checkForStreaming finds all streaming aggregates (i.e. Aggregate logical operators with streaming sources). Note Aggregate logical operator represents Dataset.groupBy and Dataset.groupByKey operators (and SQL's GROUP BY clause) in a logical query plan. [[only-one-streaming-aggregation-allowed]] checkForStreaming asserts that there is exactly one streaming aggregation in a streaming query. Otherwise, checkForStreaming reports a AnalysisException : Multiple streaming aggregations are not supported with streaming DataFrames/Datasets [[streaming-aggregation-append-mode-requires-watermark]] checkForStreaming asserts that watermark was defined for a streaming aggregation with Append output mode (on at least one of the grouping expressions). Otherwise, checkForStreaming reports a AnalysisException : Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark CAUTION: FIXME checkForStreaming counts all FlatMapGroupsWithState logical operators (on streaming Datasets with isMapGroupsWithState flag disabled). Note FlatMapGroupsWithState.isMapGroupsWithState flag is disabled when...FIXME [[multiple-flatMapGroupsWithState]] checkForStreaming asserts that multiple FlatMapGroupsWithState logical operators are only used when: outputMode is Append output mode outputMode of the FlatMapGroupsWithState logical operators is also Append output mode CAUTION: FIXME Reference to an example in flatMapGroupsWithState Otherwise, checkForStreaming reports a AnalysisException : Multiple flatMapGroupsWithStates are not supported when they are not all in append mode or the output mode is not append on a streaming DataFrames/Datasets CAUTION: FIXME checkForStreaming is used when StreamingQueryManager is requested to create a StreamingQueryWrapper (for starting a streaming query), but only when the internal spark.sql.streaming.unsupportedOperationCheck configuration property is enabled.","title":" checkForStreaming Method"},{"location":"spark-sql-streaming-WatermarkSupport/","text":"== [[WatermarkSupport]] WatermarkSupport Contract -- Unary Physical Operators with Streaming Watermark Support WatermarkSupport is the < > of unary physical operators ( UnaryExecNode ) with support for streaming event-time watermark. [NOTE] \u00b6 Watermark (aka \"allowed lateness\" ) is a moving threshold of event time and specifies what data to consider for aggregations, i.e. the threshold of late data so the engine can automatically drop incoming late data given event time and clean up old state accordingly. Read the official documentation of Spark in http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking[Handling Late Data and Watermarking]. \u00b6 [[properties]] .WatermarkSupport's (Lazily-Initialized) Properties [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Property | Description | [[watermarkExpression]] watermarkExpression a| Optional Catalyst expression that matches rows older than the event time watermark. Note Use withWatermark operator to specify streaming watermark. When initialized, watermarkExpression finds EventTimeWatermark.md#watermarkDelayMs[spark.watermarkDelayMs] watermark attribute in the child output's metadata. If found, watermarkExpression creates evictionExpression with the watermark attribute that is less than or equal < >. The watermark attribute may be of type StructType . If it is, watermarkExpression uses the first field as the watermark. watermarkExpression prints out the following INFO message to the logs when EventTimeWatermark.md#watermarkDelayMs[spark.watermarkDelayMs] watermark attribute is found. INFO [physicalOperator]Exec: Filtering state store on: [evictionExpression] NOTE: physicalOperator can be FlatMapGroupsWithStateExec , StateStoreSaveExec.md[StateStoreSaveExec] or physical-operators/StreamingDeduplicateExec.md[StreamingDeduplicateExec]. TIP: Enable INFO logging level for one of the stateful physical operators to see the INFO message in the logs. | [[watermarkPredicateForData]] watermarkPredicateForData | Optional Predicate that uses < > and the child output to match rows older than the event-time watermark | [[watermarkPredicateForKeys]] watermarkPredicateForKeys | Optional Predicate that uses < > to match rows older than the event time watermark. |=== === [[contract]] WatermarkSupport Contract [source, scala] \u00b6 package org.apache.spark.sql.execution.streaming trait WatermarkSupport extends UnaryExecNode { // only required methods that have no implementation def eventTimeWatermark: Option[Long] def keyExpressions: Seq[Attribute] } .WatermarkSupport Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[eventTimeWatermark]] eventTimeWatermark | Used mainly in < > to create a LessThanOrEqual Catalyst binary expression that matches rows older than the watermark. | [[keyExpressions]] keyExpressions | Grouping keys (in FlatMapGroupsWithStateExec ), duplicate keys (in physical-operators/StreamingDeduplicateExec.md#keyExpressions[StreamingDeduplicateExec]) or key attributes (in StateStoreSaveExec.md#keyExpressions[StateStoreSaveExec]) with at most one that may have EventTimeWatermark.md#watermarkDelayMs[spark.watermarkDelayMs] watermark attribute in metadata Used in < > to create a Predicate to match rows older than the event time watermark. Used also when StateStoreSaveExec.md#doExecute[StateStoreSaveExec] and physical-operators/StreamingDeduplicateExec.md#doExecute[StreamingDeduplicateExec] physical operators are executed. |=== === [[removeKeysOlderThanWatermark]][[removeKeysOlderThanWatermark-StateStore]] Removing Keys From StateStore Older Than Watermark -- removeKeysOlderThanWatermark Method [source, scala] \u00b6 removeKeysOlderThanWatermark(store: StateStore): Unit \u00b6 removeKeysOlderThanWatermark requests the input store for spark-sql-streaming-StateStore.md#getRange[all rows]. removeKeysOlderThanWatermark then uses < > to spark-sql-streaming-StateStore.md#remove[remove matching rows from the store]. NOTE: removeKeysOlderThanWatermark is used exclusively when StreamingDeduplicateExec physical operator is requested to < >. === [[removeKeysOlderThanWatermark-StreamingAggregationStateManager-store]] removeKeysOlderThanWatermark Method [source, scala] \u00b6 removeKeysOlderThanWatermark( storeManager: StreamingAggregationStateManager, store: StateStore): Unit removeKeysOlderThanWatermark ...FIXME NOTE: removeKeysOlderThanWatermark is used exclusively when StateStoreSaveExec physical operator is requested to < >.","title":"WatermarkSupport"},{"location":"spark-sql-streaming-WatermarkSupport/#note","text":"Watermark (aka \"allowed lateness\" ) is a moving threshold of event time and specifies what data to consider for aggregations, i.e. the threshold of late data so the engine can automatically drop incoming late data given event time and clean up old state accordingly.","title":"[NOTE]"},{"location":"spark-sql-streaming-WatermarkSupport/#read-the-official-documentation-of-spark-in-httpsparkapacheorgdocslateststructured-streaming-programming-guidehtmlhandling-late-data-and-watermarkinghandling-late-data-and-watermarking","text":"[[properties]] .WatermarkSupport's (Lazily-Initialized) Properties [cols=\"1,3\",options=\"header\",width=\"100%\"] |=== | Property | Description | [[watermarkExpression]] watermarkExpression a| Optional Catalyst expression that matches rows older than the event time watermark. Note Use withWatermark operator to specify streaming watermark. When initialized, watermarkExpression finds EventTimeWatermark.md#watermarkDelayMs[spark.watermarkDelayMs] watermark attribute in the child output's metadata. If found, watermarkExpression creates evictionExpression with the watermark attribute that is less than or equal < >. The watermark attribute may be of type StructType . If it is, watermarkExpression uses the first field as the watermark. watermarkExpression prints out the following INFO message to the logs when EventTimeWatermark.md#watermarkDelayMs[spark.watermarkDelayMs] watermark attribute is found. INFO [physicalOperator]Exec: Filtering state store on: [evictionExpression] NOTE: physicalOperator can be FlatMapGroupsWithStateExec , StateStoreSaveExec.md[StateStoreSaveExec] or physical-operators/StreamingDeduplicateExec.md[StreamingDeduplicateExec]. TIP: Enable INFO logging level for one of the stateful physical operators to see the INFO message in the logs. | [[watermarkPredicateForData]] watermarkPredicateForData | Optional Predicate that uses < > and the child output to match rows older than the event-time watermark | [[watermarkPredicateForKeys]] watermarkPredicateForKeys | Optional Predicate that uses < > to match rows older than the event time watermark. |=== === [[contract]] WatermarkSupport Contract","title":"Read the official documentation of Spark in http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking[Handling Late Data and Watermarking]."},{"location":"spark-sql-streaming-WatermarkSupport/#source-scala","text":"package org.apache.spark.sql.execution.streaming trait WatermarkSupport extends UnaryExecNode { // only required methods that have no implementation def eventTimeWatermark: Option[Long] def keyExpressions: Seq[Attribute] } .WatermarkSupport Contract [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | [[eventTimeWatermark]] eventTimeWatermark | Used mainly in < > to create a LessThanOrEqual Catalyst binary expression that matches rows older than the watermark. | [[keyExpressions]] keyExpressions | Grouping keys (in FlatMapGroupsWithStateExec ), duplicate keys (in physical-operators/StreamingDeduplicateExec.md#keyExpressions[StreamingDeduplicateExec]) or key attributes (in StateStoreSaveExec.md#keyExpressions[StateStoreSaveExec]) with at most one that may have EventTimeWatermark.md#watermarkDelayMs[spark.watermarkDelayMs] watermark attribute in metadata Used in < > to create a Predicate to match rows older than the event time watermark. Used also when StateStoreSaveExec.md#doExecute[StateStoreSaveExec] and physical-operators/StreamingDeduplicateExec.md#doExecute[StreamingDeduplicateExec] physical operators are executed. |=== === [[removeKeysOlderThanWatermark]][[removeKeysOlderThanWatermark-StateStore]] Removing Keys From StateStore Older Than Watermark -- removeKeysOlderThanWatermark Method","title":"[source, scala]"},{"location":"spark-sql-streaming-WatermarkSupport/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-WatermarkSupport/#removekeysolderthanwatermarkstore-statestore-unit","text":"removeKeysOlderThanWatermark requests the input store for spark-sql-streaming-StateStore.md#getRange[all rows]. removeKeysOlderThanWatermark then uses < > to spark-sql-streaming-StateStore.md#remove[remove matching rows from the store]. NOTE: removeKeysOlderThanWatermark is used exclusively when StreamingDeduplicateExec physical operator is requested to < >. === [[removeKeysOlderThanWatermark-StreamingAggregationStateManager-store]] removeKeysOlderThanWatermark Method","title":"removeKeysOlderThanWatermark(store: StateStore): Unit"},{"location":"spark-sql-streaming-WatermarkSupport/#source-scala_2","text":"removeKeysOlderThanWatermark( storeManager: StreamingAggregationStateManager, store: StateStore): Unit removeKeysOlderThanWatermark ...FIXME NOTE: removeKeysOlderThanWatermark is used exclusively when StateStoreSaveExec physical operator is requested to < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-WatermarkTracker/","text":"== [[WatermarkTracker]] WatermarkTracker WatermarkTracker tracks the < > of a streaming query (across < > in a physical query plan) based on a given < >. WatermarkTracker is used exclusively in < >. WatermarkTracker is < > (using the < >) when MicroBatchExecution is requested to < > (when requested to < >). [[policy]] [[creating-instance]] WatermarkTracker takes a single < > to be created. [[MultipleWatermarkPolicy]] MultipleWatermarkPolicy can be one of the following: [[MaxWatermark]] MaxWatermark (alias: min ) [[MinWatermark]] MinWatermark (alias: max ) [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.WatermarkTracker to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.WatermarkTracker=ALL Refer to < >. \u00b6 === [[apply]] Creating WatermarkTracker -- apply Factory Method [source, scala] \u00b6 apply(conf: RuntimeConfig): WatermarkTracker \u00b6 apply uses the < > configuration property for the global watermark policy (default: min ) and creates a < >. NOTE: apply is used exclusively when MicroBatchExecution is requested to < > (when requested to < >). === [[setWatermark]] setWatermark Method [source, scala] \u00b6 setWatermark(newWatermarkMs: Long): Unit \u00b6 setWatermark simply updates the < > to the given newWatermarkMs . NOTE: setWatermark is used exclusively when MicroBatchExecution is requested to < > (when requested to < >). === [[updateWatermark]] Updating Event-Time Watermark -- updateWatermark Method [source, scala] \u00b6 updateWatermark(executedPlan: SparkPlan): Unit \u00b6 updateWatermark requests the given physical operator ( SparkPlan ) to collect all EventTimeWatermarkExec unary physical operators. updateWatermark simply exits when no EventTimeWatermarkExec was found. updateWatermark ...FIXME NOTE: updateWatermark is used exclusively when MicroBatchExecution is requested to < > (when requested to < >). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | globalWatermarkMs a| [[globalWatermarkMs]][[currentWatermark]] Current global event-time watermark per < > (across < > in a physical query plan) Default: 0 Used when...FIXME | operatorToWatermarkMap a| [[operatorToWatermarkMap]] Event-time watermark per EventTimeWatermarkExec physical operator ( mutable.HashMap[Int, Long] ) Used when...FIXME |===","title":"WatermarkTracker"},{"location":"spark-sql-streaming-WatermarkTracker/#refer-to","text":"=== [[apply]] Creating WatermarkTracker -- apply Factory Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-WatermarkTracker/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-WatermarkTracker/#applyconf-runtimeconfig-watermarktracker","text":"apply uses the < > configuration property for the global watermark policy (default: min ) and creates a < >. NOTE: apply is used exclusively when MicroBatchExecution is requested to < > (when requested to < >). === [[setWatermark]] setWatermark Method","title":"apply(conf: RuntimeConfig): WatermarkTracker"},{"location":"spark-sql-streaming-WatermarkTracker/#source-scala_1","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-WatermarkTracker/#setwatermarknewwatermarkms-long-unit","text":"setWatermark simply updates the < > to the given newWatermarkMs . NOTE: setWatermark is used exclusively when MicroBatchExecution is requested to < > (when requested to < >). === [[updateWatermark]] Updating Event-Time Watermark -- updateWatermark Method","title":"setWatermark(newWatermarkMs: Long): Unit"},{"location":"spark-sql-streaming-WatermarkTracker/#source-scala_2","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-WatermarkTracker/#updatewatermarkexecutedplan-sparkplan-unit","text":"updateWatermark requests the given physical operator ( SparkPlan ) to collect all EventTimeWatermarkExec unary physical operators. updateWatermark simply exits when no EventTimeWatermarkExec was found. updateWatermark ...FIXME NOTE: updateWatermark is used exclusively when MicroBatchExecution is requested to < > (when requested to < >). === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | globalWatermarkMs a| [[globalWatermarkMs]][[currentWatermark]] Current global event-time watermark per < > (across < > in a physical query plan) Default: 0 Used when...FIXME | operatorToWatermarkMap a| [[operatorToWatermarkMap]] Event-time watermark per EventTimeWatermarkExec physical operator ( mutable.HashMap[Int, Long] ) Used when...FIXME |===","title":"updateWatermark(executedPlan: SparkPlan): Unit"},{"location":"spark-sql-streaming-WriteToContinuousDataSource/","text":"== [[WriteToContinuousDataSource]] WriteToContinuousDataSource Unary Logical Operator [[children]] WriteToContinuousDataSource is a unary logical operator ( LogicalPlan ) that is < > exclusively when ContinuousExecution is requested to < > (to create an < >). WriteToContinuousDataSource is planned ( translated ) to a < > unary physical operator (when DataSourceV2Strategy execution planning strategy is requested to plan a logical query). TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy-DataSourceV2Strategy.html[DataSourceV2Strategy Execution Planning Strategy] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[creating-instance]] WriteToContinuousDataSource takes the following to be created: [[writer]] < > [[query]] Child logical operator ( LogicalPlan ) [[output]] WriteToContinuousDataSource uses empty output schema (which is exactly to say that no output is expected whatsoever).","title":"WriteToContinuousDataSource Unary Logical Operator"},{"location":"spark-sql-streaming-WriteToContinuousDataSourceExec/","text":"== [[WriteToContinuousDataSourceExec]] WriteToContinuousDataSourceExec Unary Physical Operator [[children]] WriteToContinuousDataSourceExec is a unary physical operator that < >. [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 WriteToContinuousDataSourceExec is < > exclusively when DataSourceV2Strategy execution planning strategy is requested to plan a < > unary logical operator. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy-DataSourceV2Strategy.html[DataSourceV2Strategy Execution Planning Strategy] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[creating-instance]] WriteToContinuousDataSourceExec takes the following to be created: [[writer]] < > [[query]][[child]] Child physical operator ( SparkPlan ) [[output]] WriteToContinuousDataSourceExec uses empty output schema (which is exactly to say that no output is expected whatsoever). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec=ALL Refer to < >. \u00b6 === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method [source, scala] \u00b6 doExecute(): RDD[InternalRow] \u00b6 NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). doExecute requests the < > to create a DataWriterFactory . doExecute then requests the < > to execute (that gives a RDD[InternalRow] ) and uses the RDD[InternalRow] and the DataWriterFactory to create a < >. doExecute prints out the following INFO message to the logs: Start processing data source writer: [writer]. The input RDD has [partitions] partitions. doExecute requests the EpochCoordinatorRef helper for a < > (using the < >). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. doExecute requests the EpochCoordinator RPC endpoint reference to send out a < > message synchronously. In the end, doExecute requests the ContinuousWriteRDD to collect (which simply runs a Spark job on all partitions in an RDD and returns the results in an array). NOTE: Requesting the ContinuousWriteRDD to collect is how a Spark job is ran that in turn runs tasks (one per partition) that are described by the < > method. Since executing collect is meant to run a Spark job (with tasks on executors), it's in the discretion of the tasks themselves to decide when to finish (so if they want to run indefinitely, so be it). What a clever trick!","title":"WriteToContinuousDataSourceExec Unary Physical Operator"},{"location":"spark-sql-streaming-WriteToContinuousDataSourceExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"spark-sql-streaming-WriteToContinuousDataSourceExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"WriteToContinuousDataSourceExec is < > exclusively when DataSourceV2Strategy execution planning strategy is requested to plan a < > unary logical operator. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkStrategy-DataSourceV2Strategy.html[DataSourceV2Strategy Execution Planning Strategy] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [[creating-instance]] WriteToContinuousDataSourceExec takes the following to be created: [[writer]] < > [[query]][[child]] Child physical operator ( SparkPlan ) [[output]] WriteToContinuousDataSourceExec uses empty output schema (which is exactly to say that no output is expected whatsoever). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.continuous.WriteToContinuousDataSourceExec=ALL","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"spark-sql-streaming-WriteToContinuousDataSourceExec/#refer-to","text":"=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method","title":"Refer to &lt;&gt;."},{"location":"spark-sql-streaming-WriteToContinuousDataSourceExec/#source-scala","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-WriteToContinuousDataSourceExec/#doexecute-rddinternalrow","text":"NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). doExecute requests the < > to create a DataWriterFactory . doExecute then requests the < > to execute (that gives a RDD[InternalRow] ) and uses the RDD[InternalRow] and the DataWriterFactory to create a < >. doExecute prints out the following INFO message to the logs: Start processing data source writer: [writer]. The input RDD has [partitions] partitions. doExecute requests the EpochCoordinatorRef helper for a < > (using the < >). NOTE: The < > runs on the driver as the single point to coordinate epochs across partition tasks. doExecute requests the EpochCoordinator RPC endpoint reference to send out a < > message synchronously. In the end, doExecute requests the ContinuousWriteRDD to collect (which simply runs a Spark job on all partitions in an RDD and returns the results in an array). NOTE: Requesting the ContinuousWriteRDD to collect is how a Spark job is ran that in turn runs tasks (one per partition) that are described by the < > method. Since executing collect is meant to run a Spark job (with tasks on executors), it's in the discretion of the tasks themselves to decide when to finish (so if they want to run indefinitely, so be it). What a clever trick!","title":"doExecute(): RDD[InternalRow]"},{"location":"spark-sql-streaming-aggregation/","text":"Streaming Aggregation \u00b6 In Spark Structured Streaming, a streaming aggregation is a streaming query that was described ( build ) using the following high-level streaming operators : Dataset.groupBy , Dataset.rollup , Dataset.cube (that simply create a RelationalGroupedDataset ) Dataset.groupByKey (that simply creates a KeyValueGroupedDataset ) SQL's GROUP BY clause (including WITH CUBE and WITH ROLLUP ) Streaming aggregation belongs to the category of Stateful Stream Processing . === [[IncrementalExecution]] IncrementalExecution -- QueryExecution of Streaming Queries Under the covers, the high-level operators create a logical query plan with one or more Aggregate logical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Aggregate.html[Aggregate ] logical operator in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. In Spark Structured Streaming < > is responsible for planning streaming queries for execution. At < >, IncrementalExecution uses the < > execution planning strategy for planning streaming aggregations ( Aggregate unary logical operators) as pairs of < > and < > physical operators. [source, scala] \u00b6 // input data from a data source // it's rate data source // but that does not really matter // We need a streaming Dataset val input = spark .readStream .format(\"rate\") .load // Streaming aggregation with groupBy val counts = input .groupBy($\"value\" % 2) .count counts.explain(extended = true) /** == Parsed Logical Plan == 'Aggregate [('value % 2)], [('value % 2) AS (value % 2)#23, count(1) AS count#22L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Analyzed Logical Plan == (value % 2): bigint, count: bigint Aggregate [(value#16L % cast(2 as bigint))], [(value#16L % cast(2 as bigint)) AS (value % 2)#23L, count(1) AS count#22L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Optimized Logical Plan == Aggregate [(value#16L % 2)], [(value#16L % 2) AS (value % 2)#23L, count(1) AS count#22L] +- Project [value#16L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Physical Plan == *(4) HashAggregate(keys=[(value#16L % 2)#27L], functions=[count(1)], output=[(value % 2)#23L, count#22L]) +- StateStoreSave [(value#16L % 2)#27L], state info [ checkpoint = , runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2 +- *(3) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- StateStoreRestore [(value#16L % 2)#27L], state info [ checkpoint = , runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], 2 +- *(2) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- Exchange hashpartitioning((value#16L % 2)#27L, 200) +- *(1) HashAggregate(keys=[(value#16L % 2) AS (value#16L % 2)#27L], functions=[partial_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- *(1) Project [value#16L] +- StreamingRelation rate, [timestamp#15, value#16L] */ === [[demos]] Demos Use the following demos to learn more: < > < > < > < >","title":"Streaming Aggregation"},{"location":"spark-sql-streaming-aggregation/#streaming-aggregation","text":"In Spark Structured Streaming, a streaming aggregation is a streaming query that was described ( build ) using the following high-level streaming operators : Dataset.groupBy , Dataset.rollup , Dataset.cube (that simply create a RelationalGroupedDataset ) Dataset.groupByKey (that simply creates a KeyValueGroupedDataset ) SQL's GROUP BY clause (including WITH CUBE and WITH ROLLUP ) Streaming aggregation belongs to the category of Stateful Stream Processing . === [[IncrementalExecution]] IncrementalExecution -- QueryExecution of Streaming Queries Under the covers, the high-level operators create a logical query plan with one or more Aggregate logical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Aggregate.html[Aggregate ] logical operator in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. In Spark Structured Streaming < > is responsible for planning streaming queries for execution. At < >, IncrementalExecution uses the < > execution planning strategy for planning streaming aggregations ( Aggregate unary logical operators) as pairs of < > and < > physical operators.","title":"Streaming Aggregation"},{"location":"spark-sql-streaming-aggregation/#source-scala","text":"// input data from a data source // it's rate data source // but that does not really matter // We need a streaming Dataset val input = spark .readStream .format(\"rate\") .load // Streaming aggregation with groupBy val counts = input .groupBy($\"value\" % 2) .count counts.explain(extended = true) /** == Parsed Logical Plan == 'Aggregate [('value % 2)], [('value % 2) AS (value % 2)#23, count(1) AS count#22L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Analyzed Logical Plan == (value % 2): bigint, count: bigint Aggregate [(value#16L % cast(2 as bigint))], [(value#16L % cast(2 as bigint)) AS (value % 2)#23L, count(1) AS count#22L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Optimized Logical Plan == Aggregate [(value#16L % 2)], [(value#16L % 2) AS (value % 2)#23L, count(1) AS count#22L] +- Project [value#16L] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@7879348, rate, [timestamp#15, value#16L] == Physical Plan == *(4) HashAggregate(keys=[(value#16L % 2)#27L], functions=[count(1)], output=[(value % 2)#23L, count#22L]) +- StateStoreSave [(value#16L % 2)#27L], state info [ checkpoint = , runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2 +- *(3) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- StateStoreRestore [(value#16L % 2)#27L], state info [ checkpoint = , runId = 8c0ae2be-5eaa-4038-bc29-a176abfaf885, opId = 0, ver = 0, numPartitions = 200], 2 +- *(2) HashAggregate(keys=[(value#16L % 2)#27L], functions=[merge_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- Exchange hashpartitioning((value#16L % 2)#27L, 200) +- *(1) HashAggregate(keys=[(value#16L % 2) AS (value#16L % 2)#27L], functions=[partial_count(1)], output=[(value#16L % 2)#27L, count#29L]) +- *(1) Project [value#16L] +- StreamingRelation rate, [timestamp#15, value#16L] */ === [[demos]] Demos Use the following demos to learn more: < > < > < > < >","title":"[source, scala]"},{"location":"spark-sql-streaming-continuous-stream-processing/","text":"== Continuous Stream Processing (Structured Streaming V2) Continuous Stream Processing is one of the two stream processing engines in < > that is used for execution of structured streaming queries with < > trigger. NOTE: The other feature-richer stream processing engine is < >. Continuous Stream Processing execution engine uses the novel Data Source API V2 (Spark SQL) and for the very first time makes stream processing truly continuous . TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-data-source-api-v2.html[Data Source API V2] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. Because of the two innovative changes Continuous Stream Processing is often referred to as Structured Streaming V2 . [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.Continuous(15.seconds)) // \u2190 Uses ContinuousExecution for execution .queryName(\"rate2console\") .start scala> :type sq org.apache.spark.sql.streaming.StreamingQuery assert(sq.isActive) // sq.stop \u00b6 Under the covers, Continuous Stream Processing uses < > stream execution engine. When requested to < >, ContinuousExecution adds < > physical operator as the top-level operator in the physical query plan of the streaming query. [source, scala] \u00b6 scala> :type sq org.apache.spark.sql.streaming.StreamingQuery scala> sq.explain == Physical Plan == WriteToContinuousDataSource ConsoleWriter[numRows=20, truncate=false] +- *(1) Project [timestamp#758, value#759L] +- *(1) ScanV2 rate[timestamp#758, value#759L] From now on, you may think of a streaming query as a soon-to-be-generated < > - an RDD data structure that Spark developers use to describe a distributed computation. When the streaming query is started (and the top-level WriteToContinuousDataSourceExec physical operator is requested to < >), it simply requests the underlying ContinuousWriteRDD to collect. That collect operator is how a Spark job is run (as tasks over all partitions of the RDD) as described by the < > \"protocol\" (a recipe for the tasks to be scheduled to run on Spark executors). .Creating Instance of StreamExecution image::images/webui-spark-job-streaming-query-started.png[align=\"center\"] While the < > (of the ContinuousWriteRDD ), they keep running < >. And that's the ingenious design trick of how the streaming query (as a Spark job with the distributed tasks running on executors) runs continuously and indefinitely. When DataStreamReader is requested to < >, it creates...FIXME","title":"Continuous Stream Processing"},{"location":"spark-sql-streaming-continuous-stream-processing/#source-scala","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.Continuous(15.seconds)) // \u2190 Uses ContinuousExecution for execution .queryName(\"rate2console\") .start scala> :type sq org.apache.spark.sql.streaming.StreamingQuery assert(sq.isActive)","title":"[source, scala]"},{"location":"spark-sql-streaming-continuous-stream-processing/#sqstop","text":"Under the covers, Continuous Stream Processing uses < > stream execution engine. When requested to < >, ContinuousExecution adds < > physical operator as the top-level operator in the physical query plan of the streaming query.","title":"// sq.stop"},{"location":"spark-sql-streaming-continuous-stream-processing/#source-scala_1","text":"scala> :type sq org.apache.spark.sql.streaming.StreamingQuery scala> sq.explain == Physical Plan == WriteToContinuousDataSource ConsoleWriter[numRows=20, truncate=false] +- *(1) Project [timestamp#758, value#759L] +- *(1) ScanV2 rate[timestamp#758, value#759L] From now on, you may think of a streaming query as a soon-to-be-generated < > - an RDD data structure that Spark developers use to describe a distributed computation. When the streaming query is started (and the top-level WriteToContinuousDataSourceExec physical operator is requested to < >), it simply requests the underlying ContinuousWriteRDD to collect. That collect operator is how a Spark job is run (as tasks over all partitions of the RDD) as described by the < > \"protocol\" (a recipe for the tasks to be scheduled to run on Spark executors). .Creating Instance of StreamExecution image::images/webui-spark-job-streaming-query-started.png[align=\"center\"] While the < > (of the ContinuousWriteRDD ), they keep running < >. And that's the ingenious design trick of how the streaming query (as a Spark job with the distributed tasks running on executors) runs continuously and indefinitely. When DataStreamReader is requested to < >, it creates...FIXME","title":"[source, scala]"},{"location":"spark-sql-streaming-deduplication/","text":"== Streaming Deduplication Streaming Deduplication is...FIXME","title":"Streaming Deduplication"},{"location":"spark-sql-streaming-extending-new-data-sources/","text":"Extending Structured Streaming with New Data Sources \u00b6 Spark Structured Streaming uses Spark SQL for planning streaming queries ( preparing for execution ). Spark SQL is migrating from the former Data Source API V1 to a new Data Source API V2, and so is Structured Streaming. That is exactly the reason for < > and < > APIs for the two different Data Source API's class hierarchies, for streaming sources and sinks, respectively. Structured Streaming supports two stream execution engines (i.e. Micro-Batch and Continuous ) with their own APIs. < > supports the old Data Source API V1 and the new modern Data Source API V2 with micro-batch-specific APIs for streaming sources and sinks. < > supports the new modern Data Source API V2 only with continuous-specific APIs for streaming sources and sinks. The following are the questions to think of (and answer) while considering development of a new data source for Structured Streaming. They are supposed to give you a sense of how much work and time it takes as well as what Spark version to support (e.g. 2.2 vs 2.4). Data Source API V1 Data Source API V2 < > ( Structured Streaming V1 ) < > ( Structured Streaming V2 ) Read side (< >) Write side (< >)","title":"Extending Structured Streaming with New Data Sources"},{"location":"spark-sql-streaming-extending-new-data-sources/#extending-structured-streaming-with-new-data-sources","text":"Spark Structured Streaming uses Spark SQL for planning streaming queries ( preparing for execution ). Spark SQL is migrating from the former Data Source API V1 to a new Data Source API V2, and so is Structured Streaming. That is exactly the reason for < > and < > APIs for the two different Data Source API's class hierarchies, for streaming sources and sinks, respectively. Structured Streaming supports two stream execution engines (i.e. Micro-Batch and Continuous ) with their own APIs. < > supports the old Data Source API V1 and the new modern Data Source API V2 with micro-batch-specific APIs for streaming sources and sinks. < > supports the new modern Data Source API V2 only with continuous-specific APIs for streaming sources and sinks. The following are the questions to think of (and answer) while considering development of a new data source for Structured Streaming. They are supposed to give you a sense of how much work and time it takes as well as what Spark version to support (e.g. 2.2 vs 2.4). Data Source API V1 Data Source API V2 < > ( Structured Streaming V1 ) < > ( Structured Streaming V2 ) Read side (< >) Write side (< >)","title":"Extending Structured Streaming with New Data Sources"},{"location":"spark-sql-streaming-join/","text":"Streaming Join \u00b6 [[operators]] In Spark Structured Streaming, a streaming join is a streaming query that was described ( build ) using the high-level streaming operators : Dataset.crossJoin Dataset.join Dataset.joinWith SQL's JOIN clause Streaming joins can be stateless or < >: Joins of a streaming query and a batch query ( stream-static joins ) are stateless and no state management is required Joins of two streaming queries (< >) are stateful and require streaming state (with an optional < >). === [[stream-stream-joins]] Stream-Stream Joins Spark Structured Streaming supports stream-stream joins with the following: < > (i.e. https://en.wikipedia.org/wiki/Join_(SQL)#Equi-join[equi-joins ] that use only equality comparisons in the join predicate) Inner , LeftOuter , and RightOuter < > Stream-stream equi-joins are planned as < > physical operators of two ShuffleExchangeExec physical operators (per < >). === [[join-state-watermark]] Join State Watermark for State Removal Stream-stream joins may optionally define Join State Watermark for state removal (cf. < >). A join state watermark can be specified on the following: . < > ( key state ) . < > ( value state ) A join state watermark can be specified on key state, value state or both. === [[IncrementalExecution]] IncrementalExecution -- QueryExecution of Streaming Queries Under the covers, the < > create a logical query plan with one or more Join logical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Join.html[Join Logical Operator] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. In Spark Structured Streaming < > is responsible for planning streaming queries for execution. At < >, IncrementalExecution uses the < > execution planning strategy for planning < > as < > physical operators. === [[demos]] Demos Use the following demo application to learn more: https://github.com/jaceklaskowski/spark-structured-streaming-book/tree/v3.0.1/examples/src/main/scala/pl/japila/spark/StreamStreamJoinApp.scala[StreamStreamJoinApp ] === [[i-want-more]] Further Reading Or Watching https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins[Stream-stream Joins] in the official documentation of Apache Spark for Structured Streaming https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html[Introducing Stream-Stream Joins in Apache Spark 2.3] by Databricks (video) https://databricks.com/session/deep-dive-into-stateful-stream-processing-in-structured-streaming[Deep Dive into Stateful Stream Processing in Structured Streaming] by Tathagata Das","title":"Streaming Join"},{"location":"spark-sql-streaming-join/#streaming-join","text":"[[operators]] In Spark Structured Streaming, a streaming join is a streaming query that was described ( build ) using the high-level streaming operators : Dataset.crossJoin Dataset.join Dataset.joinWith SQL's JOIN clause Streaming joins can be stateless or < >: Joins of a streaming query and a batch query ( stream-static joins ) are stateless and no state management is required Joins of two streaming queries (< >) are stateful and require streaming state (with an optional < >). === [[stream-stream-joins]] Stream-Stream Joins Spark Structured Streaming supports stream-stream joins with the following: < > (i.e. https://en.wikipedia.org/wiki/Join_(SQL)#Equi-join[equi-joins ] that use only equality comparisons in the join predicate) Inner , LeftOuter , and RightOuter < > Stream-stream equi-joins are planned as < > physical operators of two ShuffleExchangeExec physical operators (per < >). === [[join-state-watermark]] Join State Watermark for State Removal Stream-stream joins may optionally define Join State Watermark for state removal (cf. < >). A join state watermark can be specified on the following: . < > ( key state ) . < > ( value state ) A join state watermark can be specified on key state, value state or both. === [[IncrementalExecution]] IncrementalExecution -- QueryExecution of Streaming Queries Under the covers, the < > create a logical query plan with one or more Join logical operators. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Join.html[Join Logical Operator] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. In Spark Structured Streaming < > is responsible for planning streaming queries for execution. At < >, IncrementalExecution uses the < > execution planning strategy for planning < > as < > physical operators. === [[demos]] Demos Use the following demo application to learn more: https://github.com/jaceklaskowski/spark-structured-streaming-book/tree/v3.0.1/examples/src/main/scala/pl/japila/spark/StreamStreamJoinApp.scala[StreamStreamJoinApp ] === [[i-want-more]] Further Reading Or Watching https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins[Stream-stream Joins] in the official documentation of Apache Spark for Structured Streaming https://databricks.com/blog/2018/03/13/introducing-stream-stream-joins-in-apache-spark-2-3.html[Introducing Stream-Stream Joins in Apache Spark 2.3] by Databricks (video) https://databricks.com/session/deep-dive-into-stateful-stream-processing-in-structured-streaming[Deep Dive into Stateful Stream Processing in Structured Streaming] by Tathagata Das","title":"Streaming Join"},{"location":"spark-sql-streaming-kafka-data-source/","text":"== Kafka Data Source -- Streaming Data Source for Apache Kafka Kafka Data Source is the streaming data source for https://kafka.apache.org/[Apache Kafka] in Spark Structured Streaming. Kafka Data Source provides a < > and a < > for < > and < > stream processing. === [[spark-sql-kafka-0-10]] spark-sql-kafka-0-10 External Module Kafka Data Source is part of the spark-sql-kafka-0-10 external module that is distributed with the official distribution of Apache Spark, but it is not included in the CLASSPATH by default. You should define spark-sql-kafka-0-10 module as part of the build definition in your Spark project, e.g. as a libraryDependency in build.sbt for sbt: libraryDependencies += \"org.apache.spark\" %% \"spark-sql-kafka-0-10\" % \"3.0.1\" For Spark environments like spark-submit (and \"derivatives\" like spark-shell ), you should use --packages command-line option: ./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 NOTE: Replace the version of spark-sql-kafka-0-10 module (e.g. 3.0.1 above) with one of the available versions found at https://search.maven.org/search?q=a:spark-sql-kafka-0-10_2.12[The Central Repository's Search] that matches your version of Apache Spark. === [[streaming-source]] Streaming Source With < > you can use kafka data source format for loading data (reading records) from one or more Kafka topics as a streaming Dataset. [source, scala] \u00b6 val records = spark .readStream .format(\"kafka\") .option(\"subscribePattern\", \"\"\"topic-\\d{2}\"\"\") // topics with two digits at the end .option(\"kafka.bootstrap.servers\", \":9092\") .load Kafka data source supports many options for reading. Internally, the kafka data source format for reading is available through < > that is a < > and < > for < > and < > stream processing, respectively. === [[schema]] Predefined (Fixed) Schema Kafka Data Source uses a predefined (fixed) schema. .Kafka Data Source's Fixed Schema (in the positional order) [cols=\"1m,2m\",options=\"header\",width=\"100%\"] |=== | Name | Type | key | BinaryType | value | BinaryType | topic | StringType | partition | IntegerType | offset | LongType | timestamp | TimestampType | timestampType | IntegerType |=== [source, scala] \u00b6 scala> records.printSchema root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) Internally, the fixed schema is defined as part of the DataSourceReader contract through < > and < > extension contracts for < > and < > stream processing, respectively. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceReader.html[DataSourceReader ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. [TIP] \u00b6 Use Column.cast operator to cast BinaryType to a StringType (for key and value columns). [source, scala] \u00b6 scala> :type records org.apache.spark.sql.DataFrame val values = records .select($\"value\" cast \"string\") // deserializing values scala> values.printSchema root |-- value: string (nullable = true) ==== === [[streaming-sink]] Streaming Sink With < > you can use kafka data source format for writing the result of executing a streaming query (a streaming Dataset) to one or more Kafka topics. [source, scala] \u00b6 val sq = records .writeStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \":9092\") .option(\"topic\", \"kafka2console-output\") .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") .start Internally, the kafka data source format for writing is available through < > that is a < >. === [[micro-batch-stream-processing]] Micro-Batch Stream Processing Kafka Data Source supports < > (i.e. < > and < > triggers) via < >. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"kafka\") .option(\"subscribepattern\", \"kafka2console.*\") .option(\"kafka.bootstrap.servers\", \":9092\") .load .withColumn(\"value\", $\"value\" cast \"string\") // deserializing values .writeStream .format(\"console\") .option(\"truncate\", false) // format-specific option .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option .trigger(Trigger.ProcessingTime(30.seconds)) .queryName(\"kafka2console-microbatch\") .start // In the end, stop the streaming query sq.stop Kafka Data Source can assign a single task per Kafka partition (using < > in < >). Kafka Data Source can reuse a Kafka consumer (using < > in < >). === [[continuous-stream-processing]] Continuous Stream Processing Kafka Data Source supports < > (i.e. < > trigger) via < >. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"kafka\") .option(\"subscribepattern\", \"kafka2console.*\") .option(\"kafka.bootstrap.servers\", \":9092\") .load .withColumn(\"value\", $\"value\" cast \"string\") // convert bytes to string for display purposes .writeStream .format(\"console\") .option(\"truncate\", false) // format-specific option .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option .queryName(\"kafka2console-continuous\") .trigger(Trigger.Continuous(10.seconds)) .start // In the end, stop the streaming query sq.stop === [[options]] Configuration Options NOTE: Options with kafka. prefix (e.g. < >) are considered configuration properties for the Kafka consumers used on the < > and < >. .Kafka Data Source's Options (Case-Insensitive) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Option | Description | assign a| [[assign]] spark-sql-streaming-ConsumerStrategy.md#AssignStrategy[Topic subscription strategy] that accepts a JSON with topic names and partitions, e.g. {\"topicA\":[0,1],\"topicB\":[0,1]} NOTE: Exactly one topic subscription strategy is allowed (that KafkaSourceProvider spark-sql-streaming-KafkaSourceProvider.md#validateGeneralOptions[validates] before creating KafkaSource ). | failOnDataLoss a| [[failOnDataLoss]] Flag to control whether...FIXME Default: true Used when KafkaSourceProvider is requested for < > | kafka.bootstrap.servers a| [[kafka.bootstrap.servers]] (required) bootstrap.servers configuration property of the Kafka consumers used on the driver and executors Default: (empty) | kafkaConsumer.pollTimeoutMs a| [[kafkaConsumer.pollTimeoutMs]][[pollTimeoutMs]] The time (in milliseconds) spent waiting in Consumer.poll if data is not available in the buffer. Default: spark.network.timeout or 120s Used when...FIXME | maxOffsetsPerTrigger a| [[maxOffsetsPerTrigger]] Number of records to fetch per trigger (to limit the number of records to fetch). Default: (undefined) Unless defined, KafkaSource requests < > for the spark-sql-streaming-KafkaOffsetReader.md#fetchLatestOffsets[latest offsets]. | minPartitions a| [[minPartitions]] Minimum number of partitions per executor (given Kafka partitions) Default: (undefined) Must be undefined (default) or greater than 0 When undefined (default) or smaller than the number of TopicPartitions with records to consume from, < > uses < > to < > for every TopicPartition (and the < >). | startingOffsets a| [[startingOffsets]] Starting offsets Default: latest Possible values: latest earliest JSON with topics, partitions and their starting offsets, e.g. + {\"topicA\":{\"part\":offset,\"p1\":-1},\"topicB\":{\"0\":-2}} [TIP] \u00b6 Use Scala's tripple quotes for the JSON for topics, partitions and offsets. [source, scala] \u00b6 option( \"startingOffsets\", \"\"\"{\"topic1\":{\"0\":5,\"4\":-1},\"topic2\":{\"0\":-2}}\"\"\") ==== | subscribe a| [[subscribe]] spark-sql-streaming-ConsumerStrategy.md#SubscribeStrategy[Topic subscription strategy] that accepts topic names as a comma-separated string, e.g. topic1,topic2,topic3 NOTE: Exactly one topic subscription strategy is allowed (that KafkaSourceProvider spark-sql-streaming-KafkaSourceProvider.md#validateGeneralOptions[validates] before creating KafkaSource ). | subscribepattern a| [[subscribepattern]] spark-sql-streaming-ConsumerStrategy.md#SubscribePatternStrategy[Topic subscription strategy] that uses Java's http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern ] for the topic subscription regex pattern of topics to subscribe to, e.g. topic\\d [TIP] \u00b6 Use Scala's tripple quotes for the regular expression for topic subscription regex pattern. [source, scala] \u00b6 option(\"subscribepattern\", \"\"\"topic\\d\"\"\") \u00b6 ==== NOTE: Exactly one topic subscription strategy is allowed (that KafkaSourceProvider spark-sql-streaming-KafkaSourceProvider.md#validateGeneralOptions[validates] before creating KafkaSource ). | topic a| [[topic]] Optional topic name to use for writing a streaming query Default: (empty) Unless defined, Kafka data source uses the topic names as defined in the topic field in the incoming data. |=== === [[logical-query-plan-for-reading]] Logical Query Plan for Reading When DataStreamReader is requested to load a dataset with kafka data source format, it creates a DataFrame with a < > leaf logical operator. [source, scala] \u00b6 scala> records.explain(extended = true) == Parsed Logical Plan == StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1a366d0, kafka, Map(maxOffsetsPerTrigger -> 1, startingOffsets -> latest, subscribepattern -> topic\\d, kafka.bootstrap.servers -> :9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@39b3de87,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 1, startingOffsets -> latest, subscribepattern -> topic\\d, kafka.bootstrap.servers -> :9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] ... === [[logical-query-plan-for-writing]] Logical Query Plan for Writing When DataStreamWriter is requested to start a streaming query with kafka data source format for writing, it requests the StreamingQueryManager to create a streaming query that in turn creates (a < > with) a < > or a < > for < > and < > stream processing, respectively. [source, scala] \u00b6 scala> sq.explain(extended = true) == Parsed Logical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@bf98b73 +- Project [key#28 AS key#7, value#29 AS value#8, topic#30 AS topic#9, partition#31 AS partition#10, offset#32L AS offset#11L, timestamp#33 AS timestamp#12, timestampType#34 AS timestampType#13] +- Streaming RelationV2 kafka[key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34] (Options: [subscribepattern=kafka2console.*,kafka.bootstrap.servers=:9092]) === [[demo]] Demo: Streaming Aggregation with Kafka Data Source Check out < >. [TIP] \u00b6 Use the following to publish events to Kafka. // 1st streaming batch $ cat /tmp/1 1,1,1 15,2,1 $ kafkacat -P -b localhost:9092 -t topic1 -l /tmp/1 // Alternatively (and slower due to JVM bootup) $ cat /tmp/1 | ./bin/kafka-console-producer.sh --topic topic1 --broker-list localhost:9092 \u00b6","title":"Kafka Data Source"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala","text":"val records = spark .readStream .format(\"kafka\") .option(\"subscribePattern\", \"\"\"topic-\\d{2}\"\"\") // topics with two digits at the end .option(\"kafka.bootstrap.servers\", \":9092\") .load Kafka data source supports many options for reading. Internally, the kafka data source format for reading is available through < > that is a < > and < > for < > and < > stream processing, respectively. === [[schema]] Predefined (Fixed) Schema Kafka Data Source uses a predefined (fixed) schema. .Kafka Data Source's Fixed Schema (in the positional order) [cols=\"1m,2m\",options=\"header\",width=\"100%\"] |=== | Name | Type | key | BinaryType | value | BinaryType | topic | StringType | partition | IntegerType | offset | LongType | timestamp | TimestampType | timestampType | IntegerType |===","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala_1","text":"scala> records.printSchema root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) Internally, the fixed schema is defined as part of the DataSourceReader contract through < > and < > extension contracts for < > and < > stream processing, respectively. TIP: Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataSourceReader.html[DataSourceReader ] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book.","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#tip","text":"Use Column.cast operator to cast BinaryType to a StringType (for key and value columns).","title":"[TIP]"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala_2","text":"scala> :type records org.apache.spark.sql.DataFrame val values = records .select($\"value\" cast \"string\") // deserializing values scala> values.printSchema root |-- value: string (nullable = true) ==== === [[streaming-sink]] Streaming Sink With < > you can use kafka data source format for writing the result of executing a streaming query (a streaming Dataset) to one or more Kafka topics.","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala_3","text":"val sq = records .writeStream .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \":9092\") .option(\"topic\", \"kafka2console-output\") .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") .start Internally, the kafka data source format for writing is available through < > that is a < >. === [[micro-batch-stream-processing]] Micro-Batch Stream Processing Kafka Data Source supports < > (i.e. < > and < > triggers) via < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala_4","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"kafka\") .option(\"subscribepattern\", \"kafka2console.*\") .option(\"kafka.bootstrap.servers\", \":9092\") .load .withColumn(\"value\", $\"value\" cast \"string\") // deserializing values .writeStream .format(\"console\") .option(\"truncate\", false) // format-specific option .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option .trigger(Trigger.ProcessingTime(30.seconds)) .queryName(\"kafka2console-microbatch\") .start // In the end, stop the streaming query sq.stop Kafka Data Source can assign a single task per Kafka partition (using < > in < >). Kafka Data Source can reuse a Kafka consumer (using < > in < >). === [[continuous-stream-processing]] Continuous Stream Processing Kafka Data Source supports < > (i.e. < > trigger) via < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala_5","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = spark .readStream .format(\"kafka\") .option(\"subscribepattern\", \"kafka2console.*\") .option(\"kafka.bootstrap.servers\", \":9092\") .load .withColumn(\"value\", $\"value\" cast \"string\") // convert bytes to string for display purposes .writeStream .format(\"console\") .option(\"truncate\", false) // format-specific option .option(\"checkpointLocation\", \"checkpointLocation-kafka2console\") // generic query option .queryName(\"kafka2console-continuous\") .trigger(Trigger.Continuous(10.seconds)) .start // In the end, stop the streaming query sq.stop === [[options]] Configuration Options NOTE: Options with kafka. prefix (e.g. < >) are considered configuration properties for the Kafka consumers used on the < > and < >. .Kafka Data Source's Options (Case-Insensitive) [cols=\"1m,3\",options=\"header\",width=\"100%\"] |=== | Option | Description | assign a| [[assign]] spark-sql-streaming-ConsumerStrategy.md#AssignStrategy[Topic subscription strategy] that accepts a JSON with topic names and partitions, e.g. {\"topicA\":[0,1],\"topicB\":[0,1]} NOTE: Exactly one topic subscription strategy is allowed (that KafkaSourceProvider spark-sql-streaming-KafkaSourceProvider.md#validateGeneralOptions[validates] before creating KafkaSource ). | failOnDataLoss a| [[failOnDataLoss]] Flag to control whether...FIXME Default: true Used when KafkaSourceProvider is requested for < > | kafka.bootstrap.servers a| [[kafka.bootstrap.servers]] (required) bootstrap.servers configuration property of the Kafka consumers used on the driver and executors Default: (empty) | kafkaConsumer.pollTimeoutMs a| [[kafkaConsumer.pollTimeoutMs]][[pollTimeoutMs]] The time (in milliseconds) spent waiting in Consumer.poll if data is not available in the buffer. Default: spark.network.timeout or 120s Used when...FIXME | maxOffsetsPerTrigger a| [[maxOffsetsPerTrigger]] Number of records to fetch per trigger (to limit the number of records to fetch). Default: (undefined) Unless defined, KafkaSource requests < > for the spark-sql-streaming-KafkaOffsetReader.md#fetchLatestOffsets[latest offsets]. | minPartitions a| [[minPartitions]] Minimum number of partitions per executor (given Kafka partitions) Default: (undefined) Must be undefined (default) or greater than 0 When undefined (default) or smaller than the number of TopicPartitions with records to consume from, < > uses < > to < > for every TopicPartition (and the < >). | startingOffsets a| [[startingOffsets]] Starting offsets Default: latest Possible values: latest earliest JSON with topics, partitions and their starting offsets, e.g. + {\"topicA\":{\"part\":offset,\"p1\":-1},\"topicB\":{\"0\":-2}}","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#tip_1","text":"Use Scala's tripple quotes for the JSON for topics, partitions and offsets.","title":"[TIP]"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala_6","text":"option( \"startingOffsets\", \"\"\"{\"topic1\":{\"0\":5,\"4\":-1},\"topic2\":{\"0\":-2}}\"\"\") ==== | subscribe a| [[subscribe]] spark-sql-streaming-ConsumerStrategy.md#SubscribeStrategy[Topic subscription strategy] that accepts topic names as a comma-separated string, e.g. topic1,topic2,topic3 NOTE: Exactly one topic subscription strategy is allowed (that KafkaSourceProvider spark-sql-streaming-KafkaSourceProvider.md#validateGeneralOptions[validates] before creating KafkaSource ). | subscribepattern a| [[subscribepattern]] spark-sql-streaming-ConsumerStrategy.md#SubscribePatternStrategy[Topic subscription strategy] that uses Java's http://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[java.util.regex.Pattern ] for the topic subscription regex pattern of topics to subscribe to, e.g. topic\\d","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#tip_2","text":"Use Scala's tripple quotes for the regular expression for topic subscription regex pattern.","title":"[TIP]"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala_7","text":"","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#optionsubscribepattern-topicd","text":"==== NOTE: Exactly one topic subscription strategy is allowed (that KafkaSourceProvider spark-sql-streaming-KafkaSourceProvider.md#validateGeneralOptions[validates] before creating KafkaSource ). | topic a| [[topic]] Optional topic name to use for writing a streaming query Default: (empty) Unless defined, Kafka data source uses the topic names as defined in the topic field in the incoming data. |=== === [[logical-query-plan-for-reading]] Logical Query Plan for Reading When DataStreamReader is requested to load a dataset with kafka data source format, it creates a DataFrame with a < > leaf logical operator.","title":"option(\"subscribepattern\", \"\"\"topic\\d\"\"\")"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala_8","text":"scala> records.explain(extended = true) == Parsed Logical Plan == StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@1a366d0, kafka, Map(maxOffsetsPerTrigger -> 1, startingOffsets -> latest, subscribepattern -> topic\\d, kafka.bootstrap.servers -> :9092), [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@39b3de87,kafka,List(),None,List(),None,Map(maxOffsetsPerTrigger -> 1, startingOffsets -> latest, subscribepattern -> topic\\d, kafka.bootstrap.servers -> :9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] ... === [[logical-query-plan-for-writing]] Logical Query Plan for Writing When DataStreamWriter is requested to start a streaming query with kafka data source format for writing, it requests the StreamingQueryManager to create a streaming query that in turn creates (a < > with) a < > or a < > for < > and < > stream processing, respectively.","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#source-scala_9","text":"scala> sq.explain(extended = true) == Parsed Logical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@bf98b73 +- Project [key#28 AS key#7, value#29 AS value#8, topic#30 AS topic#9, partition#31 AS partition#10, offset#32L AS offset#11L, timestamp#33 AS timestamp#12, timestampType#34 AS timestampType#13] +- Streaming RelationV2 kafka[key#28, value#29, topic#30, partition#31, offset#32L, timestamp#33, timestampType#34] (Options: [subscribepattern=kafka2console.*,kafka.bootstrap.servers=:9092]) === [[demo]] Demo: Streaming Aggregation with Kafka Data Source Check out < >.","title":"[source, scala]"},{"location":"spark-sql-streaming-kafka-data-source/#tip_3","text":"Use the following to publish events to Kafka.","title":"[TIP]"},{"location":"spark-sql-streaming-kafka-data-source/#1st-streaming-batch-cat-tmp1-111-1521-kafkacat-p-b-localhost9092-t-topic1-l-tmp1-alternatively-and-slower-due-to-jvm-bootup-cat-tmp1-binkafka-console-producersh-topic-topic1-broker-list-localhost9092","text":"","title":"// 1st streaming batch\n$ cat /tmp/1\n1,1,1\n15,2,1\n\n$ kafkacat -P -b localhost:9092 -t topic1 -l /tmp/1\n\n// Alternatively (and slower due to JVM bootup)\n$ cat /tmp/1 | ./bin/kafka-console-producer.sh --topic topic1 --broker-list localhost:9092\n"},{"location":"spark-sql-streaming-limit/","text":"== Streaming Limit Streaming Limit is...FIXME","title":"Streaming Limit"},{"location":"spark-sql-streaming-memory-data-source/","text":"== Memory Data Source Memory Data Source is made up of the following two base implementations to support the older DataSource API V1 and the modern DataSource API V2: < > < > Memory data source supports < > and < > stream processing modes. [cols=\"30,35,35\",options=\"header\",width=\"100%\"] |=== | Stream Processing | Source | Sink | < > | < > | < > | < > | < > | < > |=== [CAUTION] \u00b6 Memory Data Source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery. MemoryStream is designed primarily for unit tests, tutorials and debugging. \u00b6 === [[memory-sink]] Memory Sink Memory sink requires that a streaming query has a name (defined using DataStreamWriter.queryName or queryName option). Memory sink may optionally define checkpoint location using checkpointLocation option that is used to recover from for < > output mode only. Memory Sink and CreateViewCommand \u00b6 When a streaming query with memory sink is started , DataStreamWriter uses Dataset.createOrReplaceTempView operator to create or replace a local temporary view with the name of the query (which is required). Examples \u00b6 .Memory Source in Micro-Batch Stream Processing [source, scala] val spark: SparkSession = ??? implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream // It uses two implicits: Encoder[Int] and SQLContext val intsIn = MemoryStream[Int] val ints = intsIn.toDF .withColumn(\"t\", current_timestamp()) .withWatermark(\"t\", \"5 minutes\") .groupBy(window($\"t\", \"5 minutes\") as \"window\") .agg(count(\"*\") as \"total\") import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val totalsOver5mins = ints. writeStream. format(\"memory\"). queryName(\"totalsOver5mins\"). outputMode(OutputMode.Append). trigger(Trigger.ProcessingTime(10.seconds)). start val zeroOffset = intsIn.addData(0, 1, 2) totalsOver5mins.processAllAvailable() spark.table(\"totalsOver5mins\").show scala> intsOut.show +-----+ |value| +-----+ | 0| | 1| | 2| +-----+ memoryQuery.stop() \u00b6 .Memory Sink in Micro-Batch Stream Processing [source, scala] val queryName = \"memoryDemo\" val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"memory\") .queryName(queryName) .start // The name of the streaming query is an in-memory table val showAll = sql(s\"select * from $queryName\") scala> showAll.show(truncate = false) +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2019-10-10 15:19:16.431|42 | |2019-10-10 15:19:17.431|43 | +-----------------------+-----+ import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.MemorySink val sink = se.sink.asInstanceOf[MemorySink] assert(sink.toString == \"MemorySink\") sink.clear() \u00b6","title":"Memory Data Source"},{"location":"spark-sql-streaming-memory-data-source/#caution","text":"Memory Data Source is not for production use due to design contraints, e.g. infinite in-memory collection of lines read and no fault recovery.","title":"[CAUTION]"},{"location":"spark-sql-streaming-memory-data-source/#memorystream-is-designed-primarily-for-unit-tests-tutorials-and-debugging","text":"=== [[memory-sink]] Memory Sink Memory sink requires that a streaming query has a name (defined using DataStreamWriter.queryName or queryName option). Memory sink may optionally define checkpoint location using checkpointLocation option that is used to recover from for < > output mode only.","title":"MemoryStream is designed primarily for unit tests, tutorials and debugging."},{"location":"spark-sql-streaming-memory-data-source/#memory-sink-and-createviewcommand","text":"When a streaming query with memory sink is started , DataStreamWriter uses Dataset.createOrReplaceTempView operator to create or replace a local temporary view with the name of the query (which is required).","title":"Memory Sink and CreateViewCommand"},{"location":"spark-sql-streaming-memory-data-source/#examples","text":".Memory Source in Micro-Batch Stream Processing [source, scala] val spark: SparkSession = ??? implicit val ctx = spark.sqlContext import org.apache.spark.sql.execution.streaming.MemoryStream // It uses two implicits: Encoder[Int] and SQLContext val intsIn = MemoryStream[Int] val ints = intsIn.toDF .withColumn(\"t\", current_timestamp()) .withWatermark(\"t\", \"5 minutes\") .groupBy(window($\"t\", \"5 minutes\") as \"window\") .agg(count(\"*\") as \"total\") import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val totalsOver5mins = ints. writeStream. format(\"memory\"). queryName(\"totalsOver5mins\"). outputMode(OutputMode.Append). trigger(Trigger.ProcessingTime(10.seconds)). start val zeroOffset = intsIn.addData(0, 1, 2) totalsOver5mins.processAllAvailable() spark.table(\"totalsOver5mins\").show scala> intsOut.show +-----+ |value| +-----+ | 0| | 1| | 2| +-----+","title":"Examples"},{"location":"spark-sql-streaming-memory-data-source/#memoryquerystop","text":".Memory Sink in Micro-Batch Stream Processing [source, scala] val queryName = \"memoryDemo\" val sq = spark .readStream .format(\"rate\") .load .writeStream .format(\"memory\") .queryName(queryName) .start // The name of the streaming query is an in-memory table val showAll = sql(s\"select * from $queryName\") scala> showAll.show(truncate = false) +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2019-10-10 15:19:16.431|42 | |2019-10-10 15:19:17.431|43 | +-----------------------+-----+ import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery import org.apache.spark.sql.execution.streaming.MemorySink val sink = se.sink.asInstanceOf[MemorySink] assert(sink.toString == \"MemorySink\")","title":"memoryQuery.stop()"},{"location":"spark-sql-streaming-memory-data-source/#sinkclear","text":"","title":"sink.clear()"},{"location":"spark-sql-streaming-offsets-and-metadata-checkpointing/","text":"Offsets and Metadata Checkpointing \u00b6 A streaming query can be started from scratch or from checkpoint (that gives fault-tolerance as the state is preserved even when a failure happens). Stream execution engines use checkpoint location to resume stream processing and get start offsets to start query processing from. StreamExecution resumes (populates the start offsets) from the latest checkpointed offsets from the Write-Ahead Log (WAL) of Offsets that may have already been processed (and, if so, committed to the Offset Commit Log ). < > of < > < > and < > < > and StreamExecutions ( committed and available offsets) Micro-Batch Stream Processing \u00b6 In < >, the available offsets registry is populated with the < > from the Write-Ahead Log (WAL) when MicroBatchExecution stream processing engine is requested to < > (if available) when MicroBatchExecution is requested to < > ( before the first \"zero\" micro-batch ). The available offsets are then < > to the committed offsets when the latest batch ID available (as described above) is exactly the < > committed to the Offset Commit Log when MicroBatchExecution stream processing engine is requested to < >. When a streaming query is started from scratch (with no checkpoint that has offsets in the Offset Write-Ahead Log ), MicroBatchExecution prints out the following INFO message: Starting new streaming query. When a streaming query is resumed (restarted) from a checkpoint with offsets in the Offset Write-Ahead Log , MicroBatchExecution prints out the following INFO message: Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets] Every time MicroBatchExecution is requested to < > (in any of the streaming sources)...FIXME When MicroBatchExecution is requested to < > (when MicroBatchExecution requested to < >), every streaming source is requested for the latest offset available that are < > to the availableOffsets registry. < > report some offsets or none at all (if this source has never received any data). Streaming sources with no data are excluded ( filtered out ). MicroBatchExecution prints out the following TRACE message to the logs: noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch] With < > internal flag enabled, MicroBatchExecution commits (< >) the available offsets for the batch to the Write-Ahead Log (WAL) and prints out the following INFO message to the logs: Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata] When < >, MicroBatchExecution requests every Source and < > (in the availableOffsets registry) for unprocessed data (that has not been committed yet and so considered unprocessed). In the end (of < >), MicroBatchExecution commits (< >) the available offsets (to the < > registry) so they are considered processed already. MicroBatchExecution prints out the following DEBUG message to the logs: Completed batch [currentBatchId] Limitations (Assumptions) \u00b6 It is assumed that the order of streaming sources in a streaming query matches the order of the < > of < > (in offsetLog ) and availableOffsets . In other words, a streaming query can be modified and then restarted from a checkpoint (to maintain stream processing state) only when the number of streaming sources and their order are preserved across restarts.","title":"Offsets and Metadata Checkpointing"},{"location":"spark-sql-streaming-offsets-and-metadata-checkpointing/#offsets-and-metadata-checkpointing","text":"A streaming query can be started from scratch or from checkpoint (that gives fault-tolerance as the state is preserved even when a failure happens). Stream execution engines use checkpoint location to resume stream processing and get start offsets to start query processing from. StreamExecution resumes (populates the start offsets) from the latest checkpointed offsets from the Write-Ahead Log (WAL) of Offsets that may have already been processed (and, if so, committed to the Offset Commit Log ). < > of < > < > and < > < > and StreamExecutions ( committed and available offsets)","title":"Offsets and Metadata Checkpointing"},{"location":"spark-sql-streaming-offsets-and-metadata-checkpointing/#micro-batch-stream-processing","text":"In < >, the available offsets registry is populated with the < > from the Write-Ahead Log (WAL) when MicroBatchExecution stream processing engine is requested to < > (if available) when MicroBatchExecution is requested to < > ( before the first \"zero\" micro-batch ). The available offsets are then < > to the committed offsets when the latest batch ID available (as described above) is exactly the < > committed to the Offset Commit Log when MicroBatchExecution stream processing engine is requested to < >. When a streaming query is started from scratch (with no checkpoint that has offsets in the Offset Write-Ahead Log ), MicroBatchExecution prints out the following INFO message: Starting new streaming query. When a streaming query is resumed (restarted) from a checkpoint with offsets in the Offset Write-Ahead Log , MicroBatchExecution prints out the following INFO message: Resuming at batch [currentBatchId] with committed offsets [committedOffsets] and available offsets [availableOffsets] Every time MicroBatchExecution is requested to < > (in any of the streaming sources)...FIXME When MicroBatchExecution is requested to < > (when MicroBatchExecution requested to < >), every streaming source is requested for the latest offset available that are < > to the availableOffsets registry. < > report some offsets or none at all (if this source has never received any data). Streaming sources with no data are excluded ( filtered out ). MicroBatchExecution prints out the following TRACE message to the logs: noDataBatchesEnabled = [noDataBatchesEnabled], lastExecutionRequiresAnotherBatch = [lastExecutionRequiresAnotherBatch], isNewDataAvailable = [isNewDataAvailable], shouldConstructNextBatch = [shouldConstructNextBatch] With < > internal flag enabled, MicroBatchExecution commits (< >) the available offsets for the batch to the Write-Ahead Log (WAL) and prints out the following INFO message to the logs: Committed offsets for batch [currentBatchId]. Metadata [offsetSeqMetadata] When < >, MicroBatchExecution requests every Source and < > (in the availableOffsets registry) for unprocessed data (that has not been committed yet and so considered unprocessed). In the end (of < >), MicroBatchExecution commits (< >) the available offsets (to the < > registry) so they are considered processed already. MicroBatchExecution prints out the following DEBUG message to the logs: Completed batch [currentBatchId]","title":"Micro-Batch Stream Processing"},{"location":"spark-sql-streaming-offsets-and-metadata-checkpointing/#limitations-assumptions","text":"It is assumed that the order of streaming sources in a streaming query matches the order of the < > of < > (in offsetLog ) and availableOffsets . In other words, a streaming query can be modified and then restarted from a checkpoint (to maintain stream processing state) only when the number of streaming sources and their order are preserved across restarts.","title":"Limitations (Assumptions)"},{"location":"spark-sql-streaming-properties/","text":"Configuration Properties \u00b6 Configuration properties (aka settings ) allow you to fine-tune a Spark Structured Streaming application. Tip Find out more on Configuration Properties in The Internals of Spark SQL spark.sql.streaming.metricsEnabled \u00b6 Enables streaming metrics Default: false Use SQLConf.streamingMetricsEnabled to access the current value. [[properties]] .Structured Streaming's Properties [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.sql.streaming.aggregation.stateFormatVersion a| [[spark.sql.streaming.aggregation.stateFormatVersion]] (internal) Version of the state format Default: 2 Supported values: [[spark.sql.streaming.aggregation.stateFormatVersion-legacyVersion]] 1 (for the legacy < >) [[spark.sql.streaming.aggregation.stateFormatVersion-default]] 2 (for the default < >) Used when < > execution planning strategy is executed (and plans a streaming query with an aggregate that simply boils down to creating a < > with the proper implementation version of < >) Among the < > that are not supposed to be overriden after a streaming query has once been started (and could later recover from a checkpoint after being restarted) | spark.sql.streaming.checkpointFileManagerClass a| [[spark.sql.streaming.checkpointFileManagerClass]] (internal) < > to use to write checkpoint files atomically Default: < > (with < > in case of unsupported file system used for storing metadata files) | spark.sql.streaming.checkpointLocation a| [[spark.sql.streaming.checkpointLocation]] Default checkpoint directory for storing checkpoint data Default: (empty) | spark.sql.streaming.continuous.executorQueueSize a| [[spark.sql.streaming.continuous.executorQueueSize]] (internal) The size (measured in number of rows) of the queue used in continuous execution to buffer the results of a ContinuousDataReader. Default: 1024 | spark.sql.streaming.continuous.executorPollIntervalMs a| [[spark.sql.streaming.continuous.executorPollIntervalMs]] (internal) The interval (in millis) at which continuous execution readers will poll to check whether the epoch has advanced on the driver. Default: 100 (ms) | spark.sql.streaming.disabledV2MicroBatchReaders a| [[spark.sql.streaming.disabledV2MicroBatchReaders]] (internal) A comma-separated list of fully-qualified class names of data source providers for which < > is disabled. Reads from these sources will fall back to the V1 Sources. Default: (empty) Use < > to get the current value. | spark.sql.streaming.fileSource.log.cleanupDelay a| [[spark.sql.streaming.fileSource.log.cleanupDelay]] (internal) How long (in millis) a file is guaranteed to be visible for all readers. Default: 10 (minutes) Use < > to get the current value. | spark.sql.streaming.fileSource.log.compactInterval a| [[spark.sql.streaming.fileSource.log.compactInterval]] (internal) Number of log files after which all the previous files are compacted into the next log file. Default: 10 Must be a positive value (greater than 0 ) Use < > to get the current value. | spark.sql.streaming.fileSource.log.deletion a| [[spark.sql.streaming.fileSource.log.deletion]] (internal) Whether to delete the expired log files in file stream source Default: true Use < > to get the current value. | spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion a| [[spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion]] (internal) State format version used to create a < > for FlatMapGroupsWithStateExec physical operator Default: 2 Supported values: 1 2 Among the < > that are not supposed to be overriden after a streaming query has once been started (and could later recover from a checkpoint after being restarted) | spark.sql.streaming.maxBatchesToRetainInMemory a| [[spark.sql.streaming.maxBatchesToRetainInMemory]] (internal) The maximum number of batches which will be retained in memory to avoid loading from files. Default: 2 Maximum count of versions a State Store implementation should retain in memory. The value adjusts a trade-off between memory usage vs cache miss: 2 covers both success and direct failure cases 1 covers only success case 0 or negative value disables cache to maximize memory size of executors Used exclusively when HDFSBackedStateStoreProvider is requested to < >. | spark.sql.streaming.minBatchesToRetain a| [[spark.sql.streaming.minBatchesToRetain]] (internal) The minimum number of entries to retain for failure recovery Default: 100 Use < > to get the current value | spark.sql.streaming.multipleWatermarkPolicy a| [[spark.sql.streaming.multipleWatermarkPolicy]] Global watermark policy that is the policy to calculate the global watermark value when there are multiple watermark operators in a streaming query Default: min Supported values: min - chooses the minimum watermark reported across multiple operators max - chooses the maximum across multiple operators Cannot be changed between query restarts from the same checkpoint location. | spark.sql.streaming.noDataMicroBatches.enabled a| [[spark.sql.streaming.noDataMicroBatches.enabled]] Flag to control whether the < > should execute batches with no data to process for eager state management for stateful streaming queries ( true ) or not ( false ). Default: true Use < > to get the current value | spark.sql.streaming.noDataProgressEventInterval a| [[spark.sql.streaming.noDataProgressEventInterval]] (internal) How long to wait between two progress events when there is no data (in millis) when ProgressReporter is requested to finish a trigger Default: 10000L Use < > to get the current value | spark.sql.streaming.numRecentProgressUpdates a| [[spark.sql.streaming.numRecentProgressUpdates]] Number of StreamingQueryProgresses to retain in progressBuffer internal registry when ProgressReporter is requested to update progress of streaming query Default: 100 Use < > to get the current value | spark.sql.streaming.pollingDelay a| [[spark.sql.streaming.pollingDelay]] (internal) How long (in millis) to delay StreamExecution before MicroBatchExecution.md#runBatches-batchRunner-no-data[polls for new data when no data was available in a batch] Default: 10 (milliseconds) | spark.sql.streaming.stateStore.maintenanceInterval a| [[spark.sql.streaming.stateStore.maintenanceInterval]] The initial delay and how often to execute StateStore's spark-sql-streaming-StateStore.md#MaintenanceTask[maintenance task]. Default: 60s | spark.sql.streaming.stateStore.minDeltasForSnapshot a| [[spark.sql.streaming.stateStore.minDeltasForSnapshot]] (internal) Minimum number of state store delta files that need to be generated before HDFSBackedStateStore will consider generating a snapshot (consolidate the deltas into a snapshot) Default: 10 Use < > to get the current value. | spark.sql.streaming.stateStore.providerClass a| [[spark.sql.streaming.stateStore.providerClass]] (internal) The fully-qualified class name of the < > implementation that manages state data in stateful streaming queries. This class must have a zero-arg constructor. Default: < > Use < > to get the current value. | spark.sql.streaming.unsupportedOperationCheck a| [[spark.sql.streaming.unsupportedOperationCheck]] (internal) When enabled ( true ), StreamingQueryManager spark-sql-streaming-UnsupportedOperationChecker.md#checkForStreaming[makes sure that the logical plan of a streaming query uses supported operations only]. Default: true |===","title":"Configuration Properties"},{"location":"spark-sql-streaming-properties/#configuration-properties","text":"Configuration properties (aka settings ) allow you to fine-tune a Spark Structured Streaming application. Tip Find out more on Configuration Properties in The Internals of Spark SQL","title":"Configuration Properties"},{"location":"spark-sql-streaming-properties/#sparksqlstreamingmetricsenabled","text":"Enables streaming metrics Default: false Use SQLConf.streamingMetricsEnabled to access the current value. [[properties]] .Structured Streaming's Properties [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Name | Description | spark.sql.streaming.aggregation.stateFormatVersion a| [[spark.sql.streaming.aggregation.stateFormatVersion]] (internal) Version of the state format Default: 2 Supported values: [[spark.sql.streaming.aggregation.stateFormatVersion-legacyVersion]] 1 (for the legacy < >) [[spark.sql.streaming.aggregation.stateFormatVersion-default]] 2 (for the default < >) Used when < > execution planning strategy is executed (and plans a streaming query with an aggregate that simply boils down to creating a < > with the proper implementation version of < >) Among the < > that are not supposed to be overriden after a streaming query has once been started (and could later recover from a checkpoint after being restarted) | spark.sql.streaming.checkpointFileManagerClass a| [[spark.sql.streaming.checkpointFileManagerClass]] (internal) < > to use to write checkpoint files atomically Default: < > (with < > in case of unsupported file system used for storing metadata files) | spark.sql.streaming.checkpointLocation a| [[spark.sql.streaming.checkpointLocation]] Default checkpoint directory for storing checkpoint data Default: (empty) | spark.sql.streaming.continuous.executorQueueSize a| [[spark.sql.streaming.continuous.executorQueueSize]] (internal) The size (measured in number of rows) of the queue used in continuous execution to buffer the results of a ContinuousDataReader. Default: 1024 | spark.sql.streaming.continuous.executorPollIntervalMs a| [[spark.sql.streaming.continuous.executorPollIntervalMs]] (internal) The interval (in millis) at which continuous execution readers will poll to check whether the epoch has advanced on the driver. Default: 100 (ms) | spark.sql.streaming.disabledV2MicroBatchReaders a| [[spark.sql.streaming.disabledV2MicroBatchReaders]] (internal) A comma-separated list of fully-qualified class names of data source providers for which < > is disabled. Reads from these sources will fall back to the V1 Sources. Default: (empty) Use < > to get the current value. | spark.sql.streaming.fileSource.log.cleanupDelay a| [[spark.sql.streaming.fileSource.log.cleanupDelay]] (internal) How long (in millis) a file is guaranteed to be visible for all readers. Default: 10 (minutes) Use < > to get the current value. | spark.sql.streaming.fileSource.log.compactInterval a| [[spark.sql.streaming.fileSource.log.compactInterval]] (internal) Number of log files after which all the previous files are compacted into the next log file. Default: 10 Must be a positive value (greater than 0 ) Use < > to get the current value. | spark.sql.streaming.fileSource.log.deletion a| [[spark.sql.streaming.fileSource.log.deletion]] (internal) Whether to delete the expired log files in file stream source Default: true Use < > to get the current value. | spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion a| [[spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion]] (internal) State format version used to create a < > for FlatMapGroupsWithStateExec physical operator Default: 2 Supported values: 1 2 Among the < > that are not supposed to be overriden after a streaming query has once been started (and could later recover from a checkpoint after being restarted) | spark.sql.streaming.maxBatchesToRetainInMemory a| [[spark.sql.streaming.maxBatchesToRetainInMemory]] (internal) The maximum number of batches which will be retained in memory to avoid loading from files. Default: 2 Maximum count of versions a State Store implementation should retain in memory. The value adjusts a trade-off between memory usage vs cache miss: 2 covers both success and direct failure cases 1 covers only success case 0 or negative value disables cache to maximize memory size of executors Used exclusively when HDFSBackedStateStoreProvider is requested to < >. | spark.sql.streaming.minBatchesToRetain a| [[spark.sql.streaming.minBatchesToRetain]] (internal) The minimum number of entries to retain for failure recovery Default: 100 Use < > to get the current value | spark.sql.streaming.multipleWatermarkPolicy a| [[spark.sql.streaming.multipleWatermarkPolicy]] Global watermark policy that is the policy to calculate the global watermark value when there are multiple watermark operators in a streaming query Default: min Supported values: min - chooses the minimum watermark reported across multiple operators max - chooses the maximum across multiple operators Cannot be changed between query restarts from the same checkpoint location. | spark.sql.streaming.noDataMicroBatches.enabled a| [[spark.sql.streaming.noDataMicroBatches.enabled]] Flag to control whether the < > should execute batches with no data to process for eager state management for stateful streaming queries ( true ) or not ( false ). Default: true Use < > to get the current value | spark.sql.streaming.noDataProgressEventInterval a| [[spark.sql.streaming.noDataProgressEventInterval]] (internal) How long to wait between two progress events when there is no data (in millis) when ProgressReporter is requested to finish a trigger Default: 10000L Use < > to get the current value | spark.sql.streaming.numRecentProgressUpdates a| [[spark.sql.streaming.numRecentProgressUpdates]] Number of StreamingQueryProgresses to retain in progressBuffer internal registry when ProgressReporter is requested to update progress of streaming query Default: 100 Use < > to get the current value | spark.sql.streaming.pollingDelay a| [[spark.sql.streaming.pollingDelay]] (internal) How long (in millis) to delay StreamExecution before MicroBatchExecution.md#runBatches-batchRunner-no-data[polls for new data when no data was available in a batch] Default: 10 (milliseconds) | spark.sql.streaming.stateStore.maintenanceInterval a| [[spark.sql.streaming.stateStore.maintenanceInterval]] The initial delay and how often to execute StateStore's spark-sql-streaming-StateStore.md#MaintenanceTask[maintenance task]. Default: 60s | spark.sql.streaming.stateStore.minDeltasForSnapshot a| [[spark.sql.streaming.stateStore.minDeltasForSnapshot]] (internal) Minimum number of state store delta files that need to be generated before HDFSBackedStateStore will consider generating a snapshot (consolidate the deltas into a snapshot) Default: 10 Use < > to get the current value. | spark.sql.streaming.stateStore.providerClass a| [[spark.sql.streaming.stateStore.providerClass]] (internal) The fully-qualified class name of the < > implementation that manages state data in stateful streaming queries. This class must have a zero-arg constructor. Default: < > Use < > to get the current value. | spark.sql.streaming.unsupportedOperationCheck a| [[spark.sql.streaming.unsupportedOperationCheck]] (internal) When enabled ( true ), StreamingQueryManager spark-sql-streaming-UnsupportedOperationChecker.md#checkForStreaming[makes sure that the logical plan of a streaming query uses supported operations only]. Default: true |===","title":" spark.sql.streaming.metricsEnabled"},{"location":"spark-sql-streaming-stateful-stream-processing/","text":"Stateful Stream Processing \u00b6 Stateful Stream Processing is a stream processing with state (implicit or explicit). In Spark Structured Streaming, a streaming query is stateful when is one of the following (that makes use of StateStores ): Streaming Aggregation Arbitrary Stateful Streaming Aggregation Stream-Stream Join Streaming Deduplication Streaming Limit Versioned State, StateStores and StateStoreProviders \u00b6 Spark Structured Streaming uses < > for versioned and fault-tolerant key-value state stores. State stores are checkpointed incrementally to avoid state loss and for increased performance. State stores are managed by < > with < > being the default and only known implementation. HDFSBackedStateStoreProvider uses Hadoop DFS-compliant file system for < >. State store providers manage versioned state per < >. The lifecycle of a StateStoreProvider begins when StateStore utility (on a Spark executor) is requested for the < >. IMPORTANT: It is worth to notice that since StateStore and StateStoreProvider utilities are Scala objects that makes it possible that there can only be one instance of StateStore and StateStoreProvider on a single JVM. Scala objects are (sort of) singletons which means that there will be exactly one instance of each per JVM and that is exactly the JVM of a Spark executor. As long as the executor is up and running state versions are cached and no Hadoop DFS is used (except for the initial load). When requested for a < >, StateStore utility is given the version of a state store to look up. The version is either the < > (in < >) or the < > (in < >). StateStore utility requests StateStoreProvider utility to < > that creates the StateStoreProvider implementation (based on < > internal configuration property) and requests it to < >. The initialized StateStoreProvider is cached in < > internal lookup table (for a < >) for later lookups. StateStoreProvider utility then requests the StateStoreProvider for the < >. (e.g. a < > in case of < >). An instance of StateStoreProvider is requested to < > or < > (when < >) in < > that runs periodically every < > configuration property (default: 60s ). IncrementalExecution \u2014 QueryExecution of Streaming Queries \u00b6 Regardless of the query language (Dataset API or SQL), any structured query (incl. streaming queries) becomes a logical query plan. In Spark Structured Streaming it is < > that plans streaming queries for execution. While < > (aka query planning ), IncrementalExecution uses the < >. The rule fills out the following physical operators with the execution-specific configuration (with < > being the most important for stateful stream processing): FlatMapGroupsWithStateExec < > < > (used for < >) < > < > < > ==== [[IncrementalExecution-shouldRunAnotherBatch]] Micro-Batch Stream Processing and Extra Non-Data Batch for StateStoreWriter Stateful Operators In < > (with < > engine), IncrementalExecution uses < > flag that allows StateStoreWriters stateful physical operators to indicate whether the last batch execution requires another non-data batch . The < > shows the StateStoreWriters that redefine shouldRunAnotherBatch flag. [[StateStoreWriters-shouldRunAnotherBatch]] .StateStoreWriters and shouldRunAnotherBatch Flag [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateStoreWriter | shouldRunAnotherBatch Flag | FlatMapGroupsWithStateExec a| [[shouldRunAnotherBatch-FlatMapGroupsWithStateExec]] Based on GroupStateTimeout | < > a| [[shouldRunAnotherBatch-StateStoreSaveExec]] Based on < > | < > a| [[shouldRunAnotherBatch-StreamingDeduplicateExec]] Based on < > | < > a| [[shouldRunAnotherBatch-StreamingSymmetricHashJoinExec]] Based on < > |=== === [[StateStoreRDD]] StateStoreRDD Right after < >, a stateful streaming query (a single micro-batch actually) becomes an RDD with one or more < >. You can find the StateStoreRDDs of a streaming query in the RDD lineage. [source, scala] \u00b6 scala> :type streamingQuery org.apache.spark.sql.streaming.StreamingQuery scala> streamingQuery.explain == Physical Plan == *(4) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[count(1)]) +- StateStoreSave [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], Append, 10000, 2 +- *(3) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)]) +- StateStoreRestore [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], 2 +- *(2) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)]) +- Exchange hashpartitioning(window#13-T0ms, value#3L, 1) +- *(1) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[partial_count(1)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13-T0ms, value#3L] +- *(1) Filter isnotnull(time#2-T0ms) +- EventTimeWatermark time#2: timestamp, interval +- LocalTableScan , [time#2, value#3L] import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper} val se = streamingQuery.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution scala> :type se.lastExecution org.apache.spark.sql.execution.streaming.IncrementalExecution val rdd = se.lastExecution.toRdd scala> rdd.toDebugString res3: String = (1) MapPartitionsRDD[39] at toRdd at :40 [] | StateStoreRDD[38] at toRdd at :40 [] // \u2190 here | MapPartitionsRDD[37] at toRdd at :40 [] | StateStoreRDD[36] at toRdd at :40 [] // \u2190 here | MapPartitionsRDD[35] at toRdd at :40 [] | ShuffledRowRDD[17] at start at :67 [] +-(1) MapPartitionsRDD[16] at start at :67 [] | MapPartitionsRDD[15] at start at :67 [] | MapPartitionsRDD[14] at start at :67 [] | MapPartitionsRDD[13] at start at :67 [] | ParallelCollectionRDD[12] at start at :67 [] === [[StateStoreCoordinator]] StateStoreCoordinator RPC Endpoint, StateStoreRDD and Preferred Locations Since execution of a stateful streaming query happens on Spark executors whereas planning is on the driver, Spark Structured Streaming uses RPC environment for tracking locations of the state stores in use. That makes the tasks (of a structured query) to be scheduled where the state (of a partition) is. When planned for execution, the StateStoreRDD is first asked for the < > (which happens on the driver) that are later used to < > (on Spark executors). Spark Structured Streaming uses RPC environment to keep track of < > (their < > actually) for RDD planning. Every time < > is requested for the < >, it communicates with the < > that knows the locations of the required StateStores (per host and executor ID). StateStoreRDD uses < > with < > to uniquely identify the < > to use for ( associate with ) a stateful operator and a partition. === State Management The state in a stateful streaming query can be implicit or explicit.","title":"Stateful Stream Processing"},{"location":"spark-sql-streaming-stateful-stream-processing/#stateful-stream-processing","text":"Stateful Stream Processing is a stream processing with state (implicit or explicit). In Spark Structured Streaming, a streaming query is stateful when is one of the following (that makes use of StateStores ): Streaming Aggregation Arbitrary Stateful Streaming Aggregation Stream-Stream Join Streaming Deduplication Streaming Limit","title":"Stateful Stream Processing"},{"location":"spark-sql-streaming-stateful-stream-processing/#versioned-state-statestores-and-statestoreproviders","text":"Spark Structured Streaming uses < > for versioned and fault-tolerant key-value state stores. State stores are checkpointed incrementally to avoid state loss and for increased performance. State stores are managed by < > with < > being the default and only known implementation. HDFSBackedStateStoreProvider uses Hadoop DFS-compliant file system for < >. State store providers manage versioned state per < >. The lifecycle of a StateStoreProvider begins when StateStore utility (on a Spark executor) is requested for the < >. IMPORTANT: It is worth to notice that since StateStore and StateStoreProvider utilities are Scala objects that makes it possible that there can only be one instance of StateStore and StateStoreProvider on a single JVM. Scala objects are (sort of) singletons which means that there will be exactly one instance of each per JVM and that is exactly the JVM of a Spark executor. As long as the executor is up and running state versions are cached and no Hadoop DFS is used (except for the initial load). When requested for a < >, StateStore utility is given the version of a state store to look up. The version is either the < > (in < >) or the < > (in < >). StateStore utility requests StateStoreProvider utility to < > that creates the StateStoreProvider implementation (based on < > internal configuration property) and requests it to < >. The initialized StateStoreProvider is cached in < > internal lookup table (for a < >) for later lookups. StateStoreProvider utility then requests the StateStoreProvider for the < >. (e.g. a < > in case of < >). An instance of StateStoreProvider is requested to < > or < > (when < >) in < > that runs periodically every < > configuration property (default: 60s ).","title":" Versioned State, StateStores and StateStoreProviders"},{"location":"spark-sql-streaming-stateful-stream-processing/#incrementalexecution-queryexecution-of-streaming-queries","text":"Regardless of the query language (Dataset API or SQL), any structured query (incl. streaming queries) becomes a logical query plan. In Spark Structured Streaming it is < > that plans streaming queries for execution. While < > (aka query planning ), IncrementalExecution uses the < >. The rule fills out the following physical operators with the execution-specific configuration (with < > being the most important for stateful stream processing): FlatMapGroupsWithStateExec < > < > (used for < >) < > < > < > ==== [[IncrementalExecution-shouldRunAnotherBatch]] Micro-Batch Stream Processing and Extra Non-Data Batch for StateStoreWriter Stateful Operators In < > (with < > engine), IncrementalExecution uses < > flag that allows StateStoreWriters stateful physical operators to indicate whether the last batch execution requires another non-data batch . The < > shows the StateStoreWriters that redefine shouldRunAnotherBatch flag. [[StateStoreWriters-shouldRunAnotherBatch]] .StateStoreWriters and shouldRunAnotherBatch Flag [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | StateStoreWriter | shouldRunAnotherBatch Flag | FlatMapGroupsWithStateExec a| [[shouldRunAnotherBatch-FlatMapGroupsWithStateExec]] Based on GroupStateTimeout | < > a| [[shouldRunAnotherBatch-StateStoreSaveExec]] Based on < > | < > a| [[shouldRunAnotherBatch-StreamingDeduplicateExec]] Based on < > | < > a| [[shouldRunAnotherBatch-StreamingSymmetricHashJoinExec]] Based on < > |=== === [[StateStoreRDD]] StateStoreRDD Right after < >, a stateful streaming query (a single micro-batch actually) becomes an RDD with one or more < >. You can find the StateStoreRDDs of a streaming query in the RDD lineage.","title":" IncrementalExecution &mdash; QueryExecution of Streaming Queries"},{"location":"spark-sql-streaming-stateful-stream-processing/#source-scala","text":"scala> :type streamingQuery org.apache.spark.sql.streaming.StreamingQuery scala> streamingQuery.explain == Physical Plan == *(4) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[count(1)]) +- StateStoreSave [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], Append, 10000, 2 +- *(3) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)]) +- StateStoreRestore [window#13-T0ms, value#3L], state info [ checkpoint = file:/tmp/checkpoint-counts/state, runId = 1dec2d81-f2d0-45b9-8f16-39ede66e13e7, opId = 0, ver = 1, numPartitions = 1], 2 +- *(2) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[merge_count(1)]) +- Exchange hashpartitioning(window#13-T0ms, value#3L, 1) +- *(1) HashAggregate(keys=[window#13-T0ms, value#3L], functions=[partial_count(1)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#2-T0ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#13-T0ms, value#3L] +- *(1) Filter isnotnull(time#2-T0ms) +- EventTimeWatermark time#2: timestamp, interval +- LocalTableScan , [time#2, value#3L] import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper} val se = streamingQuery.asInstanceOf[StreamingQueryWrapper].streamingQuery scala> :type se org.apache.spark.sql.execution.streaming.StreamExecution scala> :type se.lastExecution org.apache.spark.sql.execution.streaming.IncrementalExecution val rdd = se.lastExecution.toRdd scala> rdd.toDebugString res3: String = (1) MapPartitionsRDD[39] at toRdd at :40 [] | StateStoreRDD[38] at toRdd at :40 [] // \u2190 here | MapPartitionsRDD[37] at toRdd at :40 [] | StateStoreRDD[36] at toRdd at :40 [] // \u2190 here | MapPartitionsRDD[35] at toRdd at :40 [] | ShuffledRowRDD[17] at start at :67 [] +-(1) MapPartitionsRDD[16] at start at :67 [] | MapPartitionsRDD[15] at start at :67 [] | MapPartitionsRDD[14] at start at :67 [] | MapPartitionsRDD[13] at start at :67 [] | ParallelCollectionRDD[12] at start at :67 [] === [[StateStoreCoordinator]] StateStoreCoordinator RPC Endpoint, StateStoreRDD and Preferred Locations Since execution of a stateful streaming query happens on Spark executors whereas planning is on the driver, Spark Structured Streaming uses RPC environment for tracking locations of the state stores in use. That makes the tasks (of a structured query) to be scheduled where the state (of a partition) is. When planned for execution, the StateStoreRDD is first asked for the < > (which happens on the driver) that are later used to < > (on Spark executors). Spark Structured Streaming uses RPC environment to keep track of < > (their < > actually) for RDD planning. Every time < > is requested for the < >, it communicates with the < > that knows the locations of the required StateStores (per host and executor ID). StateStoreRDD uses < > with < > to uniquely identify the < > to use for ( associate with ) a stateful operator and a partition. === State Management The state in a stateful streaming query can be implicit or explicit.","title":"[source, scala]"},{"location":"spark-sql-streaming-watermark/","text":"Streaming Watermark \u00b6 Streaming Watermark of a stateful streaming query is how long to wait for late and possibly out-of-order events until a streaming state can be considered final and not to change. Streaming watermark is used to mark events (modeled as a row in the streaming Dataset) that are older than the threshold as \"too late\", and not \"interesting\" to update partial non-final streaming state. In Spark Structured Streaming, streaming watermark is defined using Dataset.withWatermark high-level operator. [source, scala] \u00b6 withWatermark( eventTime: String, delayThreshold: String): Dataset[T] In Dataset.withWatermark operator, eventTime is the name of the column to use to monitor event time whereas delayThreshold is a delay threshold. Watermark Delay says how late and possibly out-of-order events are still acceptable and contribute to the final result of a stateful streaming query. Event-time watermark delay is used to calculate the difference between the event time of an event and the time in the past. Event-Time Watermark is then a time threshold ( point in time ) that is the minimum acceptable time of an event (modeled as a row in the streaming Dataset) that is accepted in a stateful streaming query. With streaming watermark, memory usage of a streaming state can be controlled as late events can easily be dropped, and old state (e.g. aggregates or join) that are never going to be updated removed. That avoids unbounded streaming state that would inevitably use up all the available memory of long-running streaming queries and end up in out of memory errors. In < > output mode the current event-time streaming watermark is used for the following: Output saved state rows that became expired ( Expired events in the demo) Dropping late events, i.e. don't save them to a state store or include in aggregation ( Late events in the demo) Streaming watermark is < > for a < > in < > output mode. === [[streaming-aggregation]] Streaming Aggregation In < >, a streaming watermark has to be defined on one or many grouping expressions of a streaming aggregation (directly or using < > standard function). Note Dataset.withWatermark operator has to be used before an aggregation operator (for the watermark to have an effect). === [[streaming-join]] Streaming Join In < >, a streaming watermark can be defined on < >. === [[demos]] Demos Use the following demos to learn more: < > Internals \u00b6 Under the covers, Dataset.withWatermark high-level operator creates a logical query plan with EventTimeWatermark logical operator. EventTimeWatermark logical operator is planned to EventTimeWatermarkExec physical operator that extracts the event times (from the data being processed) and adds them to an accumulator. Since the execution (data processing) happens on Spark executors, using the accumulator is the only Spark-approved way for communication between the tasks (on the executors) and the driver. Using accumulator updates the driver with the current event-time watermark. During the query planning phase (in < > and < >) that also happens on the driver, IncrementalExecution is given the current < > with the current event-time watermark. === [[i-want-more]] Further Reading Or Watching https://issues.apache.org/jira/browse/SPARK-18124[SPARK-18124 Observed delay based event time watermarks]","title":"Streaming Watermark"},{"location":"spark-sql-streaming-watermark/#streaming-watermark","text":"Streaming Watermark of a stateful streaming query is how long to wait for late and possibly out-of-order events until a streaming state can be considered final and not to change. Streaming watermark is used to mark events (modeled as a row in the streaming Dataset) that are older than the threshold as \"too late\", and not \"interesting\" to update partial non-final streaming state. In Spark Structured Streaming, streaming watermark is defined using Dataset.withWatermark high-level operator.","title":"Streaming Watermark"},{"location":"spark-sql-streaming-watermark/#source-scala","text":"withWatermark( eventTime: String, delayThreshold: String): Dataset[T] In Dataset.withWatermark operator, eventTime is the name of the column to use to monitor event time whereas delayThreshold is a delay threshold. Watermark Delay says how late and possibly out-of-order events are still acceptable and contribute to the final result of a stateful streaming query. Event-time watermark delay is used to calculate the difference between the event time of an event and the time in the past. Event-Time Watermark is then a time threshold ( point in time ) that is the minimum acceptable time of an event (modeled as a row in the streaming Dataset) that is accepted in a stateful streaming query. With streaming watermark, memory usage of a streaming state can be controlled as late events can easily be dropped, and old state (e.g. aggregates or join) that are never going to be updated removed. That avoids unbounded streaming state that would inevitably use up all the available memory of long-running streaming queries and end up in out of memory errors. In < > output mode the current event-time streaming watermark is used for the following: Output saved state rows that became expired ( Expired events in the demo) Dropping late events, i.e. don't save them to a state store or include in aggregation ( Late events in the demo) Streaming watermark is < > for a < > in < > output mode. === [[streaming-aggregation]] Streaming Aggregation In < >, a streaming watermark has to be defined on one or many grouping expressions of a streaming aggregation (directly or using < > standard function). Note Dataset.withWatermark operator has to be used before an aggregation operator (for the watermark to have an effect). === [[streaming-join]] Streaming Join In < >, a streaming watermark can be defined on < >. === [[demos]] Demos Use the following demos to learn more: < >","title":"[source, scala]"},{"location":"spark-sql-streaming-watermark/#internals","text":"Under the covers, Dataset.withWatermark high-level operator creates a logical query plan with EventTimeWatermark logical operator. EventTimeWatermark logical operator is planned to EventTimeWatermarkExec physical operator that extracts the event times (from the data being processed) and adds them to an accumulator. Since the execution (data processing) happens on Spark executors, using the accumulator is the only Spark-approved way for communication between the tasks (on the executors) and the driver. Using accumulator updates the driver with the current event-time watermark. During the query planning phase (in < > and < >) that also happens on the driver, IncrementalExecution is given the current < > with the current event-time watermark. === [[i-want-more]] Further Reading Or Watching https://issues.apache.org/jira/browse/SPARK-18124[SPARK-18124 Observed delay based event time watermarks]","title":" Internals"},{"location":"spark-sql-streaming-window/","text":"== [[window]] window Function -- Stream Time Windows window is a standard function that generates tumbling , sliding or delayed stream time window ranges (on a timestamp column). [source, scala] \u00b6 window( timeColumn: Column, windowDuration: String): Column // <1> window( timeColumn: Column, windowDuration: String, slideDuration: String): Column // <2> window( timeColumn: Column, windowDuration: String, slideDuration: String, startTime: String): Column // <3> <1> Creates a tumbling time window with slideDuration as windowDuration and 0 second for startTime <2> Creates a sliding time window with 0 second for startTime <3> Creates a delayed time window [NOTE] \u00b6 From https://msdn.microsoft.com/en-us/library/azure/dn835055.aspx[Tumbling Window (Azure Stream Analytics)]: > Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. \u00b6 [NOTE] \u00b6 From https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]: Tumbling windows group elements of a stream into finite sets where each set corresponds to an interval. > Tumbling windows discretize a stream into non-overlapping windows. \u00b6 [source, scala] \u00b6 scala> val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS window timeColumn should be of TimestampType , i.e. with https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html[java.sql.Timestamp ] values. TIP: Use ++ https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#from-java.time.Instant-++[java.sql.Timestamp.from ] or ++ https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#valueOf-java.time.LocalDateTime-++[java.sql.Timestamp.valueOf ] factory methods to create Timestamp instances. [source, scala] \u00b6 // https://docs.oracle.com/javase/8/docs/api/java/time/LocalDateTime.html import java.time.LocalDateTime // https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html import java.sql.Timestamp val levels = Seq( // (year, month, dayOfMonth, hour, minute, second) ((2012, 12, 12, 12, 12, 12), 5), ((2012, 12, 12, 12, 12, 14), 9), ((2012, 12, 12, 13, 13, 14), 4), ((2016, 8, 13, 0, 0, 0), 10), ((2017, 5, 27, 0, 0, 0), 15)). map { case ((yy, mm, dd, h, m, s), a) => (LocalDateTime.of(yy, mm, dd, h, m, s), a) }. map { case (ts, a) => (Timestamp.valueOf(ts), a) }. toDF(\"time\", \"level\") scala> levels.show +-------------------+-----+ | time|level| +-------------------+-----+ |2012-12-12 12:12:12| 5| |2012-12-12 12:12:14| 9| |2012-12-12 13:13:14| 4| |2016-08-13 00:00:00| 10| |2017-05-27 00:00:00| 15| +-------------------+-----+ val q = levels.select(window($\"time\", \"5 seconds\"), $\"level\") scala> q.show(truncate = false) +---------------------------------------------+-----+ |window |level| +---------------------------------------------+-----+ |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|5 | |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|9 | |[2012-12-12 13:13:10.0,2012-12-12 13:13:15.0]|4 | |[2016-08-13 00:00:00.0,2016-08-13 00:00:05.0]|10 | |[2017-05-27 00:00:00.0,2017-05-27 00:00:05.0]|15 | +---------------------------------------------+-----+ scala> q.printSchema root |-- window: struct (nullable = true) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) |-- level: integer (nullable = false) // calculating the sum of levels every 5 seconds val sums = levels. groupBy(window($\"time\", \"5 seconds\")). agg(sum(\"level\") as \"level_sum\"). select(\"window.start\", \"window.end\", \"level_sum\") scala> sums.show +-------------------+-------------------+---------+ | start| end|level_sum| +-------------------+-------------------+---------+ |2012-12-12 13:13:10|2012-12-12 13:13:15| 4| |2012-12-12 12:12:10|2012-12-12 12:12:15| 14| |2016-08-13 00:00:00|2016-08-13 00:00:05| 10| |2017-05-27 00:00:00|2017-05-27 00:00:05| 15| +-------------------+-------------------+---------+ windowDuration and slideDuration are strings specifying the width of the window for duration and sliding identifiers, respectively. TIP: Use CalendarInterval for valid window identifiers. There are a couple of rules governing the durations: The window duration must be greater than 0 The slide duration must be greater than 0. The start time must be greater than or equal to 0. The slide duration must be less than or equal to the window duration. The start time must be less than the slide duration. NOTE: Only one window expression is supported in a query. NOTE: null values are filtered out in window expression. Internally, window creates a spark-sql-Column.md[Column] with TimeWindow Catalyst expression under window alias. [source, scala] \u00b6 scala> val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS window val windowExpr = timeColumn.expr scala> println(windowExpr.numberedTreeString) 00 timewindow('time, 5000000, 5000000, 0) AS window#23 01 +- timewindow('time, 5000000, 5000000, 0) 02 +- 'time Internally, TimeWindow Catalyst expression is simply a struct type with two fields, i.e. start and end , both of TimestampType type. [source, scala] \u00b6 scala> println(windowExpr.dataType) StructType(StructField(start,TimestampType,true), StructField(end,TimestampType,true)) scala> println(windowExpr.dataType.prettyJson) { \"type\" : \"struct\", \"fields\" : [ { \"name\" : \"start\", \"type\" : \"timestamp\", \"nullable\" : true, \"metadata\" : { } }, { \"name\" : \"end\", \"type\" : \"timestamp\", \"nullable\" : true, \"metadata\" : { } } ] } [NOTE] \u00b6 TimeWindow time window Catalyst expression is planned (i.e. converted ) in TimeWindowing logical optimization rule (i.e. Rule[LogicalPlan] ) of the Spark SQL logical query plan analyzer. Find more about the\u2009Spark SQL logical query plan analyzer in https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-Analyzer.html[Mastering Apache Spark 2] gitbook. \u00b6 ==== [[window-example]] Example -- Traffic Sensor NOTE: The example is borrowed from https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]. The example shows how to use window function to model a traffic sensor that counts every 15 seconds the number of vehicles passing a certain location.","title":"window Function"},{"location":"spark-sql-streaming-window/#source-scala","text":"window( timeColumn: Column, windowDuration: String): Column // <1> window( timeColumn: Column, windowDuration: String, slideDuration: String): Column // <2> window( timeColumn: Column, windowDuration: String, slideDuration: String, startTime: String): Column // <3> <1> Creates a tumbling time window with slideDuration as windowDuration and 0 second for startTime <2> Creates a sliding time window with 0 second for startTime <3> Creates a delayed time window","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#note","text":"From https://msdn.microsoft.com/en-us/library/azure/dn835055.aspx[Tumbling Window (Azure Stream Analytics)]:","title":"[NOTE]"},{"location":"spark-sql-streaming-window/#tumbling-windows-are-a-series-of-fixed-sized-non-overlapping-and-contiguous-time-intervals","text":"","title":"&gt; Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals."},{"location":"spark-sql-streaming-window/#note_1","text":"From https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]: Tumbling windows group elements of a stream into finite sets where each set corresponds to an interval.","title":"[NOTE]"},{"location":"spark-sql-streaming-window/#tumbling-windows-discretize-a-stream-into-non-overlapping-windows","text":"","title":"&gt; Tumbling windows discretize a stream into non-overlapping windows."},{"location":"spark-sql-streaming-window/#source-scala_1","text":"scala> val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS window timeColumn should be of TimestampType , i.e. with https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html[java.sql.Timestamp ] values. TIP: Use ++ https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#from-java.time.Instant-++[java.sql.Timestamp.from ] or ++ https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#valueOf-java.time.LocalDateTime-++[java.sql.Timestamp.valueOf ] factory methods to create Timestamp instances.","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#source-scala_2","text":"// https://docs.oracle.com/javase/8/docs/api/java/time/LocalDateTime.html import java.time.LocalDateTime // https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html import java.sql.Timestamp val levels = Seq( // (year, month, dayOfMonth, hour, minute, second) ((2012, 12, 12, 12, 12, 12), 5), ((2012, 12, 12, 12, 12, 14), 9), ((2012, 12, 12, 13, 13, 14), 4), ((2016, 8, 13, 0, 0, 0), 10), ((2017, 5, 27, 0, 0, 0), 15)). map { case ((yy, mm, dd, h, m, s), a) => (LocalDateTime.of(yy, mm, dd, h, m, s), a) }. map { case (ts, a) => (Timestamp.valueOf(ts), a) }. toDF(\"time\", \"level\") scala> levels.show +-------------------+-----+ | time|level| +-------------------+-----+ |2012-12-12 12:12:12| 5| |2012-12-12 12:12:14| 9| |2012-12-12 13:13:14| 4| |2016-08-13 00:00:00| 10| |2017-05-27 00:00:00| 15| +-------------------+-----+ val q = levels.select(window($\"time\", \"5 seconds\"), $\"level\") scala> q.show(truncate = false) +---------------------------------------------+-----+ |window |level| +---------------------------------------------+-----+ |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|5 | |[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|9 | |[2012-12-12 13:13:10.0,2012-12-12 13:13:15.0]|4 | |[2016-08-13 00:00:00.0,2016-08-13 00:00:05.0]|10 | |[2017-05-27 00:00:00.0,2017-05-27 00:00:05.0]|15 | +---------------------------------------------+-----+ scala> q.printSchema root |-- window: struct (nullable = true) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) |-- level: integer (nullable = false) // calculating the sum of levels every 5 seconds val sums = levels. groupBy(window($\"time\", \"5 seconds\")). agg(sum(\"level\") as \"level_sum\"). select(\"window.start\", \"window.end\", \"level_sum\") scala> sums.show +-------------------+-------------------+---------+ | start| end|level_sum| +-------------------+-------------------+---------+ |2012-12-12 13:13:10|2012-12-12 13:13:15| 4| |2012-12-12 12:12:10|2012-12-12 12:12:15| 14| |2016-08-13 00:00:00|2016-08-13 00:00:05| 10| |2017-05-27 00:00:00|2017-05-27 00:00:05| 15| +-------------------+-------------------+---------+ windowDuration and slideDuration are strings specifying the width of the window for duration and sliding identifiers, respectively. TIP: Use CalendarInterval for valid window identifiers. There are a couple of rules governing the durations: The window duration must be greater than 0 The slide duration must be greater than 0. The start time must be greater than or equal to 0. The slide duration must be less than or equal to the window duration. The start time must be less than the slide duration. NOTE: Only one window expression is supported in a query. NOTE: null values are filtered out in window expression. Internally, window creates a spark-sql-Column.md[Column] with TimeWindow Catalyst expression under window alias.","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#source-scala_3","text":"scala> val timeColumn = window($\"time\", \"5 seconds\") timeColumn: org.apache.spark.sql.Column = timewindow(time, 5000000, 5000000, 0) AS window val windowExpr = timeColumn.expr scala> println(windowExpr.numberedTreeString) 00 timewindow('time, 5000000, 5000000, 0) AS window#23 01 +- timewindow('time, 5000000, 5000000, 0) 02 +- 'time Internally, TimeWindow Catalyst expression is simply a struct type with two fields, i.e. start and end , both of TimestampType type.","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#source-scala_4","text":"scala> println(windowExpr.dataType) StructType(StructField(start,TimestampType,true), StructField(end,TimestampType,true)) scala> println(windowExpr.dataType.prettyJson) { \"type\" : \"struct\", \"fields\" : [ { \"name\" : \"start\", \"type\" : \"timestamp\", \"nullable\" : true, \"metadata\" : { } }, { \"name\" : \"end\", \"type\" : \"timestamp\", \"nullable\" : true, \"metadata\" : { } } ] }","title":"[source, scala]"},{"location":"spark-sql-streaming-window/#note_2","text":"TimeWindow time window Catalyst expression is planned (i.e. converted ) in TimeWindowing logical optimization rule (i.e. Rule[LogicalPlan] ) of the Spark SQL logical query plan analyzer.","title":"[NOTE]"},{"location":"spark-sql-streaming-window/#find-more-about-the-spark-sql-logical-query-plan-analyzer-in-httpsjaceklaskowskigitbooksiomastering-apache-sparkspark-sql-analyzerhtmlmastering-apache-spark-2-gitbook","text":"==== [[window-example]] Example -- Traffic Sensor NOTE: The example is borrowed from https://flink.apache.org/news/2015/12/04/Introducing-windows.html[Introducing Stream Windows in Apache Flink]. The example shows how to use window function to model a traffic sensor that counts every 15 seconds the number of vehicles passing a certain location.","title":"Find more about the\u2009Spark SQL logical query plan analyzer in https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-sql-Analyzer.html[Mastering Apache Spark 2] gitbook."},{"location":"spark-structured-streaming-batch-processing-time/","text":"Batch Processing Time \u00b6 Batch Processing Time (aka Batch Timeout Threshold ) is the processing time ( processing timestamp ) of the current streaming batch. The following standard functions (and their Catalyst expressions) allow accessing the batch processing time in Micro-Batch Stream Processing : now , current_timestamp , and unix_timestamp functions ( CurrentTimestamp ) current_date function ( CurrentDate ) Note CurrentTimestamp or CurrentDate expressions are not supported in Continuous Stream Processing . Internals \u00b6 GroupStateImpl is given the batch processing time when created for a streaming query (that is actually the batch processing time of the FlatMapGroupsWithStateExec physical operator). When created, FlatMapGroupsWithStateExec physical operator has the processing time undefined and set to the current timestamp in the < > every streaming batch. The current timestamp (and other batch-specific configurations) is given as the < > (as part of the query planning phase) when a stream execution engine does the following: MicroBatchExecution is requested to < > in < > In < > the base StreamExecution is requested to run stream processing and initializes OffsetSeqMetadata to 0 s.","title":"Batch Processing Time"},{"location":"spark-structured-streaming-batch-processing-time/#batch-processing-time","text":"Batch Processing Time (aka Batch Timeout Threshold ) is the processing time ( processing timestamp ) of the current streaming batch. The following standard functions (and their Catalyst expressions) allow accessing the batch processing time in Micro-Batch Stream Processing : now , current_timestamp , and unix_timestamp functions ( CurrentTimestamp ) current_date function ( CurrentDate ) Note CurrentTimestamp or CurrentDate expressions are not supported in Continuous Stream Processing .","title":"Batch Processing Time"},{"location":"spark-structured-streaming-batch-processing-time/#internals","text":"GroupStateImpl is given the batch processing time when created for a streaming query (that is actually the batch processing time of the FlatMapGroupsWithStateExec physical operator). When created, FlatMapGroupsWithStateExec physical operator has the processing time undefined and set to the current timestamp in the < > every streaming batch. The current timestamp (and other batch-specific configurations) is given as the < > (as part of the query planning phase) when a stream execution engine does the following: MicroBatchExecution is requested to < > in < > In < > the base StreamExecution is requested to run stream processing and initializes OffsetSeqMetadata to 0 s.","title":"Internals"},{"location":"spark-structured-streaming-internals/","text":"Internals of Streaming Queries \u00b6 The page is to keep notes about how to guide readers through the codebase and may disappear if merged with the other pages or become an intro page. DataStreamReader and Streaming Data Source Data Source Resolution, Streaming Dataset and Logical Query Plan Dataset API \u2014 High-Level DSL to Build Logical Query Plan DataStreamWriter and Streaming Data Sink StreamingQuery StreamingQueryManager DataStreamReader and Streaming Data Source \u00b6 It all starts with SparkSession.readStream method which lets you define a streaming source in a stream processing pipeline ( streaming processing graph or dataflow graph ). import org.apache.spark.sql.SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val reader = spark . readStream import org.apache.spark.sql.streaming.DataStreamReader assert ( reader . isInstanceOf [ DataStreamReader ]) SparkSession.readStream method creates a DataStreamReader . The fluent API of DataStreamReader allows you to describe the input data source (e.g. DataStreamReader.format and DataStreamReader.options ) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia). reader . format ( \"csv\" ) . option ( \"delimiter\" , \"|\" ) There are a couple of built-in data source formats. Their names are the names of the corresponding DataStreamReader methods and so act like shortcuts of DataStreamReader.format (where you have to specify the format by name), i.e. csv , json , orc , parquet and text , followed by DataStreamReader.load . You may also want to use DataStreamReader.schema method to specify the schema of the streaming data source. reader . schema ( \"a INT, b STRING\" ) In the end, you use DataStreamReader.load method that simply creates a streaming Dataset (the good ol' Dataset that you may have already used in Spark SQL). val input = reader . format ( \"csv\" ) . option ( \"delimiter\" , \"\\t\" ) . schema ( \"word STRING, num INT\" ) . load ( \"data/streaming\" ) import org.apache.spark.sql.DataFrame assert ( input . isInstanceOf [ DataFrame ]) The Dataset has the isStreaming property enabled that is basically the only way you could distinguish streaming Datasets from regular, batch Datasets. assert ( input . isStreaming ) In other words, Spark Structured Streaming is designed to extend the features of Spark SQL and let your structured queries be streaming queries. Data Source Resolution, Streaming Dataset and Logical Query Plan \u00b6 Whenever you create a Dataset (be it batch in Spark SQL or streaming in Spark Structured Streaming) is when you create a logical query plan using the High-Level Dataset DSL . A logical query plan is made up of logical operators. Spark Structured Streaming gives you two logical operators to represent streaming sources ( StreamingRelationV2 and StreamingRelation ). When DataStreamReader.load method is executed, load first looks up the requested data source (that you specified using DataStreamReader.format ) and creates an instance of it ( instantiation ). That'd be data source resolution step (that I described in...FIXME). DataStreamReader.load is where you can find the intersection of the former Micro-Batch Stream Processing V1 API with the new Continuous Stream Processing V2 API. V2 Code Path \u00b6 For MicroBatchReadSupport or ContinuousReadSupport data sources, DataStreamReader.load creates a logical query plan with a StreamingRelationV2 leaf logical operator. That is the new V2 code path . // rate data source is V2 val rates = spark.readStream.format(\"rate\").load val plan = rates.queryExecution.logical scala> println(plan.numberedTreeString) 00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2ed03b1a, rate, [timestamp#12, value#13L] V1 Code Path \u00b6 For all other types of streaming data sources, DataStreamReader.load creates a logical query plan with a StreamingRelation leaf logical operator. That is the former V1 code path . // text data source is V1 val texts = spark.readStream.format(\"text\").load(\"data/streaming\") val plan = texts.queryExecution.logical scala> println(plan.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@35edd886,text,List(),None,List(),None,Map(path -> data/streaming),None), FileSource[data/streaming], [value#18] Dataset API \u2014 High-Level DSL to Build Logical Query Plan \u00b6 With a streaming Dataset created, you can now use all the methods of Dataset API, including but not limited to the following operators: Dataset.dropDuplicates for streaming deduplication Dataset.groupBy and Dataset.groupByKey for streaming aggregation Dataset.withWatermark for event time watermark Please note that a streaming Dataset is a regular Dataset ( with some streaming-related limitations ). val rates = spark .readStream .format(\"rate\") .load val countByTime = rates .withWatermark(\"timestamp\", \"10 seconds\") .groupBy($\"timestamp\") .agg(count(\"*\") as \"count\") import org.apache.spark.sql.Dataset assert(countByTime.isInstanceOf[Dataset[_]]) The point is to understand that the Dataset API is a domain-specific language (DSL) to build a more sophisticated stream processing pipeline that you could also build using the low-level logical operators directly. Use Dataset.explain to learn the underlying logical and physical query plans. assert(countByTime.isStreaming) scala> countByTime.explain(extended = true) == Parsed Logical Plan == 'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Analyzed Logical Plan == timestamp: timestamp, count: bigint Aggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Optimized Logical Plan == Aggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- Project [timestamp#88] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Physical Plan == *(5) HashAggregate(keys=[timestamp#88-T10000ms], functions=[count(1)], output=[timestamp#88-T10000ms, count#131L]) +- StateStoreSave [timestamp#88-T10000ms], state info [ checkpoint = <unknown>, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2 +- *(4) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- StateStoreRestore [timestamp#88-T10000ms], state info [ checkpoint = <unknown>, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], 2 +- *(3) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- Exchange hashpartitioning(timestamp#88-T10000ms, 200) +- *(2) HashAggregate(keys=[timestamp#88-T10000ms], functions=[partial_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- *(1) Project [timestamp#88] +- StreamingRelation rate, [timestamp#88, value#89L] Or go pro and talk to QueryExecution directly. val plan = countByTime.queryExecution.logical scala> println(plan.numberedTreeString) 00 'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L] 01 +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds 02 +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] Please note that most of the stream processing operators you may also have used in batch structured queries in Spark SQL. Again, the distinction between Spark SQL and Spark Structured Streaming is very thin from a developer's point of view. DataStreamWriter and Streaming Data Sink \u00b6 Once you're satisfied with building a stream processing pipeline (using the APIs of DataStreamReader , Dataset , RelationalGroupedDataset and KeyValueGroupedDataset ), you should define how and when the result of the streaming query is persisted in ( sent out to ) an external data system using a streaming sink . You should use Dataset.writeStream method that simply creates a DataStreamWriter . // Not only is this a Dataset, but it is also streaming assert(countByTime.isStreaming) val writer = countByTime.writeStream import org.apache.spark.sql.streaming.DataStreamWriter assert(writer.isInstanceOf[DataStreamWriter[_]]) The fluent API of DataStreamWriter allows you to describe the output data sink ( DataStreamWriter.format and DataStreamWriter.options ) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia). writer .format(\"csv\") .option(\"delimiter\", \"\\t\") Like in DataStreamReader data source formats, there are a couple of built-in data sink formats. Unlike data source formats, their names do not have corresponding DataStreamWriter methods. The reason is that you will use DataStreamWriter.start to create and immediately start a StreamingQuery . There are however two special output formats that do have corresponding DataStreamWriter methods, i.e. DataStreamWriter.foreach and DataStreamWriter.foreachBatch , that allow for persisting query results to external data systems that do not have streaming sinks available. They give you a trade-off between developing a full-blown streaming sink and simply using the methods (that lay the basis of what a custom sink would have to do anyway). DataStreamWriter API defines two new concepts (that are not available in the \"base\" Spark SQL): OutputMode that you specify using DataStreamWriter.outputMode method Trigger that you specify using DataStreamWriter.trigger method You may also want to give a streaming query a name using DataStreamWriter.queryName method. In the end, you use DataStreamWriter.start method to create and immediately start a StreamingQuery . import org.apache.spark.sql.streaming.OutputMode import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = writer .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", \"/tmp/csv-to-csv-checkpoint\") .outputMode(OutputMode.Append) .trigger(Trigger.ProcessingTime(30.seconds)) .queryName(\"csv-to-csv\") .start(\"/tmp\") import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) When DataStreamWriter is requested to start a streaming query , it allows for the following data source formats: memory with MemorySinkV2 (with ContinuousTrigger ) or MemorySink foreach with ForeachWriterProvider sink foreachBatch with ForeachBatchSink sink (that does not support ContinuousTrigger ) Any DataSourceRegister data source Custom data sources specified by their fully-qualified class names or [name].DefaultSource avro , kafka and some others (see DataSource.lookupDataSource object method) StreamWriteSupport DataSource is requested to create a streaming sink that accepts StreamSinkProvider or FileFormat data sources only With a streaming sink, DataStreamWriter requests the StreamingQueryManager to start a streaming query . StreamingQuery \u00b6 When a stream processing pipeline is started (using DataStreamWriter.start method), DataStreamWriter creates a StreamingQuery and requests the StreamingQueryManager to start a streaming query . StreamingQueryManager \u00b6 StreamingQueryManager is used to manage streaming queries.","title":"Internals of Streaming Queries"},{"location":"spark-structured-streaming-internals/#internals-of-streaming-queries","text":"The page is to keep notes about how to guide readers through the codebase and may disappear if merged with the other pages or become an intro page. DataStreamReader and Streaming Data Source Data Source Resolution, Streaming Dataset and Logical Query Plan Dataset API \u2014 High-Level DSL to Build Logical Query Plan DataStreamWriter and Streaming Data Sink StreamingQuery StreamingQueryManager","title":"Internals of Streaming Queries"},{"location":"spark-structured-streaming-internals/#datastreamreader-and-streaming-data-source","text":"It all starts with SparkSession.readStream method which lets you define a streaming source in a stream processing pipeline ( streaming processing graph or dataflow graph ). import org.apache.spark.sql.SparkSession assert ( spark . isInstanceOf [ SparkSession ]) val reader = spark . readStream import org.apache.spark.sql.streaming.DataStreamReader assert ( reader . isInstanceOf [ DataStreamReader ]) SparkSession.readStream method creates a DataStreamReader . The fluent API of DataStreamReader allows you to describe the input data source (e.g. DataStreamReader.format and DataStreamReader.options ) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia). reader . format ( \"csv\" ) . option ( \"delimiter\" , \"|\" ) There are a couple of built-in data source formats. Their names are the names of the corresponding DataStreamReader methods and so act like shortcuts of DataStreamReader.format (where you have to specify the format by name), i.e. csv , json , orc , parquet and text , followed by DataStreamReader.load . You may also want to use DataStreamReader.schema method to specify the schema of the streaming data source. reader . schema ( \"a INT, b STRING\" ) In the end, you use DataStreamReader.load method that simply creates a streaming Dataset (the good ol' Dataset that you may have already used in Spark SQL). val input = reader . format ( \"csv\" ) . option ( \"delimiter\" , \"\\t\" ) . schema ( \"word STRING, num INT\" ) . load ( \"data/streaming\" ) import org.apache.spark.sql.DataFrame assert ( input . isInstanceOf [ DataFrame ]) The Dataset has the isStreaming property enabled that is basically the only way you could distinguish streaming Datasets from regular, batch Datasets. assert ( input . isStreaming ) In other words, Spark Structured Streaming is designed to extend the features of Spark SQL and let your structured queries be streaming queries.","title":" DataStreamReader and Streaming Data Source"},{"location":"spark-structured-streaming-internals/#data-source-resolution-streaming-dataset-and-logical-query-plan","text":"Whenever you create a Dataset (be it batch in Spark SQL or streaming in Spark Structured Streaming) is when you create a logical query plan using the High-Level Dataset DSL . A logical query plan is made up of logical operators. Spark Structured Streaming gives you two logical operators to represent streaming sources ( StreamingRelationV2 and StreamingRelation ). When DataStreamReader.load method is executed, load first looks up the requested data source (that you specified using DataStreamReader.format ) and creates an instance of it ( instantiation ). That'd be data source resolution step (that I described in...FIXME). DataStreamReader.load is where you can find the intersection of the former Micro-Batch Stream Processing V1 API with the new Continuous Stream Processing V2 API.","title":" Data Source Resolution, Streaming Dataset and Logical Query Plan"},{"location":"spark-structured-streaming-internals/#v2-code-path","text":"For MicroBatchReadSupport or ContinuousReadSupport data sources, DataStreamReader.load creates a logical query plan with a StreamingRelationV2 leaf logical operator. That is the new V2 code path . // rate data source is V2 val rates = spark.readStream.format(\"rate\").load val plan = rates.queryExecution.logical scala> println(plan.numberedTreeString) 00 StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2ed03b1a, rate, [timestamp#12, value#13L]","title":"V2 Code Path"},{"location":"spark-structured-streaming-internals/#v1-code-path","text":"For all other types of streaming data sources, DataStreamReader.load creates a logical query plan with a StreamingRelation leaf logical operator. That is the former V1 code path . // text data source is V1 val texts = spark.readStream.format(\"text\").load(\"data/streaming\") val plan = texts.queryExecution.logical scala> println(plan.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@35edd886,text,List(),None,List(),None,Map(path -> data/streaming),None), FileSource[data/streaming], [value#18]","title":"V1 Code Path"},{"location":"spark-structured-streaming-internals/#dataset-api-high-level-dsl-to-build-logical-query-plan","text":"With a streaming Dataset created, you can now use all the methods of Dataset API, including but not limited to the following operators: Dataset.dropDuplicates for streaming deduplication Dataset.groupBy and Dataset.groupByKey for streaming aggregation Dataset.withWatermark for event time watermark Please note that a streaming Dataset is a regular Dataset ( with some streaming-related limitations ). val rates = spark .readStream .format(\"rate\") .load val countByTime = rates .withWatermark(\"timestamp\", \"10 seconds\") .groupBy($\"timestamp\") .agg(count(\"*\") as \"count\") import org.apache.spark.sql.Dataset assert(countByTime.isInstanceOf[Dataset[_]]) The point is to understand that the Dataset API is a domain-specific language (DSL) to build a more sophisticated stream processing pipeline that you could also build using the low-level logical operators directly. Use Dataset.explain to learn the underlying logical and physical query plans. assert(countByTime.isStreaming) scala> countByTime.explain(extended = true) == Parsed Logical Plan == 'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Analyzed Logical Plan == timestamp: timestamp, count: bigint Aggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Optimized Logical Plan == Aggregate [timestamp#88-T10000ms], [timestamp#88-T10000ms, count(1) AS count#131L] +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- Project [timestamp#88] +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] == Physical Plan == *(5) HashAggregate(keys=[timestamp#88-T10000ms], functions=[count(1)], output=[timestamp#88-T10000ms, count#131L]) +- StateStoreSave [timestamp#88-T10000ms], state info [ checkpoint = <unknown>, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], Append, 0, 2 +- *(4) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- StateStoreRestore [timestamp#88-T10000ms], state info [ checkpoint = <unknown>, runId = 28606ba5-9c7f-4f1f-ae41-e28d75c4d948, opId = 0, ver = 0, numPartitions = 200], 2 +- *(3) HashAggregate(keys=[timestamp#88-T10000ms], functions=[merge_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- Exchange hashpartitioning(timestamp#88-T10000ms, 200) +- *(2) HashAggregate(keys=[timestamp#88-T10000ms], functions=[partial_count(1)], output=[timestamp#88-T10000ms, count#136L]) +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds +- *(1) Project [timestamp#88] +- StreamingRelation rate, [timestamp#88, value#89L] Or go pro and talk to QueryExecution directly. val plan = countByTime.queryExecution.logical scala> println(plan.numberedTreeString) 00 'Aggregate ['timestamp], [unresolvedalias('timestamp, None), count(1) AS count#131L] 01 +- EventTimeWatermark timestamp#88: timestamp, interval 10 seconds 02 +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.sources.RateStreamProvider@2fcb3082, rate, [timestamp#88, value#89L] Please note that most of the stream processing operators you may also have used in batch structured queries in Spark SQL. Again, the distinction between Spark SQL and Spark Structured Streaming is very thin from a developer's point of view.","title":" Dataset API &mdash; High-Level DSL to Build Logical Query Plan"},{"location":"spark-structured-streaming-internals/#datastreamwriter-and-streaming-data-sink","text":"Once you're satisfied with building a stream processing pipeline (using the APIs of DataStreamReader , Dataset , RelationalGroupedDataset and KeyValueGroupedDataset ), you should define how and when the result of the streaming query is persisted in ( sent out to ) an external data system using a streaming sink . You should use Dataset.writeStream method that simply creates a DataStreamWriter . // Not only is this a Dataset, but it is also streaming assert(countByTime.isStreaming) val writer = countByTime.writeStream import org.apache.spark.sql.streaming.DataStreamWriter assert(writer.isInstanceOf[DataStreamWriter[_]]) The fluent API of DataStreamWriter allows you to describe the output data sink ( DataStreamWriter.format and DataStreamWriter.options ) using method chaining (with the goal of making the readability of the source code close to that of ordinary written prose, essentially creating a domain-specific language within the interface. See Fluent interface article in Wikipedia). writer .format(\"csv\") .option(\"delimiter\", \"\\t\") Like in DataStreamReader data source formats, there are a couple of built-in data sink formats. Unlike data source formats, their names do not have corresponding DataStreamWriter methods. The reason is that you will use DataStreamWriter.start to create and immediately start a StreamingQuery . There are however two special output formats that do have corresponding DataStreamWriter methods, i.e. DataStreamWriter.foreach and DataStreamWriter.foreachBatch , that allow for persisting query results to external data systems that do not have streaming sinks available. They give you a trade-off between developing a full-blown streaming sink and simply using the methods (that lay the basis of what a custom sink would have to do anyway). DataStreamWriter API defines two new concepts (that are not available in the \"base\" Spark SQL): OutputMode that you specify using DataStreamWriter.outputMode method Trigger that you specify using DataStreamWriter.trigger method You may also want to give a streaming query a name using DataStreamWriter.queryName method. In the end, you use DataStreamWriter.start method to create and immediately start a StreamingQuery . import org.apache.spark.sql.streaming.OutputMode import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = writer .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", \"/tmp/csv-to-csv-checkpoint\") .outputMode(OutputMode.Append) .trigger(Trigger.ProcessingTime(30.seconds)) .queryName(\"csv-to-csv\") .start(\"/tmp\") import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery]) When DataStreamWriter is requested to start a streaming query , it allows for the following data source formats: memory with MemorySinkV2 (with ContinuousTrigger ) or MemorySink foreach with ForeachWriterProvider sink foreachBatch with ForeachBatchSink sink (that does not support ContinuousTrigger ) Any DataSourceRegister data source Custom data sources specified by their fully-qualified class names or [name].DefaultSource avro , kafka and some others (see DataSource.lookupDataSource object method) StreamWriteSupport DataSource is requested to create a streaming sink that accepts StreamSinkProvider or FileFormat data sources only With a streaming sink, DataStreamWriter requests the StreamingQueryManager to start a streaming query .","title":" DataStreamWriter and Streaming Data Sink"},{"location":"spark-structured-streaming-internals/#streamingquery","text":"When a stream processing pipeline is started (using DataStreamWriter.start method), DataStreamWriter creates a StreamingQuery and requests the StreamingQueryManager to start a streaming query .","title":" StreamingQuery"},{"location":"spark-structured-streaming-internals/#streamingquerymanager","text":"StreamingQueryManager is used to manage streaming queries.","title":" StreamingQueryManager"},{"location":"spark-structured-streaming/","text":"Spark Structured Streaming and Streaming Queries \u00b6 Spark Structured Streaming ( Structured Streaming or Spark Streams ) is the module of Apache Spark for stream processing using streaming queries . Streaming queries can be expressed using a high-level declarative streaming API ( Dataset API ) or good ol' SQL ( SQL over stream / streaming SQL ). The declarative streaming Dataset API and SQL are executed on the underlying highly-optimized Spark SQL engine. The semantics of the Structured Streaming model is as follows (see the article Structured Streaming In Apache Spark ): At any time, the output of a continuous application is equivalent to executing a batch job on a prefix of the data. As of Spark 2.2.0, Structured Streaming has been marked stable and ready for production use. With that the other older streaming module Spark Streaming is considered obsolete and not for developing new streaming applications with Apache Spark. Spark Structured Streaming comes with two stream execution engines for executing streaming queries: MicroBatchExecution for Micro-Batch Stream Processing ContinuousExecution for Continuous Stream Processing The goal of Spark Structured Streaming is to unify streaming, interactive, and batch queries over structured datasets for developing end-to-end stream processing applications dubbed continuous applications using Spark SQL's Datasets API with additional support for the following features: Streaming Aggregation Streaming Join Streaming Watermark Arbitrary Stateful Streaming Aggregation Stateful Stream Processing In Structured Streaming, Spark developers describe custom streaming computations in the same way as with Spark SQL. Internally, Structured Streaming applies the user-defined structured query to the continuously and indefinitely arriving data to analyze real-time streaming data. Structured Streaming introduces the concept of streaming datasets that are infinite datasets with primitives like input streaming data sources and output streaming data sinks . A Dataset is streaming when its logical plan is streaming. val batchQuery = spark . read . // <-- batch non-streaming query csv ( \"sales\" ) assert ( batchQuery . isStreaming == false ) val streamingQuery = spark . readStream . // <-- streaming query format ( \"rate\" ). load assert ( streamingQuery . isStreaming ) Note Read up on Spark SQL, Datasets and logical plans in The Internals of Spark SQL online book. Structured Streaming models a stream of data as an infinite (and hence continuous) table that could be changed every streaming batch. You can specify output mode of a streaming dataset which is what gets written to a streaming sink (i.e. the infinite result table) when there is a new data available. Streaming Datasets use streaming query plans (as opposed to regular batch Datasets that are based on batch query plans). Note From this perspective, batch queries can be considered streaming Datasets executed once only (and is why some batch queries, e.g. KafkaSource , can easily work in batch mode). val batchQuery = spark . read . format ( \"rate\" ). load assert ( batchQuery . isStreaming == false ) val streamingQuery = spark . readStream . format ( \"rate\" ). load assert ( streamingQuery . isStreaming ) With Structured Streaming, Spark 2 aims at simplifying streaming analytics with little to no need to reason about effective data streaming (trying to hide the unnecessary complexity in your streaming analytics architectures). Structured streaming is defined by the following data abstractions in org.apache.spark.sql.streaming package: StreamingQuery Streaming Source Streaming Sink StreamingQueryManager Structured Streaming follows micro-batch model and periodically fetches data from the data source (and uses the DataFrame data abstraction to represent the fetched data for a certain batch). With Datasets as Spark SQL's view of structured data, structured streaming checks input sources for new data every trigger (time) and executes the (continuous) queries. Note The feature has also been called Streaming Spark SQL Query , Streaming DataFrames , Continuous DataFrame or Continuous Query . There have been lots of names before the Spark project settled on Structured Streaming. References \u00b6 Articles \u00b6 SPARK-8360 Structured Streaming (aka Streaming DataFrames) The official Structured Streaming Programming Guide Structured Streaming In Apache Spark What Spark's Structured Streaming really means Videos \u00b6 The Future of Real Time in Spark from Spark Summit East 2016 in which Reynold Xin presents the concept of Streaming DataFrames Structuring Spark: DataFrames, Datasets, and Streaming A Deep Dive Into Structured Streaming by Tathagata \"TD\" Das from Spark Summit 2016 Arbitrary Stateful Aggregations in Structured Streaming in Apache Spark by Burak Yavuz","title":"Spark Structured Streaming and Streaming Queries"},{"location":"spark-structured-streaming/#spark-structured-streaming-and-streaming-queries","text":"Spark Structured Streaming ( Structured Streaming or Spark Streams ) is the module of Apache Spark for stream processing using streaming queries . Streaming queries can be expressed using a high-level declarative streaming API ( Dataset API ) or good ol' SQL ( SQL over stream / streaming SQL ). The declarative streaming Dataset API and SQL are executed on the underlying highly-optimized Spark SQL engine. The semantics of the Structured Streaming model is as follows (see the article Structured Streaming In Apache Spark ): At any time, the output of a continuous application is equivalent to executing a batch job on a prefix of the data. As of Spark 2.2.0, Structured Streaming has been marked stable and ready for production use. With that the other older streaming module Spark Streaming is considered obsolete and not for developing new streaming applications with Apache Spark. Spark Structured Streaming comes with two stream execution engines for executing streaming queries: MicroBatchExecution for Micro-Batch Stream Processing ContinuousExecution for Continuous Stream Processing The goal of Spark Structured Streaming is to unify streaming, interactive, and batch queries over structured datasets for developing end-to-end stream processing applications dubbed continuous applications using Spark SQL's Datasets API with additional support for the following features: Streaming Aggregation Streaming Join Streaming Watermark Arbitrary Stateful Streaming Aggregation Stateful Stream Processing In Structured Streaming, Spark developers describe custom streaming computations in the same way as with Spark SQL. Internally, Structured Streaming applies the user-defined structured query to the continuously and indefinitely arriving data to analyze real-time streaming data. Structured Streaming introduces the concept of streaming datasets that are infinite datasets with primitives like input streaming data sources and output streaming data sinks . A Dataset is streaming when its logical plan is streaming. val batchQuery = spark . read . // <-- batch non-streaming query csv ( \"sales\" ) assert ( batchQuery . isStreaming == false ) val streamingQuery = spark . readStream . // <-- streaming query format ( \"rate\" ). load assert ( streamingQuery . isStreaming ) Note Read up on Spark SQL, Datasets and logical plans in The Internals of Spark SQL online book. Structured Streaming models a stream of data as an infinite (and hence continuous) table that could be changed every streaming batch. You can specify output mode of a streaming dataset which is what gets written to a streaming sink (i.e. the infinite result table) when there is a new data available. Streaming Datasets use streaming query plans (as opposed to regular batch Datasets that are based on batch query plans). Note From this perspective, batch queries can be considered streaming Datasets executed once only (and is why some batch queries, e.g. KafkaSource , can easily work in batch mode). val batchQuery = spark . read . format ( \"rate\" ). load assert ( batchQuery . isStreaming == false ) val streamingQuery = spark . readStream . format ( \"rate\" ). load assert ( streamingQuery . isStreaming ) With Structured Streaming, Spark 2 aims at simplifying streaming analytics with little to no need to reason about effective data streaming (trying to hide the unnecessary complexity in your streaming analytics architectures). Structured streaming is defined by the following data abstractions in org.apache.spark.sql.streaming package: StreamingQuery Streaming Source Streaming Sink StreamingQueryManager Structured Streaming follows micro-batch model and periodically fetches data from the data source (and uses the DataFrame data abstraction to represent the fetched data for a certain batch). With Datasets as Spark SQL's view of structured data, structured streaming checks input sources for new data every trigger (time) and executes the (continuous) queries. Note The feature has also been called Streaming Spark SQL Query , Streaming DataFrames , Continuous DataFrame or Continuous Query . There have been lots of names before the Spark project settled on Structured Streaming.","title":"Spark Structured Streaming and Streaming Queries"},{"location":"spark-structured-streaming/#references","text":"","title":"References"},{"location":"spark-structured-streaming/#articles","text":"SPARK-8360 Structured Streaming (aka Streaming DataFrames) The official Structured Streaming Programming Guide Structured Streaming In Apache Spark What Spark's Structured Streaming really means","title":"Articles"},{"location":"spark-structured-streaming/#videos","text":"The Future of Real Time in Spark from Spark Summit East 2016 in which Reynold Xin presents the concept of Streaming DataFrames Structuring Spark: DataFrames, Datasets, and Streaming A Deep Dive Into Structured Streaming by Tathagata \"TD\" Das from Spark Summit 2016 Arbitrary Stateful Aggregations in Structured Streaming in Apache Spark by Burak Yavuz","title":"Videos"},{"location":"webui/","text":"== Web UI Web UI...FIXME CAUTION: FIXME What's visible on the plan diagram in the SQL tab of the UI","title":"Web UI"},{"location":"demo/StateStoreSaveExec-Complete/","text":"== Demo: StateStoreSaveExec with Complete Output Mode The following example code shows the behaviour of StateStoreSaveExec.md#doExecute-Complete[StateStoreSaveExec in Complete output mode]. [source, scala] \u00b6 // START: Only for easier debugging // The state is then only for one partition // which should make monitoring it easier import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1) scala> spark.sessionState.conf.numShufflePartitions res1: Int = 1 // END: Only for easier debugging // Read datasets from a Kafka topic // ./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT // Streaming aggregation using groupBy operator is required to have StateStoreSaveExec operator val valuesPerGroup = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load. withColumn(\"tokens\", split('value, \",\")). withColumn(\"group\", 'tokens(0)). withColumn(\"value\", 'tokens(1) cast \"int\"). select(\"group\", \"value\"). groupBy( \"group\"). agg(collect_list(\"value\") as \"values\"). orderBy( \"group\"). agg(collect_list(\"value\") as \"values\"). orderBy( \"group\".asc) // valuesPerGroup is a streaming Dataset with just one source // so it knows nothing about output mode or watermark yet // That's why StatefulOperatorStateInfo is generic // and no batch-specific values are printed out // That will be available after the first streaming batch // Use sq.explain to know the runtime-specific values scala> valuesPerGroup.explain == Physical Plan == *Sort [group#25 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#25 ASC NULLS FIRST, 1) +- ObjectHashAggregate(keys=[group#25], functions=[collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- StateStoreSave [group#25], StatefulOperatorStateInfo( ,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0), Append, 0 +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- StateStoreRestore [group#25], StatefulOperatorStateInfo( ,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0) +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- ObjectHashAggregate(keys=[group#25], functions=[partial_collect_list(value#36, 0, 0)]) +- *Project [split(cast(value#1 as string), ,)[0] AS group#25, cast(split(cast(value#1 as string), ,)[1] as int) AS value#36] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] // Start the query and hence StateStoreSaveExec // Use Complete output mode import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = valuesPerGroup. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Complete). start Batch: 0 \u00b6 +-----+------+ |group|values| +-----+------+ +-----+------+ // there's only 1 stateful operator and hence 0 for the index in stateOperators scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 0, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 60 } // publish 1 new key-value pair in a single streaming batch // 0,1 Batch: 1 \u00b6 +-----+------+ |group|values| +-----+------+ |0 |[1] | +-----+------+ // it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier (it's just started!) and so numRowsUpdated is 0 scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 324 } // publish new key and old key in a single streaming batch // new keys // 1,1 // updates to already-stored keys // 0,2 Batch: 2 \u00b6 +-----+------+ |group|values| +-----+------+ |0 |[2, 1]| |1 |[1] | +-----+------+ // it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier and so numRowsUpdated is...0?! // Think it's a BUG as it should've been 1 (for the row 0,2) // 8/30 Sent out a question to the Spark user mailing list scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 572 } // In the end... sq.stop","title":"StateStoreSaveExec with Complete Output Mode"},{"location":"demo/StateStoreSaveExec-Complete/#source-scala","text":"// START: Only for easier debugging // The state is then only for one partition // which should make monitoring it easier import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, 1) scala> spark.sessionState.conf.numShufflePartitions res1: Int = 1 // END: Only for easier debugging // Read datasets from a Kafka topic // ./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT // Streaming aggregation using groupBy operator is required to have StateStoreSaveExec operator val valuesPerGroup = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load. withColumn(\"tokens\", split('value, \",\")). withColumn(\"group\", 'tokens(0)). withColumn(\"value\", 'tokens(1) cast \"int\"). select(\"group\", \"value\"). groupBy( \"group\"). agg(collect_list(\"value\") as \"values\"). orderBy( \"group\"). agg(collect_list(\"value\") as \"values\"). orderBy( \"group\".asc) // valuesPerGroup is a streaming Dataset with just one source // so it knows nothing about output mode or watermark yet // That's why StatefulOperatorStateInfo is generic // and no batch-specific values are printed out // That will be available after the first streaming batch // Use sq.explain to know the runtime-specific values scala> valuesPerGroup.explain == Physical Plan == *Sort [group#25 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(group#25 ASC NULLS FIRST, 1) +- ObjectHashAggregate(keys=[group#25], functions=[collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- StateStoreSave [group#25], StatefulOperatorStateInfo( ,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0), Append, 0 +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- StateStoreRestore [group#25], StatefulOperatorStateInfo( ,899f0fd1-b202-45cd-9ebd-09101ca90fa8,0,0) +- ObjectHashAggregate(keys=[group#25], functions=[merge_collect_list(value#36, 0, 0)]) +- Exchange hashpartitioning(group#25, 1) +- ObjectHashAggregate(keys=[group#25], functions=[partial_collect_list(value#36, 0, 0)]) +- *Project [split(cast(value#1 as string), ,)[0] AS group#25, cast(split(cast(value#1 as string), ,)[1] as int) AS value#36] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] // Start the query and hence StateStoreSaveExec // Use Complete output mode import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = valuesPerGroup. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Complete). start","title":"[source, scala]"},{"location":"demo/StateStoreSaveExec-Complete/#batch-0","text":"+-----+------+ |group|values| +-----+------+ +-----+------+ // there's only 1 stateful operator and hence 0 for the index in stateOperators scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 0, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 60 } // publish 1 new key-value pair in a single streaming batch // 0,1","title":"Batch: 0"},{"location":"demo/StateStoreSaveExec-Complete/#batch-1","text":"+-----+------+ |group|values| +-----+------+ |0 |[1] | +-----+------+ // it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier (it's just started!) and so numRowsUpdated is 0 scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 324 } // publish new key and old key in a single streaming batch // new keys // 1,1 // updates to already-stored keys // 0,2","title":"Batch: 1"},{"location":"demo/StateStoreSaveExec-Complete/#batch-2","text":"+-----+------+ |group|values| +-----+------+ |0 |[2, 1]| |1 |[1] | +-----+------+ // it's Complete output mode so numRowsTotal is the number of keys in the state store // no keys were available earlier and so numRowsUpdated is...0?! // Think it's a BUG as it should've been 1 (for the row 0,2) // 8/30 Sent out a question to the Spark user mailing list scala> println(sq.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 572 } // In the end... sq.stop","title":"Batch: 2"},{"location":"demo/StateStoreSaveExec-Update/","text":"== Demo: StateStoreSaveExec with Update Output Mode CAUTION: FIXME Example of Update with StateStoreSaveExec (and optional watermark)","title":"StateStoreSaveExec with Update Output Mode"},{"location":"demo/StreamingQueryManager-awaitAnyTermination-resetTerminated/","text":"Demo: Using StreamingQueryManager for Query Termination Management \u00b6 The demo shows how to use StreamingQueryManager (and specifically awaitAnyTermination and resetTerminated ) for query termination management. // Save the code as demo-StreamingQueryManager.scala // Start it using spark-shell // $ ./bin/spark-shell -i demo-StreamingQueryManager.scala // Register a StreamingQueryListener to receive notifications about state changes of streaming queries import org.apache.spark.sql.streaming.StreamingQueryListener val myQueryListener = new StreamingQueryListener { import org.apache.spark.sql.streaming.StreamingQueryListener._ def onQueryTerminated(event: QueryTerminatedEvent): Unit = { println(s\"Query ${event.id} terminated\") } def onQueryStarted(event: QueryStartedEvent): Unit = {} def onQueryProgress(event: QueryProgressEvent): Unit = {} } spark.streams.addListener(myQueryListener) import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ // Start streaming queries // Start the first query val q4s = spark.readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(4.seconds)). option(\"truncate\", false). start // Start another query that is slightly slower val q10s = spark.readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). option(\"truncate\", false). start // Both queries run concurrently // You should see different outputs in the console // q4s prints out 4 rows every batch and twice as often as q10s // q10s prints out 10 rows every batch /* ------------------------------------------- Batch: 7 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:07.462|21 | |2017-10-27 13:44:08.462|22 | |2017-10-27 13:44:09.462|23 | |2017-10-27 13:44:10.462|24 | +-----------------------+-----+ ------------------------------------------- Batch: 8 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:11.462|25 | |2017-10-27 13:44:12.462|26 | |2017-10-27 13:44:13.462|27 | |2017-10-27 13:44:14.462|28 | +-----------------------+-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:09.847|6 | |2017-10-27 13:44:10.847|7 | |2017-10-27 13:44:11.847|8 | |2017-10-27 13:44:12.847|9 | |2017-10-27 13:44:13.847|10 | |2017-10-27 13:44:14.847|11 | |2017-10-27 13:44:15.847|12 | |2017-10-27 13:44:16.847|13 | |2017-10-27 13:44:17.847|14 | |2017-10-27 13:44:18.847|15 | +-----------------------+-----+ */ // Stop q4s on a separate thread // as we're about to block the current thread awaiting query termination import java.util.concurrent.Executors import java.util.concurrent.TimeUnit.SECONDS def queryTerminator(query: StreamingQuery) = new Runnable { def run = { println(s\"Stopping streaming query: ${query.id}\") query.stop } } import java.util.concurrent.TimeUnit.SECONDS // Stop the first query after 10 seconds Executors.newSingleThreadScheduledExecutor. scheduleWithFixedDelay(queryTerminator(q4s), 10, 60 * 5, SECONDS) // Stop the other query after 20 seconds Executors.newSingleThreadScheduledExecutor. scheduleWithFixedDelay(queryTerminator(q10s), 20, 60 * 5, SECONDS) // Use StreamingQueryManager to wait for any query termination (either q1 or q2) // the current thread will block indefinitely until either streaming query has finished spark.streams.awaitAnyTermination // You are here only after either streaming query has finished // Executing spark.streams.awaitAnyTermination again would return immediately // You should have received the QueryTerminatedEvent for the query termination // reset the last terminated streaming query spark.streams.resetTerminated // You know at least one query has terminated // Wait for the other query to terminate spark.streams.awaitAnyTermination assert(spark.streams.active.isEmpty) println(\"The demo went all fine. Exiting...\") // leave spark-shell System.exit(0)","title":"Using StreamingQueryManager for Query Termination Management"},{"location":"demo/StreamingQueryManager-awaitAnyTermination-resetTerminated/#demo-using-streamingquerymanager-for-query-termination-management","text":"The demo shows how to use StreamingQueryManager (and specifically awaitAnyTermination and resetTerminated ) for query termination management. // Save the code as demo-StreamingQueryManager.scala // Start it using spark-shell // $ ./bin/spark-shell -i demo-StreamingQueryManager.scala // Register a StreamingQueryListener to receive notifications about state changes of streaming queries import org.apache.spark.sql.streaming.StreamingQueryListener val myQueryListener = new StreamingQueryListener { import org.apache.spark.sql.streaming.StreamingQueryListener._ def onQueryTerminated(event: QueryTerminatedEvent): Unit = { println(s\"Query ${event.id} terminated\") } def onQueryStarted(event: QueryStartedEvent): Unit = {} def onQueryProgress(event: QueryProgressEvent): Unit = {} } spark.streams.addListener(myQueryListener) import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ // Start streaming queries // Start the first query val q4s = spark.readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(4.seconds)). option(\"truncate\", false). start // Start another query that is slightly slower val q10s = spark.readStream. format(\"rate\"). load. writeStream. format(\"console\"). trigger(Trigger.ProcessingTime(10.seconds)). option(\"truncate\", false). start // Both queries run concurrently // You should see different outputs in the console // q4s prints out 4 rows every batch and twice as often as q10s // q10s prints out 10 rows every batch /* ------------------------------------------- Batch: 7 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:07.462|21 | |2017-10-27 13:44:08.462|22 | |2017-10-27 13:44:09.462|23 | |2017-10-27 13:44:10.462|24 | +-----------------------+-----+ ------------------------------------------- Batch: 8 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:11.462|25 | |2017-10-27 13:44:12.462|26 | |2017-10-27 13:44:13.462|27 | |2017-10-27 13:44:14.462|28 | +-----------------------+-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-10-27 13:44:09.847|6 | |2017-10-27 13:44:10.847|7 | |2017-10-27 13:44:11.847|8 | |2017-10-27 13:44:12.847|9 | |2017-10-27 13:44:13.847|10 | |2017-10-27 13:44:14.847|11 | |2017-10-27 13:44:15.847|12 | |2017-10-27 13:44:16.847|13 | |2017-10-27 13:44:17.847|14 | |2017-10-27 13:44:18.847|15 | +-----------------------+-----+ */ // Stop q4s on a separate thread // as we're about to block the current thread awaiting query termination import java.util.concurrent.Executors import java.util.concurrent.TimeUnit.SECONDS def queryTerminator(query: StreamingQuery) = new Runnable { def run = { println(s\"Stopping streaming query: ${query.id}\") query.stop } } import java.util.concurrent.TimeUnit.SECONDS // Stop the first query after 10 seconds Executors.newSingleThreadScheduledExecutor. scheduleWithFixedDelay(queryTerminator(q4s), 10, 60 * 5, SECONDS) // Stop the other query after 20 seconds Executors.newSingleThreadScheduledExecutor. scheduleWithFixedDelay(queryTerminator(q10s), 20, 60 * 5, SECONDS) // Use StreamingQueryManager to wait for any query termination (either q1 or q2) // the current thread will block indefinitely until either streaming query has finished spark.streams.awaitAnyTermination // You are here only after either streaming query has finished // Executing spark.streams.awaitAnyTermination again would return immediately // You should have received the QueryTerminatedEvent for the query termination // reset the last terminated streaming query spark.streams.resetTerminated // You know at least one query has terminated // Wait for the other query to terminate spark.streams.awaitAnyTermination assert(spark.streams.active.isEmpty) println(\"The demo went all fine. Exiting...\") // leave spark-shell System.exit(0)","title":"Demo: Using StreamingQueryManager for Query Termination Management"},{"location":"demo/arbitrary-stateful-streaming-aggregation-flatMapGroupsWithState/","text":"Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator \u00b6 The following demo shows an example of Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState operator. import java.sql.Timestamp type DeviceId = Long case class Signal(timestamp: Timestamp, deviceId: DeviceId, value: Long) // input stream import org.apache.spark.sql.functions._ val signals = spark .readStream .format(\"rate\") .option(\"rowsPerSecond\", 1) .load .withColumn(\"deviceId\", rint(rand() * 10) cast \"int\") // 10 devices randomly assigned to values .withColumn(\"value\", $\"value\" % 10) // randomize the values (just for fun) .as[Signal] // convert to our type (from \"unpleasant\" Row) import org.apache.spark.sql.streaming.GroupState type Key = Int type Count = Long type State = Map[Key, Count] case class EventsCounted(deviceId: DeviceId, count: Long) def countValuesPerDevice( deviceId: Int, signals: Iterator[Signal], state: GroupState[State]): Iterator[EventsCounted] = { val values = signals.toSeq println(s\"Device: $deviceId\") println(s\"Signals (${values.size}):\") values.zipWithIndex.foreach { case (v, idx) => println(s\"$idx. $v\") } println(s\"State: $state\") // update the state with the count of elements for the key val initialState: State = Map(deviceId -> 0) val oldState = state.getOption.getOrElse(initialState) // the name to highlight that the state is for the key only val newValue = oldState(deviceId) + values.size val newState = Map(deviceId -> newValue) state.update(newState) // you must not return as it's already consumed // that leads to a very subtle error where no elements are in an iterator // iterators are one-pass data structures Iterator(EventsCounted(deviceId, newValue)) } // stream processing using flatMapGroupsWithState operator val deviceId: Signal => DeviceId = { case Signal(_, deviceId, _) => deviceId } val signalsByDevice = signals.groupByKey(deviceId) import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val signalCounter = signalsByDevice.flatMapGroupsWithState( outputMode = OutputMode.Append, timeoutConf = GroupStateTimeout.NoTimeout)(countValuesPerDevice) import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val sq = signalCounter. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Append). start","title":"Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator"},{"location":"demo/arbitrary-stateful-streaming-aggregation-flatMapGroupsWithState/#demo-arbitrary-stateful-streaming-aggregation-with-keyvaluegroupeddatasetflatmapgroupswithstate-operator","text":"The following demo shows an example of Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState operator. import java.sql.Timestamp type DeviceId = Long case class Signal(timestamp: Timestamp, deviceId: DeviceId, value: Long) // input stream import org.apache.spark.sql.functions._ val signals = spark .readStream .format(\"rate\") .option(\"rowsPerSecond\", 1) .load .withColumn(\"deviceId\", rint(rand() * 10) cast \"int\") // 10 devices randomly assigned to values .withColumn(\"value\", $\"value\" % 10) // randomize the values (just for fun) .as[Signal] // convert to our type (from \"unpleasant\" Row) import org.apache.spark.sql.streaming.GroupState type Key = Int type Count = Long type State = Map[Key, Count] case class EventsCounted(deviceId: DeviceId, count: Long) def countValuesPerDevice( deviceId: Int, signals: Iterator[Signal], state: GroupState[State]): Iterator[EventsCounted] = { val values = signals.toSeq println(s\"Device: $deviceId\") println(s\"Signals (${values.size}):\") values.zipWithIndex.foreach { case (v, idx) => println(s\"$idx. $v\") } println(s\"State: $state\") // update the state with the count of elements for the key val initialState: State = Map(deviceId -> 0) val oldState = state.getOption.getOrElse(initialState) // the name to highlight that the state is for the key only val newValue = oldState(deviceId) + values.size val newState = Map(deviceId -> newValue) state.update(newState) // you must not return as it's already consumed // that leads to a very subtle error where no elements are in an iterator // iterators are one-pass data structures Iterator(EventsCounted(deviceId, newValue)) } // stream processing using flatMapGroupsWithState operator val deviceId: Signal => DeviceId = { case Signal(_, deviceId, _) => deviceId } val signalsByDevice = signals.groupByKey(deviceId) import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val signalCounter = signalsByDevice.flatMapGroupsWithState( outputMode = OutputMode.Append, timeoutConf = GroupStateTimeout.NoTimeout)(countValuesPerDevice) import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val sq = signalCounter. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Append). start","title":"Demo: Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator"},{"location":"demo/current_timestamp/","text":"== Demo: current_timestamp Function For Processing Time in Streaming Queries The demo shows what happens when you use current_timestamp function in your structured queries. [NOTE] \u00b6 The main motivation was to answer the question https://stackoverflow.com/q/46274593/1305344[How to achieve ingestion time?] in Spark Structured Streaming. You're very welcome to upvote the question and answers at your earliest convenience. Thanks! \u00b6 Quoting the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]: Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records before they enter Flink and that event timestamp can be extracted from the record. That is exactly how event time is considered in withWatermark operator which you use to describe what column to use for event time. The column could be part of the input dataset or...generated. And that is the moment where my confusion starts. In order to generate the event time column for withWatermark operator you could use current_timestamp or current_date standard functions. [source, scala] \u00b6 // rate format gives event time // but let's generate a brand new column with ours // for demo purposes val values = spark. readStream. format(\"rate\"). load. withColumn(\"current_timestamp\", current_timestamp) scala> values.printSchema root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) |-- current_timestamp: timestamp (nullable = false) Both are special for Spark Structured Streaming as StreamExecution MicroBatchExecution.md#runBatch-triggerLogicalPlan[replaces] their underlying Catalyst expressions, CurrentTimestamp and CurrentDate respectively, with CurrentBatchTimestamp expression and the time of the current batch. [source, scala] \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = values. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). start // note the value of current_timestamp // that corresponds to the batch time Batch: 1 \u00b6 +-----------------------+-----+-------------------+ |timestamp |value|current_timestamp | +-----------------------+-----+-------------------+ |2017-09-18 10:53:31.523|0 |2017-09-18 10:53:40| |2017-09-18 10:53:32.523|1 |2017-09-18 10:53:40| |2017-09-18 10:53:33.523|2 |2017-09-18 10:53:40| |2017-09-18 10:53:34.523|3 |2017-09-18 10:53:40| |2017-09-18 10:53:35.523|4 |2017-09-18 10:53:40| |2017-09-18 10:53:36.523|5 |2017-09-18 10:53:40| |2017-09-18 10:53:37.523|6 |2017-09-18 10:53:40| |2017-09-18 10:53:38.523|7 |2017-09-18 10:53:40| +-----------------------+-----+-------------------+ // Use web UI's SQL tab for the batch (Submitted column) // or sq.recentProgress scala> println(sq.recentProgress(1).timestamp) 2017-09-18T08:53:40.000Z // Note current_batch_timestamp scala> sq.explain(extended = true) == Parsed Logical Plan == 'Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, None) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Analyzed Logical Plan == timestamp: timestamp, value: bigint, current_timestamp: timestamp Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, Some(Europe/Berlin)) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Optimized Logical Plan == Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Physical Plan == *Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- Scan ExistingRDD[timestamp#2137,value#2138L] That seems to be closer to processing time than ingestion time given the definition from the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]: Processing time refers to the system time of the machine that is executing the respective operation. Ingestion time is the time that events enter Flink. What do you think?","title":"current_timestamp Function For Processing Time in Streaming Queries"},{"location":"demo/current_timestamp/#note","text":"The main motivation was to answer the question https://stackoverflow.com/q/46274593/1305344[How to achieve ingestion time?] in Spark Structured Streaming.","title":"[NOTE]"},{"location":"demo/current_timestamp/#youre-very-welcome-to-upvote-the-question-and-answers-at-your-earliest-convenience-thanks","text":"Quoting the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]: Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records before they enter Flink and that event timestamp can be extracted from the record. That is exactly how event time is considered in withWatermark operator which you use to describe what column to use for event time. The column could be part of the input dataset or...generated. And that is the moment where my confusion starts. In order to generate the event time column for withWatermark operator you could use current_timestamp or current_date standard functions.","title":"You're very welcome to upvote the question and answers at your earliest convenience. Thanks!"},{"location":"demo/current_timestamp/#source-scala","text":"// rate format gives event time // but let's generate a brand new column with ours // for demo purposes val values = spark. readStream. format(\"rate\"). load. withColumn(\"current_timestamp\", current_timestamp) scala> values.printSchema root |-- timestamp: timestamp (nullable = true) |-- value: long (nullable = true) |-- current_timestamp: timestamp (nullable = false) Both are special for Spark Structured Streaming as StreamExecution MicroBatchExecution.md#runBatch-triggerLogicalPlan[replaces] their underlying Catalyst expressions, CurrentTimestamp and CurrentDate respectively, with CurrentBatchTimestamp expression and the time of the current batch.","title":"[source, scala]"},{"location":"demo/current_timestamp/#source-scala_1","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sq = values. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). start // note the value of current_timestamp // that corresponds to the batch time","title":"[source, scala]"},{"location":"demo/current_timestamp/#batch-1","text":"+-----------------------+-----+-------------------+ |timestamp |value|current_timestamp | +-----------------------+-----+-------------------+ |2017-09-18 10:53:31.523|0 |2017-09-18 10:53:40| |2017-09-18 10:53:32.523|1 |2017-09-18 10:53:40| |2017-09-18 10:53:33.523|2 |2017-09-18 10:53:40| |2017-09-18 10:53:34.523|3 |2017-09-18 10:53:40| |2017-09-18 10:53:35.523|4 |2017-09-18 10:53:40| |2017-09-18 10:53:36.523|5 |2017-09-18 10:53:40| |2017-09-18 10:53:37.523|6 |2017-09-18 10:53:40| |2017-09-18 10:53:38.523|7 |2017-09-18 10:53:40| +-----------------------+-----+-------------------+ // Use web UI's SQL tab for the batch (Submitted column) // or sq.recentProgress scala> println(sq.recentProgress(1).timestamp) 2017-09-18T08:53:40.000Z // Note current_batch_timestamp scala> sq.explain(extended = true) == Parsed Logical Plan == 'Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, None) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Analyzed Logical Plan == timestamp: timestamp, value: bigint, current_timestamp: timestamp Project [timestamp#2137, value#2138L, current_batch_timestamp(1505725650005, TimestampType, Some(Europe/Berlin)) AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Optimized Logical Plan == Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- LogicalRDD [timestamp#2137, value#2138L], true == Physical Plan == *Project [timestamp#2137, value#2138L, 1505725650005000 AS current_timestamp#50] +- Scan ExistingRDD[timestamp#2137,value#2138L] That seems to be closer to processing time than ingestion time given the definition from the https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/event_time.html[Apache Flink documentation]: Processing time refers to the system time of the machine that is executing the respective operation. Ingestion time is the time that events enter Flink. What do you think?","title":"Batch: 1"},{"location":"demo/custom-sink-webui/","text":"== Demo: Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI) The demo shows the steps to develop a custom spark-sql-streaming-Sink.md[streaming sink] and use it to monitor whether and what SQL queries are executed at runtime (using web UI's SQL tab). [NOTE] \u00b6 The main motivation was to answer the question https://stackoverflow.com/q/46162143/1305344[Why does a single structured query run multiple SQL queries per batch?] that happened to have turned out fairly surprising. You're very welcome to upvote the question and answers at your earliest convenience. Thanks! \u00b6 The steps are as follows: < > < > < > < > < > < > < > Findings (aka surprises ): Custom sinks require that you define a checkpoint location using checkpointLocation option (or spark-sql-streaming-properties.md#spark.sql.streaming.checkpointLocation[spark.sql.streaming.checkpointLocation] Spark property). Remove the checkpoint directory (or use a different one every start of a streaming query) to have consistent results. === [[DemoSink]] Creating Custom Sink -- DemoSink A streaming sink follows the spark-sql-streaming-Sink.md#contract[Sink contract] and a sample implementation could look as follows. [source, scala] \u00b6 package pl.japila.spark.sql.streaming case class DemoSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode) extends Sink { override def addBatch(batchId: Long, data: DataFrame): Unit = { println(s\"addBatch($batchId)\") data.explain() // Why so many lines just to show the input DataFrame? data.sparkSession.createDataFrame( data.sparkSession.sparkContext.parallelize(data.collect()), data.schema) .show(10) } } Save the file under src/main/scala in your project. === [[DemoSinkProvider]] Creating StreamSinkProvider -- DemoSinkProvider [source, scala] \u00b6 package pl.japila.spark.sql.streaming class DemoSinkProvider extends StreamSinkProvider with DataSourceRegister { override def createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink = { DemoSink(sqlContext, parameters, partitionColumns, outputMode) } override def shortName(): String = \"demo\" } Save the file under src/main/scala in your project. === [[registering-sink]] Optional Sink Registration using META-INF/services The step is optional, but greatly improve the experience when using the custom sink so you can use it by its name (rather than a fully-qualified class name or using a special class name for the sink provider). Create org.apache.spark.sql.sources.DataSourceRegister in META-INF/services directory with the following content. [source, scala] \u00b6 pl.japila.spark.sql.streaming.DemoSinkProvider \u00b6 Save the file under src/main/resources in your project. === [[build-sbt]] build.sbt Definition If you use my beloved build tool http://www.scala-sbt.org/[sbt ] to manage the project, use the following build.sbt . [source, scala] \u00b6 organization := \"pl.japila.spark\" name := \"spark-structured-streaming-demo-sink\" version := \"0.1\" scalaVersion := \"2.11.11\" libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.2.0\" \u00b6 === [[packaging-sink]] Packaging DemoSink The step depends on what build tool you use to manage the project. Use whatever command you use to create a jar file with the above classes compiled and bundled together. $ sbt package [info] Loading settings from plugins.sbt ... [info] Loading project definition from /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/project [info] Loading settings from build.sbt ... [info] Set current project to spark-structured-streaming-demo-sink (in build file:/Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/) [info] Compiling 1 Scala source to /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/classes ... [info] Done compiling. [info] Packaging /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar ... [info] Done packaging. [success] Total time: 5 s, completed Sep 12, 2017 9:34:19 AM The jar with the sink is /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar . === [[using-sink]] Using DemoSink in Streaming Query The following code reads data from the rate source and simply outputs the result to our custom DemoSink . // Make sure the DemoSink jar is available $ ls /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar // \"Install\" the DemoSink using --jars command-line option $ ./bin/spark-shell --jars /Users/jacek/dev/sandbox/spark-structured-streaming-custom-sink/target/scala-2.11/spark-structured-streaming-custom-sink_2.11-0.1.jar scala> spark.version res0: String = 2.3.0-SNAPSHOT import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ val sq = spark. readStream. format(\"rate\"). load. writeStream. format(\"demo\"). option(\"checkpointLocation\", \"/tmp/demo-checkpoint\"). trigger(Trigger.ProcessingTime(10.seconds)). start // In the end... scala> sq.stop 17/09/12 09:59:28 INFO StreamExecution: Query [id = 03cd78e3-94e2-439c-9c12-cfed0c996812, runId = 6938af91-9806-4404-965a-5ae7525d5d3f] was stopped === [[webui-sql-queries]] Monitoring SQL Queries using web UI's SQL Tab Open http://localhost:4040/SQL/ . You should find that every trigger (aka batch ) results in 3 SQL queries. Why? .web UI's SQL Tab and Completed Queries (3 Queries per Batch) image::images/webui-sql-completed-queries-three-per-batch.png[align=\"center\"] The answer lies in what sources and sink a streaming query uses (and differs per streaming query). In our case, < > collects the rows from the input DataFrame and shows it afterwards. That gives 2 SQL queries (as you can see after executing the following batch queries). [source, scala] \u00b6 // batch non-streaming query val data = (0 to 3).toDF(\"id\") // That gives one SQL query data.collect // That gives one SQL query, too data.show The remaining query (which is the first among the queries) is executed when you load the data. That can be observed easily when you change < > to not \"touch\" the input data (in addBatch ) in any way. [source, scala] \u00b6 override def addBatch(batchId: Long, data: DataFrame): Unit = { println(s\"addBatch($batchId)\") } Re-run the streaming query (using the new DemoSink ) and use web UI's SQL tab to see the queries. You should have just one query per batch (and no Spark jobs given nothing is really done in the sink's addBatch ). .web UI's SQL Tab and Completed Queries (1 Query per Batch) image::images/webui-sql-completed-queries-one-per-batch.png[align=\"center\"]","title":"Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI)"},{"location":"demo/custom-sink-webui/#note","text":"The main motivation was to answer the question https://stackoverflow.com/q/46162143/1305344[Why does a single structured query run multiple SQL queries per batch?] that happened to have turned out fairly surprising.","title":"[NOTE]"},{"location":"demo/custom-sink-webui/#youre-very-welcome-to-upvote-the-question-and-answers-at-your-earliest-convenience-thanks","text":"The steps are as follows: < > < > < > < > < > < > < > Findings (aka surprises ): Custom sinks require that you define a checkpoint location using checkpointLocation option (or spark-sql-streaming-properties.md#spark.sql.streaming.checkpointLocation[spark.sql.streaming.checkpointLocation] Spark property). Remove the checkpoint directory (or use a different one every start of a streaming query) to have consistent results. === [[DemoSink]] Creating Custom Sink -- DemoSink A streaming sink follows the spark-sql-streaming-Sink.md#contract[Sink contract] and a sample implementation could look as follows.","title":"You're very welcome to upvote the question and answers at your earliest convenience. Thanks!"},{"location":"demo/custom-sink-webui/#source-scala","text":"package pl.japila.spark.sql.streaming case class DemoSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode) extends Sink { override def addBatch(batchId: Long, data: DataFrame): Unit = { println(s\"addBatch($batchId)\") data.explain() // Why so many lines just to show the input DataFrame? data.sparkSession.createDataFrame( data.sparkSession.sparkContext.parallelize(data.collect()), data.schema) .show(10) } } Save the file under src/main/scala in your project. === [[DemoSinkProvider]] Creating StreamSinkProvider -- DemoSinkProvider","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#source-scala_1","text":"package pl.japila.spark.sql.streaming class DemoSinkProvider extends StreamSinkProvider with DataSourceRegister { override def createSink( sqlContext: SQLContext, parameters: Map[String, String], partitionColumns: Seq[String], outputMode: OutputMode): Sink = { DemoSink(sqlContext, parameters, partitionColumns, outputMode) } override def shortName(): String = \"demo\" } Save the file under src/main/scala in your project. === [[registering-sink]] Optional Sink Registration using META-INF/services The step is optional, but greatly improve the experience when using the custom sink so you can use it by its name (rather than a fully-qualified class name or using a special class name for the sink provider). Create org.apache.spark.sql.sources.DataSourceRegister in META-INF/services directory with the following content.","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#source-scala_2","text":"","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#pljapilasparksqlstreamingdemosinkprovider","text":"Save the file under src/main/resources in your project. === [[build-sbt]] build.sbt Definition If you use my beloved build tool http://www.scala-sbt.org/[sbt ] to manage the project, use the following build.sbt .","title":"pl.japila.spark.sql.streaming.DemoSinkProvider"},{"location":"demo/custom-sink-webui/#source-scala_3","text":"organization := \"pl.japila.spark\" name := \"spark-structured-streaming-demo-sink\" version := \"0.1\" scalaVersion := \"2.11.11\"","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#librarydependencies-orgapachespark-spark-sql-220","text":"=== [[packaging-sink]] Packaging DemoSink The step depends on what build tool you use to manage the project. Use whatever command you use to create a jar file with the above classes compiled and bundled together. $ sbt package [info] Loading settings from plugins.sbt ... [info] Loading project definition from /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/project [info] Loading settings from build.sbt ... [info] Set current project to spark-structured-streaming-demo-sink (in build file:/Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/) [info] Compiling 1 Scala source to /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/classes ... [info] Done compiling. [info] Packaging /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar ... [info] Done packaging. [success] Total time: 5 s, completed Sep 12, 2017 9:34:19 AM The jar with the sink is /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar . === [[using-sink]] Using DemoSink in Streaming Query The following code reads data from the rate source and simply outputs the result to our custom DemoSink . // Make sure the DemoSink jar is available $ ls /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar /Users/jacek/dev/sandbox/spark-structured-streaming-demo-sink/target/scala-2.11/spark-structured-streaming-demo-sink_2.11-0.1.jar // \"Install\" the DemoSink using --jars command-line option $ ./bin/spark-shell --jars /Users/jacek/dev/sandbox/spark-structured-streaming-custom-sink/target/scala-2.11/spark-structured-streaming-custom-sink_2.11-0.1.jar scala> spark.version res0: String = 2.3.0-SNAPSHOT import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ val sq = spark. readStream. format(\"rate\"). load. writeStream. format(\"demo\"). option(\"checkpointLocation\", \"/tmp/demo-checkpoint\"). trigger(Trigger.ProcessingTime(10.seconds)). start // In the end... scala> sq.stop 17/09/12 09:59:28 INFO StreamExecution: Query [id = 03cd78e3-94e2-439c-9c12-cfed0c996812, runId = 6938af91-9806-4404-965a-5ae7525d5d3f] was stopped === [[webui-sql-queries]] Monitoring SQL Queries using web UI's SQL Tab Open http://localhost:4040/SQL/ . You should find that every trigger (aka batch ) results in 3 SQL queries. Why? .web UI's SQL Tab and Completed Queries (3 Queries per Batch) image::images/webui-sql-completed-queries-three-per-batch.png[align=\"center\"] The answer lies in what sources and sink a streaming query uses (and differs per streaming query). In our case, < > collects the rows from the input DataFrame and shows it afterwards. That gives 2 SQL queries (as you can see after executing the following batch queries).","title":"libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.2.0\""},{"location":"demo/custom-sink-webui/#source-scala_4","text":"// batch non-streaming query val data = (0 to 3).toDF(\"id\") // That gives one SQL query data.collect // That gives one SQL query, too data.show The remaining query (which is the first among the queries) is executed when you load the data. That can be observed easily when you change < > to not \"touch\" the input data (in addBatch ) in any way.","title":"[source, scala]"},{"location":"demo/custom-sink-webui/#source-scala_5","text":"override def addBatch(batchId: Long, data: DataFrame): Unit = { println(s\"addBatch($batchId)\") } Re-run the streaming query (using the new DemoSink ) and use web UI's SQL tab to see the queries. You should have just one query per batch (and no Spark jobs given nothing is really done in the sink's addBatch ). .web UI's SQL Tab and Completed Queries (1 Query per Batch) image::images/webui-sql-completed-queries-one-per-batch.png[align=\"center\"]","title":"[source, scala]"},{"location":"demo/exploring-checkpointed-state/","text":"== Demo: Exploring Checkpointed State The following demo shows the internals of the checkpointed state of a < >. The demo uses the state checkpoint directory that was used in < >. [source, scala] \u00b6 // Change the path to match your configuration val checkpointRootLocation = \"/tmp/checkpoint-watermark_demo/state\" val version = 1L import org.apache.spark.sql.execution.streaming.state.StateStoreId val storeId = StateStoreId( checkpointRootLocation, operatorId = 0, partitionId = 0) // The key and value schemas should match the watermark demo // .groupBy(window($\"time\", windowDuration.toString) as \"sliding_window\") import org.apache.spark.sql.types.{TimestampType, StructField, StructType} val keySchema = StructType( StructField(\"sliding_window\", StructType( StructField(\"start\", TimestampType, nullable = true) :: StructField(\"end\", TimestampType, nullable = true) :: Nil), nullable = false) :: Nil) scala> keySchema.printTreeString root |-- sliding_window: struct (nullable = false) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) // .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\") import org.apache.spark.sql.types.{ArrayType, LongType} val valueSchema = StructType( StructField(\"batches\", ArrayType(LongType, true), true) :: StructField(\"values\", ArrayType(LongType, true), true) :: Nil) scala> valueSchema.printTreeString root |-- batches: array (nullable = true) | |-- element: long (containsNull = true) |-- values: array (nullable = true) | |-- element: long (containsNull = true) val indexOrdinal = None import org.apache.spark.sql.execution.streaming.state.StateStoreConf val storeConf = StateStoreConf(spark.sessionState.conf) val hadoopConf = spark.sessionState.newHadoopConf() import org.apache.spark.sql.execution.streaming.state.StateStoreProvider val provider = StateStoreProvider.createAndInit( storeId, keySchema, valueSchema, indexOrdinal, storeConf, hadoopConf) // You may want to use the following higher-level code instead import java.util.UUID val queryRunId = UUID.randomUUID import org.apache.spark.sql.execution.streaming.state.StateStoreProviderId val storeProviderId = StateStoreProviderId(storeId, queryRunId) import org.apache.spark.sql.execution.streaming.state.StateStore val store = StateStore.get( storeProviderId, keySchema, valueSchema, indexOrdinal, version, storeConf, hadoopConf) import org.apache.spark.sql.execution.streaming.state.UnsafeRowPair def formatRowPair(rowPair: UnsafeRowPair) = { s\"(${rowPair.key.getLong(0)}, ${rowPair.value.getLong(0)})\" } store.iterator.map(formatRowPair).foreach(println) // WIP: Missing value (per window) def formatRowPair(rowPair: UnsafeRowPair) = { val window = rowPair.key.getStruct(0, 2) import scala.concurrent.duration._ val begin = window.getLong(0).millis.toSeconds val end = window.getLong(1).millis.toSeconds val value = rowPair.value.getStruct(0, 4) // input is (time, value, batch) all longs val t = value.getLong(1).millis.toSeconds val v = value.getLong(2) val b = value.getLong(3) s\"(key: [$begin, end], ( end], ( t, $v, $b))\" } store.iterator.map(formatRowPair).foreach(println)","title":"Exploring Checkpointed State"},{"location":"demo/exploring-checkpointed-state/#source-scala","text":"// Change the path to match your configuration val checkpointRootLocation = \"/tmp/checkpoint-watermark_demo/state\" val version = 1L import org.apache.spark.sql.execution.streaming.state.StateStoreId val storeId = StateStoreId( checkpointRootLocation, operatorId = 0, partitionId = 0) // The key and value schemas should match the watermark demo // .groupBy(window($\"time\", windowDuration.toString) as \"sliding_window\") import org.apache.spark.sql.types.{TimestampType, StructField, StructType} val keySchema = StructType( StructField(\"sliding_window\", StructType( StructField(\"start\", TimestampType, nullable = true) :: StructField(\"end\", TimestampType, nullable = true) :: Nil), nullable = false) :: Nil) scala> keySchema.printTreeString root |-- sliding_window: struct (nullable = false) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) // .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\") import org.apache.spark.sql.types.{ArrayType, LongType} val valueSchema = StructType( StructField(\"batches\", ArrayType(LongType, true), true) :: StructField(\"values\", ArrayType(LongType, true), true) :: Nil) scala> valueSchema.printTreeString root |-- batches: array (nullable = true) | |-- element: long (containsNull = true) |-- values: array (nullable = true) | |-- element: long (containsNull = true) val indexOrdinal = None import org.apache.spark.sql.execution.streaming.state.StateStoreConf val storeConf = StateStoreConf(spark.sessionState.conf) val hadoopConf = spark.sessionState.newHadoopConf() import org.apache.spark.sql.execution.streaming.state.StateStoreProvider val provider = StateStoreProvider.createAndInit( storeId, keySchema, valueSchema, indexOrdinal, storeConf, hadoopConf) // You may want to use the following higher-level code instead import java.util.UUID val queryRunId = UUID.randomUUID import org.apache.spark.sql.execution.streaming.state.StateStoreProviderId val storeProviderId = StateStoreProviderId(storeId, queryRunId) import org.apache.spark.sql.execution.streaming.state.StateStore val store = StateStore.get( storeProviderId, keySchema, valueSchema, indexOrdinal, version, storeConf, hadoopConf) import org.apache.spark.sql.execution.streaming.state.UnsafeRowPair def formatRowPair(rowPair: UnsafeRowPair) = { s\"(${rowPair.key.getLong(0)}, ${rowPair.value.getLong(0)})\" } store.iterator.map(formatRowPair).foreach(println) // WIP: Missing value (per window) def formatRowPair(rowPair: UnsafeRowPair) = { val window = rowPair.key.getStruct(0, 2) import scala.concurrent.duration._ val begin = window.getLong(0).millis.toSeconds val end = window.getLong(1).millis.toSeconds val value = rowPair.value.getStruct(0, 4) // input is (time, value, batch) all longs val t = value.getLong(1).millis.toSeconds val v = value.getLong(2) val b = value.getLong(3) s\"(key: [$begin, end], ( end], ( t, $v, $b))\" } store.iterator.map(formatRowPair).foreach(println)","title":"[source, scala]"},{"location":"demo/groupBy-running-count-complete/","text":"== Demo: Streaming Query for Running Counts (Socket Source and Complete Output Mode) The following code shows a streaming aggregation (with Dataset.groupBy operator) in complete output mode that reads text lines from a socket (using socket data source) and outputs running counts of the words. NOTE: The example is \"borrowed\" from http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html[the official documentation of Spark]. Changes and errors are only mine. IMPORTANT: Run nc -lk 9999 first before running the demo. // START: Only for easier debugging // Reduce the number of partitions // The state is then only for one partition // which should make monitoring easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // END: Only for easier debugging val lines = spark .readStream .format(\"socket\") .option(\"host\", \"localhost\") .option(\"port\", 9999) .load scala> lines.printSchema root |-- value: string (nullable = true) import org.apache.spark.sql.functions.explode val words = lines .select(explode(split($\"value\", \"\"\"\\W+\"\"\")) as \"word\") val counts = words.groupBy(\"word\").count scala> counts.printSchema root |-- word: string (nullable = true) |-- count: long (nullable = false) // nc -lk 9999 is supposed to be up at this point val queryName = \"running_counts\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } import org.apache.spark.sql.streaming.OutputMode.Complete val runningCounts = counts .writeStream .format(\"console\") .option(\"checkpointLocation\", checkpointLocation) .outputMode(Complete) .start scala> runningCounts.explain == Physical Plan == WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWriter@205f195c +- *(5) HashAggregate(keys=[word#72], functions=[count(1)]) +- StateStoreSave [word#72], state info [ checkpoint = file:/tmp/checkpoint-running_counts/state, runId = f3b2e642-1790-4a17-ab61-3d894110b063, opId = 0, ver = 0, numPartitions = 1], Complete, 0, 2 +- *(4) HashAggregate(keys=[word#72], functions=[merge_count(1)]) +- StateStoreRestore [word#72], state info [ checkpoint = file:/tmp/checkpoint-running_counts/state, runId = f3b2e642-1790-4a17-ab61-3d894110b063, opId = 0, ver = 0, numPartitions = 1], 2 +- *(3) HashAggregate(keys=[word#72], functions=[merge_count(1)]) +- Exchange hashpartitioning(word#72, 1) +- *(2) HashAggregate(keys=[word#72], functions=[partial_count(1)]) +- Generate explode(split(value#83, \\W+)), false, [word#72] +- *(1) Project [value#83] +- *(1) ScanV2 socket[value#83] (Options: [host=localhost,port=9999]) // Type lines (words) in the terminal with nc // Observe the counts in spark-shell // Use web UI to monitor the state of state (no pun intended) // StateStoreSave and StateStoreRestore operators all have state metrics // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-running_counts/state/0/0 // Eventually... runningCounts.stop()","title":"Streaming Query for Running Counts (Socket Source and Complete Output Mode)"},{"location":"demo/groupByKey-count-Update/","text":"Demo: groupByKey Streaming Aggregation in Update Mode \u00b6 The example shows Dataset.groupByKey streaming operator to count rows in Update output mode. In other words, it is an example of using Dataset.groupByKey with count aggregation function to count customer orders ( T ) per zip code ( K ). package pl.japila.spark.examples import org.apache.spark.sql.SparkSession import org.apache.spark.sql.streaming.{OutputMode, Trigger} object GroupByKeyStreamingApp extends App { val inputTopic = \"GroupByKeyApp-input\" val appName = this.getClass.getSimpleName.replace(\"$\", \"\") val spark = SparkSession.builder .master(\"local[*]\") .appName(appName) .getOrCreate import spark.implicits._ case class Order(id: Long, zipCode: String) // Input (source node) val orders = spark .readStream .format(\"kafka\") .option(\"startingOffsets\", \"latest\") .option(\"subscribe\", inputTopic) .option(\"kafka.bootstrap.servers\", \":9092\") .load .select($\"offset\" as \"id\", $\"value\" as \"zipCode\") // FIXME Use csv, json, avro .as[Order] // Processing logic // groupByKey + count val byZipCode = (o: Order) => o.zipCode val ordersByZipCode = orders.groupByKey(byZipCode) import org.apache.spark.sql.functions.count val typedCountCol = (count(\"zipCode\") as \"count\").as[String] val counts = ordersByZipCode .agg(typedCountCol) .select($\"value\" as \"zip_code\", $\"count\") // Output (sink node) import scala.concurrent.duration._ counts .writeStream .format(\"console\") .outputMode(OutputMode.Update) // FIXME Use Complete .queryName(appName) .trigger(Trigger.ProcessingTime(5.seconds)) .start .awaitTermination() } Credits \u00b6 The example with customer orders and postal codes is borrowed from Apache Beam's Using GroupByKey Programming Guide.","title":"groupByKey Streaming Aggregation in Update Mode"},{"location":"demo/groupByKey-count-Update/#demo-groupbykey-streaming-aggregation-in-update-mode","text":"The example shows Dataset.groupByKey streaming operator to count rows in Update output mode. In other words, it is an example of using Dataset.groupByKey with count aggregation function to count customer orders ( T ) per zip code ( K ). package pl.japila.spark.examples import org.apache.spark.sql.SparkSession import org.apache.spark.sql.streaming.{OutputMode, Trigger} object GroupByKeyStreamingApp extends App { val inputTopic = \"GroupByKeyApp-input\" val appName = this.getClass.getSimpleName.replace(\"$\", \"\") val spark = SparkSession.builder .master(\"local[*]\") .appName(appName) .getOrCreate import spark.implicits._ case class Order(id: Long, zipCode: String) // Input (source node) val orders = spark .readStream .format(\"kafka\") .option(\"startingOffsets\", \"latest\") .option(\"subscribe\", inputTopic) .option(\"kafka.bootstrap.servers\", \":9092\") .load .select($\"offset\" as \"id\", $\"value\" as \"zipCode\") // FIXME Use csv, json, avro .as[Order] // Processing logic // groupByKey + count val byZipCode = (o: Order) => o.zipCode val ordersByZipCode = orders.groupByKey(byZipCode) import org.apache.spark.sql.functions.count val typedCountCol = (count(\"zipCode\") as \"count\").as[String] val counts = ordersByZipCode .agg(typedCountCol) .select($\"value\" as \"zip_code\", $\"count\") // Output (sink node) import scala.concurrent.duration._ counts .writeStream .format(\"console\") .outputMode(OutputMode.Update) // FIXME Use Complete .queryName(appName) .trigger(Trigger.ProcessingTime(5.seconds)) .start .awaitTermination() }","title":"Demo: groupByKey Streaming Aggregation in Update Mode"},{"location":"demo/groupByKey-count-Update/#credits","text":"The example with customer orders and postal codes is borrowed from Apache Beam's Using GroupByKey Programming Guide.","title":"Credits"},{"location":"demo/kafka-data-source/","text":"Demo: Streaming Aggregation with Kafka Data Source \u00b6 The following example code shows a streaming aggregation (with Dataset.groupBy operator) that reads records from Kafka (with Kafka Data Source ). IMPORTANT: Start up Kafka cluster and spark-shell with spark-sql-kafka-0-10 package before running the demo. TIP: You may want to consider copying the following code to append.txt and using :load append.txt command in spark-shell to load it (rather than copying and pasting it). // START: Only for easier debugging // The state is then only for one partition // which should make monitoring easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // END: Only for easier debugging val records = spark .readStream .format(\"kafka\") .option(\"subscribePattern\", \"\"\"topic-\\d{2}\"\"\") // topics with two digits at the end .option(\"kafka.bootstrap.servers\", \":9092\") .load scala> records.printSchema root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) // Since the streaming query uses Append output mode // it has to define a streaming event-time watermark (using Dataset.withWatermark operator) // UnsupportedOperationChecker makes sure that the requirement holds val ids = records .withColumn(\"tokens\", split($\"value\", \",\")) .withColumn(\"seconds\", 'tokens(0) cast \"long\") .withColumn(\"event_time\", to_timestamp(from_unixtime('seconds))) // <-- Event time has to be a timestamp .withColumn(\"id\", 'tokens(1)) .withColumn(\"batch\", 'tokens(2) cast \"int\") .withWatermark(eventTime = \"event_time\", delayThreshold = \"10 seconds\") // <-- define watermark (before groupBy!) .groupBy($\"event_time\") // <-- use event_time for grouping .agg(collect_list(\"batch\") as \"batches\", collect_list(\"id\") as \"ids\") .withColumn(\"event_time\", to_timestamp($\"event_time\")) // <-- convert to human-readable date scala> ids.printSchema root |-- event_time: timestamp (nullable = true) |-- batches: array (nullable = true) | |-- element: integer (containsNull = true) |-- ids: array (nullable = true) | |-- element: string (containsNull = true) assert(ids.isStreaming, \"ids is a streaming query\") // ids knows nothing about the output mode or the current streaming watermark yet // - Output mode is defined on writing side // - streaming watermark is read from rows at runtime // That's why StatefulOperatorStateInfo is generic (and uses the default Append for output mode) // and no batch-specific values are printed out // They will be available right after the first streaming batch // Use explain on a streaming query to know the trigger-specific values scala> ids.explain == Physical Plan == ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[collect_list(batch#141, 0, 0), collect_list(id#129, 0, 0)]) +- StateStoreSave [event_time#118-T10000ms], state info [ checkpoint = <unknown>, runId = a870e6e2-b925-4104-9886-b211c0be1b73, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[merge_collect_list(batch#141, 0, 0), merge_collect_list(id#129, 0, 0)]) +- StateStoreRestore [event_time#118-T10000ms], state info [ checkpoint = <unknown>, runId = a870e6e2-b925-4104-9886-b211c0be1b73, opId = 0, ver = 0, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[merge_collect_list(batch#141, 0, 0), merge_collect_list(id#129, 0, 0)]) +- Exchange hashpartitioning(event_time#118-T10000ms, 1) +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[partial_collect_list(batch#141, 0, 0), partial_collect_list(id#129, 0, 0)]) +- EventTimeWatermark event_time#118: timestamp, interval 10 seconds +- *(1) Project [cast(from_unixtime(cast(split(cast(value#8 as string), ,)[0] as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Warsaw)) as timestamp) AS event_time#118, split(cast(value#8 as string), ,)[1] AS id#129, cast(split(cast(value#8 as string), ,)[2] as int) AS batch#141] +- StreamingRelation kafka, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] val queryName = \"ids-kafka\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } // The following make for an easier demo // Kafka cluster is supposed to be up at this point // Make sure that a Kafka topic is available, e.g. topic-00 // Use ./bin/kafka-console-producer.sh --broker-list :9092 --topic topic-00 // And send a record, e.g. 1,1,1 // Define the output mode // and start the query import scala.concurrent.duration._ import org.apache.spark.sql.streaming.OutputMode.Append import org.apache.spark.sql.streaming.Trigger val streamingQuery = ids .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .queryName(queryName) .outputMode(Append) .start val lastProgress = streamingQuery.lastProgress scala> :type lastProgress org.apache.spark.sql.streaming.StreamingQueryProgress assert(lastProgress.stateOperators.length == 1, \"There should be one stateful operator\") scala> println(lastProgress.stateOperators.head.prettyJson) { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 742, \"customMetrics\" : { \"loadedMapCacheHitCount\" : 1, \"loadedMapCacheMissCount\" : 1, \"stateOnCurrentVersionSizeBytes\" : 374 } } assert(lastProgress.sources.length == 1, \"There should be one streaming source only\") scala> println(lastProgress.sources.head.prettyJson) { \"description\" : \"KafkaV2[SubscribePattern[topic-\\\\d{2}]]\", \"startOffset\" : { \"topic-00\" : { \"0\" : 1 } }, \"endOffset\" : { \"topic-00\" : { \"0\" : 1 } }, \"numInputRows\" : 0, \"inputRowsPerSecond\" : 0.0, \"processedRowsPerSecond\" : 0.0 } // Eventually... streamingQuery.stop()","title":"Streaming Aggregation with Kafka Data Source"},{"location":"demo/kafka-data-source/#demo-streaming-aggregation-with-kafka-data-source","text":"The following example code shows a streaming aggregation (with Dataset.groupBy operator) that reads records from Kafka (with Kafka Data Source ). IMPORTANT: Start up Kafka cluster and spark-shell with spark-sql-kafka-0-10 package before running the demo. TIP: You may want to consider copying the following code to append.txt and using :load append.txt command in spark-shell to load it (rather than copying and pasting it). // START: Only for easier debugging // The state is then only for one partition // which should make monitoring easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // END: Only for easier debugging val records = spark .readStream .format(\"kafka\") .option(\"subscribePattern\", \"\"\"topic-\\d{2}\"\"\") // topics with two digits at the end .option(\"kafka.bootstrap.servers\", \":9092\") .load scala> records.printSchema root |-- key: binary (nullable = true) |-- value: binary (nullable = true) |-- topic: string (nullable = true) |-- partition: integer (nullable = true) |-- offset: long (nullable = true) |-- timestamp: timestamp (nullable = true) |-- timestampType: integer (nullable = true) // Since the streaming query uses Append output mode // it has to define a streaming event-time watermark (using Dataset.withWatermark operator) // UnsupportedOperationChecker makes sure that the requirement holds val ids = records .withColumn(\"tokens\", split($\"value\", \",\")) .withColumn(\"seconds\", 'tokens(0) cast \"long\") .withColumn(\"event_time\", to_timestamp(from_unixtime('seconds))) // <-- Event time has to be a timestamp .withColumn(\"id\", 'tokens(1)) .withColumn(\"batch\", 'tokens(2) cast \"int\") .withWatermark(eventTime = \"event_time\", delayThreshold = \"10 seconds\") // <-- define watermark (before groupBy!) .groupBy($\"event_time\") // <-- use event_time for grouping .agg(collect_list(\"batch\") as \"batches\", collect_list(\"id\") as \"ids\") .withColumn(\"event_time\", to_timestamp($\"event_time\")) // <-- convert to human-readable date scala> ids.printSchema root |-- event_time: timestamp (nullable = true) |-- batches: array (nullable = true) | |-- element: integer (containsNull = true) |-- ids: array (nullable = true) | |-- element: string (containsNull = true) assert(ids.isStreaming, \"ids is a streaming query\") // ids knows nothing about the output mode or the current streaming watermark yet // - Output mode is defined on writing side // - streaming watermark is read from rows at runtime // That's why StatefulOperatorStateInfo is generic (and uses the default Append for output mode) // and no batch-specific values are printed out // They will be available right after the first streaming batch // Use explain on a streaming query to know the trigger-specific values scala> ids.explain == Physical Plan == ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[collect_list(batch#141, 0, 0), collect_list(id#129, 0, 0)]) +- StateStoreSave [event_time#118-T10000ms], state info [ checkpoint = <unknown>, runId = a870e6e2-b925-4104-9886-b211c0be1b73, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[merge_collect_list(batch#141, 0, 0), merge_collect_list(id#129, 0, 0)]) +- StateStoreRestore [event_time#118-T10000ms], state info [ checkpoint = <unknown>, runId = a870e6e2-b925-4104-9886-b211c0be1b73, opId = 0, ver = 0, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[merge_collect_list(batch#141, 0, 0), merge_collect_list(id#129, 0, 0)]) +- Exchange hashpartitioning(event_time#118-T10000ms, 1) +- ObjectHashAggregate(keys=[event_time#118-T10000ms], functions=[partial_collect_list(batch#141, 0, 0), partial_collect_list(id#129, 0, 0)]) +- EventTimeWatermark event_time#118: timestamp, interval 10 seconds +- *(1) Project [cast(from_unixtime(cast(split(cast(value#8 as string), ,)[0] as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Warsaw)) as timestamp) AS event_time#118, split(cast(value#8 as string), ,)[1] AS id#129, cast(split(cast(value#8 as string), ,)[2] as int) AS batch#141] +- StreamingRelation kafka, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] val queryName = \"ids-kafka\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } // The following make for an easier demo // Kafka cluster is supposed to be up at this point // Make sure that a Kafka topic is available, e.g. topic-00 // Use ./bin/kafka-console-producer.sh --broker-list :9092 --topic topic-00 // And send a record, e.g. 1,1,1 // Define the output mode // and start the query import scala.concurrent.duration._ import org.apache.spark.sql.streaming.OutputMode.Append import org.apache.spark.sql.streaming.Trigger val streamingQuery = ids .writeStream .format(\"console\") .option(\"truncate\", false) .option(\"checkpointLocation\", checkpointLocation) .queryName(queryName) .outputMode(Append) .start val lastProgress = streamingQuery.lastProgress scala> :type lastProgress org.apache.spark.sql.streaming.StreamingQueryProgress assert(lastProgress.stateOperators.length == 1, \"There should be one stateful operator\") scala> println(lastProgress.stateOperators.head.prettyJson) { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 742, \"customMetrics\" : { \"loadedMapCacheHitCount\" : 1, \"loadedMapCacheMissCount\" : 1, \"stateOnCurrentVersionSizeBytes\" : 374 } } assert(lastProgress.sources.length == 1, \"There should be one streaming source only\") scala> println(lastProgress.sources.head.prettyJson) { \"description\" : \"KafkaV2[SubscribePattern[topic-\\\\d{2}]]\", \"startOffset\" : { \"topic-00\" : { \"0\" : 1 } }, \"endOffset\" : { \"topic-00\" : { \"0\" : 1 } }, \"numInputRows\" : 0, \"inputRowsPerSecond\" : 0.0, \"processedRowsPerSecond\" : 0.0 } // Eventually... streamingQuery.stop()","title":"Demo: Streaming Aggregation with Kafka Data Source"},{"location":"demo/spark-sql-streaming-demo-FlatMapGroupsWithStateExec/","text":"Demo: Internals of FlatMapGroupsWithStateExec Physical Operator \u00b6 The following demo shows the internals of FlatMapGroupsWithStateExec physical operator in a Arbitrary Stateful Streaming Aggregation . // Reduce the number of partitions and hence the state stores // That is supposed to make debugging state checkpointing easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // Define event \"format\" // Use :paste mode in spark-shell import java.sql.Timestamp case class Event(time: Timestamp, value: Long) import scala.concurrent.duration._ object Event { def apply(secs: Long, value: Long): Event = { Event(new Timestamp(secs.seconds.toMillis), value) } } // Using memory data source for full control of the input import org.apache.spark.sql.execution.streaming.MemoryStream implicit val sqlCtx = spark.sqlContext val events = MemoryStream[Event] val values = events.toDS assert(values.isStreaming, \"values must be a streaming Dataset\") values.printSchema /** root |-- time: timestamp (nullable = true) |-- value: long (nullable = false) */ import scala.concurrent.duration._ val delayThreshold = 10.seconds val valuesWatermarked = values .withWatermark(eventTime = \"time\", delayThreshold.toString) // required for EventTimeTimeout // Could use Long directly, but... // Let's use case class to make the demo a bit more advanced case class Count(value: Long) import java.sql.Timestamp import org.apache.spark.sql.streaming.GroupState val keyCounts = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Count]) => { println(s\"\"\">>> keyCounts(key = $key, state = ${state.getOption.getOrElse(\"<empty>\")})\"\"\") println(s\">>> >>> currentProcessingTimeMs: ${state.getCurrentProcessingTimeMs}\") println(s\">>> >>> currentWatermarkMs: ${state.getCurrentWatermarkMs}\") println(s\">>> >>> hasTimedOut: ${state.hasTimedOut}\") val count = Count(values.length) Iterator((key, count)) } import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val valuesCounted = valuesWatermarked .as[(Timestamp, Long)] // convert DataFrame to Dataset to make groupByKey easier to write .groupByKey { case (time, value) => value } .flatMapGroupsWithState( OutputMode.Update, timeoutConf = GroupStateTimeout.EventTimeTimeout)(func = keyCounts) .toDF(\"value\", \"count\") valuesCounted.explain /** == Physical Plan == *(2) Project [_1#928L AS value#931L, _2#929 AS count#932] +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929] +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = <unknown>, runId = 9af3d00c-fe1f-46a0-8630-4e0d0af88042, opId = 0, ver = 0, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 0, 0 +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#923L, 1) +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L] +- EventTimeWatermark time#915: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#915,value#916L], [time#915, value#916L] */ val queryName = \"FlatMapGroupsWithStateExec_demo\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } import org.apache.spark.sql.streaming.OutputMode.Update val streamingQuery = valuesCounted .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .outputMode(Update) .start assert(streamingQuery.status.message == \"Waiting for data to arrive\") // Use web UI to monitor the metrics of the streaming query // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state/0/0 val batch = Seq( Event(secs = 1, value = 1), Event(secs = 15, value = 2)) events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 1, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881557237 >>> >>> currentWatermarkMs: 0 >>> >>> hasTimedOut: false >>> keyCounts(key = 2, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881557237 >>> >>> currentWatermarkMs: 0 >>> >>> hasTimedOut: false */ spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | +-----+-----+ */ // With at least one execution we can review the execution plan streamingQuery.explain /** == Physical Plan == *(2) Project [_1#928L AS value#931L, _2#929 AS count#932] +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929] +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = file:/tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state, runId = 95c3917c-2fd7-45b2-86f6-6c01f0115e1d, opId = 0, ver = 1, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 1561881557499, 5000 +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#923L, 1) +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L] +- EventTimeWatermark time#915: timestamp, interval 10 seconds +- LocalTableScan <empty>, [time#915, value#916L] */ type Millis = Long def toMillis(datetime: String): Millis = { import java.time.format.DateTimeFormatter import java.time.LocalDateTime import java.time.ZoneOffset LocalDateTime .parse(datetime, DateTimeFormatter.ISO_DATE_TIME) .toInstant(ZoneOffset.UTC) .toEpochMilli } val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds val expectedWatermarkSecs = 5.seconds assert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\") // Let's access the FlatMapGroupsWithStateExec physical operator import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper import org.apache.spark.sql.execution.streaming.StreamExecution val engine: StreamExecution = streamingQuery .asInstanceOf[StreamingQueryWrapper] .streamingQuery import org.apache.spark.sql.execution.streaming.IncrementalExecution val lastMicroBatch: IncrementalExecution = engine.lastExecution // Access executedPlan that is the optimized physical query plan ready for execution // All streaming optimizations have been applied at this point val plan = lastMicroBatch.executedPlan // Find the FlatMapGroupsWithStateExec physical operator import org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec val flatMapOp = plan.collect { case op: FlatMapGroupsWithStateExec => op }.head // Display metrics import org.apache.spark.sql.execution.metric.SQLMetric def formatMetrics(name: String, metric: SQLMetric) = { val desc = metric.name.getOrElse(\"\") val value = metric.value f\"| $name%-30s | $desc%-69s | $value%-10s\" } flatMapOp.metrics.map { case (name, metric) => formatMetrics(name, metric) }.foreach(println) /** | numTotalStateRows | number of total state rows | 0 | stateMemory | memory used by state total (min, med, max) | 390 | loadedMapCacheHitCount | count of cache hit on states cache in provider | 1 | numOutputRows | number of output rows | 0 | stateOnCurrentVersionSizeBytes | estimated size of state only on current version total (min, med, max) | 102 | loadedMapCacheMissCount | count of cache miss on states cache in provider | 0 | commitTimeMs | time to commit changes total (min, med, max) | -2 | allRemovalsTimeMs | total time to remove rows total (min, med, max) | -2 | numUpdatedStateRows | number of updated state rows | 0 | allUpdatesTimeMs | total time to update rows total (min, med, max) | -2 */ val batch = Seq( Event(secs = 1, value = 1), // under the watermark (5000 ms) so it's disregarded Event(secs = 6, value = 3)) // above the watermark so it should be counted events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881643568 >>> >>> currentWatermarkMs: 5000 >>> >>> hasTimedOut: false */ spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | |3 |[1] | +-----+-----+ */ val batch = Seq( Event(secs = 17, value = 3)) // advances the watermark events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881672887 >>> >>> currentWatermarkMs: 5000 >>> >>> hasTimedOut: false */ val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds val expectedWatermarkSecs = 7.seconds assert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\") spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | |3 |[1] | |3 |[1] | +-----+-----+ */ val batch = Seq( Event(secs = 18, value = 3)) // advances the watermark events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881778165 >>> >>> currentWatermarkMs: 7000 >>> >>> hasTimedOut: false */ // Eventually... streamingQuery.stop()","title":"Internals of FlatMapGroupsWithStateExec Physical Operator"},{"location":"demo/spark-sql-streaming-demo-FlatMapGroupsWithStateExec/#demo-internals-of-flatmapgroupswithstateexec-physical-operator","text":"The following demo shows the internals of FlatMapGroupsWithStateExec physical operator in a Arbitrary Stateful Streaming Aggregation . // Reduce the number of partitions and hence the state stores // That is supposed to make debugging state checkpointing easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // Define event \"format\" // Use :paste mode in spark-shell import java.sql.Timestamp case class Event(time: Timestamp, value: Long) import scala.concurrent.duration._ object Event { def apply(secs: Long, value: Long): Event = { Event(new Timestamp(secs.seconds.toMillis), value) } } // Using memory data source for full control of the input import org.apache.spark.sql.execution.streaming.MemoryStream implicit val sqlCtx = spark.sqlContext val events = MemoryStream[Event] val values = events.toDS assert(values.isStreaming, \"values must be a streaming Dataset\") values.printSchema /** root |-- time: timestamp (nullable = true) |-- value: long (nullable = false) */ import scala.concurrent.duration._ val delayThreshold = 10.seconds val valuesWatermarked = values .withWatermark(eventTime = \"time\", delayThreshold.toString) // required for EventTimeTimeout // Could use Long directly, but... // Let's use case class to make the demo a bit more advanced case class Count(value: Long) import java.sql.Timestamp import org.apache.spark.sql.streaming.GroupState val keyCounts = (key: Long, values: Iterator[(Timestamp, Long)], state: GroupState[Count]) => { println(s\"\"\">>> keyCounts(key = $key, state = ${state.getOption.getOrElse(\"<empty>\")})\"\"\") println(s\">>> >>> currentProcessingTimeMs: ${state.getCurrentProcessingTimeMs}\") println(s\">>> >>> currentWatermarkMs: ${state.getCurrentWatermarkMs}\") println(s\">>> >>> hasTimedOut: ${state.hasTimedOut}\") val count = Count(values.length) Iterator((key, count)) } import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode} val valuesCounted = valuesWatermarked .as[(Timestamp, Long)] // convert DataFrame to Dataset to make groupByKey easier to write .groupByKey { case (time, value) => value } .flatMapGroupsWithState( OutputMode.Update, timeoutConf = GroupStateTimeout.EventTimeTimeout)(func = keyCounts) .toDF(\"value\", \"count\") valuesCounted.explain /** == Physical Plan == *(2) Project [_1#928L AS value#931L, _2#929 AS count#932] +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929] +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = <unknown>, runId = 9af3d00c-fe1f-46a0-8630-4e0d0af88042, opId = 0, ver = 0, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 0, 0 +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#923L, 1) +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L] +- EventTimeWatermark time#915: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#915,value#916L], [time#915, value#916L] */ val queryName = \"FlatMapGroupsWithStateExec_demo\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } import org.apache.spark.sql.streaming.OutputMode.Update val streamingQuery = valuesCounted .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .outputMode(Update) .start assert(streamingQuery.status.message == \"Waiting for data to arrive\") // Use web UI to monitor the metrics of the streaming query // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state/0/0 val batch = Seq( Event(secs = 1, value = 1), Event(secs = 15, value = 2)) events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 1, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881557237 >>> >>> currentWatermarkMs: 0 >>> >>> hasTimedOut: false >>> keyCounts(key = 2, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881557237 >>> >>> currentWatermarkMs: 0 >>> >>> hasTimedOut: false */ spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | +-----+-----+ */ // With at least one execution we can review the execution plan streamingQuery.explain /** == Physical Plan == *(2) Project [_1#928L AS value#931L, _2#929 AS count#932] +- *(2) SerializeFromObject [assertnotnull(input[0, scala.Tuple2, true])._1 AS _1#928L, if (isnull(assertnotnull(input[0, scala.Tuple2, true])._2)) null else named_struct(value, assertnotnull(assertnotnull(input[0, scala.Tuple2, true])._2).value) AS _2#929] +- FlatMapGroupsWithState $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4117/181063008@d2cdc82, value#923: bigint, newInstance(class scala.Tuple2), [value#923L], [time#915-T10000ms, value#916L], obj#927: scala.Tuple2, state info [ checkpoint = file:/tmp/checkpoint-FlatMapGroupsWithStateExec_demo/state, runId = 95c3917c-2fd7-45b2-86f6-6c01f0115e1d, opId = 0, ver = 1, numPartitions = 1], class[value[0]: bigint], 2, Update, EventTimeTimeout, 1561881557499, 5000 +- *(1) Sort [value#923L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(value#923L, 1) +- AppendColumns $line140.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$4118/2131767153@3e606b4c, newInstance(class scala.Tuple2), [input[0, bigint, false] AS value#923L] +- EventTimeWatermark time#915: timestamp, interval 10 seconds +- LocalTableScan <empty>, [time#915, value#916L] */ type Millis = Long def toMillis(datetime: String): Millis = { import java.time.format.DateTimeFormatter import java.time.LocalDateTime import java.time.ZoneOffset LocalDateTime .parse(datetime, DateTimeFormatter.ISO_DATE_TIME) .toInstant(ZoneOffset.UTC) .toEpochMilli } val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds val expectedWatermarkSecs = 5.seconds assert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\") // Let's access the FlatMapGroupsWithStateExec physical operator import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper import org.apache.spark.sql.execution.streaming.StreamExecution val engine: StreamExecution = streamingQuery .asInstanceOf[StreamingQueryWrapper] .streamingQuery import org.apache.spark.sql.execution.streaming.IncrementalExecution val lastMicroBatch: IncrementalExecution = engine.lastExecution // Access executedPlan that is the optimized physical query plan ready for execution // All streaming optimizations have been applied at this point val plan = lastMicroBatch.executedPlan // Find the FlatMapGroupsWithStateExec physical operator import org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec val flatMapOp = plan.collect { case op: FlatMapGroupsWithStateExec => op }.head // Display metrics import org.apache.spark.sql.execution.metric.SQLMetric def formatMetrics(name: String, metric: SQLMetric) = { val desc = metric.name.getOrElse(\"\") val value = metric.value f\"| $name%-30s | $desc%-69s | $value%-10s\" } flatMapOp.metrics.map { case (name, metric) => formatMetrics(name, metric) }.foreach(println) /** | numTotalStateRows | number of total state rows | 0 | stateMemory | memory used by state total (min, med, max) | 390 | loadedMapCacheHitCount | count of cache hit on states cache in provider | 1 | numOutputRows | number of output rows | 0 | stateOnCurrentVersionSizeBytes | estimated size of state only on current version total (min, med, max) | 102 | loadedMapCacheMissCount | count of cache miss on states cache in provider | 0 | commitTimeMs | time to commit changes total (min, med, max) | -2 | allRemovalsTimeMs | total time to remove rows total (min, med, max) | -2 | numUpdatedStateRows | number of updated state rows | 0 | allUpdatesTimeMs | total time to update rows total (min, med, max) | -2 */ val batch = Seq( Event(secs = 1, value = 1), // under the watermark (5000 ms) so it's disregarded Event(secs = 6, value = 3)) // above the watermark so it should be counted events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881643568 >>> >>> currentWatermarkMs: 5000 >>> >>> hasTimedOut: false */ spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | |3 |[1] | +-----+-----+ */ val batch = Seq( Event(secs = 17, value = 3)) // advances the watermark events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881672887 >>> >>> currentWatermarkMs: 5000 >>> >>> hasTimedOut: false */ val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkSecs = toMillis(currentWatermark).millis.toSeconds.seconds val expectedWatermarkSecs = 7.seconds assert(currentWatermarkSecs == expectedWatermarkSecs, s\"Current event-time watermark is $currentWatermarkSecs, but should be $expectedWatermarkSecs (maximum event time - delayThreshold ${delayThreshold.toMillis})\") spark.table(queryName).show(truncate = false) /** +-----+-----+ |value|count| +-----+-----+ |1 |[1] | |2 |[1] | |3 |[1] | |3 |[1] | +-----+-----+ */ val batch = Seq( Event(secs = 18, value = 3)) // advances the watermark events.addData(batch) streamingQuery.processAllAvailable() /** >>> keyCounts(key = 3, state = <empty>) >>> >>> currentProcessingTimeMs: 1561881778165 >>> >>> currentWatermarkMs: 7000 >>> >>> hasTimedOut: false */ // Eventually... streamingQuery.stop()","title":"Demo: Internals of FlatMapGroupsWithStateExec Physical Operator"},{"location":"demo/watermark-aggregation-append/","text":"== Demo: Streaming Watermark with Aggregation in Append Output Mode The following demo shows the behaviour and the internals of < > with a < > in < > output mode. The demo also shows the behaviour and the internals of < > physical operator in < >. TIP: The below code is part of https://github.com/jaceklaskowski/spark-structured-streaming-book/blob/v3.0.1/examples/src/main/scala/pl/japila/spark/StreamingAggregationAppendMode.scala[StreamingAggregationAppendMode ] streaming application. [source, scala] \u00b6 // Reduce the number of partitions and hence the state stores // That is supposed to make debugging state checkpointing easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // Define event \"format\" // Use :paste mode in spark-shell import java.sql.Timestamp case class Event(time: Timestamp, value: Long, batch: Long) import scala.concurrent.duration._ object Event { def apply(secs: Long, value: Long, batch: Long): Event = { Event(new Timestamp(secs.seconds.toMillis), value, batch) } } // Using memory data source for full control of the input import org.apache.spark.sql.execution.streaming.MemoryStream implicit val sqlCtx = spark.sqlContext val events = MemoryStream[Event] val values = events.toDS assert(values.isStreaming, \"values must be a streaming Dataset\") values.printSchema /** root |-- time: timestamp (nullable = true) |-- value: long (nullable = false) |-- batch: long (nullable = false) */ // Streaming aggregation using groupBy operator to demo StateStoreSaveExec operator // Define required watermark for late events for Append output mode import scala.concurrent.duration._ val delayThreshold = 10.seconds val eventTime = \"time\" val valuesWatermarked = values .withWatermark(eventTime, delayThreshold.toString) // defines watermark (before groupBy!) // EventTimeWatermark logical operator is planned as EventTimeWatermarkExec physical operator // Note that as a physical operator EventTimeWatermarkExec shows itself without the Exec suffix valuesWatermarked.explain /** == Physical Plan == EventTimeWatermark time#3: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L] */ val windowDuration = 5.seconds import org.apache.spark.sql.functions.window val countsPer5secWindow = valuesWatermarked .groupBy(window(col(eventTime), windowDuration.toString) as \"sliding_window\") .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\") countsPer5secWindow.printSchema /** root |-- sliding_window: struct (nullable = false) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) |-- batches: array (nullable = true) | |-- element: long (containsNull = true) |-- values: array (nullable = true) | |-- element: long (containsNull = true) */ // valuesPerGroupWindowed is a streaming Dataset with just one source // It knows nothing about output mode or watermark yet // That's why StatefulOperatorStateInfo is generic // and no batch-specific values are printed out // That will be available after the first streaming batch // Use sq.explain to know the runtime-specific values countsPer5secWindow.explain /** == Physical Plan == ObjectHashAggregate(keys=[window#23-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)]) +- StateStoreSave [window#23-T10000ms], state info [ checkpoint = , runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- StateStoreRestore [window#23-T10000ms], state info [ checkpoint = , runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- Exchange hashpartitioning(window#23-T10000ms, 1) +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#23-T10000ms, value#4L, batch#5L] +- *(1) Filter isnotnull(time#3-T10000ms) +- EventTimeWatermark time#3: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L] */ val queryName = \"watermark_demo\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } // FIXME Use foreachBatch for batchId and the output Dataset // Start the query and hence StateStoreSaveExec import scala.concurrent.duration._ import org.apache.spark.sql.streaming.OutputMode val streamingQuery = countsPer5secWindow .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .outputMode(OutputMode.Append) // \u2190 Use Append output mode .start assert(streamingQuery.status.message == \"Waiting for data to arrive\") type Millis = Long def toMillis(datetime: String): Millis = { import java.time.format.DateTimeFormatter import java.time.LocalDateTime import java.time.ZoneOffset LocalDateTime .parse(datetime, DateTimeFormatter.ISO_DATE_TIME) .toInstant(ZoneOffset.UTC) .toEpochMilli } // Use web UI to monitor the state of state (no pun intended) // StateStoreSave and StateStoreRestore operators all have state metrics // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-watermark_demo/state/0/0 // The demo is aimed to show the following: // 1. The current watermark // 2. Check out the stats: // - expired state (below the current watermark, goes to output and purged later) // - late state (dropped as if never received and processed) // - saved state rows (above the current watermark) val batch = Seq( Event(1, 1, batch = 1), Event(15, 2, batch = 1)) events.addData(batch) streamingQuery.processAllAvailable() println(streamingQuery.lastProgress.stateOperators(0).prettyJson) /** { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 1102, \"customMetrics\" : { \"loadedMapCacheHitCount\" : 2, \"loadedMapCacheMissCount\" : 0, \"stateOnCurrentVersionSizeBytes\" : 414 } } */ val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 15 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 5.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Saved State Rows // Use the metrics of the StateStoreSave operator // Or simply streamingQuery.lastProgress.stateOperators.head spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | +------------------------------------------+-------+------+ */ // With at least one execution we can review the execution plan streamingQuery.explain /** scala> streamingQuery.explain == Physical Plan == ObjectHashAggregate(keys=[window#18-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)]) +- StateStoreSave [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], Append, 5000, 2 +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- StateStoreRestore [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- Exchange hashpartitioning(window#18-T10000ms, 1) +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#18-T10000ms, value#4L, batch#5L] +- *(1) Filter isnotnull(time#3-T10000ms) +- EventTimeWatermark time#3: timestamp, interval 10 seconds +- LocalTableScan , [time#3, value#4L, batch#5L] */ import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = streamingQuery .asInstanceOf[StreamingQueryWrapper] .streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) val lastMicroBatch = engine.lastExecution import org.apache.spark.sql.execution.streaming.IncrementalExecution assert(lastMicroBatch.isInstanceOf[IncrementalExecution]) // Access executedPlan that is the optimized physical query plan ready for execution // All streaming optimizations have been applied at this point // We just need the EventTimeWatermarkExec physical operator val plan = lastMicroBatch.executedPlan // Let's find the EventTimeWatermarkExec physical operator in the plan // There should be one only import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head // Let's check out the event-time watermark stats // They correspond to the concrete EventTimeWatermarkExec operator for a micro-batch val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(1, 1, batch = 2), Event(15, 2, batch = 2), Event(35, 3, batch = 2)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 35 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 25.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(15,1, batch = 3), Event(15,2, batch = 3), Event(20,3, batch = 3), Event(26,4, batch = 3)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 26 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") // Current event-time watermark should be the same as previously // val expectedWatermarkMs = 25.seconds.toMillis // The current max time is merely 26 so subtracting delayThreshold gives merely 16 assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(26000,15000,19000.0,4) */ val batch = Seq( Event(36, 1, batch = 4)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 36 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 26.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(50, 1, batch = 5) ) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 50 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 40.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| |[1970-01-01 01:00:25, 1970-01-01 01:00:30]|[3] |[4] | |[1970-01-01 01:00:35, 1970-01-01 01:00:40]|[2, 4] |[3, 1]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ // Eventually... streamingQuery.stop()","title":"Streaming Watermark with Aggregation in Append Output Mode"},{"location":"demo/watermark-aggregation-append/#source-scala","text":"// Reduce the number of partitions and hence the state stores // That is supposed to make debugging state checkpointing easier val numShufflePartitions = 1 import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS spark.sessionState.conf.setConf(SHUFFLE_PARTITIONS, numShufflePartitions) assert(spark.sessionState.conf.numShufflePartitions == numShufflePartitions) // Define event \"format\" // Use :paste mode in spark-shell import java.sql.Timestamp case class Event(time: Timestamp, value: Long, batch: Long) import scala.concurrent.duration._ object Event { def apply(secs: Long, value: Long, batch: Long): Event = { Event(new Timestamp(secs.seconds.toMillis), value, batch) } } // Using memory data source for full control of the input import org.apache.spark.sql.execution.streaming.MemoryStream implicit val sqlCtx = spark.sqlContext val events = MemoryStream[Event] val values = events.toDS assert(values.isStreaming, \"values must be a streaming Dataset\") values.printSchema /** root |-- time: timestamp (nullable = true) |-- value: long (nullable = false) |-- batch: long (nullable = false) */ // Streaming aggregation using groupBy operator to demo StateStoreSaveExec operator // Define required watermark for late events for Append output mode import scala.concurrent.duration._ val delayThreshold = 10.seconds val eventTime = \"time\" val valuesWatermarked = values .withWatermark(eventTime, delayThreshold.toString) // defines watermark (before groupBy!) // EventTimeWatermark logical operator is planned as EventTimeWatermarkExec physical operator // Note that as a physical operator EventTimeWatermarkExec shows itself without the Exec suffix valuesWatermarked.explain /** == Physical Plan == EventTimeWatermark time#3: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L] */ val windowDuration = 5.seconds import org.apache.spark.sql.functions.window val countsPer5secWindow = valuesWatermarked .groupBy(window(col(eventTime), windowDuration.toString) as \"sliding_window\") .agg(collect_list(\"batch\") as \"batches\", collect_list(\"value\") as \"values\") countsPer5secWindow.printSchema /** root |-- sliding_window: struct (nullable = false) | |-- start: timestamp (nullable = true) | |-- end: timestamp (nullable = true) |-- batches: array (nullable = true) | |-- element: long (containsNull = true) |-- values: array (nullable = true) | |-- element: long (containsNull = true) */ // valuesPerGroupWindowed is a streaming Dataset with just one source // It knows nothing about output mode or watermark yet // That's why StatefulOperatorStateInfo is generic // and no batch-specific values are printed out // That will be available after the first streaming batch // Use sq.explain to know the runtime-specific values countsPer5secWindow.explain /** == Physical Plan == ObjectHashAggregate(keys=[window#23-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)]) +- StateStoreSave [window#23-T10000ms], state info [ checkpoint = , runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], Append, 0, 2 +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- StateStoreRestore [window#23-T10000ms], state info [ checkpoint = , runId = 50e62943-fe5d-4a02-8498-7134ecbf5122, opId = 0, ver = 0, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- Exchange hashpartitioning(window#23-T10000ms, 1) +- ObjectHashAggregate(keys=[window#23-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#23-T10000ms, value#4L, batch#5L] +- *(1) Filter isnotnull(time#3-T10000ms) +- EventTimeWatermark time#3: timestamp, interval 10 seconds +- StreamingRelation MemoryStream[time#3,value#4L,batch#5L], [time#3, value#4L, batch#5L] */ val queryName = \"watermark_demo\" val checkpointLocation = s\"/tmp/checkpoint-$queryName\" // Delete the checkpoint location from previous executions import java.nio.file.{Files, FileSystems} import java.util.Comparator import scala.collection.JavaConverters._ val path = FileSystems.getDefault.getPath(checkpointLocation) if (Files.exists(path)) { Files.walk(path) .sorted(Comparator.reverseOrder()) .iterator .asScala .foreach(p => p.toFile.delete) } // FIXME Use foreachBatch for batchId and the output Dataset // Start the query and hence StateStoreSaveExec import scala.concurrent.duration._ import org.apache.spark.sql.streaming.OutputMode val streamingQuery = countsPer5secWindow .writeStream .format(\"memory\") .queryName(queryName) .option(\"checkpointLocation\", checkpointLocation) .outputMode(OutputMode.Append) // \u2190 Use Append output mode .start assert(streamingQuery.status.message == \"Waiting for data to arrive\") type Millis = Long def toMillis(datetime: String): Millis = { import java.time.format.DateTimeFormatter import java.time.LocalDateTime import java.time.ZoneOffset LocalDateTime .parse(datetime, DateTimeFormatter.ISO_DATE_TIME) .toInstant(ZoneOffset.UTC) .toEpochMilli } // Use web UI to monitor the state of state (no pun intended) // StateStoreSave and StateStoreRestore operators all have state metrics // Go to http://localhost:4040/SQL/ and click one of the Completed Queries with Job IDs // You may also want to check out checkpointed state // in /tmp/checkpoint-watermark_demo/state/0/0 // The demo is aimed to show the following: // 1. The current watermark // 2. Check out the stats: // - expired state (below the current watermark, goes to output and purged later) // - late state (dropped as if never received and processed) // - saved state rows (above the current watermark) val batch = Seq( Event(1, 1, batch = 1), Event(15, 2, batch = 1)) events.addData(batch) streamingQuery.processAllAvailable() println(streamingQuery.lastProgress.stateOperators(0).prettyJson) /** { \"numRowsTotal\" : 1, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 1102, \"customMetrics\" : { \"loadedMapCacheHitCount\" : 2, \"loadedMapCacheMissCount\" : 0, \"stateOnCurrentVersionSizeBytes\" : 414 } } */ val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 15 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 5.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Saved State Rows // Use the metrics of the StateStoreSave operator // Or simply streamingQuery.lastProgress.stateOperators.head spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | +------------------------------------------+-------+------+ */ // With at least one execution we can review the execution plan streamingQuery.explain /** scala> streamingQuery.explain == Physical Plan == ObjectHashAggregate(keys=[window#18-T10000ms], functions=[collect_list(batch#5L, 0, 0), collect_list(value#4L, 0, 0)]) +- StateStoreSave [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], Append, 5000, 2 +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- StateStoreRestore [window#18-T10000ms], state info [ checkpoint = file:/tmp/checkpoint-watermark_demo/state, runId = 73bb0ede-20f2-400d-8003-aa2fbebdd2e1, opId = 0, ver = 1, numPartitions = 1], 2 +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[merge_collect_list(batch#5L, 0, 0), merge_collect_list(value#4L, 0, 0)]) +- Exchange hashpartitioning(window#18-T10000ms, 1) +- ObjectHashAggregate(keys=[window#18-T10000ms], functions=[partial_collect_list(batch#5L, 0, 0), partial_collect_list(value#4L, 0, 0)]) +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) as double) = (cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) THEN (CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) + 1) ELSE CEIL((cast((precisetimestampconversion(time#3-T10000ms, TimestampType, LongType) - 0) as double) / 5000000.0)) END + 0) - 1) * 5000000) + 5000000), LongType, TimestampType)) AS window#18-T10000ms, value#4L, batch#5L] +- *(1) Filter isnotnull(time#3-T10000ms) +- EventTimeWatermark time#3: timestamp, interval 10 seconds +- LocalTableScan , [time#3, value#4L, batch#5L] */ import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val engine = streamingQuery .asInstanceOf[StreamingQueryWrapper] .streamingQuery import org.apache.spark.sql.execution.streaming.StreamExecution assert(engine.isInstanceOf[StreamExecution]) val lastMicroBatch = engine.lastExecution import org.apache.spark.sql.execution.streaming.IncrementalExecution assert(lastMicroBatch.isInstanceOf[IncrementalExecution]) // Access executedPlan that is the optimized physical query plan ready for execution // All streaming optimizations have been applied at this point // We just need the EventTimeWatermarkExec physical operator val plan = lastMicroBatch.executedPlan // Let's find the EventTimeWatermarkExec physical operator in the plan // There should be one only import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head // Let's check out the event-time watermark stats // They correspond to the concrete EventTimeWatermarkExec operator for a micro-batch val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(1, 1, batch = 2), Event(15, 2, batch = 2), Event(35, 3, batch = 2)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 35 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 25.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(15,1, batch = 3), Event(15,2, batch = 3), Event(20,3, batch = 3), Event(26,4, batch = 3)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 26 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") // Current event-time watermark should be the same as previously // val expectedWatermarkMs = 25.seconds.toMillis // The current max time is merely 26 so subtracting delayThreshold gives merely 16 assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(26000,15000,19000.0,4) */ val batch = Seq( Event(36, 1, batch = 4)) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 36 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 26.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ val batch = Seq( Event(50, 1, batch = 5) ) events.addData(batch) streamingQuery.processAllAvailable() val currentWatermark = streamingQuery.lastProgress.eventTime.get(\"watermark\") val currentWatermarkMs = toMillis(currentWatermark) val maxTime = batch.maxBy(_.time.toInstant.toEpochMilli).time.toInstant.toEpochMilli.millis.toSeconds val expectedMaxTime = 50 assert(maxTime == expectedMaxTime, s\"Maximum time across events per batch is $maxTime, but should be $expectedMaxTime\") val expectedWatermarkMs = 40.seconds.toMillis assert(currentWatermarkMs == expectedWatermarkMs, s\"Current event-time watermark is $currentWatermarkMs, but should be $expectedWatermarkMs (maximum event time ${maxTime.seconds.toMillis} minus delayThreshold ${delayThreshold.toMillis})\") // FIXME Expired State // FIXME Late Events // FIXME Saved State Rows spark.table(queryName).orderBy(\"sliding_window\").show(truncate = false) /** +------------------------------------------+-------+------+ |sliding_window |batches|values| +------------------------------------------+-------+------+ |[1970-01-01 01:00:00, 1970-01-01 01:00:05]|[1] |[1] | |[1970-01-01 01:00:15, 1970-01-01 01:00:20]|[1, 2] |[2, 2]| |[1970-01-01 01:00:25, 1970-01-01 01:00:30]|[3] |[4] | |[1970-01-01 01:00:35, 1970-01-01 01:00:40]|[2, 4] |[3, 1]| +------------------------------------------+-------+------+ */ // Check out the event-time watermark stats val plan = engine.lastExecution.executedPlan import org.apache.spark.sql.execution.streaming.EventTimeWatermarkExec val watermarkOp = plan.collect { case op: EventTimeWatermarkExec => op }.head val stats = watermarkOp.eventTimeStats.value import org.apache.spark.sql.execution.streaming.EventTimeStats assert(stats.isInstanceOf[EventTimeStats]) println(stats) /** EventTimeStats(-9223372036854775808,9223372036854775807,0.0,0) */ // Eventually... streamingQuery.stop()","title":"[source, scala]"},{"location":"logical-operators/FlatMapGroupsWithState/","text":"FlatMapGroupsWithState Unary Logical Operator \u00b6 FlatMapGroupsWithState is a unary logical operator that represents the following operators in a logical query plan of a streaming query: KeyValueGroupedDataset.mapGroupsWithState KeyValueGroupedDataset.flatMapGroupsWithState Note A unary logical operator ( UnaryNode ) is a logical operator with a single < > logical operator. Read up on UnaryNode (and logical operators in general) in The Internals of Spark SQL online book. Execution Planning \u00b6 FlatMapGroupsWithState is resolved ( planned ) to: FlatMapGroupsWithStateExec unary physical operator for streaming datasets (in FlatMapGroupsWithStateStrategy execution planning strategy) MapGroupsExec physical operator for batch datasets (in BasicOperators execution planning strategy) Creating Instance \u00b6 FlatMapGroupsWithState takes the following to be created: State function ( (Any, Iterator[Any], LogicalGroupState[Any]) => Iterator[Any] ) Catalyst Expression for keys Catalyst Expression for values Grouping Attributes Data Attributes Output Object Attribute State ExpressionEncoder OutputMode isMapGroupsWithState flag (default: false ) GroupStateTimeout Child logical operator FlatMapGroupsWithState is created (using apply factory method) for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators. Creating SerializeFromObject with FlatMapGroupsWithState \u00b6 apply [ K: Encoder , V: Encoder , S: Encoder , U: Encoder ]( func : ( Any , Iterator [ Any ], LogicalGroupState [ Any ]) => Iterator [ Any ], groupingAttributes : Seq [ Attribute ], dataAttributes : Seq [ Attribute ], outputMode : OutputMode , isMapGroupsWithState : Boolean , timeout : GroupStateTimeout , child : LogicalPlan ) : LogicalPlan apply creates a SerializeFromObject logical operator with a FlatMapGroupsWithState as its child logical operator. Internally, apply creates SerializeFromObject object consumer (aka unary logical operator) with FlatMapGroupsWithState logical plan. Internally, apply finds ExpressionEncoder for the type S and creates a FlatMapGroupsWithState with UnresolvedDeserializer for the types K and V . In the end, apply creates a SerializeFromObject object consumer with the FlatMapGroupsWithState . apply is used for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators.","title":"FlatMapGroupsWithState Unary Logical Operator"},{"location":"logical-operators/FlatMapGroupsWithState/#flatmapgroupswithstate-unary-logical-operator","text":"FlatMapGroupsWithState is a unary logical operator that represents the following operators in a logical query plan of a streaming query: KeyValueGroupedDataset.mapGroupsWithState KeyValueGroupedDataset.flatMapGroupsWithState Note A unary logical operator ( UnaryNode ) is a logical operator with a single < > logical operator. Read up on UnaryNode (and logical operators in general) in The Internals of Spark SQL online book.","title":"FlatMapGroupsWithState Unary Logical Operator"},{"location":"logical-operators/FlatMapGroupsWithState/#execution-planning","text":"FlatMapGroupsWithState is resolved ( planned ) to: FlatMapGroupsWithStateExec unary physical operator for streaming datasets (in FlatMapGroupsWithStateStrategy execution planning strategy) MapGroupsExec physical operator for batch datasets (in BasicOperators execution planning strategy)","title":"Execution Planning"},{"location":"logical-operators/FlatMapGroupsWithState/#creating-instance","text":"FlatMapGroupsWithState takes the following to be created: State function ( (Any, Iterator[Any], LogicalGroupState[Any]) => Iterator[Any] ) Catalyst Expression for keys Catalyst Expression for values Grouping Attributes Data Attributes Output Object Attribute State ExpressionEncoder OutputMode isMapGroupsWithState flag (default: false ) GroupStateTimeout Child logical operator FlatMapGroupsWithState is created (using apply factory method) for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators.","title":"Creating Instance"},{"location":"logical-operators/FlatMapGroupsWithState/#creating-serializefromobject-with-flatmapgroupswithstate","text":"apply [ K: Encoder , V: Encoder , S: Encoder , U: Encoder ]( func : ( Any , Iterator [ Any ], LogicalGroupState [ Any ]) => Iterator [ Any ], groupingAttributes : Seq [ Attribute ], dataAttributes : Seq [ Attribute ], outputMode : OutputMode , isMapGroupsWithState : Boolean , timeout : GroupStateTimeout , child : LogicalPlan ) : LogicalPlan apply creates a SerializeFromObject logical operator with a FlatMapGroupsWithState as its child logical operator. Internally, apply creates SerializeFromObject object consumer (aka unary logical operator) with FlatMapGroupsWithState logical plan. Internally, apply finds ExpressionEncoder for the type S and creates a FlatMapGroupsWithState with UnresolvedDeserializer for the types K and V . In the end, apply creates a SerializeFromObject object consumer with the FlatMapGroupsWithState . apply is used for KeyValueGroupedDataset.mapGroupsWithState and KeyValueGroupedDataset.flatMapGroupsWithState operators.","title":" Creating SerializeFromObject with FlatMapGroupsWithState"},{"location":"monitoring/ExecutionStats/","text":"== [[ExecutionStats]] ExecutionStats ExecutionStats is...FIXME","title":"ExecutionStats"},{"location":"monitoring/MetricsReporter/","text":"MetricsReporter \u00b6 MetricsReporter is a Metrics Source for streaming queries . MetricsReporter uses the last StreamingQueryProgress (of the StreamExecution ) if available or simply defaults to a \"zero\" value. Creating Instance \u00b6 MetricsReporter takes the following to be created: StreamExecution Source Name MetricsReporter is created for stream execution engines . Gauges \u00b6 inputRate-total \u00b6 Reports inputRowsPerSecond (across all streaming sources) processingRate-total \u00b6 Reports processedRowsPerSecond (across all streaming sources) latency \u00b6 Reports triggerExecution duration of the last StreamingQueryProgress eventTime-watermark \u00b6 Reports watermark of the last StreamingQueryProgress Format: yyyy-MM-dd'T'HH:mm:ss.SSS'Z' states-rowsTotal \u00b6 Reports the total of numRowsTotal of all StateOperatorProgress es of the last StreamingQueryProgress states-usedBytes \u00b6 Reports the total of memoryUsedBytes of all StateOperatorProgress es of the last StreamingQueryProgress","title":"MetricsReporter"},{"location":"monitoring/MetricsReporter/#metricsreporter","text":"MetricsReporter is a Metrics Source for streaming queries . MetricsReporter uses the last StreamingQueryProgress (of the StreamExecution ) if available or simply defaults to a \"zero\" value.","title":"MetricsReporter"},{"location":"monitoring/MetricsReporter/#creating-instance","text":"MetricsReporter takes the following to be created: StreamExecution Source Name MetricsReporter is created for stream execution engines .","title":"Creating Instance"},{"location":"monitoring/MetricsReporter/#gauges","text":"","title":"Gauges"},{"location":"monitoring/MetricsReporter/#inputrate-total","text":"Reports inputRowsPerSecond (across all streaming sources)","title":"inputRate-total"},{"location":"monitoring/MetricsReporter/#processingrate-total","text":"Reports processedRowsPerSecond (across all streaming sources)","title":"processingRate-total"},{"location":"monitoring/MetricsReporter/#latency","text":"Reports triggerExecution duration of the last StreamingQueryProgress","title":"latency"},{"location":"monitoring/MetricsReporter/#eventtime-watermark","text":"Reports watermark of the last StreamingQueryProgress Format: yyyy-MM-dd'T'HH:mm:ss.SSS'Z'","title":"eventTime-watermark"},{"location":"monitoring/MetricsReporter/#states-rowstotal","text":"Reports the total of numRowsTotal of all StateOperatorProgress es of the last StreamingQueryProgress","title":"states-rowsTotal"},{"location":"monitoring/MetricsReporter/#states-usedbytes","text":"Reports the total of memoryUsedBytes of all StateOperatorProgress es of the last StreamingQueryProgress","title":"states-usedBytes"},{"location":"monitoring/ProgressReporter/","text":"ProgressReporter \u00b6 ProgressReporter is an abstraction of stream execution progress reporters that report the statistics of execution of a streaming query. Contract \u00b6 currentBatchId \u00b6 currentBatchId : Long ID of the active ( current ) streaming micro-batch Used when...FIXME id \u00b6 id : UUID Universally unique identifier (UUID) of the streaming query (that remains unchanged between restarts) Used when...FIXME lastExecution \u00b6 lastExecution : QueryExecution QueryExecution of the streaming query Used when...FIXME logicalPlan \u00b6 logicalPlan : LogicalPlan Logical query plan of the streaming query Used when ProgressReporter is requested for the following: extract statistics from the most recent query execution (to add watermark metric for streaming watermark ) extractSourceToNumInputRows name \u00b6 name : String Name of the streaming query newData \u00b6 newData : Map [ SparkDataStream , LogicalPlan ] SparkDataStream s with the new data (as a LogicalPlan ) offsetSeqMetadata \u00b6 offsetSeqMetadata : OffsetSeqMetadata OffsetSeqMetadata (with the current micro-batch event-time watermark and timestamp ) postEvent \u00b6 postEvent ( event : StreamingQueryListener.Event ) : Unit Posts StreamingQueryListener.Event runId \u00b6 runId : UUID Universally unique identifier (UUID) of a single run of the streaming query (that changes every restart) sink \u00b6 sink : Table The one and only Table of the streaming query sinkCommitProgress \u00b6 sinkCommitProgress : Option [ StreamWriterCommitProgress ] sources \u00b6 sources : Seq [ SparkDataStream ] sparkSession \u00b6 sparkSession : SparkSession SparkSession of the streaming query Tip Find out more on SparkSession in The Internals of Spark SQL online book. triggerClock \u00b6 triggerClock : Clock Clock of the streaming query Implementations \u00b6 StreamExecution spark.sql.streaming.noDataProgressEventInterval \u00b6 ProgressReporter uses the spark.sql.streaming.noDataProgressEventInterval configuration property to control how long to wait between two progress events when there is no data (default: 10000L ) when finishing a trigger . Demo \u00b6 import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sampleQuery = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.ProcessingTime(10.seconds)) .start // Using public API import org.apache.spark.sql.streaming.SourceProgress scala> sampleQuery. | lastProgress. | sources. | map { case sp: SourceProgress => | s\"source = ${sp.description} => endOffset = ${sp.endOffset}\" }. | foreach(println) source = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] => endOffset = 663 scala> println(sampleQuery.lastProgress.sources(0)) res40: org.apache.spark.sql.streaming.SourceProgress = { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : 333, \"endOffset\" : 343, \"numInputRows\" : 10, \"inputRowsPerSecond\" : 0.9998000399920015, \"processedRowsPerSecond\" : 200.0 } // With a hack import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val offsets = sampleQuery. asInstanceOf[StreamingQueryWrapper]. streamingQuery. availableOffsets. map { case (source, offset) => s\"source = $source => offset = $offset\" } scala> offsets.foreach(println) source = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] => offset = 293 StreamingQueryProgress Queue \u00b6 progressBuffer : Queue [ StreamingQueryProgress ] progressBuffer is a scala.collection.mutable.Queue of StreamingQueryProgress es. progressBuffer has a new StreamingQueryProgress added when ProgressReporter is requested to update progress of a streaming query . The oldest StreamingQueryProgress is removed ( dequeued ) above spark.sql.streaming.numRecentProgressUpdates threshold. progressBuffer is used when ProgressReporter is requested for the last and the recent StreamingQueryProgresses . Current StreamingQueryStatus \u00b6 status : StreamingQueryStatus status is the current StreamingQueryStatus . status is used when StreamingQueryWrapper is requested for the current status of a streaming query . Updating Progress of Streaming Query \u00b6 updateProgress ( newProgress : StreamingQueryProgress ) : Unit updateProgress records the input newProgress and posts a QueryProgressEvent event. updateProgress adds the input newProgress to progressBuffer . updateProgress removes elements from progressBuffer if their number is or exceeds the value of spark.sql.streaming.numRecentProgressUpdates configuration property. updateProgress posts a QueryProgressEvent (with the input newProgress ). updateProgress prints out the following INFO message to the logs: Streaming query made progress: [newProgress] updateProgress is used when ProgressReporter is requested to finish up a trigger . Initializing Query Progress for New Trigger \u00b6 startTrigger () : Unit startTrigger prints out the following DEBUG message to the logs: Starting Trigger Calculation .startTrigger's Internal Registry Changes For New Trigger [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Registry | New Value | < > | < > | < > | Requests the < > for the current timestamp (in millis) | < > | Enables ( true ) the isTriggerActive flag of the < > | < > | null | < > | null | < > | Clears the < > |=== startTrigger is used when: MicroBatchExecution stream execution engine is requested to run an activated streaming query (at the beginning of every trigger ) ContinuousExecution stream execution engine is requested to run an activated streaming query (at the beginning of every trigger) StreamExecution starts running batches (as part of TriggerExecutor executing a batch runner). Finishing Up Streaming Batch (Trigger) \u00b6 finishTrigger ( hasNewData : Boolean ) : Unit finishTrigger sets currentTriggerEndTimestamp to the current time (using triggerClock ). finishTrigger < >. finishTrigger calculates the processing time (in seconds) as the difference between the < > and < > timestamps. finishTrigger calculates the input time (in seconds) as the difference between the start time of the < > and < > triggers. .ProgressReporter's finishTrigger and Timestamps image::images/ProgressReporter-finishTrigger-timestamps.png[align=\"center\"] finishTrigger prints out the following DEBUG message to the logs: Execution stats: [executionStats] finishTrigger creates a < > (aka source statistics) for < >. finishTrigger creates a < > (aka sink statistics) for the < >. finishTrigger creates a StreamingQueryProgress . If there was any data (using the input hasNewData flag), finishTrigger resets < > (i.e. becomes the minimum possible time) and < >. Otherwise, when no data was available (using the input hasNewData flag), finishTrigger < > only when < > passed. In the end, finishTrigger disables isTriggerActive flag of < > (i.e. sets it to false ). NOTE: finishTrigger is used exclusively when MicroBatchExecution is requested to < > (after < > at the end of a streaming batch). Time-Tracking Section (Recording Execution Time) \u00b6 reportTimeTaken [ T ]( triggerDetailKey : String )( body : => T ) : T reportTimeTaken measures the time to execute body and records it in the currentDurationsMs internal registry under triggerDetailKey key. If the triggerDetailKey key was recorded already, the current execution time is added. In the end, reportTimeTaken prints out the following DEBUG message to the logs and returns the result of executing body . [triggerDetailKey] took [time] ms reportTimeTaken is used when stream execution engines are requested to execute the following phases (that appear as triggerDetailKey in the DEBUG message in the logs): MicroBatchExecution triggerExecution getOffset setOffsetRange getEndOffset walCommit getBatch queryPlanning addBatch ContinuousExecution queryPlanning runContinuous Updating Status Message \u00b6 updateStatusMessage ( message : String ) : Unit updateStatusMessage simply updates the message in the StreamingQueryStatus internal registry. updateStatusMessage is used when: StreamExecution is requested to run stream processing MicroBatchExecution is requested to run an activated streaming query or construct the next streaming micro-batch === [[extractExecutionStats]] Generating Execution Statistics -- extractExecutionStats Internal Method [source, scala] \u00b6 extractExecutionStats(hasNewData: Boolean): ExecutionStats \u00b6 extractExecutionStats generates an < > of the < > of the streaming query. Internally, extractExecutionStats generate watermark metric (using the < > of the < >) if there is a EventTimeWatermark unary logical operator in the < > of the streaming query. extractExecutionStats < >. extractExecutionStats < >. extractExecutionStats finds the EventTimeWatermarkExec unary physical operator (with non-zero EventTimeStats ) and generates max , min , and avg statistics. In the end, extractExecutionStats creates a < > with the execution statistics. If the input hasNewData flag is turned off ( false ), extractExecutionStats returns an < > with no input rows and event-time statistics (that require data to be processed to have any sense). NOTE: extractExecutionStats is used exclusively when ProgressReporter is requested to < >. === [[extractStateOperatorMetrics]] Generating StateStoreWriter Metrics (StateOperatorProgress) -- extractStateOperatorMetrics Internal Method extractStateOperatorMetrics ( hasNewData : Boolean ) : Seq [ StateOperatorProgress ] extractStateOperatorMetrics requests the < > for the optimized execution plan ( executedPlan ) and finds all StateStoreWriter physical operators and requests them for StateOperatorProgress . extractStateOperatorMetrics clears ( zeros ) the numRowsUpdated metric for the given hasNewData turned off ( false ). extractStateOperatorMetrics returns an empty collection for the < > uninitialized ( null ). extractStateOperatorMetrics is used when ProgressReporter is requested to generate execution statistics . === [[recordTriggerOffsets]] Recording Trigger Offsets (StreamProgress) -- recordTriggerOffsets Method [source, scala] \u00b6 recordTriggerOffsets( from: StreamProgress, to: StreamProgress): Unit recordTriggerOffsets simply sets ( records ) the < > and < > internal registries to the < > representations of the from and to < >. [NOTE] \u00b6 recordTriggerOffsets is used when: MicroBatchExecution is requested to < > * ContinuousExecution is requested to < > \u00b6 Last StreamingQueryProgress \u00b6 lastProgress : StreamingQueryProgress The last StreamingQueryProgress === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentDurationsMs a| [[currentDurationsMs]] http://www.scala-lang.org/api/2.11.11/index.html#scala.collection.mutable.HashMap[scala.collection.mutable.HashMap ] of action names (aka triggerDetailKey ) and their cumulative times (in milliseconds). Starts empty when ProgressReporter < > with new entries added or updated when < > (of an action). [TIP] \u00b6 You can see the current value of currentDurationsMs in progress reports under durationMs . [options=\"wrap\"] \u00b6 scala> query.lastProgress.durationMs res3: java.util.Map[String,Long] = {triggerExecution=60, queryPlanning=1, getBatch=5, getOffset=0, addBatch=30, walCommit=23} ==== | currentStatus a| [[currentStatus]] StreamingQueryStatus with the current status of the streaming query Available using < > method message updated with < > | currentTriggerEndOffsets a| [[currentTriggerEndOffsets]] | currentTriggerEndTimestamp a| [[currentTriggerEndTimestamp]] Timestamp of when the current batch/trigger has ended Default: -1L | currentTriggerStartOffsets a| [[currentTriggerStartOffsets]] [source, scala] \u00b6 currentTriggerStartOffsets: Map[BaseStreamingSource, String] \u00b6 Start offsets (in < >) per < > Used exclusively when < > (for a SourceProgress ) Reset ( null ) when < > Initialized when < > | currentTriggerStartTimestamp a| [[currentTriggerStartTimestamp]] Timestamp of when the current batch/trigger has started Default: -1L | lastNoDataProgressEventTime a| [[lastNoDataProgressEventTime]] Default: Long.MinValue | lastTriggerStartTimestamp a| [[lastTriggerStartTimestamp]] Timestamp of when the last batch/trigger started Default: -1L | metricWarningLogged a| [[metricWarningLogged]] Flag to...FIXME Default: false |=== Logging \u00b6 Configure logging of the concrete stream execution progress reporters to see what happens inside: ContinuousExecution MicroBatchExecution","title":"ProgressReporter"},{"location":"monitoring/ProgressReporter/#progressreporter","text":"ProgressReporter is an abstraction of stream execution progress reporters that report the statistics of execution of a streaming query.","title":"ProgressReporter"},{"location":"monitoring/ProgressReporter/#contract","text":"","title":"Contract"},{"location":"monitoring/ProgressReporter/#currentbatchid","text":"currentBatchId : Long ID of the active ( current ) streaming micro-batch Used when...FIXME","title":" currentBatchId"},{"location":"monitoring/ProgressReporter/#id","text":"id : UUID Universally unique identifier (UUID) of the streaming query (that remains unchanged between restarts) Used when...FIXME","title":" id"},{"location":"monitoring/ProgressReporter/#lastexecution","text":"lastExecution : QueryExecution QueryExecution of the streaming query Used when...FIXME","title":" lastExecution"},{"location":"monitoring/ProgressReporter/#logicalplan","text":"logicalPlan : LogicalPlan Logical query plan of the streaming query Used when ProgressReporter is requested for the following: extract statistics from the most recent query execution (to add watermark metric for streaming watermark ) extractSourceToNumInputRows","title":" logicalPlan"},{"location":"monitoring/ProgressReporter/#name","text":"name : String Name of the streaming query","title":" name"},{"location":"monitoring/ProgressReporter/#newdata","text":"newData : Map [ SparkDataStream , LogicalPlan ] SparkDataStream s with the new data (as a LogicalPlan )","title":" newData"},{"location":"monitoring/ProgressReporter/#offsetseqmetadata","text":"offsetSeqMetadata : OffsetSeqMetadata OffsetSeqMetadata (with the current micro-batch event-time watermark and timestamp )","title":" offsetSeqMetadata"},{"location":"monitoring/ProgressReporter/#postevent","text":"postEvent ( event : StreamingQueryListener.Event ) : Unit Posts StreamingQueryListener.Event","title":" postEvent"},{"location":"monitoring/ProgressReporter/#runid","text":"runId : UUID Universally unique identifier (UUID) of a single run of the streaming query (that changes every restart)","title":" runId"},{"location":"monitoring/ProgressReporter/#sink","text":"sink : Table The one and only Table of the streaming query","title":" sink"},{"location":"monitoring/ProgressReporter/#sinkcommitprogress","text":"sinkCommitProgress : Option [ StreamWriterCommitProgress ]","title":" sinkCommitProgress"},{"location":"monitoring/ProgressReporter/#sources","text":"sources : Seq [ SparkDataStream ]","title":" sources"},{"location":"monitoring/ProgressReporter/#sparksession","text":"sparkSession : SparkSession SparkSession of the streaming query Tip Find out more on SparkSession in The Internals of Spark SQL online book.","title":" sparkSession"},{"location":"monitoring/ProgressReporter/#triggerclock","text":"triggerClock : Clock Clock of the streaming query","title":" triggerClock"},{"location":"monitoring/ProgressReporter/#implementations","text":"StreamExecution","title":"Implementations"},{"location":"monitoring/ProgressReporter/#sparksqlstreamingnodataprogresseventinterval","text":"ProgressReporter uses the spark.sql.streaming.noDataProgressEventInterval configuration property to control how long to wait between two progress events when there is no data (default: 10000L ) when finishing a trigger .","title":" spark.sql.streaming.noDataProgressEventInterval"},{"location":"monitoring/ProgressReporter/#demo","text":"import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val sampleQuery = spark .readStream .format(\"rate\") .load .writeStream .format(\"console\") .option(\"truncate\", false) .trigger(Trigger.ProcessingTime(10.seconds)) .start // Using public API import org.apache.spark.sql.streaming.SourceProgress scala> sampleQuery. | lastProgress. | sources. | map { case sp: SourceProgress => | s\"source = ${sp.description} => endOffset = ${sp.endOffset}\" }. | foreach(println) source = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] => endOffset = 663 scala> println(sampleQuery.lastProgress.sources(0)) res40: org.apache.spark.sql.streaming.SourceProgress = { \"description\" : \"RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]\", \"startOffset\" : 333, \"endOffset\" : 343, \"numInputRows\" : 10, \"inputRowsPerSecond\" : 0.9998000399920015, \"processedRowsPerSecond\" : 200.0 } // With a hack import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val offsets = sampleQuery. asInstanceOf[StreamingQueryWrapper]. streamingQuery. availableOffsets. map { case (source, offset) => s\"source = $source => offset = $offset\" } scala> offsets.foreach(println) source = RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8] => offset = 293","title":"Demo"},{"location":"monitoring/ProgressReporter/#streamingqueryprogress-queue","text":"progressBuffer : Queue [ StreamingQueryProgress ] progressBuffer is a scala.collection.mutable.Queue of StreamingQueryProgress es. progressBuffer has a new StreamingQueryProgress added when ProgressReporter is requested to update progress of a streaming query . The oldest StreamingQueryProgress is removed ( dequeued ) above spark.sql.streaming.numRecentProgressUpdates threshold. progressBuffer is used when ProgressReporter is requested for the last and the recent StreamingQueryProgresses .","title":" StreamingQueryProgress Queue"},{"location":"monitoring/ProgressReporter/#current-streamingquerystatus","text":"status : StreamingQueryStatus status is the current StreamingQueryStatus . status is used when StreamingQueryWrapper is requested for the current status of a streaming query .","title":" Current StreamingQueryStatus"},{"location":"monitoring/ProgressReporter/#updating-progress-of-streaming-query","text":"updateProgress ( newProgress : StreamingQueryProgress ) : Unit updateProgress records the input newProgress and posts a QueryProgressEvent event. updateProgress adds the input newProgress to progressBuffer . updateProgress removes elements from progressBuffer if their number is or exceeds the value of spark.sql.streaming.numRecentProgressUpdates configuration property. updateProgress posts a QueryProgressEvent (with the input newProgress ). updateProgress prints out the following INFO message to the logs: Streaming query made progress: [newProgress] updateProgress is used when ProgressReporter is requested to finish up a trigger .","title":" Updating Progress of Streaming Query"},{"location":"monitoring/ProgressReporter/#initializing-query-progress-for-new-trigger","text":"startTrigger () : Unit startTrigger prints out the following DEBUG message to the logs: Starting Trigger Calculation .startTrigger's Internal Registry Changes For New Trigger [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Registry | New Value | < > | < > | < > | Requests the < > for the current timestamp (in millis) | < > | Enables ( true ) the isTriggerActive flag of the < > | < > | null | < > | null | < > | Clears the < > |=== startTrigger is used when: MicroBatchExecution stream execution engine is requested to run an activated streaming query (at the beginning of every trigger ) ContinuousExecution stream execution engine is requested to run an activated streaming query (at the beginning of every trigger) StreamExecution starts running batches (as part of TriggerExecutor executing a batch runner).","title":" Initializing Query Progress for New Trigger"},{"location":"monitoring/ProgressReporter/#finishing-up-streaming-batch-trigger","text":"finishTrigger ( hasNewData : Boolean ) : Unit finishTrigger sets currentTriggerEndTimestamp to the current time (using triggerClock ). finishTrigger < >. finishTrigger calculates the processing time (in seconds) as the difference between the < > and < > timestamps. finishTrigger calculates the input time (in seconds) as the difference between the start time of the < > and < > triggers. .ProgressReporter's finishTrigger and Timestamps image::images/ProgressReporter-finishTrigger-timestamps.png[align=\"center\"] finishTrigger prints out the following DEBUG message to the logs: Execution stats: [executionStats] finishTrigger creates a < > (aka source statistics) for < >. finishTrigger creates a < > (aka sink statistics) for the < >. finishTrigger creates a StreamingQueryProgress . If there was any data (using the input hasNewData flag), finishTrigger resets < > (i.e. becomes the minimum possible time) and < >. Otherwise, when no data was available (using the input hasNewData flag), finishTrigger < > only when < > passed. In the end, finishTrigger disables isTriggerActive flag of < > (i.e. sets it to false ). NOTE: finishTrigger is used exclusively when MicroBatchExecution is requested to < > (after < > at the end of a streaming batch).","title":" Finishing Up Streaming Batch (Trigger)"},{"location":"monitoring/ProgressReporter/#time-tracking-section-recording-execution-time","text":"reportTimeTaken [ T ]( triggerDetailKey : String )( body : => T ) : T reportTimeTaken measures the time to execute body and records it in the currentDurationsMs internal registry under triggerDetailKey key. If the triggerDetailKey key was recorded already, the current execution time is added. In the end, reportTimeTaken prints out the following DEBUG message to the logs and returns the result of executing body . [triggerDetailKey] took [time] ms reportTimeTaken is used when stream execution engines are requested to execute the following phases (that appear as triggerDetailKey in the DEBUG message in the logs): MicroBatchExecution triggerExecution getOffset setOffsetRange getEndOffset walCommit getBatch queryPlanning addBatch ContinuousExecution queryPlanning runContinuous","title":" Time-Tracking Section (Recording Execution Time)"},{"location":"monitoring/ProgressReporter/#updating-status-message","text":"updateStatusMessage ( message : String ) : Unit updateStatusMessage simply updates the message in the StreamingQueryStatus internal registry. updateStatusMessage is used when: StreamExecution is requested to run stream processing MicroBatchExecution is requested to run an activated streaming query or construct the next streaming micro-batch === [[extractExecutionStats]] Generating Execution Statistics -- extractExecutionStats Internal Method","title":" Updating Status Message"},{"location":"monitoring/ProgressReporter/#source-scala","text":"","title":"[source, scala]"},{"location":"monitoring/ProgressReporter/#extractexecutionstatshasnewdata-boolean-executionstats","text":"extractExecutionStats generates an < > of the < > of the streaming query. Internally, extractExecutionStats generate watermark metric (using the < > of the < >) if there is a EventTimeWatermark unary logical operator in the < > of the streaming query. extractExecutionStats < >. extractExecutionStats < >. extractExecutionStats finds the EventTimeWatermarkExec unary physical operator (with non-zero EventTimeStats ) and generates max , min , and avg statistics. In the end, extractExecutionStats creates a < > with the execution statistics. If the input hasNewData flag is turned off ( false ), extractExecutionStats returns an < > with no input rows and event-time statistics (that require data to be processed to have any sense). NOTE: extractExecutionStats is used exclusively when ProgressReporter is requested to < >. === [[extractStateOperatorMetrics]] Generating StateStoreWriter Metrics (StateOperatorProgress) -- extractStateOperatorMetrics Internal Method extractStateOperatorMetrics ( hasNewData : Boolean ) : Seq [ StateOperatorProgress ] extractStateOperatorMetrics requests the < > for the optimized execution plan ( executedPlan ) and finds all StateStoreWriter physical operators and requests them for StateOperatorProgress . extractStateOperatorMetrics clears ( zeros ) the numRowsUpdated metric for the given hasNewData turned off ( false ). extractStateOperatorMetrics returns an empty collection for the < > uninitialized ( null ). extractStateOperatorMetrics is used when ProgressReporter is requested to generate execution statistics . === [[recordTriggerOffsets]] Recording Trigger Offsets (StreamProgress) -- recordTriggerOffsets Method","title":"extractExecutionStats(hasNewData: Boolean): ExecutionStats"},{"location":"monitoring/ProgressReporter/#source-scala_1","text":"recordTriggerOffsets( from: StreamProgress, to: StreamProgress): Unit recordTriggerOffsets simply sets ( records ) the < > and < > internal registries to the < > representations of the from and to < >.","title":"[source, scala]"},{"location":"monitoring/ProgressReporter/#note","text":"recordTriggerOffsets is used when: MicroBatchExecution is requested to < >","title":"[NOTE]"},{"location":"monitoring/ProgressReporter/#continuousexecution-is-requested-to","text":"","title":"* ContinuousExecution is requested to &lt;&gt;"},{"location":"monitoring/ProgressReporter/#last-streamingqueryprogress","text":"lastProgress : StreamingQueryProgress The last StreamingQueryProgress === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | currentDurationsMs a| [[currentDurationsMs]] http://www.scala-lang.org/api/2.11.11/index.html#scala.collection.mutable.HashMap[scala.collection.mutable.HashMap ] of action names (aka triggerDetailKey ) and their cumulative times (in milliseconds). Starts empty when ProgressReporter < > with new entries added or updated when < > (of an action).","title":" Last StreamingQueryProgress"},{"location":"monitoring/ProgressReporter/#tip","text":"You can see the current value of currentDurationsMs in progress reports under durationMs .","title":"[TIP]"},{"location":"monitoring/ProgressReporter/#optionswrap","text":"scala> query.lastProgress.durationMs res3: java.util.Map[String,Long] = {triggerExecution=60, queryPlanning=1, getBatch=5, getOffset=0, addBatch=30, walCommit=23} ==== | currentStatus a| [[currentStatus]] StreamingQueryStatus with the current status of the streaming query Available using < > method message updated with < > | currentTriggerEndOffsets a| [[currentTriggerEndOffsets]] | currentTriggerEndTimestamp a| [[currentTriggerEndTimestamp]] Timestamp of when the current batch/trigger has ended Default: -1L | currentTriggerStartOffsets a| [[currentTriggerStartOffsets]]","title":"[options=\"wrap\"]"},{"location":"monitoring/ProgressReporter/#source-scala_2","text":"","title":"[source, scala]"},{"location":"monitoring/ProgressReporter/#currenttriggerstartoffsets-mapbasestreamingsource-string","text":"Start offsets (in < >) per < > Used exclusively when < > (for a SourceProgress ) Reset ( null ) when < > Initialized when < > | currentTriggerStartTimestamp a| [[currentTriggerStartTimestamp]] Timestamp of when the current batch/trigger has started Default: -1L | lastNoDataProgressEventTime a| [[lastNoDataProgressEventTime]] Default: Long.MinValue | lastTriggerStartTimestamp a| [[lastTriggerStartTimestamp]] Timestamp of when the last batch/trigger started Default: -1L | metricWarningLogged a| [[metricWarningLogged]] Flag to...FIXME Default: false |===","title":"currentTriggerStartOffsets: Map[BaseStreamingSource, String]"},{"location":"monitoring/ProgressReporter/#logging","text":"Configure logging of the concrete stream execution progress reporters to see what happens inside: ContinuousExecution MicroBatchExecution","title":"Logging"},{"location":"monitoring/SinkProgress/","text":"SinkProgress \u00b6 SinkProgress is...FIXME","title":"SinkProgress"},{"location":"monitoring/SinkProgress/#sinkprogress","text":"SinkProgress is...FIXME","title":"SinkProgress"},{"location":"monitoring/SourceProgress/","text":"SourceProgress \u00b6 SourceProgress is...FIXME","title":"SourceProgress"},{"location":"monitoring/SourceProgress/#sourceprogress","text":"SourceProgress is...FIXME","title":"SourceProgress"},{"location":"monitoring/StateOperatorProgress/","text":"StateOperatorProgress \u00b6 StateOperatorProgress is information about updates made to stateful operators in a StreamingQuery during a trigger: numRowsTotal numRowsUpdated memoryUsedBytes Custom Metrics (default: empty) StateOperatorProgress is created when StateStoreWriter is requested to getProgress .","title":"StateOperatorProgress"},{"location":"monitoring/StateOperatorProgress/#stateoperatorprogress","text":"StateOperatorProgress is information about updates made to stateful operators in a StreamingQuery during a trigger: numRowsTotal numRowsUpdated memoryUsedBytes Custom Metrics (default: empty) StateOperatorProgress is created when StateStoreWriter is requested to getProgress .","title":"StateOperatorProgress"},{"location":"monitoring/StreamingQueryListener/","text":"StreamingQueryListener \u2014 Intercepting Life Cycle Events of Streaming Queries \u00b6 StreamingQueryListener is an abstraction of listeners to be notified about the life cycle events of all the streaming queries in a Spark Structured Streaming application: Query started Query made progress Query terminated StreamingQueryListener is used internally by StreamingQueryListenerBus to post a streaming event to all registered StreamingQueryListeners . StreamingQueryListener can be used by Spark developers to intercept events in Spark Structured Streaming applications. Contract \u00b6 onQueryProgress \u00b6 onQueryProgress ( event : QueryProgressEvent ) : Unit Informs that MicroBatchExecution has finished triggerExecution phase (the end of a streaming batch) onQueryStarted \u00b6 onQueryStarted ( event : QueryStartedEvent ) : Unit Informs that DataStreamWriter was requested to start execution of the streaming query (on the stream execution thread ) Note onQueryStarted is used internally to unblock the starting thread of StreamExecution . onQueryTerminated \u00b6 onQueryTerminated ( event : QueryTerminatedEvent ) : Unit Informs that a streaming query was < > or terminated due to an error Lifecycle Events \u00b6 StreamingQueryListener is informed about the life cycle events when StreamingQueryListenerBus is requested to doPostEvent . QueryStartedEvent \u00b6 id runId name Intercepted by onQueryStarted Posted when StreamExecution is requested to run stream processing (when DataStreamWriter is requested to start execution of the streaming query on the stream execution thread ) QueryProgressEvent \u00b6 StreamingQueryProgress Intercepted by onQueryProgress Posted when ProgressReporter is requested to update progress of a streaming query (after MicroBatchExecution has finished triggerExecution phase at the end of a streaming batch) QueryTerminatedEvent \u00b6 id runId exception if terminated due to an error Intercepted by onQueryTerminated Posted when StreamExecution is requested to run stream processing (and the streaming query was stopped or terminated due to an error) Registering StreamingQueryListener \u00b6 StreamingQueryListener can be registered using StreamingQueryManager.addListener method. val queryListener: StreamingQueryListener = ... spark.streams.addListener(queryListener) Deregistering StreamingQueryListener \u00b6 StreamingQueryListener can be deregistered using StreamingQueryManager.removeListener method. val queryListener : StreamingQueryListener = ... spark . streams . removeListener ( queryListener )","title":"StreamingQueryListener"},{"location":"monitoring/StreamingQueryListener/#streamingquerylistener-intercepting-life-cycle-events-of-streaming-queries","text":"StreamingQueryListener is an abstraction of listeners to be notified about the life cycle events of all the streaming queries in a Spark Structured Streaming application: Query started Query made progress Query terminated StreamingQueryListener is used internally by StreamingQueryListenerBus to post a streaming event to all registered StreamingQueryListeners . StreamingQueryListener can be used by Spark developers to intercept events in Spark Structured Streaming applications.","title":"StreamingQueryListener &mdash; Intercepting Life Cycle Events of Streaming Queries"},{"location":"monitoring/StreamingQueryListener/#contract","text":"","title":"Contract"},{"location":"monitoring/StreamingQueryListener/#onqueryprogress","text":"onQueryProgress ( event : QueryProgressEvent ) : Unit Informs that MicroBatchExecution has finished triggerExecution phase (the end of a streaming batch)","title":" onQueryProgress"},{"location":"monitoring/StreamingQueryListener/#onquerystarted","text":"onQueryStarted ( event : QueryStartedEvent ) : Unit Informs that DataStreamWriter was requested to start execution of the streaming query (on the stream execution thread ) Note onQueryStarted is used internally to unblock the starting thread of StreamExecution .","title":" onQueryStarted"},{"location":"monitoring/StreamingQueryListener/#onqueryterminated","text":"onQueryTerminated ( event : QueryTerminatedEvent ) : Unit Informs that a streaming query was < > or terminated due to an error","title":" onQueryTerminated"},{"location":"monitoring/StreamingQueryListener/#lifecycle-events","text":"StreamingQueryListener is informed about the life cycle events when StreamingQueryListenerBus is requested to doPostEvent .","title":" Lifecycle Events"},{"location":"monitoring/StreamingQueryListener/#querystartedevent","text":"id runId name Intercepted by onQueryStarted Posted when StreamExecution is requested to run stream processing (when DataStreamWriter is requested to start execution of the streaming query on the stream execution thread )","title":"QueryStartedEvent"},{"location":"monitoring/StreamingQueryListener/#queryprogressevent","text":"StreamingQueryProgress Intercepted by onQueryProgress Posted when ProgressReporter is requested to update progress of a streaming query (after MicroBatchExecution has finished triggerExecution phase at the end of a streaming batch)","title":"QueryProgressEvent"},{"location":"monitoring/StreamingQueryListener/#queryterminatedevent","text":"id runId exception if terminated due to an error Intercepted by onQueryTerminated Posted when StreamExecution is requested to run stream processing (and the streaming query was stopped or terminated due to an error)","title":"QueryTerminatedEvent"},{"location":"monitoring/StreamingQueryListener/#registering-streamingquerylistener","text":"StreamingQueryListener can be registered using StreamingQueryManager.addListener method. val queryListener: StreamingQueryListener = ... spark.streams.addListener(queryListener)","title":"Registering StreamingQueryListener"},{"location":"monitoring/StreamingQueryListener/#deregistering-streamingquerylistener","text":"StreamingQueryListener can be deregistered using StreamingQueryManager.removeListener method. val queryListener : StreamingQueryListener = ... spark . streams . removeListener ( queryListener )","title":"Deregistering StreamingQueryListener"},{"location":"monitoring/StreamingQueryProgress/","text":"StreamingQueryProgress \u00b6 StreamingQueryProgress is information about a single micro-batch ( progress ) of a StreamingQuery : Unique identifier Unique identifier of a query execution Name Time when a trigger has started (in ISO8601 format) Unique ID of a micro-batch Batch Duration Durations of the internal phases (in ms) Statistics of the event time as seen in a batch StateOperatorProgress for every stateful operator SourceProgress for every streaming source SinkProgress Observed Metrics StreamingQueryProgress is created when StreamExecution is requested to finish a trigger . Last and Recent Progresses \u00b6 Use lastProgress property of a StreamingQuery to access the most recent StreamingQueryProgress update. val sq : StreamingQuery = ... sq . lastProgress Use recentProgress property of a StreamingQuery to access the most recent StreamingQueryProgress updates. val sq : StreamingQuery = ... sq . recentProgress StreamingQueryListener \u00b6 Use StreamingQueryListener to be notified about StreamingQueryProgress updates while a streaming query is executed.","title":"StreamingQueryProgress"},{"location":"monitoring/StreamingQueryProgress/#streamingqueryprogress","text":"StreamingQueryProgress is information about a single micro-batch ( progress ) of a StreamingQuery : Unique identifier Unique identifier of a query execution Name Time when a trigger has started (in ISO8601 format) Unique ID of a micro-batch Batch Duration Durations of the internal phases (in ms) Statistics of the event time as seen in a batch StateOperatorProgress for every stateful operator SourceProgress for every streaming source SinkProgress Observed Metrics StreamingQueryProgress is created when StreamExecution is requested to finish a trigger .","title":"StreamingQueryProgress"},{"location":"monitoring/StreamingQueryProgress/#last-and-recent-progresses","text":"Use lastProgress property of a StreamingQuery to access the most recent StreamingQueryProgress update. val sq : StreamingQuery = ... sq . lastProgress Use recentProgress property of a StreamingQuery to access the most recent StreamingQueryProgress updates. val sq : StreamingQuery = ... sq . recentProgress","title":"Last and Recent Progresses"},{"location":"monitoring/StreamingQueryProgress/#streamingquerylistener","text":"Use StreamingQueryListener to be notified about StreamingQueryProgress updates while a streaming query is executed.","title":"StreamingQueryListener"},{"location":"monitoring/StreamingQueryStatus/","text":"StreamingQueryStatus \u00b6 StreamingQueryStatus is...FIXME","title":"StreamingQueryStatus"},{"location":"monitoring/StreamingQueryStatus/#streamingquerystatus","text":"StreamingQueryStatus is...FIXME","title":"StreamingQueryStatus"},{"location":"operators/","text":"Streaming Operators \u2014 High-Level Declarative Streaming Dataset API \u00b6 Dataset API defines a set of operators that are used in Spark Structured Streaming and together constitute the High-Level Declarative Streaming Dataset API .","title":"Streaming Operators"},{"location":"operators/#streaming-operators-high-level-declarative-streaming-dataset-api","text":"Dataset API defines a set of operators that are used in Spark Structured Streaming and together constitute the High-Level Declarative Streaming Dataset API .","title":"Streaming Operators &mdash; High-Level Declarative Streaming Dataset API"},{"location":"operators/crossJoin/","text":"crossJoin Operator \u2014 Streaming Join \u00b6 crossJoin ( right : Dataset [ _ ]) : DataFrame crossJoin operator...FIXME","title":"crossJoin"},{"location":"operators/crossJoin/#crossjoin-operator-streaming-join","text":"crossJoin ( right : Dataset [ _ ]) : DataFrame crossJoin operator...FIXME","title":"crossJoin Operator &mdash; Streaming Join"},{"location":"operators/dropDuplicates/","text":"dropDuplicates Operator \u2014 Streaming Deduplication \u00b6 dropDuplicates () : Dataset [ T ] dropDuplicates ( colNames : Seq [ String ]) : Dataset [ T ] dropDuplicates ( col1 : String , cols : String* ) : Dataset [ T ] dropDuplicates operator drops duplicate records (given a subset of columns) Note For a streaming Dataset, dropDuplicates will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark operator to limit how late the duplicate data can be and system will accordingly limit the state. In addition, too late data older than watermark will be dropped to avoid any possibility of duplicates. Demo \u00b6 // Start a streaming query // Using old-fashioned MemoryStream (with the deprecated SQLContext) import org.apache.spark.sql.execution.streaming.MemoryStream import org.apache.spark.sql.SQLContext implicit val sqlContext: SQLContext = spark.sqlContext val source = MemoryStream[(Int, Int)] val ids = source.toDS.toDF(\"time\", \"id\"). withColumn(\"time\", $\"time\" cast \"timestamp\"). // <-- convert time column from Int to Timestamp dropDuplicates(\"id\"). withColumn(\"time\", $\"time\" cast \"long\") // <-- convert time column back from Timestamp to Int // Conversions are only for display purposes // Internally we need timestamps for watermark to work // Displaying timestamps could be too much for such a simple task scala> println(ids.queryExecution.analyzed.numberedTreeString) 00 Project [cast(time#10 as bigint) AS time#15L, id#6] 01 +- Deduplicate [id#6], true 02 +- Project [cast(time#5 as timestamp) AS time#10, id#6] 03 +- Project [_1#2 AS time#5, _2#3 AS id#6] 04 +- StreamingExecutionRelation MemoryStream[_1#2,_2#3], [_1#2, _2#3] import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = ids. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Append). trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start // Publish duplicate records source.addData(1 -> 1) source.addData(2 -> 1) source.addData(3 -> 1) q.processAllAvailable() // Check out how dropDuplicates removes duplicates // --> per single streaming batch (easy) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| +----+---+ source.addData(4 -> 1) source.addData(5 -> 2) // --> across streaming batches (harder) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| | 5| 2| +----+---+ // Check out the internal state scala> println(q.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 1, \"memoryUsedBytes\" : 17751 } // You could use web UI's SQL tab instead // Use Details for Query source.addData(6 -> 2) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| | 5| 2| +----+---+ // Check out the internal state scala> println(q.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 17751 } // Restart the streaming query q.stop val q = ids. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Complete). // <-- memory sink supports checkpointing for Complete output mode only trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start // Doh! MemorySink is fine, but Complete is only available with a streaming aggregation // Answer it if you know why --> https://stackoverflow.com/q/45756997/1305344 // It's a high time to work on https://issues.apache.org/jira/browse/SPARK-21667 // to understand the low-level details (and the reason, it seems) // Disabling operation checks and starting over // ./bin/spark-shell -c spark.sql.streaming.unsupportedOperationCheck=false // it works now --> no exception! scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ +----+---+ source.addData(0 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 0| 1| +----+---+ source.addData(1 -> 1) source.addData(2 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ +----+---+ // What?! No rows?! It doesn't look as if it worked fine :( // Use groupBy to pass the requirement of having streaming aggregation for Complete output mode val counts = ids.groupBy(\"id\").agg(first($\"time\") as \"first_time\") scala> counts.explain == Physical Plan == *HashAggregate(keys=[id#246], functions=[first(time#255L, false)]) +- StateStoreSave [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,0,0), Append, 0 +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)]) +- StateStoreRestore [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,0,0) +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)]) +- *HashAggregate(keys=[id#246], functions=[partial_first(time#255L, false)]) +- *Project [cast(time#250 as bigint) AS time#255L, id#246] +- StreamingDeduplicate [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,1,0), 0 +- Exchange hashpartitioning(id#246, 200) +- *Project [cast(_1#242 as timestamp) AS time#250, _2#243 AS id#246] +- StreamingRelation MemoryStream[_1#242,_2#243], [_1#242, _2#243] val q = counts. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Complete). // <-- memory sink supports checkpointing for Complete output mode only trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start source.addData(0 -> 1) source.addData(1 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +---+----------+ | id|first_time| +---+----------+ | 1| 0| +---+----------+ // Publish duplicates // Check out how dropDuplicates removes duplicates // Stop the streaming query // Specify event time watermark to remove old duplicates","title":"dropDuplicates"},{"location":"operators/dropDuplicates/#dropduplicates-operator-streaming-deduplication","text":"dropDuplicates () : Dataset [ T ] dropDuplicates ( colNames : Seq [ String ]) : Dataset [ T ] dropDuplicates ( col1 : String , cols : String* ) : Dataset [ T ] dropDuplicates operator drops duplicate records (given a subset of columns) Note For a streaming Dataset, dropDuplicates will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark operator to limit how late the duplicate data can be and system will accordingly limit the state. In addition, too late data older than watermark will be dropped to avoid any possibility of duplicates.","title":"dropDuplicates Operator &mdash; Streaming Deduplication"},{"location":"operators/dropDuplicates/#demo","text":"// Start a streaming query // Using old-fashioned MemoryStream (with the deprecated SQLContext) import org.apache.spark.sql.execution.streaming.MemoryStream import org.apache.spark.sql.SQLContext implicit val sqlContext: SQLContext = spark.sqlContext val source = MemoryStream[(Int, Int)] val ids = source.toDS.toDF(\"time\", \"id\"). withColumn(\"time\", $\"time\" cast \"timestamp\"). // <-- convert time column from Int to Timestamp dropDuplicates(\"id\"). withColumn(\"time\", $\"time\" cast \"long\") // <-- convert time column back from Timestamp to Int // Conversions are only for display purposes // Internally we need timestamps for watermark to work // Displaying timestamps could be too much for such a simple task scala> println(ids.queryExecution.analyzed.numberedTreeString) 00 Project [cast(time#10 as bigint) AS time#15L, id#6] 01 +- Deduplicate [id#6], true 02 +- Project [cast(time#5 as timestamp) AS time#10, id#6] 03 +- Project [_1#2 AS time#5, _2#3 AS id#6] 04 +- StreamingExecutionRelation MemoryStream[_1#2,_2#3], [_1#2, _2#3] import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val q = ids. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Append). trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start // Publish duplicate records source.addData(1 -> 1) source.addData(2 -> 1) source.addData(3 -> 1) q.processAllAvailable() // Check out how dropDuplicates removes duplicates // --> per single streaming batch (easy) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| +----+---+ source.addData(4 -> 1) source.addData(5 -> 2) // --> across streaming batches (harder) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| | 5| 2| +----+---+ // Check out the internal state scala> println(q.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 1, \"memoryUsedBytes\" : 17751 } // You could use web UI's SQL tab instead // Use Details for Query source.addData(6 -> 2) scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 1| 1| | 5| 2| +----+---+ // Check out the internal state scala> println(q.lastProgress.stateOperators(0).prettyJson) { \"numRowsTotal\" : 2, \"numRowsUpdated\" : 0, \"memoryUsedBytes\" : 17751 } // Restart the streaming query q.stop val q = ids. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Complete). // <-- memory sink supports checkpointing for Complete output mode only trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start // Doh! MemorySink is fine, but Complete is only available with a streaming aggregation // Answer it if you know why --> https://stackoverflow.com/q/45756997/1305344 // It's a high time to work on https://issues.apache.org/jira/browse/SPARK-21667 // to understand the low-level details (and the reason, it seems) // Disabling operation checks and starting over // ./bin/spark-shell -c spark.sql.streaming.unsupportedOperationCheck=false // it works now --> no exception! scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ +----+---+ source.addData(0 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ | 0| 1| +----+---+ source.addData(1 -> 1) source.addData(2 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +----+---+ |time| id| +----+---+ +----+---+ // What?! No rows?! It doesn't look as if it worked fine :( // Use groupBy to pass the requirement of having streaming aggregation for Complete output mode val counts = ids.groupBy(\"id\").agg(first($\"time\") as \"first_time\") scala> counts.explain == Physical Plan == *HashAggregate(keys=[id#246], functions=[first(time#255L, false)]) +- StateStoreSave [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,0,0), Append, 0 +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)]) +- StateStoreRestore [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,0,0) +- *HashAggregate(keys=[id#246], functions=[merge_first(time#255L, false)]) +- *HashAggregate(keys=[id#246], functions=[partial_first(time#255L, false)]) +- *Project [cast(time#250 as bigint) AS time#255L, id#246] +- StreamingDeduplicate [id#246], StatefulOperatorStateInfo(<unknown>,3585583b-42d7-4547-8d62-255581c48275,1,0), 0 +- Exchange hashpartitioning(id#246, 200) +- *Project [cast(_1#242 as timestamp) AS time#250, _2#243 AS id#246] +- StreamingRelation MemoryStream[_1#242,_2#243], [_1#242, _2#243] val q = counts. writeStream. format(\"memory\"). queryName(\"dups\"). outputMode(OutputMode.Complete). // <-- memory sink supports checkpointing for Complete output mode only trigger(Trigger.ProcessingTime(30.seconds)). option(\"checkpointLocation\", \"checkpoint-dir\"). // <-- use checkpointing to save state between restarts start source.addData(0 -> 1) source.addData(1 -> 1) // wait till the batch is triggered scala> spark.table(\"dups\").show +---+----------+ | id|first_time| +---+----------+ | 1| 0| +---+----------+ // Publish duplicates // Check out how dropDuplicates removes duplicates // Stop the streaming query // Specify event time watermark to remove old duplicates","title":"Demo"},{"location":"operators/explain/","text":"Dataset.explain Operator \u2014 Explaining Streaming Queries \u00b6 explain () : Unit // <1> explain ( extended: Boolean ) : Unit <1> Calls explain with extended flag disabled Dataset.explain operator explains query plans, i.e. prints the logical and (with extended flag enabled) physical query plans to the console. Internally, explain creates a ExplainCommand runnable command with the logical plan and extended flag. explain then executes the plan with ExplainCommand runnable command and collects the results that are printed out to the standard output. [NOTE] \u00b6 explain uses SparkSession to access the current SessionState to execute the plan. [source, scala] \u00b6 import org.apache.spark.sql.execution.command.ExplainCommand val explain = ExplainCommand(...) spark.sessionState.executePlan(explain) ==== For streaming Datasets, ExplainCommand command simply creates a spark-sql-streaming-IncrementalExecution.md[IncrementalExecution] for the SparkSession and the logical plan. NOTE: For the purpose of explain , IncrementalExecution is created with the output mode Append , checkpoint location <unknown> , run id a random number, current batch id 0 and offset metadata empty. They do not really matter when explaining the load-part of a streaming query. Demo \u00b6 val records = spark. readStream. format(\"rate\"). load scala> records.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] scala> records.explain(extended = true) == Parsed Logical Plan == StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Analyzed Logical Plan == timestamp: timestamp, value: bigint StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Optimized Logical Plan == StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L]","title":"explain"},{"location":"operators/explain/#datasetexplain-operator-explaining-streaming-queries","text":"explain () : Unit // <1> explain ( extended: Boolean ) : Unit <1> Calls explain with extended flag disabled Dataset.explain operator explains query plans, i.e. prints the logical and (with extended flag enabled) physical query plans to the console. Internally, explain creates a ExplainCommand runnable command with the logical plan and extended flag. explain then executes the plan with ExplainCommand runnable command and collects the results that are printed out to the standard output.","title":"Dataset.explain Operator &mdash; Explaining Streaming Queries"},{"location":"operators/explain/#note","text":"explain uses SparkSession to access the current SessionState to execute the plan.","title":"[NOTE]"},{"location":"operators/explain/#source-scala","text":"import org.apache.spark.sql.execution.command.ExplainCommand val explain = ExplainCommand(...) spark.sessionState.executePlan(explain) ==== For streaming Datasets, ExplainCommand command simply creates a spark-sql-streaming-IncrementalExecution.md[IncrementalExecution] for the SparkSession and the logical plan. NOTE: For the purpose of explain , IncrementalExecution is created with the output mode Append , checkpoint location <unknown> , run id a random number, current batch id 0 and offset metadata empty. They do not really matter when explaining the load-part of a streaming query.","title":"[source, scala]"},{"location":"operators/explain/#demo","text":"val records = spark. readStream. format(\"rate\"). load scala> records.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] scala> records.explain(extended = true) == Parsed Logical Plan == StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Analyzed Logical Plan == timestamp: timestamp, value: bigint StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Optimized Logical Plan == StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4071aa13,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L]","title":"Demo"},{"location":"operators/flatMapGroupsWithState/","text":"flatMapGroupsWithState \u00b6 flatMapGroupsWithState is...FIXME","title":"flatMapGroupsWithState"},{"location":"operators/flatMapGroupsWithState/#flatmapgroupswithstate","text":"flatMapGroupsWithState is...FIXME","title":"flatMapGroupsWithState"},{"location":"operators/groupBy/","text":"groupBy Operator \u2014 Streaming Aggregation \u00b6 groupBy ( cols : Column* ) : RelationalGroupedDataset groupBy ( col1 : String , cols : String* ) : RelationalGroupedDataset groupBy operator aggregates rows by zero, one or more columns. Demo \u00b6 val fromTopic1 = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load // extract event time et al // time,key,value /* 2017-08-23T00:00:00.002Z,1,now 2017-08-23T00:05:00.002Z,1,5 mins later 2017-08-23T00:09:00.002Z,1,9 mins later 2017-08-23T00:11:00.002Z,1,11 mins later 2017-08-23T01:00:00.002Z,1,1 hour later // late event = watermark should be (1 hour - 10 minutes) already 2017-08-23T00:49:59.002Z,1,==> SHOULD NOT BE INCLUDED in aggregation as too late <== CAUTION: FIXME SHOULD NOT BE INCLUDED is included contrary to my understanding?! */ val timedValues = fromTopic1. select('value cast \"string\"). withColumn(\"tokens\", split('value, \",\")). withColumn(\"time\", to_timestamp('tokens(0))). withColumn(\"key\", 'tokens(1) cast \"int\"). withColumn(\"value\", 'tokens(2)). select(\"time\", \"key\", \"value\") // aggregation with watermark val counts = timedValues. withWatermark(\"time\", \"10 minutes\"). groupBy(\"key\"). agg(collect_list('value) as \"values\", collect_list('time) as \"times\") // Note that StatefulOperatorStateInfo is mostly generic // since no batch-specific values are currently available // only after the first streaming batch scala> counts.explain == Physical Plan == ObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreSave [key#27], StatefulOperatorStateInfo(<unknown>,25149816-1f14-4901-af13-896286a26d42,0,0), Append, 0 +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreRestore [key#27], StatefulOperatorStateInfo(<unknown>,25149816-1f14-4901-af13-896286a26d42,0,0) +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)]) +- EventTimeWatermark time#22: timestamp, interval 10 minutes +- *Project [cast(split(cast(value#1 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#1 as string), ,)[1] as int) AS key#27, split(cast(value#1 as string), ,)[2] AS value#33] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ val sq = counts.writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(30.seconds)). outputMode(OutputMode.Update). // <-- only Update or Complete acceptable because of groupBy aggregation start // After StreamingQuery was started, // the physical plan is complete (with batch-specific values) scala> sq.explain == Physical Plan == ObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreSave [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0), Update, 0 +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreRestore [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0) +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)]) +- EventTimeWatermark time#22: timestamp, interval 10 minutes +- *Project [cast(split(cast(value#76 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#76 as string), ,)[1] as int) AS key#27, split(cast(value#76 as string), ,)[2] AS value#33] +- Scan ExistingRDD[key#75,value#76,topic#77,partition#78,offset#79L,timestamp#80,timestampType#81]","title":"groupBy"},{"location":"operators/groupBy/#groupby-operator-streaming-aggregation","text":"groupBy ( cols : Column* ) : RelationalGroupedDataset groupBy ( col1 : String , cols : String* ) : RelationalGroupedDataset groupBy operator aggregates rows by zero, one or more columns.","title":"groupBy Operator &mdash; Streaming Aggregation"},{"location":"operators/groupBy/#demo","text":"val fromTopic1 = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). load // extract event time et al // time,key,value /* 2017-08-23T00:00:00.002Z,1,now 2017-08-23T00:05:00.002Z,1,5 mins later 2017-08-23T00:09:00.002Z,1,9 mins later 2017-08-23T00:11:00.002Z,1,11 mins later 2017-08-23T01:00:00.002Z,1,1 hour later // late event = watermark should be (1 hour - 10 minutes) already 2017-08-23T00:49:59.002Z,1,==> SHOULD NOT BE INCLUDED in aggregation as too late <== CAUTION: FIXME SHOULD NOT BE INCLUDED is included contrary to my understanding?! */ val timedValues = fromTopic1. select('value cast \"string\"). withColumn(\"tokens\", split('value, \",\")). withColumn(\"time\", to_timestamp('tokens(0))). withColumn(\"key\", 'tokens(1) cast \"int\"). withColumn(\"value\", 'tokens(2)). select(\"time\", \"key\", \"value\") // aggregation with watermark val counts = timedValues. withWatermark(\"time\", \"10 minutes\"). groupBy(\"key\"). agg(collect_list('value) as \"values\", collect_list('time) as \"times\") // Note that StatefulOperatorStateInfo is mostly generic // since no batch-specific values are currently available // only after the first streaming batch scala> counts.explain == Physical Plan == ObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreSave [key#27], StatefulOperatorStateInfo(<unknown>,25149816-1f14-4901-af13-896286a26d42,0,0), Append, 0 +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreRestore [key#27], StatefulOperatorStateInfo(<unknown>,25149816-1f14-4901-af13-896286a26d42,0,0) +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)]) +- EventTimeWatermark time#22: timestamp, interval 10 minutes +- *Project [cast(split(cast(value#1 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#1 as string), ,)[1] as int) AS key#27, split(cast(value#1 as string), ,)[2] AS value#33] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] import org.apache.spark.sql.streaming._ import scala.concurrent.duration._ val sq = counts.writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(30.seconds)). outputMode(OutputMode.Update). // <-- only Update or Complete acceptable because of groupBy aggregation start // After StreamingQuery was started, // the physical plan is complete (with batch-specific values) scala> sq.explain == Physical Plan == ObjectHashAggregate(keys=[key#27], functions=[collect_list(value#33, 0, 0), collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreSave [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0), Update, 0 +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- StateStoreRestore [key#27], StatefulOperatorStateInfo(file:/private/var/folders/0w/kb0d3rqn4zb9fcc91pxhgn8w0000gn/T/temporary-635d6519-b6ca-4686-9b6b-5db0e83cfd51/state,855cec1c-25dc-4a86-ae54-c6cdd4ed02ec,0,0) +- ObjectHashAggregate(keys=[key#27], functions=[merge_collect_list(value#33, 0, 0), merge_collect_list(time#22-T600000ms, 0, 0)]) +- Exchange hashpartitioning(key#27, 200) +- ObjectHashAggregate(keys=[key#27], functions=[partial_collect_list(value#33, 0, 0), partial_collect_list(time#22-T600000ms, 0, 0)]) +- EventTimeWatermark time#22: timestamp, interval 10 minutes +- *Project [cast(split(cast(value#76 as string), ,)[0] as timestamp) AS time#22, cast(split(cast(value#76 as string), ,)[1] as int) AS key#27, split(cast(value#76 as string), ,)[2] AS value#33] +- Scan ExistingRDD[key#75,value#76,topic#77,partition#78,offset#79L,timestamp#80,timestampType#81]","title":"Demo"},{"location":"operators/groupByKey/","text":"groupByKey Operator \u2014 Streaming Aggregation \u00b6 groupByKey ( func : T => K ) : KeyValueGroupedDataset [ K , T ] groupByKey operator aggregates rows by a typed grouping function for Arbitrary Stateful Streaming Aggregation . groupByKey creates a KeyValueGroupedDataset (with keys of type K and rows of type T ) to apply aggregation functions over groups of rows (of type T ) by key (of type K ) per the given func key-generating function. Note The type of the input argument of func is the type of rows in the Dataset (i.e. Dataset[T] ). groupByKey simply applies the func function to every row (of type T ) and associates it with a logical group per key (of type K ). func : T => K Internally, groupByKey creates a structured query with the AppendColumns unary logical operator (with the given func and the analyzed logical plan of the target Dataset that groupByKey was executed on) and creates a new QueryExecution . In the end, groupByKey creates a KeyValueGroupedDataset with the following: Encoders for K keys and T rows The new QueryExecution (with the AppendColumns unary logical operator) The output schema of the analyzed logical plan The new columns of the AppendColumns logical operator (i.e. the attributes of the key) scala> :type sq org.apache.spark.sql.Dataset[Long] val baseCode = 'A'.toInt val byUpperChar = (n: java.lang.Long) => (n % 3 + baseCode).toString val kvs = sq.groupByKey(byUpperChar) scala> :type kvs org.apache.spark.sql.KeyValueGroupedDataset[String,Long] // Peeking under the surface of KeyValueGroupedDataset import org.apache.spark.sql.catalyst.plans.logical.AppendColumns val appendColumnsOp = kvs.queryExecution.analyzed.collect { case ac: AppendColumns => ac }.head scala> println(appendColumnsOp.newColumns) List(value#7) Demo: Aggregating Orders Per Zip Code \u00b6 Go to Demo: groupByKey Streaming Aggregation in Update Mode . Demo: Aggregating Metrics Per Device \u00b6 The following example code shows how to apply groupByKey operator to a structured stream of timestamped values of different devices. // input stream import java.sql.Timestamp val signals = spark. readStream. format(\"rate\"). option(\"rowsPerSecond\", 1). load. withColumn(\"value\", $\"value\" % 10) // <-- randomize the values (just for fun) withColumn(\"deviceId\", lit(util.Random.nextInt(10))). // <-- 10 devices randomly assigned to values as[(Timestamp, Long, Int)] // <-- convert to a \"better\" type (from \"unpleasant\" Row) // stream processing using groupByKey operator // groupByKey(func: ((Timestamp, Long, Int)) => K): KeyValueGroupedDataset[K, (Timestamp, Long, Int)] // K becomes Int which is a device id val deviceId: ((Timestamp, Long, Int)) => Int = { case (_, _, deviceId) => deviceId } scala> val signalsByDevice = signals.groupByKey(deviceId) signalsByDevice: org.apache.spark.sql.KeyValueGroupedDataset[Int,(java.sql.Timestamp, Long, Int)] = org.apache.spark.sql.KeyValueGroupedDataset@19d40bc6","title":"groupByKey"},{"location":"operators/groupByKey/#groupbykey-operator-streaming-aggregation","text":"groupByKey ( func : T => K ) : KeyValueGroupedDataset [ K , T ] groupByKey operator aggregates rows by a typed grouping function for Arbitrary Stateful Streaming Aggregation . groupByKey creates a KeyValueGroupedDataset (with keys of type K and rows of type T ) to apply aggregation functions over groups of rows (of type T ) by key (of type K ) per the given func key-generating function. Note The type of the input argument of func is the type of rows in the Dataset (i.e. Dataset[T] ). groupByKey simply applies the func function to every row (of type T ) and associates it with a logical group per key (of type K ). func : T => K Internally, groupByKey creates a structured query with the AppendColumns unary logical operator (with the given func and the analyzed logical plan of the target Dataset that groupByKey was executed on) and creates a new QueryExecution . In the end, groupByKey creates a KeyValueGroupedDataset with the following: Encoders for K keys and T rows The new QueryExecution (with the AppendColumns unary logical operator) The output schema of the analyzed logical plan The new columns of the AppendColumns logical operator (i.e. the attributes of the key) scala> :type sq org.apache.spark.sql.Dataset[Long] val baseCode = 'A'.toInt val byUpperChar = (n: java.lang.Long) => (n % 3 + baseCode).toString val kvs = sq.groupByKey(byUpperChar) scala> :type kvs org.apache.spark.sql.KeyValueGroupedDataset[String,Long] // Peeking under the surface of KeyValueGroupedDataset import org.apache.spark.sql.catalyst.plans.logical.AppendColumns val appendColumnsOp = kvs.queryExecution.analyzed.collect { case ac: AppendColumns => ac }.head scala> println(appendColumnsOp.newColumns) List(value#7)","title":"groupByKey Operator &mdash; Streaming Aggregation"},{"location":"operators/groupByKey/#demo-aggregating-orders-per-zip-code","text":"Go to Demo: groupByKey Streaming Aggregation in Update Mode .","title":"Demo: Aggregating Orders Per Zip Code"},{"location":"operators/groupByKey/#demo-aggregating-metrics-per-device","text":"The following example code shows how to apply groupByKey operator to a structured stream of timestamped values of different devices. // input stream import java.sql.Timestamp val signals = spark. readStream. format(\"rate\"). option(\"rowsPerSecond\", 1). load. withColumn(\"value\", $\"value\" % 10) // <-- randomize the values (just for fun) withColumn(\"deviceId\", lit(util.Random.nextInt(10))). // <-- 10 devices randomly assigned to values as[(Timestamp, Long, Int)] // <-- convert to a \"better\" type (from \"unpleasant\" Row) // stream processing using groupByKey operator // groupByKey(func: ((Timestamp, Long, Int)) => K): KeyValueGroupedDataset[K, (Timestamp, Long, Int)] // K becomes Int which is a device id val deviceId: ((Timestamp, Long, Int)) => Int = { case (_, _, deviceId) => deviceId } scala> val signalsByDevice = signals.groupByKey(deviceId) signalsByDevice: org.apache.spark.sql.KeyValueGroupedDataset[Int,(java.sql.Timestamp, Long, Int)] = org.apache.spark.sql.KeyValueGroupedDataset@19d40bc6","title":"Demo: Aggregating Metrics Per Device"},{"location":"operators/join/","text":"join Operator \u2014 Streaming Join \u00b6 join ( right : Dataset [ _ ]) : DataFrame join ( right : Dataset [ _ ], joinExprs : Column ) : DataFrame join ( right : Dataset [ _ ], joinExprs : Column , joinType : String ) : DataFrame join ( right : Dataset [ _ ], usingColumns : Seq [ String ]) : DataFrame join ( right : Dataset [ _ ], usingColumns : Seq [ String ], joinType : String ) : DataFrame join ( right : Dataset [ _ ], usingColumn : String ) : DataFrame Streaming Join","title":"join"},{"location":"operators/join/#join-operator-streaming-join","text":"join ( right : Dataset [ _ ]) : DataFrame join ( right : Dataset [ _ ], joinExprs : Column ) : DataFrame join ( right : Dataset [ _ ], joinExprs : Column , joinType : String ) : DataFrame join ( right : Dataset [ _ ], usingColumns : Seq [ String ]) : DataFrame join ( right : Dataset [ _ ], usingColumns : Seq [ String ], joinType : String ) : DataFrame join ( right : Dataset [ _ ], usingColumn : String ) : DataFrame Streaming Join","title":"join Operator &mdash; Streaming Join"},{"location":"operators/joinWith/","text":"joinWith Operator \u2014 Streaming Join \u00b6 joinWith [ U ]( other : Dataset [ U ], condition : Column ) : Dataset [( T , U )] joinWith [ U ]( other : Dataset [ U ], condition : Column , joinType : String ) : Dataset [( T , U )] Streaming Join","title":"joinWith"},{"location":"operators/joinWith/#joinwith-operator-streaming-join","text":"joinWith [ U ]( other : Dataset [ U ], condition : Column ) : Dataset [( T , U )] joinWith [ U ]( other : Dataset [ U ], condition : Column , joinType : String ) : Dataset [( T , U )] Streaming Join","title":"joinWith Operator &mdash; Streaming Join"},{"location":"operators/withWatermark/","text":"withWatermark Operator \u2014 Event-Time Watermark \u00b6 withWatermark ( eventTime : String , delayThreshold : String ) : Dataset [ T ] withWatermark specifies a streaming watermark (on the given eventTime column with a delay threshold). withWatermark specifies the eventTime column for event time watermark and delayThreshold for event lateness . eventTime specifies the column to use for watermark and can be either part of Dataset from the source or custom-generated using current_time or current_timestamp functions. Note Watermark tracks a point in time before which it is assumed no more late events are supposed to arrive (and if they have, the late events are considered really late and simply dropped). Note Spark Structured Streaming uses watermark for the following: To know when a given time window aggregation (using groupBy operator with window standard function) can be finalized and thus emitted when using output modes that do not allow updates, like Append output mode. To minimize the amount of state that we need to keep for ongoing aggregations, e.g. mapGroupsWithState (for implicit state management), flatMapGroupsWithState (for user-defined state management) and dropDuplicates operators. The current watermark is computed by looking at the maximum eventTime seen across all of the partitions in a query minus a user-specified delayThreshold . Due to the cost of coordinating this value across partitions, the actual watermark used is only guaranteed to be at least delayThreshold behind the actual event time. Note In some cases Spark may still process records that arrive more than delayThreshold late.","title":"withWatermark"},{"location":"operators/withWatermark/#withwatermark-operator-event-time-watermark","text":"withWatermark ( eventTime : String , delayThreshold : String ) : Dataset [ T ] withWatermark specifies a streaming watermark (on the given eventTime column with a delay threshold). withWatermark specifies the eventTime column for event time watermark and delayThreshold for event lateness . eventTime specifies the column to use for watermark and can be either part of Dataset from the source or custom-generated using current_time or current_timestamp functions. Note Watermark tracks a point in time before which it is assumed no more late events are supposed to arrive (and if they have, the late events are considered really late and simply dropped). Note Spark Structured Streaming uses watermark for the following: To know when a given time window aggregation (using groupBy operator with window standard function) can be finalized and thus emitted when using output modes that do not allow updates, like Append output mode. To minimize the amount of state that we need to keep for ongoing aggregations, e.g. mapGroupsWithState (for implicit state management), flatMapGroupsWithState (for user-defined state management) and dropDuplicates operators. The current watermark is computed by looking at the maximum eventTime seen across all of the partitions in a query minus a user-specified delayThreshold . Due to the cost of coordinating this value across partitions, the actual watermark used is only guaranteed to be at least delayThreshold behind the actual event time. Note In some cases Spark may still process records that arrive more than delayThreshold late.","title":"withWatermark Operator &mdash; Event-Time Watermark"},{"location":"operators/writeStream/","text":"writeStream Operator \u00b6 writeStream : DataStreamWriter [ T ] writeStream creates a DataStreamWriter for persisting the result of a streaming query to an external data system","title":"writeStream"},{"location":"operators/writeStream/#writestream-operator","text":"writeStream : DataStreamWriter [ T ] writeStream creates a DataStreamWriter for persisting the result of a streaming query to an external data system","title":"writeStream Operator"},{"location":"physical-operators/EventTimeWatermarkExec/","text":"EventTimeWatermarkExec Unary Physical Operator \u00b6 EventTimeWatermarkExec is a unary physical operator that represents < > logical operator at execution time. [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 The < > of the EventTimeWatermarkExec operator is to simply extract ( project ) the values of the < > and add them directly to the < > internal accumulator. [NOTE] \u00b6 Since the execution (data processing) happens on Spark executors, the only way to establish communication between the tasks (on the executors) and the driver is to use an accumulator. Read up on https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-accumulators.html[Accumulators ] in https://bit.ly/apache-spark-internals[The Internals of Apache Spark] book. \u00b6 EventTimeWatermarkExec uses < > internal accumulator as a way to send the statistics (the maximum, minimum, average and update count) of the values in the < > that is later used in: ProgressReporter for creating execution statistics for the most recent query execution (for monitoring the max , min , avg , and watermark event-time watermark statistics) StreamExecution to observe and possibly update event-time watermark when < >. EventTimeWatermarkExec is < > exclusively when < > execution planning strategy is requested to plan a logical plan with < > logical operators for execution. TIP: Check out < > to deep dive into the internals of < >. === [[creating-instance]] Creating EventTimeWatermarkExec Instance EventTimeWatermarkExec takes the following to be created: [[eventTime]] Event time column - the column with the (event) time for event-time watermark [[delay]] Delay interval ( CalendarInterval ) [[child]] Child physical operator ( SparkPlan ) While < >, EventTimeWatermarkExec registers the < > internal accumulator (with the current SparkContext ). === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method [source, scala] \u00b6 doExecute(): RDD[InternalRow] \u00b6 NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute executes the < > physical operator and maps over the partitions (using RDD.mapPartitions ). doExecute creates an unsafe projection (one per partition) for the < > in the output schema of the < > physical operator. The unsafe projection is to extract event times from the (stream of) internal rows of the child physical operator. For every row ( InternalRow ) per partition, doExecute requests the < > accumulator to < >. NOTE: The event time value is in seconds (not millis as the value is divided by 1000 ). === [[output]] Output Attributes (Schema) -- output Property [source, scala] \u00b6 output: Seq[Attribute] \u00b6 NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output requests the < > physical operator for the output attributes to find the < > and any other column with metadata that contains < > key. For the < >, output updates the metadata to include the < > for the < > key. For any other column (not the < >) with the < > key, output simply removes the key from the metadata. [source, scala] \u00b6 // FIXME: Would be nice to have a demo. Anyone? \u00b6 === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | delayMs a| [[delayMs]] Delay interval - the < > interval in milliseconds Used when: EventTimeWatermarkExec is requested for the < > WatermarkTracker is requested to < > | eventTimeStats a| [[eventTimeStats]] < > accumulator to accumulate < > values from every row in a streaming batch (when EventTimeWatermarkExec < >). NOTE: EventTimeStatsAccum is a Spark accumulator of EventTimeStats from Longs (i.e. AccumulatorV2[Long, EventTimeStats] ). NOTE: Every Spark accumulator has to be registered before use, and eventTimeStats is registered when EventTimeWatermarkExec < >. |===","title":"EventTimeWatermarkExec"},{"location":"physical-operators/EventTimeWatermarkExec/#eventtimewatermarkexec-unary-physical-operator","text":"EventTimeWatermarkExec is a unary physical operator that represents < > logical operator at execution time.","title":"EventTimeWatermarkExec Unary Physical Operator"},{"location":"physical-operators/EventTimeWatermarkExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"physical-operators/EventTimeWatermarkExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"The < > of the EventTimeWatermarkExec operator is to simply extract ( project ) the values of the < > and add them directly to the < > internal accumulator.","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/EventTimeWatermarkExec/#note_1","text":"Since the execution (data processing) happens on Spark executors, the only way to establish communication between the tasks (on the executors) and the driver is to use an accumulator.","title":"[NOTE]"},{"location":"physical-operators/EventTimeWatermarkExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-apache-sparkspark-accumulatorshtmlaccumulators-in-httpsbitlyapache-spark-internalsthe-internals-of-apache-spark-book","text":"EventTimeWatermarkExec uses < > internal accumulator as a way to send the statistics (the maximum, minimum, average and update count) of the values in the < > that is later used in: ProgressReporter for creating execution statistics for the most recent query execution (for monitoring the max , min , avg , and watermark event-time watermark statistics) StreamExecution to observe and possibly update event-time watermark when < >. EventTimeWatermarkExec is < > exclusively when < > execution planning strategy is requested to plan a logical plan with < > logical operators for execution. TIP: Check out < > to deep dive into the internals of < >. === [[creating-instance]] Creating EventTimeWatermarkExec Instance EventTimeWatermarkExec takes the following to be created: [[eventTime]] Event time column - the column with the (event) time for event-time watermark [[delay]] Delay interval ( CalendarInterval ) [[child]] Child physical operator ( SparkPlan ) While < >, EventTimeWatermarkExec registers the < > internal accumulator (with the current SparkContext ). === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-accumulators.html[Accumulators] in https://bit.ly/apache-spark-internals[The Internals of Apache Spark] book."},{"location":"physical-operators/EventTimeWatermarkExec/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/EventTimeWatermarkExec/#doexecute-rddinternalrow","text":"NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute executes the < > physical operator and maps over the partitions (using RDD.mapPartitions ). doExecute creates an unsafe projection (one per partition) for the < > in the output schema of the < > physical operator. The unsafe projection is to extract event times from the (stream of) internal rows of the child physical operator. For every row ( InternalRow ) per partition, doExecute requests the < > accumulator to < >. NOTE: The event time value is in seconds (not millis as the value is divided by 1000 ). === [[output]] Output Attributes (Schema) -- output Property","title":"doExecute(): RDD[InternalRow]"},{"location":"physical-operators/EventTimeWatermarkExec/#source-scala_1","text":"","title":"[source, scala]"},{"location":"physical-operators/EventTimeWatermarkExec/#output-seqattribute","text":"NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output requests the < > physical operator for the output attributes to find the < > and any other column with metadata that contains < > key. For the < >, output updates the metadata to include the < > for the < > key. For any other column (not the < >) with the < > key, output simply removes the key from the metadata.","title":"output: Seq[Attribute]"},{"location":"physical-operators/EventTimeWatermarkExec/#source-scala_2","text":"","title":"[source, scala]"},{"location":"physical-operators/EventTimeWatermarkExec/#fixme-would-be-nice-to-have-a-demo-anyone","text":"=== [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | delayMs a| [[delayMs]] Delay interval - the < > interval in milliseconds Used when: EventTimeWatermarkExec is requested for the < > WatermarkTracker is requested to < > | eventTimeStats a| [[eventTimeStats]] < > accumulator to accumulate < > values from every row in a streaming batch (when EventTimeWatermarkExec < >). NOTE: EventTimeStatsAccum is a Spark accumulator of EventTimeStats from Longs (i.e. AccumulatorV2[Long, EventTimeStats] ). NOTE: Every Spark accumulator has to be registered before use, and eventTimeStats is registered when EventTimeWatermarkExec < >. |===","title":"// FIXME: Would be nice to have a demo. Anyone?"},{"location":"physical-operators/FlatMapGroupsWithStateExec/","text":"FlatMapGroupsWithStateExec Unary Physical Operator \u00b6 FlatMapGroupsWithStateExec is a unary physical operator that represents FlatMapGroupsWithState logical operator at execution time. Note A unary physical operator ( UnaryExecNode ) is a physical operator with a single child physical operator. Read up on UnaryExecNode (and physical operators in general) in The Internals of Spark SQL online book. FlatMapGroupsWithStateExec is an ObjectProducerExec physical operator and so produces a single output object . Tip Read up on ObjectProducerExec physical operator in The Internals of Spark SQL online book. Tip Check out Demo: Internals of FlatMapGroupsWithStateExec Physical Operator . Note FlatMapGroupsWithStateExec is given an OutputMode when created, but it does not seem to be used at all. Check out the question What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used? on StackOverflow. Creating Instance \u00b6 FlatMapGroupsWithStateExec takes the following to be created: User-defined state function that is applied to every group (of type (Any, Iterator[Any], LogicalGroupState[Any]) => Iterator[Any] ) Deserializer expression for keys Deserializer expression for values Grouping attributes (as used for grouping in KeyValueGroupedDataset for mapGroupsWithState or flatMapGroupsWithState operators) Data attributes Output object attribute (that is the reference to the single object field this operator outputs) Optional StatefulOperatorStateInfo State encoder ( ExpressionEncoder[Any] ) State format version OutputMode GroupStateTimeout Optional Batch Processing Time Optional Event-Time Watermark Child physical operator FlatMapGroupsWithStateExec is created when FlatMapGroupsWithStateStrategy execution planning strategy is executed (and plans a FlatMapGroupsWithState logical operator for execution). Executing Physical Operator \u00b6 doExecute () : RDD [ InternalRow ] doExecute first initializes the metrics (which happens on the driver). doExecute then requests the child physical operator to execute (and generate an RDD[InternalRow] ). doExecute uses StateStoreOps to create a StateStoreRDD with a storeUpdateFunction that does the following (for a partition): Creates an InputProcessor for a given StateStore (only when the GroupStateTimeout is EventTimeTimeout ) Filters out late data based on the event-time watermark , i.e. rows from a given Iterator[InternalRow] that are older than the event-time watermark are excluded from the steps that follow Requests the InputProcessor to create an iterator of a new data processed from the (possibly filtered) iterator Requests the InputProcessor to create an iterator of a timed-out state data Creates an iterator by concatenating the above iterators (with the new data processed first) In the end, creates a CompletionIterator that executes a completion function ( completionFunction ) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). The completion method requests the given StateStore to commit changes followed by setting the store-specific metrics doExecute is part of Spark SQL's SparkPlan abstraction. Performance Metrics \u00b6 FlatMapGroupsWithStateExec uses the performance metrics of StateStoreWriter . StateStoreWriter \u00b6 FlatMapGroupsWithStateExec is a stateful physical operator that can write to a state store (and MicroBatchExecution requests whether to run another batch or not based on the GroupStateTimeout ). FlatMapGroupsWithStateExec uses the GroupStateTimeout (and possibly the updated metadata ) when asked whether to run another batch or not (when MicroBatchExecution is requested to construct the next streaming micro-batch when requested to run the activated streaming query ). Streaming Event-Time Watermark Support \u00b6 FlatMapGroupsWithStateExec is a physical operator that supports streaming event-time watermark . FlatMapGroupsWithStateExec is given the optional event time watermark when created. The event-time watermark is initially undefined ( None ) when planned for execution (in FlatMapGroupsWithStateStrategy execution planning strategy). Note FlatMapGroupsWithStateStrategy converts FlatMapGroupsWithState unary logical operator to FlatMapGroupsWithStateExec physical operator with undefined StatefulOperatorStateInfo , batchTimestampMs , and eventTimeWatermark . The event-time watermark (with the StatefulOperatorStateInfo and the batchTimestampMs ) is only defined to the current event-time watermark of the given OffsetSeqMetadata when IncrementalExecution query execution pipeline is requested to apply the state preparation rule (as part of the preparations rules). Note The preparations rules are executed (applied to a physical query plan) at the executedPlan phase of Structured Query Execution Pipeline to generate an optimized physical query plan ready for execution). Read up on Structured Query Execution Pipeline in The Internals of Spark SQL online book. IncrementalExecution is used as the lastExecution of the available streaming query execution engines . It is created in the queryPlanning phase (of the MicroBatchExecution and ContinuousExecution execution engines) based on the current OffsetSeqMetadata . Note The optional event-time watermark can only be defined when the state preparation rule is executed which is at the executedPlan phase of Structured Query Execution Pipeline which is also part of the queryPlanning phase. StateManager \u00b6 stateManager : StateManager While being created, FlatMapGroupsWithStateExec creates a StateManager (with the state encoder and the isTimeoutEnabled flag). A StateManager is created per state format version that is given while creating a FlatMapGroupsWithStateExec (to choose between the available implementations ). The state format version is controlled by spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion internal configuration property. The StateManager is used exclusively when FlatMapGroupsWithStateExec physical operator is executed for the following: State schema (for the value schema of a StateStoreRDD ) State data for a key in a StateStore while processing new data All state data (for all keys) in a StateStore while processing timed-out state data Removing the state for a key from a StateStore when all rows have been processed Persisting the state for a key in a StateStore when all rows have been processed keyExpressions Method \u00b6 keyExpressions : Seq [ Attribute ] keyExpressions simply returns the grouping attributes . keyExpressions is part of the WatermarkSupport abstraction. Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not \u00b6 shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ) : Boolean shouldRunAnotherBatch uses the GroupStateTimeout as follows: With EventTimeTimeout , shouldRunAnotherBatch is true only when the event-time watermark is defined and is older (below) the event-time watermark of the given OffsetSeqMetadata With NoTimeout (and other GroupStateTimeouts if there were any), shouldRunAnotherBatch is always false With ProcessingTimeTimeout , shouldRunAnotherBatch is always true shouldRunAnotherBatch is part of the StateStoreWriter abstraction. Internal Properties \u00b6 isTimeoutEnabled Flag \u00b6 Flag that says whether the GroupStateTimeout is not NoTimeout Used when: FlatMapGroupsWithStateExec is created (and creates the internal StateManager ) InputProcessor is requested to processTimedOutState watermarkPresent Flag \u00b6 Flag that says whether the child physical operator has a watermark attribute (among the output attributes). Used when InputProcessor is requested to callFunctionAndUpdateState Logging \u00b6 Enable ALL logging level for org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec=ALL Refer to Logging .","title":"FlatMapGroupsWithStateExec"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#flatmapgroupswithstateexec-unary-physical-operator","text":"FlatMapGroupsWithStateExec is a unary physical operator that represents FlatMapGroupsWithState logical operator at execution time. Note A unary physical operator ( UnaryExecNode ) is a physical operator with a single child physical operator. Read up on UnaryExecNode (and physical operators in general) in The Internals of Spark SQL online book. FlatMapGroupsWithStateExec is an ObjectProducerExec physical operator and so produces a single output object . Tip Read up on ObjectProducerExec physical operator in The Internals of Spark SQL online book. Tip Check out Demo: Internals of FlatMapGroupsWithStateExec Physical Operator . Note FlatMapGroupsWithStateExec is given an OutputMode when created, but it does not seem to be used at all. Check out the question What's the purpose of OutputMode in flatMapGroupsWithState? How/where is it used? on StackOverflow.","title":"FlatMapGroupsWithStateExec Unary Physical Operator"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#creating-instance","text":"FlatMapGroupsWithStateExec takes the following to be created: User-defined state function that is applied to every group (of type (Any, Iterator[Any], LogicalGroupState[Any]) => Iterator[Any] ) Deserializer expression for keys Deserializer expression for values Grouping attributes (as used for grouping in KeyValueGroupedDataset for mapGroupsWithState or flatMapGroupsWithState operators) Data attributes Output object attribute (that is the reference to the single object field this operator outputs) Optional StatefulOperatorStateInfo State encoder ( ExpressionEncoder[Any] ) State format version OutputMode GroupStateTimeout Optional Batch Processing Time Optional Event-Time Watermark Child physical operator FlatMapGroupsWithStateExec is created when FlatMapGroupsWithStateStrategy execution planning strategy is executed (and plans a FlatMapGroupsWithState logical operator for execution).","title":"Creating Instance"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#executing-physical-operator","text":"doExecute () : RDD [ InternalRow ] doExecute first initializes the metrics (which happens on the driver). doExecute then requests the child physical operator to execute (and generate an RDD[InternalRow] ). doExecute uses StateStoreOps to create a StateStoreRDD with a storeUpdateFunction that does the following (for a partition): Creates an InputProcessor for a given StateStore (only when the GroupStateTimeout is EventTimeTimeout ) Filters out late data based on the event-time watermark , i.e. rows from a given Iterator[InternalRow] that are older than the event-time watermark are excluded from the steps that follow Requests the InputProcessor to create an iterator of a new data processed from the (possibly filtered) iterator Requests the InputProcessor to create an iterator of a timed-out state data Creates an iterator by concatenating the above iterators (with the new data processed first) In the end, creates a CompletionIterator that executes a completion function ( completionFunction ) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). The completion method requests the given StateStore to commit changes followed by setting the store-specific metrics doExecute is part of Spark SQL's SparkPlan abstraction.","title":" Executing Physical Operator"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#performance-metrics","text":"FlatMapGroupsWithStateExec uses the performance metrics of StateStoreWriter .","title":" Performance Metrics"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#statestorewriter","text":"FlatMapGroupsWithStateExec is a stateful physical operator that can write to a state store (and MicroBatchExecution requests whether to run another batch or not based on the GroupStateTimeout ). FlatMapGroupsWithStateExec uses the GroupStateTimeout (and possibly the updated metadata ) when asked whether to run another batch or not (when MicroBatchExecution is requested to construct the next streaming micro-batch when requested to run the activated streaming query ).","title":" StateStoreWriter"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#streaming-event-time-watermark-support","text":"FlatMapGroupsWithStateExec is a physical operator that supports streaming event-time watermark . FlatMapGroupsWithStateExec is given the optional event time watermark when created. The event-time watermark is initially undefined ( None ) when planned for execution (in FlatMapGroupsWithStateStrategy execution planning strategy). Note FlatMapGroupsWithStateStrategy converts FlatMapGroupsWithState unary logical operator to FlatMapGroupsWithStateExec physical operator with undefined StatefulOperatorStateInfo , batchTimestampMs , and eventTimeWatermark . The event-time watermark (with the StatefulOperatorStateInfo and the batchTimestampMs ) is only defined to the current event-time watermark of the given OffsetSeqMetadata when IncrementalExecution query execution pipeline is requested to apply the state preparation rule (as part of the preparations rules). Note The preparations rules are executed (applied to a physical query plan) at the executedPlan phase of Structured Query Execution Pipeline to generate an optimized physical query plan ready for execution). Read up on Structured Query Execution Pipeline in The Internals of Spark SQL online book. IncrementalExecution is used as the lastExecution of the available streaming query execution engines . It is created in the queryPlanning phase (of the MicroBatchExecution and ContinuousExecution execution engines) based on the current OffsetSeqMetadata . Note The optional event-time watermark can only be defined when the state preparation rule is executed which is at the executedPlan phase of Structured Query Execution Pipeline which is also part of the queryPlanning phase.","title":" Streaming Event-Time Watermark Support"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#statemanager","text":"stateManager : StateManager While being created, FlatMapGroupsWithStateExec creates a StateManager (with the state encoder and the isTimeoutEnabled flag). A StateManager is created per state format version that is given while creating a FlatMapGroupsWithStateExec (to choose between the available implementations ). The state format version is controlled by spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion internal configuration property. The StateManager is used exclusively when FlatMapGroupsWithStateExec physical operator is executed for the following: State schema (for the value schema of a StateStoreRDD ) State data for a key in a StateStore while processing new data All state data (for all keys) in a StateStore while processing timed-out state data Removing the state for a key from a StateStore when all rows have been processed Persisting the state for a key in a StateStore when all rows have been processed","title":" StateManager"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#keyexpressions-method","text":"keyExpressions : Seq [ Attribute ] keyExpressions simply returns the grouping attributes . keyExpressions is part of the WatermarkSupport abstraction.","title":" keyExpressions Method"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#checking-out-whether-last-batch-execution-requires-another-non-data-batch-or-not","text":"shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ) : Boolean shouldRunAnotherBatch uses the GroupStateTimeout as follows: With EventTimeTimeout , shouldRunAnotherBatch is true only when the event-time watermark is defined and is older (below) the event-time watermark of the given OffsetSeqMetadata With NoTimeout (and other GroupStateTimeouts if there were any), shouldRunAnotherBatch is always false With ProcessingTimeTimeout , shouldRunAnotherBatch is always true shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":" Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#internal-properties","text":"","title":"Internal Properties"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#istimeoutenabled-flag","text":"Flag that says whether the GroupStateTimeout is not NoTimeout Used when: FlatMapGroupsWithStateExec is created (and creates the internal StateManager ) InputProcessor is requested to processTimedOutState","title":" isTimeoutEnabled Flag"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#watermarkpresent-flag","text":"Flag that says whether the child physical operator has a watermark attribute (among the output attributes). Used when InputProcessor is requested to callFunctionAndUpdateState","title":" watermarkPresent Flag"},{"location":"physical-operators/FlatMapGroupsWithStateExec/#logging","text":"Enable ALL logging level for org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec logger to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec=ALL Refer to Logging .","title":"Logging"},{"location":"physical-operators/StateStoreReader/","text":"== [[StateStoreReader]] StateStoreReader StateStoreReader is...FIXME","title":"StateStoreReader"},{"location":"physical-operators/StateStoreRestoreExec/","text":"== [[StateStoreRestoreExec]] StateStoreRestoreExec Unary Physical Operator -- Restoring Streaming State From State Store StateStoreRestoreExec is a unary physical operator that < > (for the keys from the < > physical operator). [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 StateStoreRestoreExec is < > exclusively when < > execution planning strategy is requested to plan a < > for execution ( Aggregate logical operators in the logical plan of a streaming query). .StateStoreRestoreExec and StatefulAggregationStrategy image::images/StateStoreRestoreExec-StatefulAggregationStrategy.png[align=\"center\"] The optional < > is initially undefined (i.e. when StateStoreRestoreExec is < >). StateStoreRestoreExec is updated to hold the streaming batch-specific execution property when IncrementalExecution spark-sql-streaming-IncrementalExecution.md#preparations[prepares a streaming physical plan for execution] (and spark-sql-streaming-IncrementalExecution.md#state[state] preparation rule is executed when StreamExecution MicroBatchExecution.md#runBatch-queryPlanning[plans a streaming query] for a streaming batch). .StateStoreRestoreExec and IncrementalExecution image::images/StateStoreRestoreExec-IncrementalExecution.png[align=\"center\"] When < >, StateStoreRestoreExec executes the < > physical operator and < > with storeUpdateFunction that restores the state for the keys in the input rows if available. [[output]] The output schema of StateStoreRestoreExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StateStoreRestoreExec is exactly the < >'s output partitioning. === [[metrics]] Performance Metrics (SQLMetrics) [cols=\"1m,1,3\",options=\"header\",width=\"100%\"] |=== | Key | Name (in UI) | Description | numOutputRows | number of output rows | [[numOutputRows]] The number of input rows from the < > physical operator (for which StateStoreRestoreExec tried to find the state) |=== .StateStoreRestoreExec in web UI (Details for Query) image::images/StateStoreRestoreExec-webui-query-details.png[align=\"center\"] === [[creating-instance]] Creating StateStoreRestoreExec Instance StateStoreRestoreExec takes the following to be created: [[keyExpressions]] Key expressions , i.e. Catalyst attributes for the grouping keys [[stateInfo]] Optional < > (default: None ) [[stateFormatVersion]] Version of the state format (based on the < > configuration property) [[child]] Child physical operator ( SparkPlan ) === [[stateManager]] StateStoreRestoreExec and StreamingAggregationStateManager -- stateManager Property [source, scala] \u00b6 stateManager: StreamingAggregationStateManager \u00b6 stateManager is a < > that is created together with StateStoreRestoreExec . The StreamingAggregationStateManager is created for the < >, the output schema of the < > physical operator and the < >. The StreamingAggregationStateManager is used when StateStoreRestoreExec is requested to < > for the following: < > < > < > === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method [source, scala] \u00b6 doExecute(): RDD[InternalRow] \u00b6 NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute executes < > physical operator and spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD] with storeUpdateFunction that does the following per < > operator's RDD partition: Generates an unsafe projection to access the key field (using < > and the output schema of < > operator). For every input row (as InternalRow ) Extracts the key from the row (using the unsafe projection above) spark-sql-streaming-StateStore.md#get[Gets the saved state] in StateStore for the key if available (it might not be if the key appeared in the input the first time) Increments < > metric (that in the end is the number of rows from the < > operator) Generates collection made up of the current row and possibly the state for the key if available NOTE: The number of rows from StateStoreRestoreExec is the number of rows from the < > operator with additional rows for the saved state. NOTE: There is no way in StateStoreRestoreExec to find out how many rows had associated state available in a state store. You would have to use the corresponding StateStoreSaveExec operator's StateStoreSaveExec.md#metrics[metrics] (most likely number of total state rows but that could depend on the output mode).","title":"StateStoreRestoreExec"},{"location":"physical-operators/StateStoreRestoreExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"physical-operators/StateStoreRestoreExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"StateStoreRestoreExec is < > exclusively when < > execution planning strategy is requested to plan a < > for execution ( Aggregate logical operators in the logical plan of a streaming query). .StateStoreRestoreExec and StatefulAggregationStrategy image::images/StateStoreRestoreExec-StatefulAggregationStrategy.png[align=\"center\"] The optional < > is initially undefined (i.e. when StateStoreRestoreExec is < >). StateStoreRestoreExec is updated to hold the streaming batch-specific execution property when IncrementalExecution spark-sql-streaming-IncrementalExecution.md#preparations[prepares a streaming physical plan for execution] (and spark-sql-streaming-IncrementalExecution.md#state[state] preparation rule is executed when StreamExecution MicroBatchExecution.md#runBatch-queryPlanning[plans a streaming query] for a streaming batch). .StateStoreRestoreExec and IncrementalExecution image::images/StateStoreRestoreExec-IncrementalExecution.png[align=\"center\"] When < >, StateStoreRestoreExec executes the < > physical operator and < > with storeUpdateFunction that restores the state for the keys in the input rows if available. [[output]] The output schema of StateStoreRestoreExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StateStoreRestoreExec is exactly the < >'s output partitioning. === [[metrics]] Performance Metrics (SQLMetrics) [cols=\"1m,1,3\",options=\"header\",width=\"100%\"] |=== | Key | Name (in UI) | Description | numOutputRows | number of output rows | [[numOutputRows]] The number of input rows from the < > physical operator (for which StateStoreRestoreExec tried to find the state) |=== .StateStoreRestoreExec in web UI (Details for Query) image::images/StateStoreRestoreExec-webui-query-details.png[align=\"center\"] === [[creating-instance]] Creating StateStoreRestoreExec Instance StateStoreRestoreExec takes the following to be created: [[keyExpressions]] Key expressions , i.e. Catalyst attributes for the grouping keys [[stateInfo]] Optional < > (default: None ) [[stateFormatVersion]] Version of the state format (based on the < > configuration property) [[child]] Child physical operator ( SparkPlan ) === [[stateManager]] StateStoreRestoreExec and StreamingAggregationStateManager -- stateManager Property","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/StateStoreRestoreExec/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/StateStoreRestoreExec/#statemanager-streamingaggregationstatemanager","text":"stateManager is a < > that is created together with StateStoreRestoreExec . The StreamingAggregationStateManager is created for the < >, the output schema of the < > physical operator and the < >. The StreamingAggregationStateManager is used when StateStoreRestoreExec is requested to < > for the following: < > < > < > === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method","title":"stateManager: StreamingAggregationStateManager"},{"location":"physical-operators/StateStoreRestoreExec/#source-scala_1","text":"","title":"[source, scala]"},{"location":"physical-operators/StateStoreRestoreExec/#doexecute-rddinternalrow","text":"NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute executes < > physical operator and spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD] with storeUpdateFunction that does the following per < > operator's RDD partition: Generates an unsafe projection to access the key field (using < > and the output schema of < > operator). For every input row (as InternalRow ) Extracts the key from the row (using the unsafe projection above) spark-sql-streaming-StateStore.md#get[Gets the saved state] in StateStore for the key if available (it might not be if the key appeared in the input the first time) Increments < > metric (that in the end is the number of rows from the < > operator) Generates collection made up of the current row and possibly the state for the key if available NOTE: The number of rows from StateStoreRestoreExec is the number of rows from the < > operator with additional rows for the saved state. NOTE: There is no way in StateStoreRestoreExec to find out how many rows had associated state available in a state store. You would have to use the corresponding StateStoreSaveExec operator's StateStoreSaveExec.md#metrics[metrics] (most likely number of total state rows but that could depend on the output mode).","title":"doExecute(): RDD[InternalRow]"},{"location":"physical-operators/StateStoreSaveExec/","text":"StateStoreSaveExec Unary Physical Operator \u00b6 StateStoreSaveExec is a unary physical operator that saves a streaming state to a state store with support for streaming watermark . [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 StateStoreSaveExec is < > exclusively when < > execution planning strategy is requested to plan a < > for execution ( Aggregate logical operators in the logical plan of a streaming query). The optional properties, i.e. the < >, the < >, and the < >, are initially undefined when StateStoreSaveExec is < >. StateStoreSaveExec is updated to hold execution-specific configuration when IncrementalExecution is requested to < > (when the < > is executed). NOTE: Unlike spark-sql-streaming-StateStoreRestoreExec.md[StateStoreRestoreExec] operator, StateStoreSaveExec takes < > and < > when < >. When < >, StateStoreSaveExec spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD to map over partitions] with storeUpdateFunction that manages the StateStore . [NOTE] \u00b6 The number of partitions of spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[StateStoreRDD] (and hence the number of Spark tasks) is what was defined for the < > physical plan. There will be that many StateStores as there are partitions in StateStoreRDD . \u00b6 NOTE: StateStoreSaveExec < > differently per output mode. When < >, StateStoreSaveExec executes the < > physical operator and spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD] (with storeUpdateFunction specific to the output mode). [[output]] The output schema of StateStoreSaveExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StateStoreSaveExec is exactly the < >'s output partitioning. [[stateManager]] StateStoreRestoreExec uses a < > (that is < > for the < >, the output of the < > physical operator and the < >). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.StateStoreSaveExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StateStoreSaveExec=ALL Refer to < >. \u00b6 === [[metrics]] Performance Metrics (SQLMetrics) StateStoreSaveExec uses the performance metrics as other stateful physical operators that write to a state store . The following table shows how the performance metrics are computed (and so their exact meaning). [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | total time to update rows a| [[allUpdatesTimeMs]] Time taken to read the input rows and < > (possibly filtering out expired rows per < > predicate) The number of rows stored is the < > metric For < > output mode, the time taken to filter out expired rows (per the required < > predicate) and the < > to < > For < > output mode, the time taken to go over all the input rows and request the < > to < > For < > output mode, the time taken to filter out expired rows (per the optional < > predicate) and the < > to < > | total time to remove rows a| [[allRemovalsTimeMs]] For < > output mode, the time taken for the < > to < > (per < > predicate) that is the total time of iterating over < > (the number of entries < > is the difference between the number of output rows of the < > and the < > metric) For < > output mode, always 0 For < > output mode, the time taken for the < > to < > (per < > predicate) | time to commit changes a| [[commitTimeMs]] Time taken for the < > to < > | number of output rows a| [[numOutputRows]] For < > output mode, the metric does not seem to be used For < > output mode, the number of rows in a < > (i.e. all < > in a < > in the < > that should be equivalent to the < > metric) For < > output mode, the number of rows that the < > was requested to < > (that did not expire per the optional < > predicate) that is equivalent to the < > metric) | number of total state rows a| [[numTotalStateRows]] Number of entries in a < > at the very end of < > (aka numTotalStateRows ) Corresponds to numRowsTotal attribute in stateOperators in StreamingQueryProgress (and is available as sq.lastProgress.stateOperators for an operator). | number of updated state rows a| [[numUpdatedStateRows]] Number of the entries that < > in a trigger and for the keys in the result rows of the upstream physical operator (aka numUpdatedStateRows ) For < > output mode, the number of input rows that have not expired yet (per the required < > predicate) and that the < > was requested to < > (the time taken is the < > metric) For < > output mode, the number of input rows (which should be exactly the number of output rows from the < >) For < > output mode, the number of rows that the < > was requested to < > (that did not expire per the optional < > predicate) that is equivalent to the < > metric) Corresponds to numRowsUpdated attribute in stateOperators in StreamingQueryProgress (and is available as sq.lastProgress.stateOperators for an operator). | memory used by state a| [[stateMemory]] Estimated memory used by a < > (aka stateMemory ) after StateStoreSaveExec finished < > (per the < > of the < >) |=== === [[creating-instance]] Creating StateStoreSaveExec Instance StateStoreSaveExec takes the following to be created: [[keyExpressions]] Key expressions , i.e. Catalyst attributes for the grouping keys [[stateInfo]] Execution-specific < > (default: None ) [[outputMode]] Execution-specific < > (default: None ) [[eventTimeWatermark]] < > (default: None ) [[stateFormatVersion]] Version of the state format (based on the < > configuration property) [[child]] Child physical operator ( SparkPlan ) === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method [source, scala] \u00b6 doExecute(): RDD[InternalRow] \u00b6 NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute initializes metrics . NOTE: doExecute requires that the optional < > is at this point defined (that should have happened when IncrementalExecution spark-sql-streaming-IncrementalExecution.md#preparations[had prepared a streaming aggregation for execution]). doExecute executes < > physical operator and spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD] with storeUpdateFunction that: Generates an unsafe projection to access the key field (using < > and the output schema of < >). Branches off per < >: < >, < > and < >. doExecute throws an UnsupportedOperationException when executed with an invalid < >: Invalid output mode: [outputMode] ==== [[doExecute-Append]] Append Output Mode NOTE: < > is the default output mode when not specified explicitly. NOTE: Append output mode requires that a streaming query defines < > (e.g. using withWatermark operator) on the event-time column that is used in aggregation (directly or using < > standard function). For < > output mode, doExecute does the following: Finds late (aggregate) rows from < > physical operator (that have expired per < >) < > and increments the < > metric < > Creates an iterator that < > when requested the next row and in the end < > TIP: Refer to < > for an example of StateStoreSaveExec with Append output mode. CAUTION: FIXME When is \"Filtering state store on:\" printed out? Uses spark-sql-streaming-WatermarkSupport.md#watermarkPredicateForData[watermarkPredicateForData] predicate to exclude matching rows and (like in < > output mode) spark-sql-streaming-StateStore.md#put[stores all the remaining rows] in StateStore . (like in < > output mode) While storing the rows, increments < > metric (for every row) and records the total time in < > metric. spark-sql-streaming-StateStore.md#getRange[Takes all the rows] from StateStore and returns a NextIterator that: In getNext , finds the first row that matches < > predicate, spark-sql-streaming-StateStore.md#remove[removes it] from StateStore , and returns it back. + If no row was found, getNext also marks the iterator as finished. In close , records the time to iterate over all the rows in < > metric, spark-sql-streaming-StateStore.md#commit[commits the updates] to StateStore followed by recording the time in < > metric and recording StateStore metrics . ==== [[doExecute-Complete]] Complete Output Mode For < > output mode, doExecute does the following: Takes all UnsafeRow rows (from the parent iterator) < > eagerly (i.e. all rows that are available in the parent iterator before proceeding) < > In the end, < > and passes the rows along (i.e. to the following physical operator) The number of keys stored in the state store is recorded in < > metric. NOTE: In Complete output mode the < > metric is exactly the < > metric. TIP: Refer to < > for an example of StateStoreSaveExec with Complete output mode. spark-sql-streaming-StateStore.md#put[Stores all rows] (as UnsafeRow ) in StateStore . While storing the rows, increments < > metric (for every row) and records the total time in < > metric. Records 0 in < > metric. spark-sql-streaming-StateStore.md#commit[Commits the state updates] to StateStore and records the time in < > metric. Records StateStore metrics In the end, takes all the rows stored in StateStore and increments numOutputRows metric. ==== [[doExecute-Update]] Update Output Mode For < > output mode, doExecute returns an iterator that filters out late aggregate rows (per < > if defined) and < > (one by one, i.e. every next ). With no more rows available, that < > (all at once) and < >. TIP: Refer to < > for an example of StateStoreSaveExec with Update output mode. doExecute returns Iterator of rows that uses < > predicate to filter out late rows. In hasNext , when rows are no longer available: Records the total time to iterate over all the rows in < > metric. spark-sql-streaming-WatermarkSupport.md#removeKeysOlderThanWatermark[removeKeysOlderThanWatermark] and records the time in < > metric. spark-sql-streaming-StateStore.md#commit[Commits the updates] to StateStore and records the time in < > metric. Records StateStore metrics In next , stores a row in StateStore and increments numOutputRows and numUpdatedStateRows metrics. === [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- shouldRunAnotherBatch Method shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ) : Boolean shouldRunAnotherBatch is positive ( true ) when all of the following are met: < > is either < > or < > < > is defined and is older (below) the current < > (of the given OffsetSeqMetadata ) Otherwise, shouldRunAnotherBatch is negative ( false ). shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":"StateStoreSaveExec"},{"location":"physical-operators/StateStoreSaveExec/#statestoresaveexec-unary-physical-operator","text":"StateStoreSaveExec is a unary physical operator that saves a streaming state to a state store with support for streaming watermark .","title":"StateStoreSaveExec Unary Physical Operator"},{"location":"physical-operators/StateStoreSaveExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"physical-operators/StateStoreSaveExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"StateStoreSaveExec is < > exclusively when < > execution planning strategy is requested to plan a < > for execution ( Aggregate logical operators in the logical plan of a streaming query). The optional properties, i.e. the < >, the < >, and the < >, are initially undefined when StateStoreSaveExec is < >. StateStoreSaveExec is updated to hold execution-specific configuration when IncrementalExecution is requested to < > (when the < > is executed). NOTE: Unlike spark-sql-streaming-StateStoreRestoreExec.md[StateStoreRestoreExec] operator, StateStoreSaveExec takes < > and < > when < >. When < >, StateStoreSaveExec spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD to map over partitions] with storeUpdateFunction that manages the StateStore .","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/StateStoreSaveExec/#note_1","text":"The number of partitions of spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[StateStoreRDD] (and hence the number of Spark tasks) is what was defined for the < > physical plan.","title":"[NOTE]"},{"location":"physical-operators/StateStoreSaveExec/#there-will-be-that-many-statestores-as-there-are-partitions-in-statestorerdd","text":"NOTE: StateStoreSaveExec < > differently per output mode. When < >, StateStoreSaveExec executes the < > physical operator and spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD] (with storeUpdateFunction specific to the output mode). [[output]] The output schema of StateStoreSaveExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StateStoreSaveExec is exactly the < >'s output partitioning. [[stateManager]] StateStoreRestoreExec uses a < > (that is < > for the < >, the output of the < > physical operator and the < >). [[logging]] [TIP] ==== Enable ALL logging level for org.apache.spark.sql.execution.streaming.StateStoreSaveExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StateStoreSaveExec=ALL","title":"There will be that many StateStores as there are partitions in StateStoreRDD."},{"location":"physical-operators/StateStoreSaveExec/#refer-to","text":"=== [[metrics]] Performance Metrics (SQLMetrics) StateStoreSaveExec uses the performance metrics as other stateful physical operators that write to a state store . The following table shows how the performance metrics are computed (and so their exact meaning). [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | total time to update rows a| [[allUpdatesTimeMs]] Time taken to read the input rows and < > (possibly filtering out expired rows per < > predicate) The number of rows stored is the < > metric For < > output mode, the time taken to filter out expired rows (per the required < > predicate) and the < > to < > For < > output mode, the time taken to go over all the input rows and request the < > to < > For < > output mode, the time taken to filter out expired rows (per the optional < > predicate) and the < > to < > | total time to remove rows a| [[allRemovalsTimeMs]] For < > output mode, the time taken for the < > to < > (per < > predicate) that is the total time of iterating over < > (the number of entries < > is the difference between the number of output rows of the < > and the < > metric) For < > output mode, always 0 For < > output mode, the time taken for the < > to < > (per < > predicate) | time to commit changes a| [[commitTimeMs]] Time taken for the < > to < > | number of output rows a| [[numOutputRows]] For < > output mode, the metric does not seem to be used For < > output mode, the number of rows in a < > (i.e. all < > in a < > in the < > that should be equivalent to the < > metric) For < > output mode, the number of rows that the < > was requested to < > (that did not expire per the optional < > predicate) that is equivalent to the < > metric) | number of total state rows a| [[numTotalStateRows]] Number of entries in a < > at the very end of < > (aka numTotalStateRows ) Corresponds to numRowsTotal attribute in stateOperators in StreamingQueryProgress (and is available as sq.lastProgress.stateOperators for an operator). | number of updated state rows a| [[numUpdatedStateRows]] Number of the entries that < > in a trigger and for the keys in the result rows of the upstream physical operator (aka numUpdatedStateRows ) For < > output mode, the number of input rows that have not expired yet (per the required < > predicate) and that the < > was requested to < > (the time taken is the < > metric) For < > output mode, the number of input rows (which should be exactly the number of output rows from the < >) For < > output mode, the number of rows that the < > was requested to < > (that did not expire per the optional < > predicate) that is equivalent to the < > metric) Corresponds to numRowsUpdated attribute in stateOperators in StreamingQueryProgress (and is available as sq.lastProgress.stateOperators for an operator). | memory used by state a| [[stateMemory]] Estimated memory used by a < > (aka stateMemory ) after StateStoreSaveExec finished < > (per the < > of the < >) |=== === [[creating-instance]] Creating StateStoreSaveExec Instance StateStoreSaveExec takes the following to be created: [[keyExpressions]] Key expressions , i.e. Catalyst attributes for the grouping keys [[stateInfo]] Execution-specific < > (default: None ) [[outputMode]] Execution-specific < > (default: None ) [[eventTimeWatermark]] < > (default: None ) [[stateFormatVersion]] Version of the state format (based on the < > configuration property) [[child]] Child physical operator ( SparkPlan ) === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method","title":"Refer to &lt;&gt;."},{"location":"physical-operators/StateStoreSaveExec/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/StateStoreSaveExec/#doexecute-rddinternalrow","text":"NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute initializes metrics . NOTE: doExecute requires that the optional < > is at this point defined (that should have happened when IncrementalExecution spark-sql-streaming-IncrementalExecution.md#preparations[had prepared a streaming aggregation for execution]). doExecute executes < > physical operator and spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD] with storeUpdateFunction that: Generates an unsafe projection to access the key field (using < > and the output schema of < >). Branches off per < >: < >, < > and < >. doExecute throws an UnsupportedOperationException when executed with an invalid < >: Invalid output mode: [outputMode] ==== [[doExecute-Append]] Append Output Mode NOTE: < > is the default output mode when not specified explicitly. NOTE: Append output mode requires that a streaming query defines < > (e.g. using withWatermark operator) on the event-time column that is used in aggregation (directly or using < > standard function). For < > output mode, doExecute does the following: Finds late (aggregate) rows from < > physical operator (that have expired per < >) < > and increments the < > metric < > Creates an iterator that < > when requested the next row and in the end < > TIP: Refer to < > for an example of StateStoreSaveExec with Append output mode. CAUTION: FIXME When is \"Filtering state store on:\" printed out? Uses spark-sql-streaming-WatermarkSupport.md#watermarkPredicateForData[watermarkPredicateForData] predicate to exclude matching rows and (like in < > output mode) spark-sql-streaming-StateStore.md#put[stores all the remaining rows] in StateStore . (like in < > output mode) While storing the rows, increments < > metric (for every row) and records the total time in < > metric. spark-sql-streaming-StateStore.md#getRange[Takes all the rows] from StateStore and returns a NextIterator that: In getNext , finds the first row that matches < > predicate, spark-sql-streaming-StateStore.md#remove[removes it] from StateStore , and returns it back. + If no row was found, getNext also marks the iterator as finished. In close , records the time to iterate over all the rows in < > metric, spark-sql-streaming-StateStore.md#commit[commits the updates] to StateStore followed by recording the time in < > metric and recording StateStore metrics . ==== [[doExecute-Complete]] Complete Output Mode For < > output mode, doExecute does the following: Takes all UnsafeRow rows (from the parent iterator) < > eagerly (i.e. all rows that are available in the parent iterator before proceeding) < > In the end, < > and passes the rows along (i.e. to the following physical operator) The number of keys stored in the state store is recorded in < > metric. NOTE: In Complete output mode the < > metric is exactly the < > metric. TIP: Refer to < > for an example of StateStoreSaveExec with Complete output mode. spark-sql-streaming-StateStore.md#put[Stores all rows] (as UnsafeRow ) in StateStore . While storing the rows, increments < > metric (for every row) and records the total time in < > metric. Records 0 in < > metric. spark-sql-streaming-StateStore.md#commit[Commits the state updates] to StateStore and records the time in < > metric. Records StateStore metrics In the end, takes all the rows stored in StateStore and increments numOutputRows metric. ==== [[doExecute-Update]] Update Output Mode For < > output mode, doExecute returns an iterator that filters out late aggregate rows (per < > if defined) and < > (one by one, i.e. every next ). With no more rows available, that < > (all at once) and < >. TIP: Refer to < > for an example of StateStoreSaveExec with Update output mode. doExecute returns Iterator of rows that uses < > predicate to filter out late rows. In hasNext , when rows are no longer available: Records the total time to iterate over all the rows in < > metric. spark-sql-streaming-WatermarkSupport.md#removeKeysOlderThanWatermark[removeKeysOlderThanWatermark] and records the time in < > metric. spark-sql-streaming-StateStore.md#commit[Commits the updates] to StateStore and records the time in < > metric. Records StateStore metrics In next , stores a row in StateStore and increments numOutputRows and numUpdatedStateRows metrics. === [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- shouldRunAnotherBatch Method shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ) : Boolean shouldRunAnotherBatch is positive ( true ) when all of the following are met: < > is either < > or < > < > is defined and is older (below) the current < > (of the given OffsetSeqMetadata ) Otherwise, shouldRunAnotherBatch is negative ( false ). shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":"doExecute(): RDD[InternalRow]"},{"location":"physical-operators/StateStoreWriter/","text":"StateStoreWriter Physical Operators \u00b6 StateStoreWriter is an extension of the StatefulOperator abstraction for stateful physical operators that write to a state store and collect the write metrics for execution progress reporting . Implementations \u00b6 FlatMapGroupsWithStateExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StreamingSymmetricHashJoinExec Performance Metrics \u00b6 ID Name numOutputRows number of output rows numTotalStateRows number of total state rows numUpdatedStateRows number of updated state rows allUpdatesTimeMs time to update allRemovalsTimeMs time to remove commitTimeMs time to commit changes stateMemory memory used by state Setting StateStore-Specific Metrics for Stateful Physical Operator \u00b6 setStoreMetrics ( store : StateStore ) : Unit setStoreMetrics requests the specified StateStore for the metrics and records the following metrics of a physical operator: numTotalStateRows as the number of keys stateMemory as the memory used (in bytes) setStoreMetrics records the custom metrics . setStoreMetrics is used when the following physical operators are executed: FlatMapGroupsWithStateExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StateOperatorProgress \u00b6 getProgress () : StateOperatorProgress getProgress ...FIXME getProgress is used when ProgressReporter is requested to extractStateOperatorMetrics (when MicroBatchExecution is requested to run the activated streaming query ). Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not \u00b6 shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ) : Boolean shouldRunAnotherBatch is negative ( false ) by default (to indicate that another non-data batch is not required given the OffsetSeqMetadata with the event-time watermark and the batch timestamp). shouldRunAnotherBatch is used when IncrementalExecution is requested to check out whether the last batch execution requires another batch (when MicroBatchExecution is requested to run the activated streaming query ). stateStoreCustomMetrics Internal Method \u00b6 stateStoreCustomMetrics : Map [ String , SQLMetric ] stateStoreCustomMetrics ...FIXME stateStoreCustomMetrics is used when StateStoreWriter is requested for the metrics and getProgress .","title":"StateStoreWriter"},{"location":"physical-operators/StateStoreWriter/#statestorewriter-physical-operators","text":"StateStoreWriter is an extension of the StatefulOperator abstraction for stateful physical operators that write to a state store and collect the write metrics for execution progress reporting .","title":"StateStoreWriter Physical Operators"},{"location":"physical-operators/StateStoreWriter/#implementations","text":"FlatMapGroupsWithStateExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec StreamingSymmetricHashJoinExec","title":"Implementations"},{"location":"physical-operators/StateStoreWriter/#performance-metrics","text":"ID Name numOutputRows number of output rows numTotalStateRows number of total state rows numUpdatedStateRows number of updated state rows allUpdatesTimeMs time to update allRemovalsTimeMs time to remove commitTimeMs time to commit changes stateMemory memory used by state","title":" Performance Metrics"},{"location":"physical-operators/StateStoreWriter/#setting-statestore-specific-metrics-for-stateful-physical-operator","text":"setStoreMetrics ( store : StateStore ) : Unit setStoreMetrics requests the specified StateStore for the metrics and records the following metrics of a physical operator: numTotalStateRows as the number of keys stateMemory as the memory used (in bytes) setStoreMetrics records the custom metrics . setStoreMetrics is used when the following physical operators are executed: FlatMapGroupsWithStateExec StateStoreSaveExec StreamingDeduplicateExec StreamingGlobalLimitExec","title":" Setting StateStore-Specific Metrics for Stateful Physical Operator"},{"location":"physical-operators/StateStoreWriter/#stateoperatorprogress","text":"getProgress () : StateOperatorProgress getProgress ...FIXME getProgress is used when ProgressReporter is requested to extractStateOperatorMetrics (when MicroBatchExecution is requested to run the activated streaming query ).","title":" StateOperatorProgress"},{"location":"physical-operators/StateStoreWriter/#checking-out-whether-last-batch-execution-requires-another-non-data-batch-or-not","text":"shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ) : Boolean shouldRunAnotherBatch is negative ( false ) by default (to indicate that another non-data batch is not required given the OffsetSeqMetadata with the event-time watermark and the batch timestamp). shouldRunAnotherBatch is used when IncrementalExecution is requested to check out whether the last batch execution requires another batch (when MicroBatchExecution is requested to run the activated streaming query ).","title":" Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not"},{"location":"physical-operators/StateStoreWriter/#statestorecustommetrics-internal-method","text":"stateStoreCustomMetrics : Map [ String , SQLMetric ] stateStoreCustomMetrics ...FIXME stateStoreCustomMetrics is used when StateStoreWriter is requested for the metrics and getProgress .","title":" stateStoreCustomMetrics Internal Method"},{"location":"physical-operators/StatefulOperator/","text":"StatefulOperator Physical Operators \u00b6 StatefulOperator is the < > of < > that < > or < > state (described by < >). [[contract]] .StatefulOperator Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | stateInfo a| [[stateInfo]] [source, scala] \u00b6 stateInfo: Option[StatefulOperatorStateInfo] \u00b6 The < > of the physical operator |=== [[extensions]] .StatefulOperators (Direct Implementations) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StatefulOperator | Description | < > | [[StateStoreReader]] | StateStoreWriter | [[StateStoreWriter]] Physical operator that writes to a state store and collects the write metrics for execution progress reporting |===","title":"StatefulOperator"},{"location":"physical-operators/StatefulOperator/#statefuloperator-physical-operators","text":"StatefulOperator is the < > of < > that < > or < > state (described by < >). [[contract]] .StatefulOperator Contract [cols=\"1m,2\",options=\"header\",width=\"100%\"] |=== | Method | Description | stateInfo a| [[stateInfo]]","title":"StatefulOperator Physical Operators"},{"location":"physical-operators/StatefulOperator/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/StatefulOperator/#stateinfo-optionstatefuloperatorstateinfo","text":"The < > of the physical operator |=== [[extensions]] .StatefulOperators (Direct Implementations) [cols=\"1,2\",options=\"header\",width=\"100%\"] |=== | StatefulOperator | Description | < > | [[StateStoreReader]] | StateStoreWriter | [[StateStoreWriter]] Physical operator that writes to a state store and collects the write metrics for execution progress reporting |===","title":"stateInfo: Option[StatefulOperatorStateInfo]"},{"location":"physical-operators/StreamingDeduplicateExec/","text":"StreamingDeduplicateExec Unary Physical Operator \u00b6 StreamingDeduplicateExec is a unary physical operator that writes state to StateStore with support for streaming watermark . [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 StreamingDeduplicateExec is < > exclusively when StreamingDeduplicationStrategy spark-sql-streaming-StreamingDeduplicationStrategy.md#apply[plans Deduplicate unary logical operators]. val uniqueValues = spark. readStream. format(\"rate\"). load. dropDuplicates(\"value\") // <-- creates Deduplicate logical operator scala> println(uniqueValues.queryExecution.logical.numberedTreeString) 00 Deduplicate [value#214L], true 01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#213, value#214L] scala> uniqueValues.explain == Physical Plan == StreamingDeduplicate [value#214L], StatefulOperatorStateInfo(<unknown>,5a65879c-67bc-4e77-b417-6100db6a52a2,0,0), 0 +- Exchange hashpartitioning(value#214L, 200) +- StreamingRelation rate, [timestamp#213, value#214L] // Start the query and hence StreamingDeduplicateExec import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = uniqueValues. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // sorting not supported for non-aggregate queries // and so values are unsorted ------------------------------------------- Batch: 0 ------------------------------------------- +---------+-----+ |timestamp|value| +---------+-----+ +---------+-----+ ------------------------------------------- Batch: 1 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-07-25 22:12:03.018|0 | |2017-07-25 22:12:08.018|5 | |2017-07-25 22:12:04.018|1 | |2017-07-25 22:12:06.018|3 | |2017-07-25 22:12:05.018|2 | |2017-07-25 22:12:07.018|4 | +-----------------------+-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-07-25 22:12:10.018|7 | |2017-07-25 22:12:09.018|6 | |2017-07-25 22:12:12.018|9 | |2017-07-25 22:12:13.018|10 | |2017-07-25 22:12:15.018|12 | |2017-07-25 22:12:11.018|8 | |2017-07-25 22:12:14.018|11 | |2017-07-25 22:12:16.018|13 | |2017-07-25 22:12:17.018|14 | |2017-07-25 22:12:18.018|15 | +-----------------------+-----+ // Eventually... sq.stop [[metrics]] StreamingDeduplicateExec uses the performance metrics of StateStoreWriter . .StreamingDeduplicateExec in web UI (Details for Query) image::images/StreamingDeduplicateExec-webui-query-details.png[align=\"center\"] [[output]] The output schema of StreamingDeduplicateExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StreamingDeduplicateExec is exactly the < >'s output partitioning. [source, scala] \u00b6 /** // Start spark-shell with debugging and Kafka support SPARK_SUBMIT_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\" \\ ./bin/spark-shell \\ --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT */ // Reading val topic1 = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). option(\"startingOffsets\", \"earliest\"). load // Processing with deduplication // Don't use watermark // The following won't work due to https://issues.apache.org/jira/browse/SPARK-21546 /** val records = topic1. withColumn(\"eventtime\", 'timestamp). // \u2190 just to put the right name given the purpose withWatermark(eventTime = \"eventtime\", delayThreshold = \"30 seconds\"). // \u2190 use the renamed eventtime column dropDuplicates(\"value\"). // dropDuplicates will use watermark // only when eventTime column exists // include the watermark column => internal design leak? select('key cast \"string\", 'value cast \"string\", 'eventtime). as[(String, String, java.sql.Timestamp)] */ val records = topic1. dropDuplicates(\"value\"). select('key cast \"string\", 'value cast \"string\"). as[(String, String)] scala> records.explain == Physical Plan == *Project [cast(key#0 as string) AS key#249, cast(value#1 as string) AS value#250] +- StreamingDeduplicate [value#1], StatefulOperatorStateInfo( ,68198b93-6184-49ae-8098-006c32cc6192,0,0), 0 +- Exchange hashpartitioning(value#1, 200) +- *Project [key#0, value#1] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] // Writing import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val sq = records. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). queryName(\"from-kafka-topic1-to-console\"). outputMode(OutputMode.Update). start // Eventually... sq.stop [TIP] \u00b6 Enable INFO logging level for org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec=INFO Refer to spark-sql-streaming-logging.md[Logging]. \u00b6 === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method [source, scala] \u00b6 doExecute(): RDD[InternalRow] \u00b6 NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute initializes metrics . doExecute executes < > physical operator and spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD] with storeUpdateFunction that: Generates an unsafe projection to access the key field (using < > and the output schema of < >). Filters out rows from Iterator[InternalRow] that match watermarkPredicateForData (when defined and < > is EventTimeTimeout ) For every row (as InternalRow ) Extracts the key from the row (using the unsafe projection above) spark-sql-streaming-StateStore.md#get[Gets the saved state] in StateStore for the key (when there was a state for the key in the row) Filters out (aka drops ) the row (when there was no state for the key in the row) Stores a new (and empty) state for the key and increments < > and < > metrics. In the end, storeUpdateFunction creates a CompletionIterator that executes a completion function (aka completionFunction ) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). + The completion function does the following: Updates < > metric (that is the total time to execute storeUpdateFunction ) Updates < > metric with the time taken to spark-sql-streaming-WatermarkSupport.md#removeKeysOlderThanWatermark[remove keys older than the watermark from the StateStore] Updates < > metric with the time taken to spark-sql-streaming-StateStore.md#commit[commit the changes to the StateStore] Sets StateStore-specific metrics === [[creating-instance]] Creating StreamingDeduplicateExec Instance StreamingDeduplicateExec takes the following when created: [[keyExpressions]] Duplicate keys (as used in dropDuplicates operator) [[child]] Child physical operator ( SparkPlan ) [[stateInfo]] < > [[eventTimeWatermark]] Event-time watermark Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not \u00b6 shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ) : Boolean shouldRunAnotherBatch ...FIXME shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":"StreamingDeduplicateExec"},{"location":"physical-operators/StreamingDeduplicateExec/#streamingdeduplicateexec-unary-physical-operator","text":"StreamingDeduplicateExec is a unary physical operator that writes state to StateStore with support for streaming watermark .","title":"StreamingDeduplicateExec Unary Physical Operator"},{"location":"physical-operators/StreamingDeduplicateExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"physical-operators/StreamingDeduplicateExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"StreamingDeduplicateExec is < > exclusively when StreamingDeduplicationStrategy spark-sql-streaming-StreamingDeduplicationStrategy.md#apply[plans Deduplicate unary logical operators]. val uniqueValues = spark. readStream. format(\"rate\"). load. dropDuplicates(\"value\") // <-- creates Deduplicate logical operator scala> println(uniqueValues.queryExecution.logical.numberedTreeString) 00 Deduplicate [value#214L], true 01 +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4785f176,rate,List(),None,List(),None,Map(),None), rate, [timestamp#213, value#214L] scala> uniqueValues.explain == Physical Plan == StreamingDeduplicate [value#214L], StatefulOperatorStateInfo(<unknown>,5a65879c-67bc-4e77-b417-6100db6a52a2,0,0), 0 +- Exchange hashpartitioning(value#214L, 200) +- StreamingRelation rate, [timestamp#213, value#214L] // Start the query and hence StreamingDeduplicateExec import scala.concurrent.duration._ import org.apache.spark.sql.streaming.{OutputMode, Trigger} val sq = uniqueValues. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). outputMode(OutputMode.Update). start // sorting not supported for non-aggregate queries // and so values are unsorted ------------------------------------------- Batch: 0 ------------------------------------------- +---------+-----+ |timestamp|value| +---------+-----+ +---------+-----+ ------------------------------------------- Batch: 1 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-07-25 22:12:03.018|0 | |2017-07-25 22:12:08.018|5 | |2017-07-25 22:12:04.018|1 | |2017-07-25 22:12:06.018|3 | |2017-07-25 22:12:05.018|2 | |2017-07-25 22:12:07.018|4 | +-----------------------+-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----------------------+-----+ |timestamp |value| +-----------------------+-----+ |2017-07-25 22:12:10.018|7 | |2017-07-25 22:12:09.018|6 | |2017-07-25 22:12:12.018|9 | |2017-07-25 22:12:13.018|10 | |2017-07-25 22:12:15.018|12 | |2017-07-25 22:12:11.018|8 | |2017-07-25 22:12:14.018|11 | |2017-07-25 22:12:16.018|13 | |2017-07-25 22:12:17.018|14 | |2017-07-25 22:12:18.018|15 | +-----------------------+-----+ // Eventually... sq.stop [[metrics]] StreamingDeduplicateExec uses the performance metrics of StateStoreWriter . .StreamingDeduplicateExec in web UI (Details for Query) image::images/StreamingDeduplicateExec-webui-query-details.png[align=\"center\"] [[output]] The output schema of StreamingDeduplicateExec is exactly the < >'s output schema. [[outputPartitioning]] The output partitioning of StreamingDeduplicateExec is exactly the < >'s output partitioning.","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/StreamingDeduplicateExec/#source-scala","text":"/** // Start spark-shell with debugging and Kafka support SPARK_SUBMIT_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005\" \\ ./bin/spark-shell \\ --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0-SNAPSHOT */ // Reading val topic1 = spark. readStream. format(\"kafka\"). option(\"subscribe\", \"topic1\"). option(\"kafka.bootstrap.servers\", \"localhost:9092\"). option(\"startingOffsets\", \"earliest\"). load // Processing with deduplication // Don't use watermark // The following won't work due to https://issues.apache.org/jira/browse/SPARK-21546 /** val records = topic1. withColumn(\"eventtime\", 'timestamp). // \u2190 just to put the right name given the purpose withWatermark(eventTime = \"eventtime\", delayThreshold = \"30 seconds\"). // \u2190 use the renamed eventtime column dropDuplicates(\"value\"). // dropDuplicates will use watermark // only when eventTime column exists // include the watermark column => internal design leak? select('key cast \"string\", 'value cast \"string\", 'eventtime). as[(String, String, java.sql.Timestamp)] */ val records = topic1. dropDuplicates(\"value\"). select('key cast \"string\", 'value cast \"string\"). as[(String, String)] scala> records.explain == Physical Plan == *Project [cast(key#0 as string) AS key#249, cast(value#1 as string) AS value#250] +- StreamingDeduplicate [value#1], StatefulOperatorStateInfo( ,68198b93-6184-49ae-8098-006c32cc6192,0,0), 0 +- Exchange hashpartitioning(value#1, 200) +- *Project [key#0, value#1] +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6] // Writing import org.apache.spark.sql.streaming.{OutputMode, Trigger} import scala.concurrent.duration._ val sq = records. writeStream. format(\"console\"). option(\"truncate\", false). trigger(Trigger.ProcessingTime(10.seconds)). queryName(\"from-kafka-topic1-to-console\"). outputMode(OutputMode.Update). start // Eventually... sq.stop","title":"[source, scala]"},{"location":"physical-operators/StreamingDeduplicateExec/#tip","text":"Enable INFO logging level for org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec to see what happens inside. Add the following line to conf/log4j.properties : log4j.logger.org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec=INFO","title":"[TIP]"},{"location":"physical-operators/StreamingDeduplicateExec/#refer-to-spark-sql-streaming-loggingmdlogging","text":"=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method","title":"Refer to spark-sql-streaming-logging.md[Logging]."},{"location":"physical-operators/StreamingDeduplicateExec/#source-scala_1","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingDeduplicateExec/#doexecute-rddinternalrow","text":"NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a distributed computation over internal binary rows on Apache Spark (i.e. RDD[InternalRow] ). Internally, doExecute initializes metrics . doExecute executes < > physical operator and spark-sql-streaming-StateStoreOps.md#mapPartitionsWithStateStore[creates a StateStoreRDD] with storeUpdateFunction that: Generates an unsafe projection to access the key field (using < > and the output schema of < >). Filters out rows from Iterator[InternalRow] that match watermarkPredicateForData (when defined and < > is EventTimeTimeout ) For every row (as InternalRow ) Extracts the key from the row (using the unsafe projection above) spark-sql-streaming-StateStore.md#get[Gets the saved state] in StateStore for the key (when there was a state for the key in the row) Filters out (aka drops ) the row (when there was no state for the key in the row) Stores a new (and empty) state for the key and increments < > and < > metrics. In the end, storeUpdateFunction creates a CompletionIterator that executes a completion function (aka completionFunction ) after it has successfully iterated through all the elements (i.e. when a client has consumed all the rows). + The completion function does the following: Updates < > metric (that is the total time to execute storeUpdateFunction ) Updates < > metric with the time taken to spark-sql-streaming-WatermarkSupport.md#removeKeysOlderThanWatermark[remove keys older than the watermark from the StateStore] Updates < > metric with the time taken to spark-sql-streaming-StateStore.md#commit[commit the changes to the StateStore] Sets StateStore-specific metrics === [[creating-instance]] Creating StreamingDeduplicateExec Instance StreamingDeduplicateExec takes the following when created: [[keyExpressions]] Duplicate keys (as used in dropDuplicates operator) [[child]] Child physical operator ( SparkPlan ) [[stateInfo]] < > [[eventTimeWatermark]] Event-time watermark","title":"doExecute(): RDD[InternalRow]"},{"location":"physical-operators/StreamingDeduplicateExec/#checking-out-whether-last-batch-execution-requires-another-non-data-batch-or-not","text":"shouldRunAnotherBatch ( newMetadata : OffsetSeqMetadata ) : Boolean shouldRunAnotherBatch ...FIXME shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":" Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not"},{"location":"physical-operators/StreamingGlobalLimitExec/","text":"== [[StreamingGlobalLimitExec]] StreamingGlobalLimitExec Unary Physical Operator StreamingGlobalLimitExec is a unary physical operator that represents a Limit logical operator of a streaming query at execution time. [NOTE] \u00b6 A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 StreamingGlobalLimitExec is < > exclusively when < > execution planning strategy is requested to plan a Limit logical operator (in the logical plan of a streaming query) for execution. [NOTE] \u00b6 Limit logical operator represents Dataset.limit operator in a logical query plan. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Limit.html[Limit Logical Operator] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book. \u00b6 StreamingGlobalLimitExec is a < >. StreamingGlobalLimitExec supports < > output mode only. The optional properties, i.e. the < > and the < >, are initially undefined when StreamingGlobalLimitExec is < >. StreamingGlobalLimitExec is updated to hold execution-specific configuration when IncrementalExecution is requested to < > (when the < > is executed). === [[creating-instance]] Creating StreamingGlobalLimitExec Instance StreamingGlobalLimitExec takes the following to be created: [[streamLimit]] Streaming Limit [[child]] Child physical operator ( SparkPlan ) [[stateInfo]] < > (default: None ) [[outputMode]] < > (default: None ) StreamingGlobalLimitExec initializes the < >. === [[StateStoreWriter]] StreamingGlobalLimitExec as StateStoreWriter StreamingGlobalLimitExec is a stateful physical operator that can write to a state store . === [[metrics]] Performance Metrics StreamingGlobalLimitExec uses the performance metrics of the parent StateStoreWriter . === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method [source, scala] \u00b6 doExecute(): RDD[InternalRow] \u00b6 NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a recipe for distributed computation over internal binary rows on Apache Spark ( RDD[InternalRow] ). doExecute ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keySchema a| [[keySchema]] FIXME Used when...FIXME | valueSchema a| [[valueSchema]] FIXME Used when...FIXME |===","title":"StreamingGlobalLimitExec"},{"location":"physical-operators/StreamingGlobalLimitExec/#note","text":"A unary physical operator ( UnaryExecNode ) is a physical operator with a single < > physical operator.","title":"[NOTE]"},{"location":"physical-operators/StreamingGlobalLimitExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlunaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"StreamingGlobalLimitExec is < > exclusively when < > execution planning strategy is requested to plan a Limit logical operator (in the logical plan of a streaming query) for execution.","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[UnaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/StreamingGlobalLimitExec/#note_1","text":"Limit logical operator represents Dataset.limit operator in a logical query plan.","title":"[NOTE]"},{"location":"physical-operators/StreamingGlobalLimitExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-logicalplan-limithtmllimit-logical-operator-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-book","text":"StreamingGlobalLimitExec is a < >. StreamingGlobalLimitExec supports < > output mode only. The optional properties, i.e. the < > and the < >, are initially undefined when StreamingGlobalLimitExec is < >. StreamingGlobalLimitExec is updated to hold execution-specific configuration when IncrementalExecution is requested to < > (when the < > is executed). === [[creating-instance]] Creating StreamingGlobalLimitExec Instance StreamingGlobalLimitExec takes the following to be created: [[streamLimit]] Streaming Limit [[child]] Child physical operator ( SparkPlan ) [[stateInfo]] < > (default: None ) [[outputMode]] < > (default: None ) StreamingGlobalLimitExec initializes the < >. === [[StateStoreWriter]] StreamingGlobalLimitExec as StateStoreWriter StreamingGlobalLimitExec is a stateful physical operator that can write to a state store . === [[metrics]] Performance Metrics StreamingGlobalLimitExec uses the performance metrics of the parent StateStoreWriter . === [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- doExecute Method","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-LogicalPlan-Limit.html[Limit Logical Operator] in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] book."},{"location":"physical-operators/StreamingGlobalLimitExec/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingGlobalLimitExec/#doexecute-rddinternalrow","text":"NOTE: doExecute is part of SparkPlan Contract to generate the runtime representation of an physical operator as a recipe for distributed computation over internal binary rows on Apache Spark ( RDD[InternalRow] ). doExecute ...FIXME === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | keySchema a| [[keySchema]] FIXME Used when...FIXME | valueSchema a| [[valueSchema]] FIXME Used when...FIXME |===","title":"doExecute(): RDD[InternalRow]"},{"location":"physical-operators/StreamingRelationExec/","text":"StreamingRelationExec Leaf Physical Operator \u00b6 StreamingRelationExec is a leaf physical operator (i.e. LeafExecNode ) that...FIXME StreamingRelationExec is < > when StreamingRelationStrategy spark-sql-streaming-StreamingRelationStrategy.md#apply[plans] StreamingRelation and StreamingExecutionRelation logical operators. [source, scala] \u00b6 scala> spark.version res0: String = 2.3.0-SNAPSHOT val rates = spark. readStream. format(\"rate\"). load // StreamingRelation logical operator scala> println(rates.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] // StreamingRelationExec physical operator (shown without \"Exec\" suffix) scala> rates.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] [[doExecute]] StreamingRelationExec is not supposed to be executed and is used...FIXME === [[creating-instance]] Creating StreamingRelationExec Instance StreamingRelationExec takes the following when created: [[sourceName]] The name of a streaming source [[output]] Output attributes","title":"StreamingRelationExec"},{"location":"physical-operators/StreamingRelationExec/#streamingrelationexec-leaf-physical-operator","text":"StreamingRelationExec is a leaf physical operator (i.e. LeafExecNode ) that...FIXME StreamingRelationExec is < > when StreamingRelationStrategy spark-sql-streaming-StreamingRelationStrategy.md#apply[plans] StreamingRelation and StreamingExecutionRelation logical operators.","title":"StreamingRelationExec Leaf Physical Operator"},{"location":"physical-operators/StreamingRelationExec/#source-scala","text":"scala> spark.version res0: String = 2.3.0-SNAPSHOT val rates = spark. readStream. format(\"rate\"). load // StreamingRelation logical operator scala> println(rates.queryExecution.logical.numberedTreeString) 00 StreamingRelation DataSource(org.apache.spark.sql.SparkSession@31ba0af0,rate,List(),None,List(),None,Map(),None), rate, [timestamp#0, value#1L] // StreamingRelationExec physical operator (shown without \"Exec\" suffix) scala> rates.explain == Physical Plan == StreamingRelation rate, [timestamp#0, value#1L] [[doExecute]] StreamingRelationExec is not supposed to be executed and is used...FIXME === [[creating-instance]] Creating StreamingRelationExec Instance StreamingRelationExec takes the following when created: [[sourceName]] The name of a streaming source [[output]] Output attributes","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/","text":"StreamingSymmetricHashJoinExec Binary Physical Operator \u2014 Stream-Stream Joins \u00b6 StreamingSymmetricHashJoinExec is a binary physical operator for stream-stream equi-join at execution time. [NOTE] \u00b6 A binary physical operator ( BinaryExecNode ) is a physical operator with < > and < > child physical operators. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[BinaryExecNode ] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. \u00b6 [[supported-join-types]][[joinType]] StreamingSymmetricHashJoinExec supports Inner , LeftOuter , and RightOuter join types (with the < > and the < > keys using the exact same data types). StreamingSymmetricHashJoinExec is < > exclusively when < > execution planning strategy is requested to plan a logical query plan with a Join logical operator of two streaming queries with equality predicates ( EqualTo and EqualNullSafe ). StreamingSymmetricHashJoinExec is given execution-specific configuration (i.e. < >, < >, and < >) when IncrementalExecution is requested to plan a streaming query for execution (and uses the < >). StreamingSymmetricHashJoinExec uses two < > (for the < > and < > sides of the join) to manage join state when < >. StreamingSymmetricHashJoinExec is a stateful physical operator that writes to a state store . Creating Instance \u00b6 StreamingSymmetricHashJoinExec takes the following to be created: [[leftKeys]] Left keys (Catalyst expressions of the keys on the left side) [[rightKeys]] Right keys (Catalyst expressions of the keys on the right side) < > [[condition]] Join condition ( JoinConditionSplitPredicates ) [[stateInfo]] < > < > < > [[left]] Physical operator on the left side ( SparkPlan ) [[right]] Physical operator on the right side ( SparkPlan ) StreamingSymmetricHashJoinExec initializes the < >. === [[output]] Output Schema -- output Method [source, scala] \u00b6 output: Seq[Attribute] \u00b6 NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output schema depends on the < >: For Cross and Inner ( InnerLike ) joins, it is the output schema of the < > and < > operators For LeftOuter joins, it is the output schema of the < > operator with the attributes of the < > operator with nullability flag enabled ( true ) For RightOuter joins, it is the output schema of the < > operator with the attributes of the < > operator with nullability flag enabled ( true ) output throws an IllegalArgumentException for other join types: [className] should not take [joinType] as the JoinType === [[outputPartitioning]] Output Partitioning -- outputPartitioning Method [source, scala] \u00b6 outputPartitioning: Partitioning \u00b6 NOTE: outputPartitioning is part of the SparkPlan Contract to specify how data should be partitioned across different nodes in the cluster. outputPartitioning depends on the < >: For Cross and Inner ( InnerLike ) joins, it is a PartitioningCollection of the output partitioning of the < > and < > operators For LeftOuter joins, it is a PartitioningCollection of the output partitioning of the < > operator For RightOuter joins, it is a PartitioningCollection of the output partitioning of the < > operator outputPartitioning throws an IllegalArgumentException for other join types: [className] should not take [joinType] as the JoinType === [[eventTimeWatermark]] Event-Time Watermark -- eventTimeWatermark Internal Property [source, scala] \u00b6 eventTimeWatermark: Option[Long] \u00b6 When < >, StreamingSymmetricHashJoinExec can be given the < > of the current streaming micro-batch. eventTimeWatermark is an optional property that is specified only after < > was requested to apply the < > to a physical query plan of a streaming query (to < > once for < > and every trigger for < > in their queryPlanning phases). [NOTE] \u00b6 eventTimeWatermark is used when: StreamingSymmetricHashJoinExec is requested to < > * OneSideHashJoiner is requested to < > \u00b6 === [[stateWatermarkPredicates]] Watermark Predicates for State Removal -- stateWatermarkPredicates Internal Property [source, scala] \u00b6 stateWatermarkPredicates: JoinStateWatermarkPredicates \u00b6 When < >, StreamingSymmetricHashJoinExec is given a < > for the < > and < > join sides (using the < > utility). stateWatermarkPredicates contains the left and right predicates only when < > is requested to apply the < > to a physical query plan of a streaming query (to < > once for < > and every trigger for < > in their queryPlanning phases). [NOTE] \u00b6 stateWatermarkPredicates is used when StreamingSymmetricHashJoinExec is requested for the following: < > (and creating < >) * < > \u00b6 === [[requiredChildDistribution]] Required Partition Requirements -- requiredChildDistribution Method [source, scala] \u00b6 requiredChildDistribution: Seq[Distribution] \u00b6 [NOTE] \u00b6 requiredChildDistribution is part of the SparkPlan Contract for the required partition requirements (aka required child distribution ) of the input data, i.e. how the output of the children physical operators is split across partitions before this operator can be executed. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[SparkPlan Contract]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. \u00b6 requiredChildDistribution returns two HashClusteredDistributions for the < > and < > keys with the required < > based on the < >. [NOTE] \u00b6 requiredChildDistribution is used exclusively when EnsureRequirements physical query plan optimization is executed (and enforces partition requirements). Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-EnsureRequirements.html[EnsureRequirements Physical Query Optimization]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. \u00b6 [NOTE] \u00b6 HashClusteredDistribution becomes HashPartitioning at execution that distributes rows across partitions (generates partition IDs of rows) based on Murmur3Hash of the join expressions (separately for the < > and < > keys) modulo the required number of partitions. Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Distribution-HashClusteredDistribution.html[HashClusteredDistribution ]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book. \u00b6 === [[metrics]] Performance Metrics (SQLMetrics) StreamingSymmetricHashJoinExec uses the performance metrics as other stateful physical operators that write to a state store . The following table shows how the performance metrics are computed (and so their exact meaning). [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | total time to update rows a| [[allUpdatesTimeMs]] Processing time of all rows | total time to remove rows a| [[allRemovalsTimeMs]] | time to commit changes a| [[commitTimeMs]] | number of output rows a| [[numOutputRows]] Total number of output rows | number of total state rows a| [[numTotalStateRows]] | number of updated state rows a| [[numUpdatedStateRows]] < > of the < > and < > OneSideHashJoiners | memory used by state a| [[stateMemory]] |=== === [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- shouldRunAnotherBatch Method [source, scala] \u00b6 shouldRunAnotherBatch( newMetadata: OffsetSeqMetadata): Boolean shouldRunAnotherBatch is positive ( true ) when all of the following are positive: Either the < > or < > join state watermark predicates are defined (in the < >) < > threshold (of the StreamingSymmetricHashJoinExec operator) is defined and the current < > threshold of the given OffsetSeqMetadata is above ( greater than ) it, i.e. moved above shouldRunAnotherBatch is negative ( false ) otherwise. shouldRunAnotherBatch is part of the StateStoreWriter abstraction. Executing Physical Operator \u00b6 doExecute () : RDD [ InternalRow ] doExecute is part of the SparkPlan abstraction ( Spark SQL ). doExecute first requests the StreamingQueryManager for the StateStoreCoordinatorRef to the StateStoreCoordinator RPC endpoint (for the driver). doExecute then uses SymmetricHashJoinStateManager utility to < > for the < > and < > sides of the streaming join. In the end, doExecute requests the < > and < > child physical operators to execute (generate an RDD) and then < > with < > (and with the StateStoreCoordinatorRef and the state stores). === [[processPartitions]] Processing Partitions of Left and Right Sides of Stream-Stream Join -- processPartitions Internal Method [source, scala] \u00b6 processPartitions( leftInputIter: Iterator[InternalRow], rightInputIter: Iterator[InternalRow]): Iterator[InternalRow] [[processPartitions-updateStartTimeNs]] processPartitions records the current time (as updateStartTimeNs for the < > performance metric in < >). [[processPartitions-postJoinFilter]] processPartitions creates a new predicate ( postJoinFilter ) based on the bothSides of the < > if defined or true literal. [[processPartitions-leftSideJoiner]] processPartitions creates a < > for the < > and all other properties for the left-hand join side ( leftSideJoiner ). [[processPartitions-rightSideJoiner]] processPartitions creates a < > for the < > and all other properties for the right-hand join side ( rightSideJoiner ). [[processPartitions-leftOutputIter]][[processPartitions-rightOutputIter]] processPartitions requests the OneSideHashJoiner for the left-hand join side to < > with the right-hand side one (that creates a leftOutputIter row iterator) and the OneSideHashJoiner for the right-hand join side to do the same with the left-hand side one (and creates a rightOutputIter row iterator). [[processPartitions-innerOutputCompletionTimeNs]] processPartitions records the current time (as innerOutputCompletionTimeNs for the < > performance metric in < >). [[processPartitions-innerOutputIter]] processPartitions creates a CompletionIterator with the left and right output iterators (with the rows of the leftOutputIter first followed by rightOutputIter ). When no rows are left to process, the CompletionIterator records the completion time. [[processPartitions-outputIter]] processPartitions creates a join-specific output Iterator[InternalRow] of the output rows based on the < > (of the StreamingSymmetricHashJoinExec ): For Inner joins, processPartitions simply uses the < > For LeftOuter joins, processPartitions ... For RightOuter joins, processPartitions ... For other joins, processPartitions simply throws an IllegalArgumentException . [[processPartitions-outputIterWithMetrics]] processPartitions creates an UnsafeProjection for the < > (and the output of the < > and < > child operators) that counts all the rows of the < > (as the < > metric) and generate an output projection. In the end, processPartitions returns a CompletionIterator with with the < > and < > completion function. NOTE: processPartitions is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. ==== [[processPartitions-onOutputCompletion]][[onOutputCompletion]] Calculating Performance Metrics (Output Completion Callback) -- onOutputCompletion Internal Method [source, scala] \u00b6 onOutputCompletion: Unit \u00b6 onOutputCompletion calculates the < > performance metric (that is the time since the < > was executed). onOutputCompletion adds the time for the inner join to complete (since < > time marker) to the < > performance metric. onOutputCompletion records the time to < > (per the < > for the < > and the < > streaming queries) and adds it to the < > performance metric. NOTE: onOutputCompletion triggers the < > eagerly by iterating over the state rows to be deleted. onOutputCompletion records the time for the < > and < > OneSideHashJoiners to < > that becomes the < > performance metric. onOutputCompletion calculates the < > performance metric (as the < > of the < > and < > streaming queries). onOutputCompletion calculates the < > performance metric (as the sum of the < > in the < > of the < > and < > streaming queries). onOutputCompletion calculates the < > performance metric (as the sum of the < > by the < > and < > of the < > and < > streams). In the end, onOutputCompletion calculates the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | hadoopConfBcast a| [[hadoopConfBcast]] Hadoop Configuration broadcast (to the Spark cluster) Used exclusively to < > | joinStateManager a| [[joinStateManager]] < > Used when OneSideHashJoiner is requested to < >, < >, < >, and for the < > | nullLeft a| [[nullLeft]] GenericInternalRow of the size of the output schema of the < > | nullRight a| [[nullRight]] GenericInternalRow of the size of the output schema of the < > | storeConf a| [[storeConf]] < > Used exclusively to < > |===","title":"StreamingSymmetricHashJoinExec"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#streamingsymmetrichashjoinexec-binary-physical-operator-stream-stream-joins","text":"StreamingSymmetricHashJoinExec is a binary physical operator for stream-stream equi-join at execution time.","title":"StreamingSymmetricHashJoinExec Binary Physical Operator &mdash; Stream-Stream Joins"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#note","text":"A binary physical operator ( BinaryExecNode ) is a physical operator with < > and < > child physical operators.","title":"[NOTE]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlbinaryexecnode-and-physical-operators-in-general-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-online-book","text":"[[supported-join-types]][[joinType]] StreamingSymmetricHashJoinExec supports Inner , LeftOuter , and RightOuter join types (with the < > and the < > keys using the exact same data types). StreamingSymmetricHashJoinExec is < > exclusively when < > execution planning strategy is requested to plan a logical query plan with a Join logical operator of two streaming queries with equality predicates ( EqualTo and EqualNullSafe ). StreamingSymmetricHashJoinExec is given execution-specific configuration (i.e. < >, < >, and < >) when IncrementalExecution is requested to plan a streaming query for execution (and uses the < >). StreamingSymmetricHashJoinExec uses two < > (for the < > and < > sides of the join) to manage join state when < >. StreamingSymmetricHashJoinExec is a stateful physical operator that writes to a state store .","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[BinaryExecNode] (and physical operators in general) in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book."},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#creating-instance","text":"StreamingSymmetricHashJoinExec takes the following to be created: [[leftKeys]] Left keys (Catalyst expressions of the keys on the left side) [[rightKeys]] Right keys (Catalyst expressions of the keys on the right side) < > [[condition]] Join condition ( JoinConditionSplitPredicates ) [[stateInfo]] < > < > < > [[left]] Physical operator on the left side ( SparkPlan ) [[right]] Physical operator on the right side ( SparkPlan ) StreamingSymmetricHashJoinExec initializes the < >. === [[output]] Output Schema -- output Method","title":"Creating Instance"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#output-seqattribute","text":"NOTE: output is part of the QueryPlan Contract to describe the attributes of (the schema of) the output. output schema depends on the < >: For Cross and Inner ( InnerLike ) joins, it is the output schema of the < > and < > operators For LeftOuter joins, it is the output schema of the < > operator with the attributes of the < > operator with nullability flag enabled ( true ) For RightOuter joins, it is the output schema of the < > operator with the attributes of the < > operator with nullability flag enabled ( true ) output throws an IllegalArgumentException for other join types: [className] should not take [joinType] as the JoinType === [[outputPartitioning]] Output Partitioning -- outputPartitioning Method","title":"output: Seq[Attribute]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_1","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#outputpartitioning-partitioning","text":"NOTE: outputPartitioning is part of the SparkPlan Contract to specify how data should be partitioned across different nodes in the cluster. outputPartitioning depends on the < >: For Cross and Inner ( InnerLike ) joins, it is a PartitioningCollection of the output partitioning of the < > and < > operators For LeftOuter joins, it is a PartitioningCollection of the output partitioning of the < > operator For RightOuter joins, it is a PartitioningCollection of the output partitioning of the < > operator outputPartitioning throws an IllegalArgumentException for other join types: [className] should not take [joinType] as the JoinType === [[eventTimeWatermark]] Event-Time Watermark -- eventTimeWatermark Internal Property","title":"outputPartitioning: Partitioning"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_2","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#eventtimewatermark-optionlong","text":"When < >, StreamingSymmetricHashJoinExec can be given the < > of the current streaming micro-batch. eventTimeWatermark is an optional property that is specified only after < > was requested to apply the < > to a physical query plan of a streaming query (to < > once for < > and every trigger for < > in their queryPlanning phases).","title":"eventTimeWatermark: Option[Long]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#note_1","text":"eventTimeWatermark is used when: StreamingSymmetricHashJoinExec is requested to < >","title":"[NOTE]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#onesidehashjoiner-is-requested-to","text":"=== [[stateWatermarkPredicates]] Watermark Predicates for State Removal -- stateWatermarkPredicates Internal Property","title":"* OneSideHashJoiner is requested to &lt;&gt;"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_3","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#statewatermarkpredicates-joinstatewatermarkpredicates","text":"When < >, StreamingSymmetricHashJoinExec is given a < > for the < > and < > join sides (using the < > utility). stateWatermarkPredicates contains the left and right predicates only when < > is requested to apply the < > to a physical query plan of a streaming query (to < > once for < > and every trigger for < > in their queryPlanning phases).","title":"stateWatermarkPredicates: JoinStateWatermarkPredicates"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#note_2","text":"stateWatermarkPredicates is used when StreamingSymmetricHashJoinExec is requested for the following: < > (and creating < >)","title":"[NOTE]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#_1","text":"=== [[requiredChildDistribution]] Required Partition Requirements -- requiredChildDistribution Method","title":"* &lt;&gt;"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_4","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#requiredchilddistribution-seqdistribution","text":"","title":"requiredChildDistribution: Seq[Distribution]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#note_3","text":"requiredChildDistribution is part of the SparkPlan Contract for the required partition requirements (aka required child distribution ) of the input data, i.e. how the output of the children physical operators is split across partitions before this operator can be executed.","title":"[NOTE]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-sparkplanhtmlsparkplan-contract-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-online-book","text":"requiredChildDistribution returns two HashClusteredDistributions for the < > and < > keys with the required < > based on the < >.","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan.html[SparkPlan Contract]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book."},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#note_4","text":"requiredChildDistribution is used exclusively when EnsureRequirements physical query plan optimization is executed (and enforces partition requirements).","title":"[NOTE]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-ensurerequirementshtmlensurerequirements-physical-query-optimization-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-online-book","text":"","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-EnsureRequirements.html[EnsureRequirements Physical Query Optimization]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book."},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#note_5","text":"HashClusteredDistribution becomes HashPartitioning at execution that distributes rows across partitions (generates partition IDs of rows) based on Murmur3Hash of the join expressions (separately for the < > and < > keys) modulo the required number of partitions.","title":"[NOTE]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#read-up-on-httpsjaceklaskowskigitbooksiomastering-spark-sqlspark-sql-distribution-hashclustereddistributionhtmlhashclustereddistribution-in-httpsbitlyspark-sql-internalsthe-internals-of-spark-sql-online-book","text":"=== [[metrics]] Performance Metrics (SQLMetrics) StreamingSymmetricHashJoinExec uses the performance metrics as other stateful physical operators that write to a state store . The following table shows how the performance metrics are computed (and so their exact meaning). [cols=\"30,70\",options=\"header\",width=\"100%\"] |=== | Name (in web UI) | Description | total time to update rows a| [[allUpdatesTimeMs]] Processing time of all rows | total time to remove rows a| [[allRemovalsTimeMs]] | time to commit changes a| [[commitTimeMs]] | number of output rows a| [[numOutputRows]] Total number of output rows | number of total state rows a| [[numTotalStateRows]] | number of updated state rows a| [[numUpdatedStateRows]] < > of the < > and < > OneSideHashJoiners | memory used by state a| [[stateMemory]] |=== === [[shouldRunAnotherBatch]] Checking Out Whether Last Batch Execution Requires Another Non-Data Batch or Not -- shouldRunAnotherBatch Method","title":"Read up on https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Distribution-HashClusteredDistribution.html[HashClusteredDistribution]\u2009in https://bit.ly/spark-sql-internals[The Internals of Spark SQL] online book."},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_5","text":"shouldRunAnotherBatch( newMetadata: OffsetSeqMetadata): Boolean shouldRunAnotherBatch is positive ( true ) when all of the following are positive: Either the < > or < > join state watermark predicates are defined (in the < >) < > threshold (of the StreamingSymmetricHashJoinExec operator) is defined and the current < > threshold of the given OffsetSeqMetadata is above ( greater than ) it, i.e. moved above shouldRunAnotherBatch is negative ( false ) otherwise. shouldRunAnotherBatch is part of the StateStoreWriter abstraction.","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#executing-physical-operator","text":"doExecute () : RDD [ InternalRow ] doExecute is part of the SparkPlan abstraction ( Spark SQL ). doExecute first requests the StreamingQueryManager for the StateStoreCoordinatorRef to the StateStoreCoordinator RPC endpoint (for the driver). doExecute then uses SymmetricHashJoinStateManager utility to < > for the < > and < > sides of the streaming join. In the end, doExecute requests the < > and < > child physical operators to execute (generate an RDD) and then < > with < > (and with the StateStoreCoordinatorRef and the state stores). === [[processPartitions]] Processing Partitions of Left and Right Sides of Stream-Stream Join -- processPartitions Internal Method","title":" Executing Physical Operator"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_6","text":"processPartitions( leftInputIter: Iterator[InternalRow], rightInputIter: Iterator[InternalRow]): Iterator[InternalRow] [[processPartitions-updateStartTimeNs]] processPartitions records the current time (as updateStartTimeNs for the < > performance metric in < >). [[processPartitions-postJoinFilter]] processPartitions creates a new predicate ( postJoinFilter ) based on the bothSides of the < > if defined or true literal. [[processPartitions-leftSideJoiner]] processPartitions creates a < > for the < > and all other properties for the left-hand join side ( leftSideJoiner ). [[processPartitions-rightSideJoiner]] processPartitions creates a < > for the < > and all other properties for the right-hand join side ( rightSideJoiner ). [[processPartitions-leftOutputIter]][[processPartitions-rightOutputIter]] processPartitions requests the OneSideHashJoiner for the left-hand join side to < > with the right-hand side one (that creates a leftOutputIter row iterator) and the OneSideHashJoiner for the right-hand join side to do the same with the left-hand side one (and creates a rightOutputIter row iterator). [[processPartitions-innerOutputCompletionTimeNs]] processPartitions records the current time (as innerOutputCompletionTimeNs for the < > performance metric in < >). [[processPartitions-innerOutputIter]] processPartitions creates a CompletionIterator with the left and right output iterators (with the rows of the leftOutputIter first followed by rightOutputIter ). When no rows are left to process, the CompletionIterator records the completion time. [[processPartitions-outputIter]] processPartitions creates a join-specific output Iterator[InternalRow] of the output rows based on the < > (of the StreamingSymmetricHashJoinExec ): For Inner joins, processPartitions simply uses the < > For LeftOuter joins, processPartitions ... For RightOuter joins, processPartitions ... For other joins, processPartitions simply throws an IllegalArgumentException . [[processPartitions-outputIterWithMetrics]] processPartitions creates an UnsafeProjection for the < > (and the output of the < > and < > child operators) that counts all the rows of the < > (as the < > metric) and generate an output projection. In the end, processPartitions returns a CompletionIterator with with the < > and < > completion function. NOTE: processPartitions is used exclusively when StreamingSymmetricHashJoinExec physical operator is requested to < >. ==== [[processPartitions-onOutputCompletion]][[onOutputCompletion]] Calculating Performance Metrics (Output Completion Callback) -- onOutputCompletion Internal Method","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#source-scala_7","text":"","title":"[source, scala]"},{"location":"physical-operators/StreamingSymmetricHashJoinExec/#onoutputcompletion-unit","text":"onOutputCompletion calculates the < > performance metric (that is the time since the < > was executed). onOutputCompletion adds the time for the inner join to complete (since < > time marker) to the < > performance metric. onOutputCompletion records the time to < > (per the < > for the < > and the < > streaming queries) and adds it to the < > performance metric. NOTE: onOutputCompletion triggers the < > eagerly by iterating over the state rows to be deleted. onOutputCompletion records the time for the < > and < > OneSideHashJoiners to < > that becomes the < > performance metric. onOutputCompletion calculates the < > performance metric (as the < > of the < > and < > streaming queries). onOutputCompletion calculates the < > performance metric (as the sum of the < > in the < > of the < > and < > streaming queries). onOutputCompletion calculates the < > performance metric (as the sum of the < > by the < > and < > of the < > and < > streams). In the end, onOutputCompletion calculates the < >. === [[internal-properties]] Internal Properties [cols=\"30m,70\",options=\"header\",width=\"100%\"] |=== | Name | Description | hadoopConfBcast a| [[hadoopConfBcast]] Hadoop Configuration broadcast (to the Spark cluster) Used exclusively to < > | joinStateManager a| [[joinStateManager]] < > Used when OneSideHashJoiner is requested to < >, < >, < >, and for the < > | nullLeft a| [[nullLeft]] GenericInternalRow of the size of the output schema of the < > | nullRight a| [[nullRight]] GenericInternalRow of the size of the output schema of the < > | storeConf a| [[storeConf]] < > Used exclusively to < > |===","title":"onOutputCompletion: Unit"}]}