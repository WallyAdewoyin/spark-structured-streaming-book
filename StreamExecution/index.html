<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Demystifying inner-workings of Spark Structured Streaming"><link href=https://jaceklaskowski.github.io/spark-structured-streaming-book/StreamExecution/ rel=canonical><meta name=author content="Jacek Laskowski"><link rel="shortcut icon" href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.1.2, mkdocs-material-6.1.0"><title>StreamExecution - The Internals of Spark Structured Streaming</title><link rel=stylesheet href=../assets/stylesheets/main.bc7e593a.min.css><link rel=stylesheet href=../assets/stylesheets/palette.ab28b872.min.css><link href=https://fonts.gstatic.com rel=preconnect crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-151208281-3","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview",document.location.pathname)})</script><script async src=https://www.google-analytics.com/analytics.js></script></head> <body dir=ltr data-md-color-scheme data-md-color-primary=none data-md-color-accent=none> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#streamexecution-stream-execution-engines class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header-nav md-grid" aria-label=Header> <a href=https://jaceklaskowski.github.io/spark-structured-streaming-book/ title="The Internals of Spark Structured Streaming" class="md-header-nav__button md-logo" aria-label="The Internals of Spark Structured Streaming"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 2l-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> <label class="md-header-nav__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header-nav__title data-md-component=header-title> <div class=md-header-nav__ellipsis> <span class="md-header-nav__topic md-ellipsis"> The Internals of Spark Structured Streaming </span> <span class="md-header-nav__topic md-ellipsis"> StreamExecution </span> </div> </div> <label class="md-header-nav__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query data-md-state=active> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <button type=reset class="md-search__icon md-icon" aria-label=Clear data-md-component=search-reset tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header-nav__source> <a href=https://github.com/jaceklaskowski/spark-structured-streaming-book/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> spark-structured-streaming-book </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class="md-tabs md-tabs--active" aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class="md-tabs__link md-tabs__link--active"> Home </a> </li> <li class=md-tabs__item> <a href=../operators/ class=md-tabs__link> Streaming Operators </a> </li> <li class=md-tabs__item> <a href=../datasources/ class=md-tabs__link> Data Sources </a> </li> <li class=md-tabs__item> <a href=../monitoring/StreamingQueryListener/ class=md-tabs__link> Monitoring </a> </li> <li class=md-tabs__item> <a href=../webui/ class=md-tabs__link> Web UI </a> </li> <li class=md-tabs__item> <a href=../demo/spark-sql-streaming-demo-FlatMapGroupsWithStateExec/ class=md-tabs__link> Demos </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=https://jaceklaskowski.github.io/spark-structured-streaming-book/ title="The Internals of Spark Structured Streaming" class="md-nav__button md-logo" aria-label="The Internals of Spark Structured Streaming"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 2l-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5z"/></svg> </a> The Internals of Spark Structured Streaming </label> <div class=md-nav__source> <a href=https://github.com/jaceklaskowski/spark-structured-streaming-book/ title="Go to repository" class=md-source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> spark-structured-streaming-book </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1 type=checkbox id=nav-1 checked> <label class=md-nav__link for=nav-1> Home <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Home data-md-level=1> <label class=md-nav__title for=nav-1> <span class="md-nav__icon md-icon"></span> Home </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> Welcome </a> </li> <li class=md-nav__item> <a href=../spark-structured-streaming/ class=md-nav__link> Spark Structured Streaming and Streaming Queries </a> </li> <li class=md-nav__item> <a href=../StreamingQuery/ class=md-nav__link> StreamingQuery </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-properties/ class=md-nav__link> Configuration Properties </a> </li> <li class=md-nav__item> <a href=../spark-structured-streaming-batch-processing-time/ class=md-nav__link> Batch Processing Time </a> </li> <li class=md-nav__item> <a href=../spark-structured-streaming-internals/ class=md-nav__link> Internals of Streaming Queries </a> </li> <li class=md-nav__item> <a href=../MicroBatchStream/ class=md-nav__link> MicroBatchStream </a> </li> <li class=md-nav__item> <a href=../ContinuousStream/ class=md-nav__link> ContinuousStream </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9 type=checkbox id=nav-1-9> <label class=md-nav__link for=nav-1-9> Streaming Join <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Streaming Join" data-md-level=2> <label class=md-nav__title for=nav-1-9> <span class="md-nav__icon md-icon"></span> Streaming Join </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-join/ class=md-nav__link> Streaming Join </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreAwareZipPartitionsRDD/ class=md-nav__link> StateStoreAwareZipPartitionsRDD </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-3 type=checkbox id=nav-1-9-3> <label class=md-nav__link for=nav-1-9-3> SymmetricHashJoinStateManager <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=SymmetricHashJoinStateManager data-md-level=3> <label class=md-nav__title for=nav-1-9-3> <span class="md-nav__icon md-icon"></span> SymmetricHashJoinStateManager </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-SymmetricHashJoinStateManager/ class=md-nav__link> SymmetricHashJoinStateManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreHandler/ class=md-nav__link> StateStoreHandler </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KeyToNumValuesStore/ class=md-nav__link> KeyToNumValuesStore </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KeyWithIndexToValueStore/ class=md-nav__link> KeyWithIndexToValueStore </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-OneSideHashJoiner/ class=md-nav__link> OneSideHashJoiner </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-JoinStateWatermarkPredicates/ class=md-nav__link> JoinStateWatermarkPredicates </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-JoinStateWatermarkPredicate/ class=md-nav__link> JoinStateWatermarkPredicate </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-9-7 type=checkbox id=nav-1-9-7> <label class=md-nav__link for=nav-1-9-7> StateStoreAwareZipPartitionsHelper <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateStoreAwareZipPartitionsHelper data-md-level=3> <label class=md-nav__title for=nav-1-9-7> <span class="md-nav__icon md-icon"></span> StateStoreAwareZipPartitionsHelper </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreAwareZipPartitionsHelper/ class=md-nav__link> StateStoreAwareZipPartitionsHelper </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingSymmetricHashJoinHelper/ class=md-nav__link> StreamingSymmetricHashJoinHelper </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingJoinHelper/ class=md-nav__link> StreamingJoinHelper </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-10 type=checkbox id=nav-1-10> <label class=md-nav__link for=nav-1-10> Extending Structured Streaming with New Data Sources <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Extending Structured Streaming with New Data Sources" data-md-level=2> <label class=md-nav__title for=nav-1-10> <span class="md-nav__icon md-icon"></span> Extending Structured Streaming with New Data Sources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-extending-new-data-sources/ class=md-nav__link> Extending Structured Streaming with New Data Sources </a> </li> <li class=md-nav__item> <a href=../DataSource/ class=md-nav__link> DataSource </a> </li> <li class=md-nav__item> <a href=../SparkDataStream/ class=md-nav__link> SparkDataStream </a> </li> <li class=md-nav__item> <a href=../SupportsAdmissionControl/ class=md-nav__link> SupportsAdmissionControl </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-11 type=checkbox id=nav-1-11> <label class=md-nav__link for=nav-1-11> Streaming Aggregation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Streaming Aggregation" data-md-level=2> <label class=md-nav__title for=nav-1-11> <span class="md-nav__icon md-icon"></span> Streaming Aggregation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-aggregation/ class=md-nav__link> Streaming Aggregation </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreRDD/ class=md-nav__link> StateStoreRDD </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreOps/ class=md-nav__link> StateStoreOps </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingAggregationStateManager/ class=md-nav__link> StreamingAggregationStateManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingAggregationStateManagerBaseImpl/ class=md-nav__link> StreamingAggregationStateManagerBaseImpl </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingAggregationStateManagerImplV1/ class=md-nav__link> StreamingAggregationStateManagerImplV1 </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingAggregationStateManagerImplV2/ class=md-nav__link> StreamingAggregationStateManagerImplV2 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12 type=checkbox id=nav-1-12> <label class=md-nav__link for=nav-1-12> Stateful Stream Processing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Stateful Stream Processing" data-md-level=2> <label class=md-nav__title for=nav-1-12> <span class="md-nav__icon md-icon"></span> Stateful Stream Processing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-stateful-stream-processing/ class=md-nav__link> Stateful Stream Processing </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-watermark/ class=md-nav__link> Streaming Watermark </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-deduplication/ class=md-nav__link> Streaming Deduplication </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-limit/ class=md-nav__link> Streaming Limit </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-5 type=checkbox id=nav-1-12-5> <label class=md-nav__link for=nav-1-12-5> StateStore <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateStore data-md-level=3> <label class=md-nav__title for=nav-1-12-5> <span class="md-nav__icon md-icon"></span> StateStore </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStore/ class=md-nav__link> StateStore </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreId/ class=md-nav__link> StateStoreId </a> </li> <li class=md-nav__item> <a href=../HDFSBackedStateStore/ class=md-nav__link> HDFSBackedStateStore </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-6 type=checkbox id=nav-1-12-6> <label class=md-nav__link for=nav-1-12-6> StateStoreProvider <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateStoreProvider data-md-level=3> <label class=md-nav__title for=nav-1-12-6> <span class="md-nav__icon md-icon"></span> StateStoreProvider </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreProvider/ class=md-nav__link> StateStoreProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreProviderId/ class=md-nav__link> StateStoreProviderId </a> </li> <li class=md-nav__item> <a href=../HDFSBackedStateStoreProvider/ class=md-nav__link> HDFSBackedStateStoreProvider </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-12-7 type=checkbox id=nav-1-12-7> <label class=md-nav__link for=nav-1-12-7> StateStoreCoordinator <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateStoreCoordinator data-md-level=3> <label class=md-nav__title for=nav-1-12-7> <span class="md-nav__icon md-icon"></span> StateStoreCoordinator </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreCoordinator/ class=md-nav__link> StateStoreCoordinator </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreCoordinatorRef/ class=md-nav__link> StateStoreCoordinatorRef </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-WatermarkSupport/ class=md-nav__link> WatermarkSupport </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StatefulOperatorStateInfo/ class=md-nav__link> StatefulOperatorStateInfo </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreMetrics/ class=md-nav__link> StateStoreMetrics </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreCustomMetric/ class=md-nav__link> StateStoreCustomMetric </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateStoreUpdater/ class=md-nav__link> StateStoreUpdater </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-EventTimeStatsAccum/ class=md-nav__link> EventTimeStatsAccum </a> </li> <li class=md-nav__item> <a href=../StateStoreConf/ class=md-nav__link> StateStoreConf </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-13 type=checkbox id=nav-1-13> <label class=md-nav__link for=nav-1-13> Arbitrary Stateful Streaming Aggregation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Arbitrary Stateful Streaming Aggregation" data-md-level=2> <label class=md-nav__title for=nav-1-13> <span class="md-nav__icon md-icon"></span> Arbitrary Stateful Streaming Aggregation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../arbitrary-stateful-streaming-aggregation/ class=md-nav__link> Arbitrary Stateful Streaming Aggregation </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-13-2 type=checkbox id=nav-1-13-2> <label class=md-nav__link for=nav-1-13-2> KeyValueGroupedDataset <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=KeyValueGroupedDataset data-md-level=3> <label class=md-nav__title for=nav-1-13-2> <span class="md-nav__icon md-icon"></span> KeyValueGroupedDataset </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../KeyValueGroupedDataset/ class=md-nav__link> KeyValueGroupedDataset </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KeyValueGroupedDataset-mapGroupsWithState/ class=md-nav__link> mapGroupsWithState Operator </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-KeyValueGroupedDataset-flatMapGroupsWithState/ class=md-nav__link> flatMapGroupsWithState Operator </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-13-3 type=checkbox id=nav-1-13-3> <label class=md-nav__link for=nav-1-13-3> GroupState <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=GroupState data-md-level=3> <label class=md-nav__title for=nav-1-13-3> <span class="md-nav__icon md-icon"></span> GroupState </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../GroupState/ class=md-nav__link> GroupState </a> </li> <li class=md-nav__item> <a href=../GroupStateImpl/ class=md-nav__link> GroupStateImpl </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-GroupStateTimeout/ class=md-nav__link> GroupStateTimeout </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-13-5 type=checkbox id=nav-1-13-5> <label class=md-nav__link for=nav-1-13-5> StateManager <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StateManager data-md-level=3> <label class=md-nav__title for=nav-1-13-5> <span class="md-nav__icon md-icon"></span> StateManager </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-StateManager/ class=md-nav__link> StateManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateManagerImplV2/ class=md-nav__link> StateManagerImplV2 </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateManagerImplBase/ class=md-nav__link> StateManagerImplBase </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StateManagerImplV1/ class=md-nav__link> StateManagerImplV1 </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-FlatMapGroupsWithStateExecHelper/ class=md-nav__link> FlatMapGroupsWithStateExecHelper Helper Class </a> </li> <li class=md-nav__item> <a href=../InputProcessor/ class=md-nav__link> InputProcessor </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../StreamingQueryManager/ class=md-nav__link> StreamingQueryManager </a> </li> <li class=md-nav__item> <a href=../StreamingQueryListenerBus/ class=md-nav__link> StreamingQueryListenerBus </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-16 type=checkbox id=nav-1-16> <label class=md-nav__link for=nav-1-16> Developing Streaming Applications <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Developing Streaming Applications" data-md-level=2> <label class=md-nav__title for=nav-1-16> <span class="md-nav__icon md-icon"></span> Developing Streaming Applications </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../DataStreamReader/ class=md-nav__link> DataStreamReader </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-16-2 type=checkbox id=nav-1-16-2> <label class=md-nav__link for=nav-1-16-2> DataStreamWriter <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=DataStreamWriter data-md-level=3> <label class=md-nav__title for=nav-1-16-2> <span class="md-nav__icon md-icon"></span> DataStreamWriter </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../DataStreamWriter/ class=md-nav__link> DataStreamWriter </a> </li> <li class=md-nav__item> <a href=../OutputMode/ class=md-nav__link> OutputMode </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-Trigger/ class=md-nav__link> Trigger </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-window/ class=md-nav__link> window Function </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-17 type=checkbox id=nav-1-17 checked> <label class=md-nav__link for=nav-1-17> Query Planning and Execution <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Query Planning and Execution" data-md-level=2> <label class=md-nav__title for=nav-1-17> <span class="md-nav__icon md-icon"></span> Query Planning and Execution </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> StreamExecution <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> StreamExecution </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#source-scala class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#logicalplan-logicalplan class=md-nav__link> logicalPlan: LogicalPlan </a> </li> <li class=md-nav__item> <a href=#source-scala_1 class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#source-scala_2 class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#source-scala_3 class=md-nav__link> [source, scala] </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-TriggerExecutor/ class=md-nav__link> TriggerExecutor </a> </li> <li class=md-nav__item> <a href=../IncrementalExecution/ class=md-nav__link> IncrementalExecution </a> </li> <li class=md-nav__item> <a href=../StreamMetadata/ class=md-nav__link> StreamMetadata </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-17-5 type=checkbox id=nav-1-17-5> <label class=md-nav__link for=nav-1-17-5> Logical Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Logical Operators" data-md-level=3> <label class=md-nav__title for=nav-1-17-5> <span class="md-nav__icon md-icon"></span> Logical Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../EventTimeWatermark/ class=md-nav__link> EventTimeWatermark </a> </li> <li class=md-nav__item> <a href=../logical-operators/FlatMapGroupsWithState/ class=md-nav__link> FlatMapGroupsWithState </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-Deduplicate/ class=md-nav__link> Deduplicate </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemoryPlan/ class=md-nav__link> MemoryPlan </a> </li> <li class=md-nav__item> <a href=../StreamingDataSourceV2Relation/ class=md-nav__link> StreamingDataSourceV2Relation </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingRelation/ class=md-nav__link> StreamingRelation </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingRelationV2/ class=md-nav__link> StreamingRelationV2 </a> </li> <li class=md-nav__item> <a href=../StreamingExecutionRelation/ class=md-nav__link> StreamingExecutionRelation </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-17-6 type=checkbox id=nav-1-17-6> <label class=md-nav__link for=nav-1-17-6> Physical Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Physical Operators" data-md-level=3> <label class=md-nav__title for=nav-1-17-6> <span class="md-nav__icon md-icon"></span> Physical Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../physical-operators/EventTimeWatermarkExec/ class=md-nav__link> EventTimeWatermarkExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/FlatMapGroupsWithStateExec/ class=md-nav__link> FlatMapGroupsWithStateExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StatefulOperator/ class=md-nav__link> StatefulOperator </a> </li> <li class=md-nav__item> <a href=../physical-operators/StateStoreReader/ class=md-nav__link> StateStoreReader </a> </li> <li class=md-nav__item> <a href=../physical-operators/StateStoreRestoreExec/ class=md-nav__link> StateStoreRestoreExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StateStoreSaveExec/ class=md-nav__link> StateStoreSaveExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StateStoreWriter/ class=md-nav__link> StateStoreWriter </a> </li> <li class=md-nav__item> <a href=../physical-operators/StreamingDeduplicateExec/ class=md-nav__link> StreamingDeduplicateExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StreamingGlobalLimitExec/ class=md-nav__link> StreamingGlobalLimitExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StreamingRelationExec/ class=md-nav__link> StreamingRelationExec </a> </li> <li class=md-nav__item> <a href=../physical-operators/StreamingSymmetricHashJoinExec/ class=md-nav__link> StreamingSymmetricHashJoinExec </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-17-7 type=checkbox id=nav-1-17-7> <label class=md-nav__link for=nav-1-17-7> Execution Planning Strategies <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Execution Planning Strategies" data-md-level=3> <label class=md-nav__title for=nav-1-17-7> <span class="md-nav__icon md-icon"></span> Execution Planning Strategies </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-FlatMapGroupsWithStateStrategy/ class=md-nav__link> FlatMapGroupsWithStateStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StatefulAggregationStrategy/ class=md-nav__link> StatefulAggregationStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingDeduplicationStrategy/ class=md-nav__link> StreamingDeduplicationStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingGlobalLimitStrategy/ class=md-nav__link> StreamingGlobalLimitStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingJoinStrategy/ class=md-nav__link> StreamingJoinStrategy </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamingRelationStrategy/ class=md-nav__link> StreamingRelationStrategy </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../StreamingQueryWrapper/ class=md-nav__link> StreamingQueryWrapper </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-18 type=checkbox id=nav-1-18> <label class=md-nav__link for=nav-1-18> Offsets and Metadata Checkpointing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Offsets and Metadata Checkpointing" data-md-level=2> <label class=md-nav__title for=nav-1-18> <span class="md-nav__icon md-icon"></span> Offsets and Metadata Checkpointing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-offsets-and-metadata-checkpointing/ class=md-nav__link> Offsets and Metadata Checkpointing </a> </li> <li class=md-nav__item> <a href=../MetadataLog/ class=md-nav__link> MetadataLog </a> </li> <li class=md-nav__item> <a href=../HDFSMetadataLog/ class=md-nav__link> HDFSMetadataLog </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-18-4 type=checkbox id=nav-1-18-4> <label class=md-nav__link for=nav-1-18-4> CommitLog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=CommitLog data-md-level=3> <label class=md-nav__title for=nav-1-18-4> <span class="md-nav__icon md-icon"></span> CommitLog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-CommitLog/ class=md-nav__link> CommitLog </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-CommitMetadata/ class=md-nav__link> CommitMetadata </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-18-5 type=checkbox id=nav-1-18-5> <label class=md-nav__link for=nav-1-18-5> OffsetSeqLog <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=OffsetSeqLog data-md-level=3> <label class=md-nav__title for=nav-1-18-5> <span class="md-nav__icon md-icon"></span> OffsetSeqLog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-OffsetSeqLog/ class=md-nav__link> OffsetSeqLog </a> </li> <li class=md-nav__item> <a href=../OffsetSeq/ class=md-nav__link> OffsetSeq </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-OffsetSeqMetadata/ class=md-nav__link> OffsetSeqMetadata </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-18-7 type=checkbox id=nav-1-18-7> <label class=md-nav__link for=nav-1-18-7> CheckpointFileManager <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=CheckpointFileManager data-md-level=3> <label class=md-nav__title for=nav-1-18-7> <span class="md-nav__icon md-icon"></span> CheckpointFileManager </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../CheckpointFileManager/ class=md-nav__link> CheckpointFileManager </a> </li> <li class=md-nav__item> <a href=../FileContextBasedCheckpointFileManager/ class=md-nav__link> FileContextBasedCheckpointFileManager </a> </li> <li class=md-nav__item> <a href=../FileSystemBasedCheckpointFileManager/ class=md-nav__link> FileSystemBasedCheckpointFileManager </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-Offset/ class=md-nav__link> Offset </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-StreamProgress/ class=md-nav__link> StreamProgress </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-19 type=checkbox id=nav-1-19> <label class=md-nav__link for=nav-1-19> Micro-Batch Stream Processing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Micro-Batch Stream Processing" data-md-level=2> <label class=md-nav__title for=nav-1-19> <span class="md-nav__icon md-icon"></span> Micro-Batch Stream Processing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../micro-batch-stream-processing/ class=md-nav__link> Micro-Batch Stream Processing </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-19-2 type=checkbox id=nav-1-19-2> <label class=md-nav__link for=nav-1-19-2> MicroBatchExecution <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=MicroBatchExecution data-md-level=3> <label class=md-nav__title for=nav-1-19-2> <span class="md-nav__icon md-icon"></span> MicroBatchExecution </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../MicroBatchExecution/ class=md-nav__link> MicroBatchExecution </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MicroBatchWriter/ class=md-nav__link> MicroBatchWriter </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-19-3 type=checkbox id=nav-1-19-3> <label class=md-nav__link for=nav-1-19-3> MicroBatchReadSupport <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=MicroBatchReadSupport data-md-level=3> <label class=md-nav__title for=nav-1-19-3> <span class="md-nav__icon md-icon"></span> MicroBatchReadSupport </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../MicroBatchReadSupport/ class=md-nav__link> MicroBatchReadSupport </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MicroBatchReader/ class=md-nav__link> MicroBatchReader </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-WatermarkTracker/ class=md-nav__link> WatermarkTracker </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-19-5 type=checkbox id=nav-1-19-5> <label class=md-nav__link for=nav-1-19-5> Source <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Source data-md-level=3> <label class=md-nav__title for=nav-1-19-5> <span class="md-nav__icon md-icon"></span> Source </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Source/ class=md-nav__link> Source </a> </li> <li class=md-nav__item> <a href=../StreamSourceProvider/ class=md-nav__link> StreamSourceProvider </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../Sink/ class=md-nav__link> Sink </a> </li> <li class=md-nav__item> <a href=../StreamSinkProvider/ class=md-nav__link> StreamSinkProvider </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-20 type=checkbox id=nav-1-20> <label class=md-nav__link for=nav-1-20> Continuous Stream Processing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Continuous Stream Processing" data-md-level=2> <label class=md-nav__title for=nav-1-20> <span class="md-nav__icon md-icon"></span> Continuous Stream Processing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-continuous-stream-processing/ class=md-nav__link> Continuous Stream Processing </a> </li> <li class=md-nav__item> <a href=../ContinuousExecution/ class=md-nav__link> ContinuousExecution </a> </li> <li class=md-nav__item> <a href=../ContinuousReadSupport/ class=md-nav__link> ContinuousReadSupport </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousReader/ class=md-nav__link> ContinuousReader </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-RateStreamContinuousReader/ class=md-nav__link> RateStreamContinuousReader </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-20-6 type=checkbox id=nav-1-20-6> <label class=md-nav__link for=nav-1-20-6> EpochCoordinator <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=EpochCoordinator data-md-level=3> <label class=md-nav__title for=nav-1-20-6> <span class="md-nav__icon md-icon"></span> EpochCoordinator </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-EpochCoordinator/ class=md-nav__link> EpochCoordinator RPC Endpoint </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-EpochCoordinatorRef/ class=md-nav__link> EpochCoordinatorRef </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-EpochTracker/ class=md-nav__link> EpochTracker </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-20-7 type=checkbox id=nav-1-20-7> <label class=md-nav__link for=nav-1-20-7> ContinuousQueuedDataReader <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=ContinuousQueuedDataReader data-md-level=3> <label class=md-nav__title for=nav-1-20-7> <span class="md-nav__icon md-icon"></span> ContinuousQueuedDataReader </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousQueuedDataReader/ class=md-nav__link> ContinuousQueuedDataReader </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousQueuedDataReader-DataReaderThread/ class=md-nav__link> DataReaderThread </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousQueuedDataReader-EpochMarkerGenerator/ class=md-nav__link> EpochMarkerGenerator </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-PartitionOffset/ class=md-nav__link> PartitionOffset </a> </li> <li class=md-nav__item> <a href=../ContinuousExecutionRelation/ class=md-nav__link> ContinuousExecutionRelation </a> </li> <li class=md-nav__item> <a href=../WriteToContinuousDataSource/ class=md-nav__link> WriteToContinuousDataSource </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-1-20-11 type=checkbox id=nav-1-20-11> <label class=md-nav__link for=nav-1-20-11> WriteToContinuousDataSourceExec <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=WriteToContinuousDataSourceExec data-md-level=3> <label class=md-nav__title for=nav-1-20-11> <span class="md-nav__icon md-icon"></span> WriteToContinuousDataSourceExec </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-WriteToContinuousDataSourceExec/ class=md-nav__link> WriteToContinuousDataSourceExec Unary Physical Operator </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousWriteRDD/ class=md-nav__link> ContinuousWriteRDD </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousDataSourceRDD/ class=md-nav__link> ContinuousDataSourceRDD </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-UnsupportedOperationChecker/ class=md-nav__link> UnsupportedOperationChecker </a> </li> <li class=md-nav__item> <a href=../SQLConf/ class=md-nav__link> SQLConf </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-2 type=checkbox id=nav-2> <label class=md-nav__link for=nav-2> Streaming Operators <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Streaming Operators" data-md-level=1> <label class=md-nav__title for=nav-2> <span class="md-nav__icon md-icon"></span> Streaming Operators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../operators/ class=md-nav__link> Streaming Operators </a> </li> <li class=md-nav__item> <a href=../operators/crossJoin/ class=md-nav__link> crossJoin </a> </li> <li class=md-nav__item> <a href=../operators/dropDuplicates/ class=md-nav__link> dropDuplicates </a> </li> <li class=md-nav__item> <a href=../operators/explain/ class=md-nav__link> explain </a> </li> <li class=md-nav__item> <a href=../operators/flatMapGroupsWithState/ class=md-nav__link> flatMapGroupsWithState </a> </li> <li class=md-nav__item> <a href=../operators/groupBy/ class=md-nav__link> groupBy </a> </li> <li class=md-nav__item> <a href=../operators/groupByKey/ class=md-nav__link> groupByKey </a> </li> <li class=md-nav__item> <a href=../operators/join/ class=md-nav__link> join </a> </li> <li class=md-nav__item> <a href=../operators/joinWith/ class=md-nav__link> joinWith </a> </li> <li class=md-nav__item> <a href=../operators/withWatermark/ class=md-nav__link> withWatermark </a> </li> <li class=md-nav__item> <a href=../operators/writeStream/ class=md-nav__link> writeStream </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3 type=checkbox id=nav-3> <label class=md-nav__link for=nav-3> Data Sources <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Data Sources" data-md-level=1> <label class=md-nav__title for=nav-3> <span class="md-nav__icon md-icon"></span> Data Sources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/ class=md-nav__link> Data Sources </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-2 type=checkbox id=nav-3-2> <label class=md-nav__link for=nav-3-2> File <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=File data-md-level=2> <label class=md-nav__title for=nav-3-2> <span class="md-nav__icon md-icon"></span> File </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/file/ class=md-nav__link> File Data Source </a> </li> <li class=md-nav__item> <a href=../datasources/file/FileStreamSource/ class=md-nav__link> FileStreamSource </a> </li> <li class=md-nav__item> <a href=../datasources/file/FileStreamSink/ class=md-nav__link> FileStreamSink </a> </li> <li class=md-nav__item> <a href=../datasources/file/CompactibleFileStreamLog/ class=md-nav__link> CompactibleFileStreamLog </a> </li> <li class=md-nav__item> <a href=../datasources/file/SinkFileStatus/ class=md-nav__link> SinkFileStatus </a> </li> <li class=md-nav__item> <a href=../datasources/file/ManifestFileCommitProtocol/ class=md-nav__link> ManifestFileCommitProtocol </a> </li> <li class=md-nav__item> <a href=../datasources/file/MetadataLogFileIndex/ class=md-nav__link> MetadataLogFileIndex </a> </li> <li class=md-nav__item> <a href=../datasources/file/FileStreamSourceCleaner/ class=md-nav__link> FileStreamSourceCleaner </a> </li> <li class=md-nav__item> <a href=../datasources/file/FileStreamSourceLog/ class=md-nav__link> FileStreamSourceLog </a> </li> <li class=md-nav__item> <a href=../datasources/file/FileStreamSinkLog/ class=md-nav__link> FileStreamSinkLog </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-3 type=checkbox id=nav-3-3> <label class=md-nav__link for=nav-3-3> Kafka <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Kafka data-md-level=2> <label class=md-nav__title for=nav-3-3> <span class="md-nav__icon md-icon"></span> Kafka </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/kafka/ class=md-nav__link> Kafka Data Source </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceProvider/ class=md-nav__link> KafkaSourceProvider </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaTable/ class=md-nav__link> KafkaTable </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSource/ class=md-nav__link> KafkaSource </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaRelation/ class=md-nav__link> KafkaRelation </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceRDD/ class=md-nav__link> KafkaSourceRDD </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/CachedKafkaConsumer/ class=md-nav__link> CachedKafkaConsumer </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceOffset/ class=md-nav__link> KafkaSourceOffset </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaOffsetReader/ class=md-nav__link> KafkaOffsetReader </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/ConsumerStrategy/ class=md-nav__link> ConsumerStrategy </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSink/ class=md-nav__link> KafkaSink </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaBatch/ class=md-nav__link> KafkaBatch </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaScan/ class=md-nav__link> KafkaScan </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaOffsetRangeLimit/ class=md-nav__link> KafkaOffsetRangeLimit </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaDataConsumer/ class=md-nav__link> KafkaDataConsumer </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-3-16 type=checkbox id=nav-3-3-16> <label class=md-nav__link for=nav-3-3-16> KafkaMicroBatchReader <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=KafkaMicroBatchReader data-md-level=3> <label class=md-nav__title for=nav-3-3-16> <span class="md-nav__icon md-icon"></span> KafkaMicroBatchReader </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/kafka/KafkaMicroBatchReader/ class=md-nav__link> KafkaMicroBatchReader </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaOffsetRangeCalculator/ class=md-nav__link> KafkaOffsetRangeCalculator </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaMicroBatchInputPartition/ class=md-nav__link> KafkaMicroBatchInputPartition </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaMicroBatchInputPartitionReader/ class=md-nav__link> KafkaMicroBatchInputPartitionReader </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaSourceInitialOffsetWriter/ class=md-nav__link> KafkaSourceInitialOffsetWriter </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-3-17 type=checkbox id=nav-3-3-17> <label class=md-nav__link for=nav-3-3-17> KafkaContinuousReader <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=KafkaContinuousReader data-md-level=3> <label class=md-nav__title for=nav-3-3-17> <span class="md-nav__icon md-icon"></span> KafkaContinuousReader </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../datasources/kafka/KafkaContinuousReader/ class=md-nav__link> KafkaContinuousReader </a> </li> <li class=md-nav__item> <a href=../datasources/kafka/KafkaContinuousInputPartition/ class=md-nav__link> KafkaContinuousInputPartition </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-4 type=checkbox id=nav-3-4> <label class=md-nav__link for=nav-3-4> Socket <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Socket data-md-level=2> <label class=md-nav__title for=nav-3-4> <span class="md-nav__icon md-icon"></span> Socket </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-TextSocketSourceProvider/ class=md-nav__link> TextSocketSourceProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-TextSocketSource/ class=md-nav__link> TextSocketSource </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-5 type=checkbox id=nav-3-5> <label class=md-nav__link for=nav-3-5> Rate <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Rate data-md-level=2> <label class=md-nav__title for=nav-3-5> <span class="md-nav__icon md-icon"></span> Rate </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-RateSourceProvider/ class=md-nav__link> RateSourceProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-RateStreamProvider/ class=md-nav__link> RateStreamProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-RateStreamSource/ class=md-nav__link> RateStreamSource </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-RateStreamMicroBatchReader/ class=md-nav__link> RateStreamMicroBatchReader </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-6 type=checkbox id=nav-3-6> <label class=md-nav__link for=nav-3-6> Console Sink <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Console Sink" data-md-level=2> <label class=md-nav__title for=nav-3-6> <span class="md-nav__icon md-icon"></span> Console Sink </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-ConsoleSinkProvider/ class=md-nav__link> ConsoleSinkProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ConsoleWriter/ class=md-nav__link> ConsoleWriter </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-7 type=checkbox id=nav-3-7> <label class=md-nav__link for=nav-3-7> Foreach Sink <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Foreach Sink" data-md-level=2> <label class=md-nav__title for=nav-3-7> <span class="md-nav__icon md-icon"></span> Foreach Sink </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-ForeachWriterProvider/ class=md-nav__link> ForeachWriterProvider </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ForeachWriter/ class=md-nav__link> ForeachWriter </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ForeachSink/ class=md-nav__link> ForeachSink </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-8 type=checkbox id=nav-3-8> <label class=md-nav__link for=nav-3-8> ForeachBatch Sink <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="ForeachBatch Sink" data-md-level=2> <label class=md-nav__title for=nav-3-8> <span class="md-nav__icon md-icon"></span> ForeachBatch Sink </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-ForeachBatchSink/ class=md-nav__link> ForeachBatchSink </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-9 type=checkbox id=nav-3-9> <label class=md-nav__link for=nav-3-9> Memory <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Memory data-md-level=2> <label class=md-nav__title for=nav-3-9> <span class="md-nav__icon md-icon"></span> Memory </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-memory-data-source/ class=md-nav__link> Memory Data Source </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemoryStream/ class=md-nav__link> MemoryStream </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-ContinuousMemoryStream/ class=md-nav__link> ContinuousMemoryStream </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemorySink/ class=md-nav__link> MemorySink </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-3-9-5 type=checkbox id=nav-3-9-5> <label class=md-nav__link for=nav-3-9-5> MemorySinkV2 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=MemorySinkV2 data-md-level=3> <label class=md-nav__title for=nav-3-9-5> <span class="md-nav__icon md-icon"></span> MemorySinkV2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../spark-sql-streaming-MemorySinkV2/ class=md-nav__link> MemorySinkV2 </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemoryStreamWriter/ class=md-nav__link> MemoryStreamWriter </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemoryStreamBase/ class=md-nav__link> MemoryStreamBase </a> </li> <li class=md-nav__item> <a href=../spark-sql-streaming-MemorySinkBase/ class=md-nav__link> MemorySinkBase </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4 type=checkbox id=nav-4> <label class=md-nav__link for=nav-4> Monitoring <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Monitoring data-md-level=1> <label class=md-nav__title for=nav-4> <span class="md-nav__icon md-icon"></span> Monitoring </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../monitoring/StreamingQueryListener/ class=md-nav__link> StreamingQueryListener </a> </li> <li class=md-nav__item> <a href=../monitoring/ProgressReporter/ class=md-nav__link> ProgressReporter </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-4-3 type=checkbox id=nav-4-3> <label class=md-nav__link for=nav-4-3> StreamingQueryProgress <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=StreamingQueryProgress data-md-level=2> <label class=md-nav__title for=nav-4-3> <span class="md-nav__icon md-icon"></span> StreamingQueryProgress </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../monitoring/StreamingQueryProgress/ class=md-nav__link> StreamingQueryProgress </a> </li> <li class=md-nav__item> <a href=../monitoring/StateOperatorProgress/ class=md-nav__link> StateOperatorProgress </a> </li> <li class=md-nav__item> <a href=../monitoring/ExecutionStats/ class=md-nav__link> ExecutionStats </a> </li> <li class=md-nav__item> <a href=../monitoring/SourceProgress/ class=md-nav__link> SourceProgress </a> </li> <li class=md-nav__item> <a href=../monitoring/SinkProgress/ class=md-nav__link> SinkProgress </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../monitoring/StreamingQueryStatus/ class=md-nav__link> StreamingQueryStatus </a> </li> <li class=md-nav__item> <a href=../monitoring/MetricsReporter/ class=md-nav__link> MetricsReporter </a> </li> <li class=md-nav__item> <a href=../spark-logging/ class=md-nav__link> Logging </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-5 type=checkbox id=nav-5> <label class=md-nav__link for=nav-5> Web UI <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Web UI" data-md-level=1> <label class=md-nav__title for=nav-5> <span class="md-nav__icon md-icon"></span> Web UI </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../webui/ class=md-nav__link> Web UI </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=nav-6 type=checkbox id=nav-6> <label class=md-nav__link for=nav-6> Demos <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Demos data-md-level=1> <label class=md-nav__title for=nav-6> <span class="md-nav__icon md-icon"></span> Demos </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../demo/spark-sql-streaming-demo-FlatMapGroupsWithStateExec/ class=md-nav__link> Internals of FlatMapGroupsWithStateExec Physical Operator </a> </li> <li class=md-nav__item> <a href=../demo/arbitrary-stateful-streaming-aggregation-flatMapGroupsWithState/ class=md-nav__link> Arbitrary Stateful Streaming Aggregation with KeyValueGroupedDataset.flatMapGroupsWithState Operator </a> </li> <li class=md-nav__item> <a href=../demo/exploring-checkpointed-state/ class=md-nav__link> Exploring Checkpointed State </a> </li> <li class=md-nav__item> <a href=../demo/watermark-aggregation-append/ class=md-nav__link> Streaming Watermark with Aggregation in Append Output Mode </a> </li> <li class=md-nav__item> <a href=../demo/groupBy-running-count-complete/ class=md-nav__link> Streaming Query for Running Counts (Socket Source and Complete Output Mode) </a> </li> <li class=md-nav__item> <a href=../demo/kafka-data-source/ class=md-nav__link> Streaming Aggregation with Kafka Data Source </a> </li> <li class=md-nav__item> <a href=../demo/groupByKey-count-Update/ class=md-nav__link> groupByKey Streaming Aggregation in Update Mode </a> </li> <li class=md-nav__item> <a href=../demo/StateStoreSaveExec-Complete/ class=md-nav__link> StateStoreSaveExec with Complete Output Mode </a> </li> <li class=md-nav__item> <a href=../demo/StateStoreSaveExec-Update/ class=md-nav__link> StateStoreSaveExec with Update Output Mode </a> </li> <li class=md-nav__item> <a href=../demo/custom-sink-webui/ class=md-nav__link> Developing Custom Streaming Sink (and Monitoring SQL Queries in web UI) </a> </li> <li class=md-nav__item> <a href=../demo/current_timestamp/ class=md-nav__link> current_timestamp Function For Processing Time in Streaming Queries </a> </li> <li class=md-nav__item> <a href=../demo/StreamingQueryManager-awaitAnyTermination-resetTerminated/ class=md-nav__link> Using StreamingQueryManager for Query Termination Management </a> </li> <li class=md-nav__item> <a href=../demo/using-file-streaming-source/ class=md-nav__link> Using File Streaming Source </a> </li> <li class=md-nav__item> <a href=../demo/deep-dive-into-filestreamsink/ class=md-nav__link> Deep Dive into FileStreamSink </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=#source-scala class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#logicalplan-logicalplan class=md-nav__link> logicalPlan: LogicalPlan </a> </li> <li class=md-nav__item> <a href=#source-scala_1 class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#source-scala_2 class=md-nav__link> [source, scala] </a> </li> <li class=md-nav__item> <a href=#source-scala_3 class=md-nav__link> [source, scala] </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content> <article class="md-content__inner md-typeset"> <a href=https://github.com/jaceklaskowski/spark-structured-streaming-book/edit/mkdocs-material/docs/StreamExecution.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=streamexecution-stream-execution-engines>StreamExecution &mdash; Stream Execution Engines<a class=headerlink href=#streamexecution-stream-execution-engines title="Permanent link">&para;</a></h1> <p><code>StreamExecution</code> is the &lt;<contract, base>&gt; of &lt;<extensions, stream execution engines>&gt; (aka <em>streaming query processing engines</em>) that can &lt;<runactivatedstream, run>&gt; a &lt;<logicalplan, structured query>&gt; (on a &lt;<queryexecutionthread, stream execution thread>&gt;).</p> <p>NOTE: <em>Continuous query</em>, <em>streaming query</em>, <em>continuous Dataset</em>, <em>streaming Dataset</em> are all considered high-level synonyms for an executable entity that stream execution engines run using the &lt;<logicalplan, analyzed logical plan>&gt; internally.</p> <p>[[contract]] .StreamExecution Contract (Abstract Methods Only) [cols="30m,70",options="header",width="100%"] |=== | Property | Description</p> <p>| logicalPlan a| [[logicalPlan]]</p> <h2 id=source-scala>[source, scala]<a class=headerlink href=#source-scala title="Permanent link">&para;</a></h2> <h2 id=logicalplan-logicalplan>logicalPlan: LogicalPlan<a class=headerlink href=#logicalplan-logicalplan title="Permanent link">&para;</a></h2> <p>Analyzed logical plan of the streaming query to execute</p> <p>Used when <code>StreamExecution</code> is requested to &lt;<runstream, run stream processing>&gt;</p> <p><code>logicalPlan</code> is part of the <a href=../monitoring/ProgressReporter/#logicalPlan>ProgressReporter</a> abstraction.</p> <p>| runActivatedStream a| [[runActivatedStream]]</p> <h2 id=source-scala_1>[source, scala]<a class=headerlink href=#source-scala_1 title="Permanent link">&para;</a></h2> <p>runActivatedStream( sparkSessionForStream: SparkSession): Unit</p> <hr> <p>Executes (<em>runs</em>) the activated &lt;<streamingquery.md#, streaming query>&gt;</p> <p>Used exclusively when <code>StreamExecution</code> is requested to &lt;<runstream, run the streaming query>&gt; (when transitioning from <code>INITIALIZING</code> to <code>ACTIVE</code> state)</p> <p>|===</p> <p>.Streaming Query and Stream Execution Engine [source, scala]</p> <hr> <p>import org.apache.spark.sql.streaming.StreamingQuery assert(sq.isInstanceOf[StreamingQuery])</p> <p>import org.apache.spark.sql.execution.streaming.StreamingQueryWrapper val se = sq.asInstanceOf[StreamingQueryWrapper].streamingQuery</p> <p>scala&gt; :type se org.apache.spark.sql.execution.streaming.StreamExecution</p> <hr> <p>[[minLogEntriesToMaintain]][[spark.sql.streaming.minBatchesToRetain]] <code>StreamExecution</code> uses the &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.minbatchestoretain, spark.sql.streaming.minbatchestoretain>&gt; configuration property to allow the &lt;<extensions, streamexecutions>&gt; to discard old log entries (from the &lt;<offsetlog, offset>&gt; and &lt;<commitlog, commit>&gt; logs).</p> <p>[[extensions]] .StreamExecutions [cols="30,70",options="header",width="100%"] |=== | StreamExecution | Description</p> <p>| &lt;<continuousexecution.md#, continuousexecution>&gt; | [[ContinuousExecution]] Used in &lt;<spark-sql-streaming-continuous-stream-processing.md#, continuous stream processing>&gt;</p> <p>| &lt;<microbatchexecution.md#, microbatchexecution>&gt; | [[MicroBatchExecution]] Used in &lt;<micro-batch-stream-processing.md#, micro-batch stream processing>&gt; |===</p> <p>NOTE: <code>StreamExecution</code> does not support adaptive query execution and cost-based optimizer (and turns them off when requested to &lt;<runstream, run stream processing>&gt;).</p> <p><code>StreamExecution</code> is the <em>execution environment</em> of a StreamingQuery.md[single streaming query] (aka <em>streaming Dataset</em>) that is executed every &lt;<trigger, trigger>&gt; and in the end &lt;<microbatchexecution.md#runbatch-addbatch, adds the results to a sink>&gt;.</p> <p>NOTE: <code>StreamExecution</code> corresponds to a StreamingQuery.md[single streaming query] with one or more <a href=../Source/ >streaming sources</a> and exactly one <a href=../Sink/ >streaming sink</a>.</p> <h2 id=source-scala_2>[source, scala]<a class=headerlink href=#source-scala_2 title="Permanent link">&para;</a></h2> <p>import org.apache.spark.sql.streaming.Trigger import scala.concurrent.duration._ val q = spark. readStream. format("rate"). load. writeStream. format("console"). trigger(Trigger.ProcessingTime(10.minutes)). start scala&gt; :type q org.apache.spark.sql.streaming.StreamingQuery</p> <p>// Pull out StreamExecution off StreamingQueryWrapper import org.apache.spark.sql.execution.streaming.{StreamExecution, StreamingQueryWrapper} val se = q.asInstanceOf[StreamingQueryWrapper].streamingQuery scala&gt; :type se org.apache.spark.sql.execution.streaming.StreamExecution</p> <hr> <p><img alt="Creating Instance of StreamExecution" src=../images/StreamExecution-creating-instance.png></p> <p>When &lt;<start, started>&gt;, <code>StreamExecution</code> starts a &lt;<queryexecutionthread, stream execution thread>&gt; that simply &lt;<runstream, runs stream processing>&gt; (and hence the streaming query).</p> <p>.StreamExecution's Starting Streaming Query (on Execution Thread) image::images/StreamExecution-start.png[align="center"]</p> <p><code>StreamExecution</code> is a <a href=../monitoring/ProgressReporter/ >ProgressReporter</a> and &lt;<postevent, reports status of the streaming query>&gt; (i.e. when it starts, progresses and terminates) by posting <code>StreamingQueryListener</code> events.</p> <div class=highlight><pre><span></span><code>import org.apache.spark.sql.streaming.Trigger
import scala.concurrent.duration._
val sq = spark
  .readStream
  .text(&quot;server-logs&quot;)
  .writeStream
  .format(&quot;console&quot;)
  .queryName(&quot;debug&quot;)
  .trigger(Trigger.ProcessingTime(20.seconds))
  .start

// Enable the log level to see the INFO and DEBUG messages
// log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=DEBUG

17/06/18 21:21:07 INFO StreamExecution: Starting new streaming query.
17/06/18 21:21:07 DEBUG StreamExecution: getOffset took 5 ms
17/06/18 21:21:07 DEBUG StreamExecution: Stream running from {} to {}
17/06/18 21:21:07 DEBUG StreamExecution: triggerExecution took 9 ms
17/06/18 21:21:07 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())
17/06/18 21:21:07 INFO StreamExecution: Streaming query made progress: {
  &quot;id&quot; : &quot;8b57b0bd-fc4a-42eb-81a3-777d7ba5e370&quot;,
  &quot;runId&quot; : &quot;920b227e-6d02-4a03-a271-c62120258cea&quot;,
  &quot;name&quot; : &quot;debug&quot;,
  &quot;timestamp&quot; : &quot;2017-06-18T19:21:07.693Z&quot;,
  &quot;numInputRows&quot; : 0,
  &quot;processedRowsPerSecond&quot; : 0.0,
  &quot;durationMs&quot; : {
    &quot;getOffset&quot; : 5,
    &quot;triggerExecution&quot; : 9
  },
  &quot;stateOperators&quot; : [ ],
  &quot;sources&quot; : [ {
    &quot;description&quot; : &quot;FileStreamSource[file:/Users/jacek/dev/oss/spark/server-logs]&quot;,
    &quot;startOffset&quot; : null,
    &quot;endOffset&quot; : null,
    &quot;numInputRows&quot; : 0,
    &quot;processedRowsPerSecond&quot; : 0.0
  } ],
  &quot;sink&quot; : {
    &quot;description&quot; : &quot;org.apache.spark.sql.execution.streaming.ConsoleSink@2460208a&quot;
  }
}
17/06/18 21:21:10 DEBUG StreamExecution: Starting Trigger Calculation
17/06/18 21:21:10 DEBUG StreamExecution: getOffset took 3 ms
17/06/18 21:21:10 DEBUG StreamExecution: triggerExecution took 3 ms
17/06/18 21:21:10 DEBUG StreamExecution: Execution stats: ExecutionStats(Map(),List(),Map())
</code></pre></div> <p><code>StreamExecution</code> tracks streaming data sources in &lt;<uniquesources, uniquesources>&gt; internal registry.</p> <p>.StreamExecution's uniqueSources Registry of Streaming Data Sources image::images/StreamExecution-uniqueSources.png[align="center"]</p> <p><code>StreamExecution</code> collects <code>durationMs</code> for the execution units of streaming batches.</p> <p>.StreamExecution's durationMs image::images/StreamExecution-durationMs.png[align="center"]</p> <h2 id=source-scala_3>[source, scala]<a class=headerlink href=#source-scala_3 title="Permanent link">&para;</a></h2> <p>scala&gt; :type q org.apache.spark.sql.streaming.StreamingQuery</p> <p>scala&gt; println(q.lastProgress) { "id" : "03fc78fc-fe19-408c-a1ae-812d0e28fcee", "runId" : "8c247071-afba-40e5-aad2-0e6f45f22488", "name" : null, "timestamp" : "2017-08-14T20:30:00.004Z", "batchId" : 1, "numInputRows" : 432, "inputRowsPerSecond" : 0.9993568953312452, "processedRowsPerSecond" : 1380.1916932907347, "durationMs" : { "addBatch" : 237, "getBatch" : 26, "getOffset" : 0, "queryPlanning" : 1, "triggerExecution" : 313, "walCommit" : 45 }, "stateOperators" : [ ], "sources" : [ { "description" : "RateSource[rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=8]", "startOffset" : 0, "endOffset" : 432, "numInputRows" : 432, "inputRowsPerSecond" : 0.9993568953312452, "processedRowsPerSecond" : 1380.1916932907347 } ], "sink" : { "description" : "ConsoleSink[numRows=20, truncate=true]" } }</p> <hr> <p><code>StreamExecution</code> uses &lt;<offsetlog, offsetseqlog>&gt; and &lt;<batchcommitlog, batchcommitlog>&gt; metadata logs for <em>write-ahead log</em> (to record offsets to be processed) and that have already been processed and committed to a streaming sink, respectively.</p> <p>TIP: Monitor <code>offsets</code> and <code>commits</code> metadata logs to know the progress of a streaming query.</p> <p><code>StreamExecution</code> &lt;<runbatches-batchrunner-no-data, delays polling for new data>&gt; for 10 milliseconds (when no data was available to process in a batch). Use spark-sql-streaming-properties.md#spark.sql.streaming.pollingDelay[spark.sql.streaming.pollingDelay] Spark property to control the delay.</p> <p>[[id]] Every <code>StreamExecution</code> is uniquely identified by an <em>ID of the streaming query</em> (which is the <code>id</code> of the &lt;<streammetadata, streammetadata>&gt;).</p> <p>NOTE: Since the &lt;<streammetadata, streammetadata>&gt; is persisted (to the <code>metadata</code> file in the &lt;<checkpointfile, checkpoint directory>&gt;), the streaming query ID "survives" query restarts as long as the checkpoint directory is preserved.</p> <p>[[runId]] <code>StreamExecution</code> is also uniquely identified by a <em>run ID of the streaming query</em>. A run ID is a randomly-generated 128-bit universally unique identifier (UUID) that is assigned at the time <code>StreamExecution</code> is created.</p> <p>NOTE: <code>runId</code> does not "survive" query restarts and will always be different yet unique (across all active queries).</p> <h1 id=note>[NOTE]<a class=headerlink href=#note title="Permanent link">&para;</a></h1> <p>The &lt;<name, name>&gt;, &lt;<id, id>&gt; and &lt;<runid, runid>&gt; are all unique across all active queries (in a <a href=../StreamingQueryManager/ >StreamingQueryManager</a>). The difference is that:</p> <ul> <li> <p>&lt;<name, name>&gt; is optional and user-defined</p> </li> <li> <p>&lt;<id, id>&gt; is a UUID that is auto-generated at the time <code>StreamExecution</code> is created and persisted to <code>metadata</code> checkpoint file</p> </li> </ul> <h1 id=is-a-uuid-that-is-auto-generated-every-time-streamexecution-is-created>* &lt;<runid, runid>&gt; is a UUID that is auto-generated every time <code>StreamExecution</code> is created<a class=headerlink href=#is-a-uuid-that-is-auto-generated-every-time-streamexecution-is-created title="Permanent link">&para;</a></h1> <p>[[streamMetadata]] <code>StreamExecution</code> uses a <a href=../StreamMetadata/ >StreamMetadata</a> that is <a href=../StreamMetadata/#write>persisted</a> in the <code>metadata</code> file in the &lt;<checkpointfile, checkpoint directory>&gt;. If the <code>metadata</code> file is available it is <a href=../StreamMetadata/#read>read</a> and is the way to recover the &lt;<id, id>&gt; of a streaming query when resumed (i.e. restarted after a failure or a planned stop).</p> <p>[[IS_CONTINUOUS_PROCESSING]] <code>StreamExecution</code> uses <em>__is_continuous_processing</em> local property (default: <code>false</code>) to differentiate between &lt;<continuousexecution.md#, continuousexecution>&gt; (<code>true</code>) and &lt;<microbatchexecution.md#, microbatchexecution>&gt; (<code>false</code>) which is used when <code>StateStoreRDD</code> is requested to &lt;<spark-sql-streaming-statestorerdd.md#compute, compute a partition>&gt; (and &lt;<spark-sql-streaming-statestore.md#get, finds a statestore>&gt; for a given version).</p> <p>[[logging]] [TIP] ==== Enable <code>ALL</code> logging level for <code>org.apache.spark.sql.execution.streaming.StreamExecution</code> to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <div class=highlight><pre><span></span><code>log4j.logger.org.apache.spark.sql.execution.streaming.StreamExecution=ALL
</code></pre></div> <h1 id=refer-to>Refer to &lt;<spark-sql-streaming-spark-logging.md#, logging>&gt;.<a class=headerlink href=#refer-to title="Permanent link">&para;</a></h1> <h2 id=creating-instance>Creating Instance<a class=headerlink href=#creating-instance title="Permanent link">&para;</a></h2> <p><code>StreamExecution</code> takes the following to be created:</p> <ul> <li>[[sparkSession]] <code>SparkSession</code></li> <li>[[name]] Name of the streaming query (can also be <code>null</code>)</li> <li>[[checkpointRoot]] Path of the checkpoint directory (aka <em>metadata directory</em>)</li> <li>[[analyzedPlan]] Streaming query (as an analyzed logical query plan, i.e. <code>LogicalPlan</code>)</li> <li>[[trigger]] <a href=../spark-sql-streaming-Trigger/ >Trigger</a></li> <li>[[triggerClock]] <code>Clock</code></li> <li>[[outputMode]] <a href=../OutputMode/ >OutputMode</a></li> <li>[[deleteCheckpointOnStop]] <code>deleteCheckpointOnStop</code> flag (to control whether to delete the checkpoint directory on stop)</li> </ul> <p><code>StreamExecution</code> initializes the &lt;<internal-properties, internal properties>&gt;.</p> <p>NOTE: <code>StreamExecution</code> is a Scala abstract class and cannot be &lt;<creating-instance, created>&gt; directly. It is created indirectly when the &lt;<extensions, concrete streamexecutions>&gt; are.</p> <p>=== [[offsetLog]] Write-Ahead Log (WAL) of Offsets -- <code>offsetLog</code> Property</p> <h2 id=source-scala_4>[source, scala]<a class=headerlink href=#source-scala_4 title="Permanent link">&para;</a></h2> <h2 id=offsetlog-offsetseqlog>offsetLog: OffsetSeqLog<a class=headerlink href=#offsetlog-offsetseqlog title="Permanent link">&para;</a></h2> <p><code>offsetLog</code> is a &lt;<spark-sql-streaming-offsetseqlog.md#, hadoop dfs-based metadata storage>&gt; (of <a href=../OffsetSeq/ >OffsetSeq</a>s) with <code>offsets</code> &lt;<checkpointfile, metadata directory>&gt;.</p> <p><code>offsetLog</code> is used as <em>Write-Ahead Log of Offsets</em> to <a href=../HDFSMetadataLog/#add>persist offsets</a> of the data about to be processed in every trigger.</p> <p>NOTE: <em>Metadata log</em> or <em>metadata checkpoint</em> are synonyms and are often used interchangeably.</p> <p>The number of entries in the <code>OffsetSeqLog</code> is controlled using &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.minbatchestoretain, spark.sql.streaming.minbatchestoretain>&gt; configuration property (default: <code>100</code>). &lt;<extensions, stream execution engines>&gt; discard (<em>purge</em>) offsets from the <code>offsets</code> metadata log when the &lt;<currentbatchid, current batch id>&gt; (in &lt;<microbatchexecution.md#, microbatchexecution>&gt;) or the &lt;<continuousexecution.md#commit, epoch committed>&gt; (in &lt;<continuousexecution.md#, continuousexecution>&gt;) is above the threshold.</p> <h1 id=note_1>[NOTE]<a class=headerlink href=#note_1 title="Permanent link">&para;</a></h1> <p><code>offsetLog</code> is used when:</p> <ul> <li><code>ContinuousExecution</code> stream execution engine is requested to &lt;<continuousexecution.md#commit, commit an epoch>&gt;, &lt;<continuousexecution.md#getstartoffsets, getstartoffsets>&gt;, and &lt;<continuousexecution.md#addoffset, addoffset>&gt;</li> </ul> <h1 id=microbatchexecution-stream-execution-engine-is-requested-to-and>* <code>MicroBatchExecution</code> stream execution engine is requested to &lt;<microbatchexecution.md#populatestartoffsets, populate start offsets>&gt; and &lt;<microbatchexecution.md#constructnextbatch, construct (or skip) the next streaming micro-batch>&gt;<a class=headerlink href=#microbatchexecution-stream-execution-engine-is-requested-to-and title="Permanent link">&para;</a></h1> <p>=== [[state]] State of Streaming Query (Execution) -- <code>state</code> Property</p> <h2 id=source-scala_5>[source, scala]<a class=headerlink href=#source-scala_5 title="Permanent link">&para;</a></h2> <h2 id=state-atomicreferencestate>state: AtomicReference[State]<a class=headerlink href=#state-atomicreferencestate title="Permanent link">&para;</a></h2> <p><code>state</code> indicates the internal state of execution of the streaming query (as <a href=https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicReference.html[java.util.concurrent.atomic.AtomicReference>https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/atomic/AtomicReference.html[java.util.concurrent.atomic.AtomicReference</a>]).</p> <p>[[states]] .States [cols="30m,70",options="header",width="100%"] |=== | Name | Description</p> <p>| ACTIVE a| [[ACTIVE]] <code>StreamExecution</code> has been requested to &lt;<runstream, run stream processing>&gt; (and is about to &lt;<runactivatedstream, run the activated streaming query>&gt;)</p> <p>| INITIALIZING a| [[INITIALIZING]] <code>StreamExecution</code> has been &lt;<creating-instance, created>&gt;</p> <p>| TERMINATED a| [[TERMINATED]] Used to indicate that:</p> <ul> <li> <p><code>MicroBatchExecution</code> has been requested to &lt;<microbatchexecution.md#stop, stop>&gt;</p> </li> <li> <p><code>ContinuousExecution</code> has been requested to &lt;<continuousexecution.md#stop, stop>&gt;</p> </li> <li> <p><code>StreamExecution</code> has been requested to &lt;<runstream, run stream processing>&gt; (and has finished &lt;<runactivatedstream, running the activated streaming query>&gt;)</p> </li> </ul> <p>| RECONFIGURING a| [[RECONFIGURING]] Used only when <code>ContinuousExecution</code> is requested to &lt;<continuousexecution.md#runcontinuous, run a streaming query in continuous mode>&gt; (and the &lt;<spark-sql-streaming-continuousreader.md#, continuousreader>&gt; indicated a &lt;<spark-sql-streaming-continuousreader.md#needsreconfiguration, need for reconfiguration>&gt;)</p> <p>|===</p> <p>=== [[availableOffsets]] Available Offsets (StreamProgress) -- <code>availableOffsets</code> Property</p> <h2 id=source-scala_6>[source, scala]<a class=headerlink href=#source-scala_6 title="Permanent link">&para;</a></h2> <h2 id=availableoffsets-streamprogress>availableOffsets: StreamProgress<a class=headerlink href=#availableoffsets-streamprogress title="Permanent link">&para;</a></h2> <p><code>availableOffsets</code> is a &lt;<spark-sql-streaming-streamprogress.md#, collection of offsets per streaming source>&gt; to track what data (by &lt;<spark-sql-streaming-offset.md#, offset>&gt;) is available for processing for every <a href=../monitoring/ProgressReporter/#sources>streaming source</a> in the &lt;<analyzedplan, streaming query>&gt; (and have not yet been &lt;<committedoffsets, committed>&gt;).</p> <p><code>availableOffsets</code> works in tandem with the &lt;<committedoffsets, committedoffsets>&gt; internal registry.</p> <p><code>availableOffsets</code> is &lt;<spark-sql-streaming-streamprogress.md#creating-instance, empty>&gt; when <code>StreamExecution</code> is &lt;<creating-instance, created>&gt; (i.e. no offsets are reported for any streaming source in the streaming query).</p> <p><code>availableOffsets</code> is used when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;<microbatchexecution.md#populatestartoffsets, resume and fetch the start offsets from checkpoint>&gt;, &lt;<microbatchexecution.md#isnewdataavailable, check whether new data is available>&gt;, &lt;<microbatchexecution.md#constructnextbatch, construct the next streaming micro-batch>&gt; and &lt;<microbatchexecution.md#runbatch, run a single streaming micro-batch>&gt;</p> </li> <li> <p><code>ContinuousExecution</code> stream execution engine is requested to &lt;<continuousexecution.md#commit, commit an epoch>&gt;</p> </li> <li> <p><code>StreamExecution</code> is requested for the &lt;<todebugstring, internal string representation>&gt;</p> </li> </ul> <p>=== [[committedOffsets]] Committed Offsets (StreamProgress) -- <code>committedOffsets</code> Property</p> <h2 id=source-scala_7>[source, scala]<a class=headerlink href=#source-scala_7 title="Permanent link">&para;</a></h2> <h2 id=committedoffsets-streamprogress>committedOffsets: StreamProgress<a class=headerlink href=#committedoffsets-streamprogress title="Permanent link">&para;</a></h2> <p><code>committedOffsets</code> is a &lt;<spark-sql-streaming-streamprogress.md#, collection of offsets per streaming source>&gt; to track what data (by &lt;<spark-sql-streaming-offset.md#, offset>&gt;) has already been processed and committed (to the sink or state stores) for every <a href=../monitoring/ProgressReporter/#sources>streaming source</a> in the &lt;<analyzedplan, streaming query>&gt;.</p> <p><code>committedOffsets</code> works in tandem with the &lt;<availableoffsets, availableoffsets>&gt; internal registry.</p> <p><code>committedOffsets</code> is used when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested for the &lt;<microbatchexecution.md#populatestartoffsets, start offsets (from checkpoint)>&gt;, to &lt;<microbatchexecution.md#isnewdataavailable, check whether new data is available>&gt; and &lt;<microbatchexecution.md#runbatch, run a single streaming micro-batch>&gt;</p> </li> <li> <p><code>ContinuousExecution</code> stream execution engine is requested for the &lt;<continuousexecution.md#getstartoffsets, start offsets (from checkpoint)>&gt; and to &lt;<continuousexecution.md#commit, commit an epoch>&gt;</p> </li> <li> <p><code>StreamExecution</code> is requested for the &lt;<todebugstring, internal string representation>&gt;</p> </li> </ul> <p>=== [[resolvedCheckpointRoot]] Fully-Qualified (Resolved) Path to Checkpoint Root Directory -- <code>resolvedCheckpointRoot</code> Property</p> <h2 id=source-scala_8>[source, scala]<a class=headerlink href=#source-scala_8 title="Permanent link">&para;</a></h2> <h2 id=resolvedcheckpointroot-string>resolvedCheckpointRoot: String<a class=headerlink href=#resolvedcheckpointroot-string title="Permanent link">&para;</a></h2> <p><code>resolvedCheckpointRoot</code> is a fully-qualified path of the given &lt;<checkpointroot, checkpoint root directory>&gt;.</p> <p>The given &lt;<checkpointroot, checkpoint root directory>&gt; is defined using <em>checkpointLocation</em> option or the &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.checkpointlocation, spark.sql.streaming.checkpointlocation>&gt; configuration property with <code>queryName</code> option.</p> <p><code>checkpointLocation</code> and <code>queryName</code> options are defined when <code>StreamingQueryManager</code> is requested to <a href=../StreamingQueryManager/#createQuery>create a streaming query</a>.</p> <p><code>resolvedCheckpointRoot</code> is used when &lt;<checkpointfile, creating the path to the checkpoint directory>&gt; and when <code>StreamExecution</code> finishes &lt;<runbatches, running streaming batches>&gt;.</p> <p><code>resolvedCheckpointRoot</code> is used for the &lt;<logicalplan, logicalplan>&gt; (while transforming &lt;<analyzedplan, analyzedplan>&gt; and planning <code>StreamingRelation</code> logical operators to corresponding <code>StreamingExecutionRelation</code> physical operators with the streaming data sources created passing in the path to <code>sources</code> directory to store checkpointing metadata).</p> <h1 id=tip>[TIP]<a class=headerlink href=#tip title="Permanent link">&para;</a></h1> <p>You can see <code>resolvedCheckpointRoot</code> in the INFO message when <code>StreamExecution</code> is &lt;<start, started>&gt;.</p> <h1 id=starting-prettyidstring-use-resolvedcheckpointroot-to-store-the-query-checkpoint><div class=highlight><pre><span></span><code>Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint.
</code></pre></div><a class=headerlink href=#starting-prettyidstring-use-resolvedcheckpointroot-to-store-the-query-checkpoint title="Permanent link">&para;</a></h1> <p>Internally, <code>resolvedCheckpointRoot</code> creates a Hadoop <code>org.apache.hadoop.fs.Path</code> for &lt;<checkpointroot, checkpointroot>&gt; and makes it qualified.</p> <p>NOTE: <code>resolvedCheckpointRoot</code> uses <code>SparkSession</code> to access <code>SessionState</code> for a Hadoop configuration.</p> <p>=== [[commitLog]] Offset Commit Log -- <code>commits</code> Metadata Checkpoint Directory</p> <p><code>StreamExecution</code> uses <em>offset commit log</em> (&lt;<spark-sql-streaming-commitlog.md#, commitlog>&gt; with <code>commits</code> &lt;<checkpointfile, metadata checkpoint directory>&gt;) for streaming batches successfully executed (with a single file per batch with a file name being the batch id) or committed epochs.</p> <p>NOTE: <em>Metadata log</em> or <em>metadata checkpoint</em> are synonyms and are often used interchangeably.</p> <p><code>commitLog</code> is used by the &lt;<extensions, stream execution engines>&gt; for the following:</p> <ul> <li> <p><code>MicroBatchExecution</code> is requested to &lt;<microbatchexecution.md#runactivatedstream, run an activated streaming query>&gt; (that in turn requests to &lt;<microbatchexecution.md#populatestartoffsets, populate the start offsets>&gt; at the very beginning of the streaming query execution and later regularly every &lt;<microbatchexecution.md#runbatch, single batch>&gt;)</p> </li> <li> <p><code>ContinuousExecution</code> is requested to &lt;<continuousexecution.md#runactivatedstream, run an activated streaming query in continuous mode>&gt; (that in turn requests to &lt;<continuousexecution.md#getstartoffsets, retrieve the start offsets>&gt; at the very beginning of the streaming query execution and later regularly every &lt;<continuousexecution.md#commit, commit>&gt;)</p> </li> </ul> <h2 id=last-query-execution-of-streaming-query-incrementalexecution><span id=lastExecution> Last Query Execution Of Streaming Query (IncrementalExecution)<a class=headerlink href=#last-query-execution-of-streaming-query-incrementalexecution title="Permanent link">&para;</a></h2> <div class=highlight><pre><span></span><code><span class=n>lastExecution</span><span class=k>:</span> <span class=kt>IncrementalExecution</span>
</code></pre></div> <p><code>lastExecution</code> is part of the <a href=../monitoring/ProgressReporter/#lastExecution>ProgressReporter</a> abstraction.</p> <p><code>lastExecution</code> is a <a href=../IncrementalExecution/ >IncrementalExecution</a> (a <code>QueryExecution</code> of a streaming query) of the most recent (<em>last</em>) execution.</p> <p><code>lastExecution</code> is created when the &lt;<extensions, stream execution engines>&gt; are requested for the following:</p> <ul> <li> <p><code>MicroBatchExecution</code> is requested to &lt;<microbatchexecution.md#runbatch, run a single streaming micro-batch>&gt; (when in &lt;<microbatchexecution.md#runbatch-queryplanning, queryplanning phase>&gt;)</p> </li> <li> <p><code>ContinuousExecution</code> stream execution engine is requested to &lt;<continuousexecution.md#runcontinuous, run a streaming query>&gt; (when in &lt;<continuousexecution.md#runcontinuous-queryplanning, queryplanning phase>&gt;)</p> </li> </ul> <p><code>lastExecution</code> is used when:</p> <ul> <li> <p><code>StreamExecution</code> is requested to &lt;<explain, explain a streaming query>&gt; (via &lt;<explaininternal, explaininternal>&gt;)</p> </li> <li> <p><code>ProgressReporter</code> is requested to <a href=../monitoring/ProgressReporter/#extractStateOperatorMetrics>extractStateOperatorMetrics</a>, <a href=../monitoring/ProgressReporter/#extractExecutionStats>extractExecutionStats</a>, and <a href=../monitoring/ProgressReporter/#extractSourceToNumInputRows>extractSourceToNumInputRows</a></p> </li> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;<microbatchexecution.md#constructnextbatch-shouldconstructnextbatch, construct or skip the next streaming micro-batch>&gt; (based on <a href=../IncrementalExecution/#shouldRunAnotherBatch>StateStoreWriters in a streaming query</a>), &lt;<microbatchexecution.md#runbatch, run a single streaming micro-batch>&gt; (when in &lt;<microbatchexecution.md#runbatch-addbatch, addbatch phase>&gt; and &lt;<microbatchexecution.md#runbatch-updatewatermark-commitlog, updating watermark and committing offsets to offset commit log>&gt;)</p> </li> <li> <p><code>ContinuousExecution</code> stream execution engine is requested to &lt;<continuousexecution.md#runcontinuous, run a streaming query>&gt; (when in &lt;<continuousexecution.md#runcontinuous-runcontinuous, runcontinuous phase>&gt;)</p> </li> <li> <p>For debugging query execution of streaming queries (using <code>debugCodegen</code>)</p> </li> </ul> <h2 id=explaining-streaming-query><span id=explain> Explaining Streaming Query<a class=headerlink href=#explaining-streaming-query title="Permanent link">&para;</a></h2> <p><div class=highlight><pre><span></span><code><span class=n>explain</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span> <span class=c1>// &lt;1&gt;</span>
<span class=kt>explain</span><span class=o>(</span><span class=kt>extended:</span> <span class=kt>Boolean</span><span class=o>)</span><span class=kt>:</span> <span class=kt>Unit</span>
</code></pre></div> &lt;1&gt; Turns the <code>extended</code> flag off (<code>false</code>)</p> <p><code>explain</code> simply prints out &lt;<explaininternal, explaininternal>&gt; to the standard output.</p> <p>=== [[explainInternal]] <code>explainInternal</code> Method</p> <h2 id=source-scala_9>[source, scala]<a class=headerlink href=#source-scala_9 title="Permanent link">&para;</a></h2> <h2 id=explaininternalextended-boolean-string>explainInternal(extended: Boolean): String<a class=headerlink href=#explaininternalextended-boolean-string title="Permanent link">&para;</a></h2> <p><code>explainInternal</code>...FIXME</p> <h1 id=note_2>[NOTE]<a class=headerlink href=#note_2 title="Permanent link">&para;</a></h1> <p><code>explainInternal</code> is used when:</p> <ul> <li><code>StreamExecution</code> is requested to &lt;<explain, explain a streaming query>&gt;</li> </ul> <h1 id=streamingquerywrapper-is-requested-to-explaininternal>* <code>StreamingQueryWrapper</code> is requested to <a href=../StreamingQueryWrapper/#explainInternal>explainInternal</a><a class=headerlink href=#streamingquerywrapper-is-requested-to-explaininternal title="Permanent link">&para;</a></h1> <p>=== [[stopSources]] Stopping Streaming Sources and Readers -- <code>stopSources</code> Method</p> <h2 id=source-scala_10>[source, scala]<a class=headerlink href=#source-scala_10 title="Permanent link">&para;</a></h2> <h2 id=stopsources-unit>stopSources(): Unit<a class=headerlink href=#stopsources-unit title="Permanent link">&para;</a></h2> <p><code>stopSources</code> requests every &lt;<uniquesources, streaming source>&gt; (in the &lt;<analyzedplan, streaming query>&gt;) to stop.</p> <p>In case of an non-fatal exception, <code>stopSources</code> prints out the following WARN message to the logs:</p> <div class=highlight><pre><span></span><code>Failed to stop streaming source: [source]. Resources may have leaked.
</code></pre></div> <p><code>stopSources</code> is used when:</p> <ul> <li> <p><code>StreamExecution</code> is requested to &lt;<runstream, run stream processing>&gt; (and &lt;<runstream-finally, terminates>&gt; successfully or not)</p> </li> <li> <p><code>ContinuousExecution</code> is requested to &lt;<continuousexecution.md#runcontinuous, run the streaming query in continuous mode>&gt; (and terminates)</p> </li> </ul> <h2 id=running-stream-processing><span id=runStream> Running Stream Processing<a class=headerlink href=#running-stream-processing title="Permanent link">&para;</a></h2> <div class=highlight><pre><span></span><code><span class=n>runStream</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span>
</code></pre></div> <p><code>runStream</code> simply prepares the environment to &lt;<runactivatedstream, execute the activated streaming query>&gt;.</p> <p>NOTE: <code>runStream</code> is used exclusively when the &lt;<queryexecutionthread, stream execution thread>&gt; is requested to &lt;<start, start>&gt; (when <code>DataStreamWriter</code> is requested to <a href=../DataStreamWriter/#start>start an execution of the streaming query</a>).</p> <p>Internally, <code>runStream</code> sets the job group (to all the Spark jobs started by this thread) as follows:</p> <ul> <li> <p>&lt;<runid, runid>&gt; for the job group ID</p> </li> <li> <p>&lt;<getbatchdescriptionstring, getbatchdescriptionstring>&gt; for the job group description (to display in web UI)</p> </li> <li> <p><code>interruptOnCancel</code> flag on</p> </li> </ul> <h1 id=note_3>[NOTE]<a class=headerlink href=#note_3 title="Permanent link">&para;</a></h1> <p><code>runStream</code> uses the &lt;<sparksession, sparksession>&gt; to access <code>SparkContext</code> and assign the job group id.</p> <h1 id=read-up-on-httpsjaceklaskowskigitbooksiomastering-apache-sparkspark-sparkcontexthtmlsetjobgroupsparkcontextsetjobgroup-method-in-httpsbitlyapache-spark-internalsthe-internals-of-apache-spark-book>Read up on <a href=https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-SparkContext.html#setJobGroup[SparkContext.setJobGroup>https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-SparkContext.html#setJobGroup[SparkContext.setJobGroup</a>] method in <a href=https://bit.ly/apache-spark-internals[The>https://bit.ly/apache-spark-internals[The</a> Internals of Apache Spark] book.<a class=headerlink href=#read-up-on-httpsjaceklaskowskigitbooksiomastering-apache-sparkspark-sparkcontexthtmlsetjobgroupsparkcontextsetjobgroup-method-in-httpsbitlyapache-spark-internalsthe-internals-of-apache-spark-book title="Permanent link">&para;</a></h1> <p><code>runStream</code> sets <code>sql.streaming.queryId</code> local property to &lt;<id, id>&gt;.</p> <p><code>runStream</code> requests the <code>MetricsSystem</code> to register the &lt;<streammetrics, metricsreporter>&gt; when &lt;<spark-sql-streaming-properties.md#spark.sql.streaming.metricsenabled, spark.sql.streaming.metricsenabled>&gt; configuration property is on (default: off / <code>false</code>).</p> <p><code>runStream</code> notifies &lt;<spark-sql-streaming-streamingquerylistener.md#, streamingquerylisteners>&gt; that the streaming query has been started (by &lt;<postevent, posting>&gt; a new &lt;<spark-sql-streaming-streamingquerylistener.md#querystartedevent, querystartedevent>&gt; event with &lt;<id, id>&gt;, &lt;<runid, runid>&gt;, and &lt;<name, name>&gt;).</p> <p>.StreamingQueryListener Notified about Query's Start (onQueryStarted) image::images/StreamingQueryListener-onQueryStarted.png[align="center"]</p> <p><code>runStream</code> unblocks the &lt;<start, main starting thread>&gt; (by decrementing the count of the &lt;<startlatch, startlatch>&gt; that when <code>0</code> lets the starting thread continue).</p> <p>CAUTION: FIXME A picture with two parallel lanes for the starting thread and daemon one for the query.</p> <p><code>runStream</code> <a href=../monitoring/ProgressReporter/#updateStatusMessage>updates the status message</a> to be <strong>Initializing sources</strong>.</p> <p>[[runStream-initializing-sources]] <code>runStream</code> initializes the &lt;<logicalplan, analyzed logical plan>&gt;.</p> <p>NOTE: The &lt;<logicalplan, analyzed logical plan>&gt; is a lazy value in Scala and is initialized when requested the very first time.</p> <p><code>runStream</code> disables <em>adaptive query execution</em> and <em>cost-based join optimization</em> (by turning <code>spark.sql.adaptive.enabled</code> and <code>spark.sql.cbo.enabled</code> configuration properties off, respectively).</p> <p><code>runStream</code> creates a new "zero" &lt;<offsetseqmetadata, offsetseqmetadata>&gt;.</p> <p>(Only when in &lt;<state, initializing>&gt; state) <code>runStream</code> enters &lt;<state, active>&gt; state:</p> <ul> <li> <p>Decrements the count of &lt;<initializationlatch, initializationlatch>&gt;</p> </li> <li> <p>[[runStream-runActivatedStream]] &lt;<runactivatedstream, executes the activated streaming query>&gt; (which is different per &lt;<extensions, streamexecution>&gt;, i.e. &lt;<continuousexecution.md#, continuousexecution>&gt; or &lt;<microbatchexecution.md#, microbatchexecution>&gt;).</p> </li> </ul> <p>NOTE: <code>runBatches</code> does the main work only when first started (i.e. when &lt;<state, state>&gt; is <code>INITIALIZING</code>).</p> <p>[[runStream-stopped]] <code>runStream</code>...FIXME (describe the failed and stop states)</p> <p>Once &lt;<triggerexecutor, triggerexecutor>&gt; has finished executing batches, <code>runBatches</code> <a href=../monitoring/ProgressReporter/#updateStatusMessage>updates the status message</a> to <strong>Stopped</strong>.</p> <p>NOTE: &lt;<triggerexecutor, triggerexecutor>&gt; finishes executing batches when &lt;<runbatches-batch-runner, batch runner>&gt; returns whether the streaming query is stopped or not (which is when the internal &lt;<state, state>&gt; is not <code>TERMINATED</code>).</p> <p>[[runBatches-catch-isInterruptedByStop]] [[runBatches-catch-IOException]] [[runStream-catch-Throwable]] CAUTION: FIXME Describe <code>catch</code> block for exception handling</p> <p>==== [[runStream-finally]] Running Stream Processing -- <code>finally</code> Block</p> <p><code>runStream</code> releases the &lt;<startlatch, startlatch>&gt; and &lt;<initializationlatch, initializationlatch>&gt; locks.</p> <p><code>runStream</code> &lt;<stopsources, stopsources>&gt;.</p> <p><code>runStream</code> sets the &lt;<state, state>&gt; to &lt;<terminated, terminated>&gt;.</p> <p><code>runStream</code> sets the <a href=../monitoring/ProgressReporter/#currentStatus>StreamingQueryStatus</a> with the <code>isTriggerActive</code> and <code>isDataAvailable</code> flags off (<code>false</code>).</p> <p><code>runStream</code> removes the &lt;<streammetrics, stream metrics reporter>&gt; from the application's <code>MetricsSystem</code>.</p> <p><code>runStream</code> requests the <code>StreamingQueryManager</code> to <a href=../StreamingQueryManager/#notifyQueryTermination>handle termination of a streaming query</a>.</p> <p><code>runStream</code> creates a new &lt;<spark-sql-streaming-streamingquerylistener.md#queryterminatedevent, queryterminatedevent>&gt; (with the &lt;<id, id>&gt; and &lt;<runid, run id>&gt; of the streaming query) and &lt;<postevent, posts it>&gt;.</p> <p>[[runStream-finally-deleteCheckpointOnStop]] With the &lt;<deletecheckpointonstop, deletecheckpointonstop>&gt; flag enabled and no &lt;<exception, streamingqueryexception>&gt; reported, <code>runStream</code> deletes the &lt;<resolvedcheckpointroot, checkpoint directory>&gt; recursively.</p> <p>In the end, <code>runStream</code> releases the &lt;<terminationlatch, terminationlatch>&gt; lock.</p> <p>==== [[runBatches-batch-runner]] TriggerExecutor's Batch Runner</p> <p><em>Batch Runner</em> (aka <code>batchRunner</code>) is an executable block executed by &lt;<triggerexecutor, triggerexecutor>&gt; in &lt;<runbatches, runbatches>&gt;.</p> <p><code>batchRunner</code> &lt;<starttrigger, starts trigger calculation>&gt;.</p> <p>As long as the query is not stopped (i.e. &lt;<state, state>&gt; is not <code>TERMINATED</code>), <code>batchRunner</code> executes the streaming batch for the trigger.</p> <p>In <strong>triggerExecution</strong> <a href=../monitoring/ProgressReporter/#reportTimeTaken>time-tracking section</a>, <code>runBatches</code> branches off per &lt;<currentbatchid, currentbatchid>&gt;.</p> <p>.Current Batch Execution per currentBatchId [cols="1,1",options="header",width="100%"] |=== | currentBatchId &lt; 0 | currentBatchId &gt;= 0</p> <p>a|</p> <ol> <li>&lt;<populatestartoffsets, populatestartoffsets>&gt;</li> <li>Setting Job Description as &lt;<getbatchdescriptionstring, getbatchdescriptionstring>&gt;</li> </ol> <div class=highlight><pre><span></span><code>DEBUG Stream running from [committedOffsets] to [availableOffsets]
</code></pre></div> <p>| 1. &lt;<constructnextbatch, constructing the next streaming micro-batch>&gt; |===</p> <p>If there is &lt;<dataavailable, data available>&gt; in the sources, <code>batchRunner</code> marks &lt;<currentstatus, currentstatus>&gt; with <code>isDataAvailable</code> enabled.</p> <h1 id=note_4>[NOTE]<a class=headerlink href=#note_4 title="Permanent link">&para;</a></h1> <p>You can check out the status of a StreamingQuery.md[streaming query] using StreamingQuery.md#status[status] method.</p> <h2 id=source-scala_11>[source, scala]<a class=headerlink href=#source-scala_11 title="Permanent link">&para;</a></h2> <p>scala&gt; spark.streams.active(0).status res1: org.apache.spark.sql.streaming.StreamingQueryStatus = { "message" : "Waiting for next trigger", "isDataAvailable" : false, "isTriggerActive" : false }</p> <hr> <p>====</p> <p><code>batchRunner</code> then <a href=../monitoring/ProgressReporter/#updateStatusMessage>updates the status message</a> to <strong>Processing new data</strong> and &lt;<runbatch, runs the current streaming batch>&gt;.</p> <p>.StreamExecution's Running Batches (on Execution Thread) image::images/StreamExecution-runBatches.png[align="center"]</p> <p>[[runBatches-batch-runner-finishTrigger]] After <strong>triggerExecution</strong> section has finished, <code>batchRunner</code> <a href=../monitoring/ProgressReporter/#finishTrigger>finishes the streaming batch for the trigger</a> (and collects query execution statistics).</p> <p>When there was &lt;<dataavailable, data available>&gt; in the sources, <code>batchRunner</code> updates committed offsets (by spark-sql-streaming-CommitLog.md#add[adding] the &lt;<currentbatchid, current batch id>&gt; to &lt;<batchcommitlog, batchcommitlog>&gt; and adding &lt;<availableoffsets, availableoffsets>&gt; to &lt;<committedoffsets, committedoffsets>&gt;).</p> <p>You should see the following DEBUG message in the logs:</p> <div class=highlight><pre><span></span><code>DEBUG batch $currentBatchId committed
</code></pre></div> <p><code>batchRunner</code> increments the &lt;<currentbatchid, current batch id>&gt; and sets the job description for all the following Spark jobs to &lt;<getbatchdescriptionstring, include the new batch id>&gt;.</p> <p>[[runBatches-batchRunner-no-data]] When no &lt;<dataavailable, data was available>&gt; in the sources to process, <code>batchRunner</code> does the following:</p> <ol> <li> <p>Marks &lt;<currentstatus, currentstatus>&gt; with <code>isDataAvailable</code> disabled</p> </li> <li> <p><a href=../monitoring/ProgressReporter/#updateStatusMessage>Updates the status message</a> to <strong>Waiting for data to arrive</strong></p> </li> <li> <p>Sleeps the current thread for &lt;<pollingdelayms, pollingdelayms>&gt; milliseconds.</p> </li> </ol> <p><code>batchRunner</code> <a href=../monitoring/ProgressReporter/#updateStatusMessage>updates the status message</a> to <strong>Waiting for next trigger</strong> and returns whether the query is currently active or not (so &lt;<triggerexecutor, triggerexecutor>&gt; can decide whether to finish executing the batches or not)</p> <p>=== [[start]] Starting Streaming Query (on Stream Execution Thread) -- <code>start</code> Method</p> <h2 id=source-scala_12>[source, scala]<a class=headerlink href=#source-scala_12 title="Permanent link">&para;</a></h2> <h2 id=start-unit>start(): Unit<a class=headerlink href=#start-unit title="Permanent link">&para;</a></h2> <p>When called, <code>start</code> prints out the following INFO message to the logs:</p> <div class=highlight><pre><span></span><code>Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint.
</code></pre></div> <p><code>start</code> then starts the &lt;<queryexecutionthread, stream execution thread>&gt; (as a daemon thread).</p> <p>NOTE: <code>start</code> uses Java's ++<a href=https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#start--++[java.lang.Thread.start>https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html#start--++[java.lang.Thread.start</a>] to run the streaming query on a separate execution thread.</p> <p>NOTE: When started, a streaming query runs in its own execution thread on JVM.</p> <p>In the end, <code>start</code> pauses the main thread (using the &lt;<startlatch, startlatch>&gt; until <code>StreamExecution</code> is requested to &lt;<runstream, run the streaming query>&gt; that in turn sends a &lt;<spark-sql-streaming-streamingquerylistener.md#querystartedevent, querystartedevent>&gt; to all streaming listeners followed by decrementing the count of the &lt;<startlatch, startlatch>&gt;).</p> <p><code>start</code> is used when <code>StreamingQueryManager</code> is requested to <a href=../StreamingQueryManager/#startQuery>start a streaming query</a> (when <code>DataStreamWriter</code> is requested to <a href=../DataStreamWriter/#start>start an execution of the streaming query</a>).</p> <p>=== [[checkpointFile]] Path to Checkpoint Directory -- <code>checkpointFile</code> Internal Method</p> <h2 id=source-scala_13>[source, scala]<a class=headerlink href=#source-scala_13 title="Permanent link">&para;</a></h2> <h2 id=checkpointfilename-string-string>checkpointFile(name: String): String<a class=headerlink href=#checkpointfilename-string-string title="Permanent link">&para;</a></h2> <p><code>checkpointFile</code> gives the path of a directory with <code>name</code> in &lt;<resolvedcheckpointroot, checkpoint directory>&gt;.</p> <p>NOTE: <code>checkpointFile</code> uses Hadoop's <code>org.apache.hadoop.fs.Path</code>.</p> <p>NOTE: <code>checkpointFile</code> is used for &lt;<streammetadata, streammetadata>&gt;, &lt;<offsetlog, offsetseqlog>&gt;, &lt;<batchcommitlog, batchcommitlog>&gt;, and &lt;<lastexecution, lastexecution>&gt; (for &lt;<runbatch, runbatch>&gt;).</p> <h2 id=posting-streamingquerylistener-event><span id=postEvent> Posting StreamingQueryListener Event<a class=headerlink href=#posting-streamingquerylistener-event title="Permanent link">&para;</a></h2> <div class=highlight><pre><span></span><code><span class=n>postEvent</span><span class=o>(</span>
  <span class=n>event</span><span class=k>:</span> <span class=kt>StreamingQueryListener.Event</span><span class=o>)</span><span class=k>:</span> <span class=kt>Unit</span>
</code></pre></div> <p><code>postEvent</code> is a part of <a href=../monitoring/ProgressReporter/#postEvent>ProgressReporter</a> abstraction.</p> <p><code>postEvent</code> simply requests the <code>StreamingQueryManager</code> to <a href=../StreamingQueryManager/#postListenerEvent>post</a> the input event (to the <a href=../StreamingQueryListenerBus/ >StreamingQueryListenerBus</a> in the current <code>SparkSession</code>).</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p><code>postEvent</code> uses <code>SparkSession</code> to access the current <code>StreamingQueryManager</code>.</p> </div> <p><code>postEvent</code> is used when:</p> <ul> <li> <p><code>ProgressReporter</code> is requested to <a href=../monitoring/ProgressReporter/#updateProgress>report update progress</a> (while <a href=../monitoring/ProgressReporter/#finishTrigger>finishing a trigger</a>)</p> </li> <li> <p><code>StreamExecution</code> &lt;<runbatches, runs streaming batches>&gt; (and announces starting a streaming query by posting a spark-sql-streaming-StreamingQueryListener.md#QueryStartedEvent[QueryStartedEvent] and query termination by posting a spark-sql-streaming-StreamingQueryListener.md#QueryTerminatedEvent[QueryTerminatedEvent])</p> </li> </ul> <p>=== [[processAllAvailable]] Waiting Until No New Data Available in Sources or Query Has Been Terminated -- <code>processAllAvailable</code> Method</p> <h2 id=source-scala_14>[source, scala]<a class=headerlink href=#source-scala_14 title="Permanent link">&para;</a></h2> <h2 id=processallavailable-unit>processAllAvailable(): Unit<a class=headerlink href=#processallavailable-unit title="Permanent link">&para;</a></h2> <p>NOTE: <code>processAllAvailable</code> is a part of &lt;<streamingquery.md#processallavailable, streamingquery contract>&gt;.</p> <p><code>processAllAvailable</code> reports the &lt;<streamdeathcause, streamingqueryexception>&gt; if reported (and returns immediately).</p> <p>NOTE: &lt;<streamdeathcause, streamdeathcause>&gt; is reported exclusively when <code>StreamExecution</code> is requested to &lt;<runstream, run stream execution>&gt; (that terminated with an exception).</p> <p><code>processAllAvailable</code> returns immediately when <code>StreamExecution</code> is no longer &lt;<isactive, active>&gt; (in <code>TERMINATED</code> state).</p> <p><code>processAllAvailable</code> acquires a lock on the &lt;<awaitprogresslock, awaitprogresslock>&gt; and turns the &lt;<nonewdata, nonewdata>&gt; internal flag off (<code>false</code>).</p> <p><code>processAllAvailable</code> keeps polling with 10-second pauses (locked on &lt;<awaitprogresslockcondition, awaitprogresslockcondition>&gt;) until &lt;<nonewdata, nonewdata>&gt; flag is turned on (<code>true</code>) or <code>StreamExecution</code> is no longer &lt;<isactive, active>&gt; (in <code>TERMINATED</code> state).</p> <p>NOTE: The 10-second pause is hardcoded and cannot be changed.</p> <p>In the end, <code>processAllAvailable</code> releases &lt;<awaitprogresslock, awaitprogresslock>&gt; lock.</p> <p><code>processAllAvailable</code> throws an <code>IllegalStateException</code> when executed on the &lt;<queryexecutionthread, stream execution thread>&gt;:</p> <div class=highlight><pre><span></span><code>Cannot wait for a query state from the same thread that is running the query
</code></pre></div> <p>=== [[queryExecutionThread]] Stream Execution Thread -- <code>queryExecutionThread</code> Property</p> <h2 id=source-scala_15>[source, scala]<a class=headerlink href=#source-scala_15 title="Permanent link">&para;</a></h2> <h2 id=queryexecutionthread-queryexecutionthread>queryExecutionThread: QueryExecutionThread<a class=headerlink href=#queryexecutionthread-queryexecutionthread title="Permanent link">&para;</a></h2> <p><code>queryExecutionThread</code> is a Java thread of execution (<a href=https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.util.Thread>https://docs.oracle.com/javase/8/docs/api/java/lang/Thread.html[java.util.Thread</a>]) that &lt;<runstream, runs a streaming query>&gt;.</p> <p><code>queryExecutionThread</code> is started (as a daemon thread) when <code>StreamExecution</code> is requested to &lt;<start, start>&gt;. At that time, <code>start</code> prints out the following INFO message to the logs (with the &lt;<prettyidstring, prettyidstring>&gt; and the &lt;<resolvedcheckpointroot, resolvedcheckpointroot>&gt;):</p> <div class=highlight><pre><span></span><code>Starting [prettyIdString]. Use [resolvedCheckpointRoot] to store the query checkpoint.
</code></pre></div> <p>When started, <code>queryExecutionThread</code> sets the &lt;<callsite, call site>&gt; and &lt;<runstream, runs the streaming query>&gt;.</p> <p><code>queryExecutionThread</code> uses the name <em>stream execution thread for [id]</em> (that uses &lt;<prettyidstring, prettyidstring>&gt; for the id, i.e. <code>queryName [id = [id], runId = [runId]]</code>).</p> <p><code>queryExecutionThread</code> is a <code>QueryExecutionThread</code> that is a custom <code>UninterruptibleThread</code> from Apache Spark with <code>runUninterruptibly</code> method for running a block of code without being interrupted by <code>Thread.interrupt()</code>.</p> <h1 id=tip_1>[TIP]<a class=headerlink href=#tip_1 title="Permanent link">&para;</a></h1> <p>Use Java's <a href=http://docs.oracle.com/javase/8/docs/technotes/guides/management/jconsole.html[jconsole>http://docs.oracle.com/javase/8/docs/technotes/guides/management/jconsole.html[jconsole</a>] or <a href=https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html[jstack>https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jstack.html[jstack</a>] to monitor stream execution threads.</p> <h1 id=jstack-driver-pid-grep-e-stream-execution-thread-stream-execution-thread-for-kafka-topic1-id><div class=highlight><pre><span></span><code>$ jstack &lt;driver-pid&gt; | grep -e &quot;stream execution thread&quot;
&quot;stream execution thread for kafka-topic1 [id =...
</code></pre></div><a class=headerlink href=#jstack-driver-pid-grep-e-stream-execution-thread-stream-execution-thread-for-kafka-topic1-id title="Permanent link">&para;</a></h1> <p>=== [[toDebugString]] Internal String Representation -- <code>toDebugString</code> Internal Method</p> <h2 id=source-scala_16>[source, scala]<a class=headerlink href=#source-scala_16 title="Permanent link">&para;</a></h2> <h2 id=todebugstringincludelogicalplan-boolean-string>toDebugString(includeLogicalPlan: Boolean): String<a class=headerlink href=#todebugstringincludelogicalplan-boolean-string title="Permanent link">&para;</a></h2> <p><code>toDebugString</code>...FIXME</p> <p><code>toDebugString</code> is used when <code>StreamExecution</code> is requested to &lt;<runstream, run stream processing>&gt; (and an exception is caught).</p> <p>=== [[offsetSeqMetadata]] Current Batch Metadata (Event-Time Watermark and Timestamp) -- <code>offsetSeqMetadata</code> Internal Property</p> <div class=highlight><pre><span></span><code><span class=n>offsetSeqMetadata</span><span class=k>:</span> <span class=kt>OffsetSeqMetadata</span>
</code></pre></div> <p><code>offsetSeqMetadata</code> is a <a href=../spark-sql-streaming-OffsetSeqMetadata/ >OffsetSeqMetadata</a>.</p> <p><code>offsetSeqMetadata</code> is used to create an <a href=../IncrementalExecution/ >IncrementalExecution</a> in the <strong>queryPlanning</strong> phase of the <a href=../MicroBatchExecution/#runBatch-queryPlanning>MicroBatchExecution</a> and <a href=../ContinuousExecution/#runContinuous-queryPlanning>ContinuousExecution</a> execution engines.</p> <p><code>offsetSeqMetadata</code> is initialized (with <code>0</code> for <code>batchWatermarkMs</code> and <code>batchTimestampMs</code>) when <code>StreamExecution</code> is requested to &lt;<runstream, run stream processing>&gt;.</p> <p><code>offsetSeqMetadata</code> is then updated (with the current event-time watermark and timestamp) when <code>MicroBatchExecution</code> is requested to &lt;<microbatchexecution.md#constructnextbatch, construct the next streaming micro-batch>&gt;.</p> <p>NOTE: <code>MicroBatchExecution</code> uses the &lt;<microbatchexecution.md#watermarktracker, watermarktracker>&gt; for the current event-time watermark and the &lt;<microbatchexecution.md#triggerclock, trigger clock>&gt; for the current batch timestamp.</p> <p><code>offsetSeqMetadata</code> is stored (<em>checkpointed</em>) in &lt;<microbatchexecution.md#constructnextbatch-walcommit, walcommit phase>&gt; of <code>MicroBatchExecution</code> (and printed out as INFO message to the logs).</p> <p><code>offsetSeqMetadata</code> is restored (<em>re-created</em>) from a checkpointed state when <code>MicroBatchExecution</code> is requested to &lt;<microbatchexecution.md#populatestartoffsets, populate start offsets>&gt;.</p> <p><code>offsetSeqMetadata</code> is part of the <a href=../monitoring/ProgressReporter/#offsetSeqMetadata>ProgressReporter</a> abstraction.</p> <p>=== [[isActive]] <code>isActive</code> Method</p> <h2 id=source-scala_17>[source, scala]<a class=headerlink href=#source-scala_17 title="Permanent link">&para;</a></h2> <h2 id=isactive-boolean>isActive: Boolean<a class=headerlink href=#isactive-boolean title="Permanent link">&para;</a></h2> <p>NOTE: <code>isActive</code> is part of the &lt;<streamingquery.md#isactive, streamingquery contract>&gt; to indicate whether a streaming query is active (<code>true</code>) or not (<code>false</code>).</p> <p><code>isActive</code> is enabled (<code>true</code>) as long as the &lt;<state, state>&gt; is not &lt;<terminated, terminated>&gt;.</p> <p>=== [[exception]] <code>exception</code> Method</p> <h2 id=source-scala_18>[source, scala]<a class=headerlink href=#source-scala_18 title="Permanent link">&para;</a></h2> <h2 id=exception-optionstreamingqueryexception>exception: Option[StreamingQueryException]<a class=headerlink href=#exception-optionstreamingqueryexception title="Permanent link">&para;</a></h2> <p>NOTE: <code>exception</code> is part of the &lt;<streamingquery.md#exception, streamingquery contract>&gt; to indicate whether a streaming query...FIXME</p> <p><code>exception</code>...FIXME</p> <p>=== [[getBatchDescriptionString]] Human-Readable HTML Description of Spark Jobs (for web UI) -- <code>getBatchDescriptionString</code> Method</p> <h2 id=source-scala_19>[source, scala]<a class=headerlink href=#source-scala_19 title="Permanent link">&para;</a></h2> <h2 id=getbatchdescriptionstring-string>getBatchDescriptionString: String<a class=headerlink href=#getbatchdescriptionstring-string title="Permanent link">&para;</a></h2> <p><code>getBatchDescriptionString</code> is a human-readable description (in HTML format) that uses the optional &lt;<name, name>&gt; if defined, the &lt;<id, id>&gt;, the &lt;<runid, runid>&gt; and <code>batchDescription</code> that can be <em>init</em> (for the &lt;<currentbatchid, current batch id>&gt; negative) or the &lt;<currentbatchid, current batch id>&gt; itself.</p> <p><code>getBatchDescriptionString</code> is of the following format:</p> <h2 id=subs-macros>[subs=-macros]<a class=headerlink href=#subs-macros title="Permanent link">&para;</a></h2> <h2 id=nameid-idrunid-runidbatch-batchdescription>[name]<br>id = [id]<br>runId = [runId]<br>batch = [batchDescription]<a class=headerlink href=#nameid-idrunid-runidbatch-batchdescription title="Permanent link">&para;</a></h2> <p>.Monitoring Streaming Query using web UI (Spark Jobs) image::images/StreamExecution-getBatchDescriptionString-webUI.png[align="center"]</p> <h1 id=note_5>[NOTE]<a class=headerlink href=#note_5 title="Permanent link">&para;</a></h1> <p><code>getBatchDescriptionString</code> is used when:</p> <ul> <li><code>MicroBatchExecution</code> stream execution engine is requested to &lt;<microbatchexecution.md#runactivatedstream, run an activated streaming query>&gt; (as the job description of any Spark jobs triggerred as part of query execution)</li> </ul> <h1 id=streamexecution-is-requested-to-as-the-job-group-description-of-any-spark-jobs-triggerred-as-part-of-query-execution>* <code>StreamExecution</code> is requested to &lt;<runstream, run stream processing>&gt; (as the job group description of any Spark jobs triggerred as part of query execution)<a class=headerlink href=#streamexecution-is-requested-to-as-the-job-group-description-of-any-spark-jobs-triggerred-as-part-of-query-execution title="Permanent link">&para;</a></h1> <p>=== [[noNewData]] No New Data Available -- <code>noNewData</code> Internal Flag</p> <h2 id=source-scala_20>[source, scala]<a class=headerlink href=#source-scala_20 title="Permanent link">&para;</a></h2> <h2 id=nonewdata-boolean>noNewData: Boolean<a class=headerlink href=#nonewdata-boolean title="Permanent link">&para;</a></h2> <p><code>noNewData</code> is a flag that indicates that a batch has completed with no new data left and &lt;<processallavailable, processallavailable>&gt; could stop waiting till all streaming data is processed.</p> <p>Default: <code>false</code></p> <p>Turned on (<code>true</code>) when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;<constructnextbatch, construct or skip the next streaming micro-batch>&gt; (while &lt;<microbatchexecution.md#constructnextbatch-shouldconstructnextbatch-disabled, skipping the next micro-batch>&gt;)</p> </li> <li> <p><code>ContinuousExecution</code> stream execution engine is requested to &lt;<continuousexecution.md#addoffset, addoffset>&gt;</p> </li> </ul> <p>Turned off (<code>false</code>) when:</p> <ul> <li> <p><code>MicroBatchExecution</code> stream execution engine is requested to &lt;<constructnextbatch, construct or skip the next streaming micro-batch>&gt; (right after the &lt;<microbatchexecution.md#constructnextbatch-walcommit, walcommit>&gt; phase)</p> </li> <li> <p><code>StreamExecution</code> is requested to &lt;<processallavailable, processallavailable>&gt;</p> </li> </ul> <p>=== [[internal-properties]] Internal Properties</p> <p>[cols="30m,70",options="header",width="100%"] |=== | Name | Description</p> <p>| awaitProgressLock | [[awaitProgressLock]] Java's fair reentrant mutual exclusion <a href=https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/ReentrantLock.html[java.util.concurrent.locks.ReentrantLock>https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/locks/ReentrantLock.html[java.util.concurrent.locks.ReentrantLock</a>] (that favors granting access to the longest-waiting thread under contention)</p> <p>| awaitProgressLockCondition a| [[awaitProgressLockCondition]] Lock</p> <p>| callSite | [[callSite]]</p> <p>| currentBatchId a| [[currentBatchId]] Current batch ID</p> <ul> <li> <p>Starts at <code>-1</code> when <code>StreamExecution</code> is &lt;<creating-instance, created>&gt;</p> </li> <li> <p><code>0</code> when <code>StreamExecution</code> &lt;<populatestartoffsets, populates start offsets>&gt; (and &lt;<offsetlog, offsetseqlog>&gt; is empty, i.e. no offset files in <code>offsets</code> directory in checkpoint)</p> </li> <li> <p>Incremented when <code>StreamExecution</code> &lt;<runbatches, runs streaming batches>&gt; and finishes a trigger that had &lt;<dataavailable, data available from sources>&gt; (right after &lt;<batchcommitlog, committing the batch>&gt;).</p> </li> </ul> <p>| initializationLatch | [[initializationLatch]]</p> <p>| newData a| [[newData]]</p> <div class=highlight><pre><span></span><code><span class=n>newData</span><span class=k>:</span> <span class=kt>Map</span><span class=o>[</span><span class=kt>BaseStreamingSource</span><span class=p>,</span> <span class=kt>LogicalPlan</span><span class=o>]</span>
</code></pre></div> <p>Registry of the streaming sources (in the &lt;<logicalplan, logical query plan>&gt;) that have new data available in the current batch. The new data is a streaming <code>DataFrame</code>.</p> <p><code>newData</code> is part of the <a href=../monitoring/ProgressReporter/#newData>ProgressReporter</a> abstraction.</p> <p>Set exclusively when <code>StreamExecution</code> is requested to &lt;<microbatchexecution.md#runbatch-getbatch, requests unprocessed data from streaming sources>&gt; (while &lt;<runbatch, running a single streaming batch>&gt;).</p> <p>Used exclusively when <code>StreamExecution</code> is requested to &lt;<microbatchexecution.md#runbatch-newbatchesplan, transform the logical plan (of the streaming query) to include the sources and the microbatchreaders with new data>&gt; (while &lt;<runbatch, running a single streaming batch>&gt;).</p> <p>| pollingDelayMs | [[pollingDelayMs]] Time delay before polling new data again when no data was available</p> <p>Set to spark-sql-streaming-properties.md#spark.sql.streaming.pollingDelay[spark.sql.streaming.pollingDelay] Spark property.</p> <p>Used when <code>StreamExecution</code> has started &lt;<runbatches, running streaming batches>&gt; (and &lt;<runbatches-batchrunner-no-data, no data was available to process in a trigger>&gt;).</p> <p>| prettyIdString a| [[prettyIdString]] Pretty-identified string for identification in logs (with &lt;<name, name>&gt; if defined).</p> <div class=highlight><pre><span></span><code>// query name set
queryName [id = xyz, runId = abc]

// no query name
[id = xyz, runId = abc]
</code></pre></div> <p>| startLatch | [[startLatch]] Java's <a href=https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountDownLatch.html[java.util.concurrent.CountDownLatch>https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountDownLatch.html[java.util.concurrent.CountDownLatch</a>] with count <code>1</code>.</p> <p>Used when <code>StreamExecution</code> is requested to &lt;<start, start>&gt; to pause the main thread until <code>StreamExecution</code> was requested to &lt;<runstream, run the streaming query>&gt;.</p> <p>| streamDeathCause | [[streamDeathCause]] <code>StreamingQueryException</code></p> <p>| uniqueSources a| [[uniqueSources]] Unique streaming sources (after being collected as <code>StreamingExecutionRelation</code> from the &lt;<logicalplan, logical query plan>&gt;).</p> <p>Used when <code>StreamExecution</code>:</p> <ul> <li> <p>&lt;<constructnextbatch, constructs the next streaming micro-batch>&gt; (and gets new offsets for every streaming data source)</p> </li> <li> <p>&lt;<stopsources, stops all streaming data sources>&gt; |===</p> </li> </ul> <h2 id=streaming-metrics><span id=streamMetrics> Streaming Metrics<a class=headerlink href=#streaming-metrics title="Permanent link">&para;</a></h2> <p><code>StreamExecution</code> uses <a href=../monitoring/MetricsReporter/ >MetricsReporter</a> for reporting streaming metrics.</p> <p><code>MetricsReporter</code> is created with the following source name (with <a href=#name>name</a> if defined or <a href=#id>id</a>):</p> <div class=highlight><pre><span></span><code>spark.streaming.[name or id]
</code></pre></div> <p><code>MetricsReporter</code> is registered only when <a href=../spark-sql-streaming-properties/#spark.sql.streaming.metricsEnabled>spark.sql.streaming.metricsEnabled</a> configuration property is enabled (when <code>StreamExecution</code> is requested to <a href=#runStream>runStream</a>).</p> <p><code>MetricsReporter</code> is deactivated (<em>removed</em>) when a streaming query is stopped (when <code>StreamExecution</code> is requested to <a href=#runStream>runStream</a>).</p> </article> </div> </div> </main> <footer class=md-footer> <div class=md-footer-nav> <nav class="md-footer-nav__inner md-grid" aria-label=Footer> <a href=../spark-sql-streaming-window/ class="md-footer-nav__link md-footer-nav__link--prev" rel=prev> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Previous </span> window Function </div> </div> </a> <a href=../spark-sql-streaming-TriggerExecutor/ class="md-footer-nav__link md-footer-nav__link--next" rel=next> <div class=md-footer-nav__title> <div class=md-ellipsis> <span class=md-footer-nav__direction> Next </span> TriggerExecutor </div> </div> <div class="md-footer-nav__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> </div> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2020 <a href=https://twitter.com/jaceklaskowski target=_blank rel=noopener>Jacek Laskowski</a> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-footer-social> <a href=https://github.com/jaceklaskowski target=_blank rel=noopener title=github.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> <a href=https://twitter.com/jaceklaskowski target=_blank rel=noopener title=twitter.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg> </a> <a href=https://linkedin.com/in/jaceklaskowski target=_blank rel=noopener title=linkedin.com class=md-footer-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg> </a> </div> </div> </div> </footer> </div> <script src=../assets/javascripts/vendor.6a3d08fc.min.js></script> <script src=../assets/javascripts/bundle.71201edf.min.js></script><script id=__lang type=application/json>{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script> <script>
        app = initialize({
          base: "..",
          features: ['navigation.tabs', 'navigation.instant'],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script> </body> </html>